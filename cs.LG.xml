<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01440</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65306;&#20174;&#20803;&#23398;&#20064;&#21040;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#22270;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26089;&#26399;&#30340;&#25216;&#26415;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#20013;&#36816;&#34892;&#65292;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#24341;&#21457;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21482;&#26377;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#26412;&#32508;&#36848;&#35797;&#22270;&#32508;&#21512;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#25552;&#20379;&#27604;&#36739;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#36827;&#34892;&#26041;&#27861;&#36873;&#25321;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;</title><link>https://arxiv.org/abs/2403.02338</link><description>&lt;p&gt;
&#29992;&#21452;&#25163;&#25197;&#24320;&#30422;&#23376;
&lt;/p&gt;
&lt;p&gt;
Twisting Lids Off with Two Hands
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02338
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20004;&#21482;&#22810;&#25351;&#25163;&#33218;&#25805;&#32437;&#29289;&#20307;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#30340;&#20016;&#23500;&#25509;&#35302;&#24615;&#36136;&#20197;&#21450;&#21327;&#35843;&#39640;&#32500;&#24230;&#21452;&#25163;&#31995;&#32479;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;&#20004;&#21482;&#25163;&#25197;&#24320;&#21508;&#31181;&#29942;&#23376;&#30422;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#36890;&#36807;&#23545;&#29289;&#29702;&#24314;&#27169;&#12289;&#23454;&#26102;&#24863;&#30693;&#21644;&#22870;&#21169;&#35774;&#35745;&#30340;&#26032;&#24037;&#31243;&#35265;&#35299;&#65292;&#35813;&#31574;&#30053;&#23637;&#31034;&#20102;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#36143;&#31359;&#21508;&#31181;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#65292;&#23637;&#31034;&#20986;&#21160;&#24577;&#21644;&#28789;&#24039;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20173;&#28982;&#26159;&#35299;&#20915;&#21069;&#25152;&#26410;&#26377;&#22797;&#26434;&#38382;&#39064;&#30340;&#25805;&#32437;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02338v1 Announce Type: cross  Abstract: Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.
&lt;/p&gt;</description></item><item><title>GCSL&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#21033;&#29992;&#19981;&#21463;&#20197;&#21069;&#20219;&#21153;&#24433;&#21709;&#30340;&#26435;&#37325;&#23376;&#31354;&#38388;&#26469;&#35757;&#32451;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.02334</link><description>&lt;p&gt;
&#26799;&#24230;&#30456;&#20851;&#23376;&#31354;&#38388;&#23398;&#20064;&#25269;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Gradient Correlation Subspace Learning against Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02334
&lt;/p&gt;
&lt;p&gt;
GCSL&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#21033;&#29992;&#19981;&#21463;&#20197;&#21069;&#20219;&#21153;&#24433;&#21709;&#30340;&#26435;&#37325;&#23376;&#31354;&#38388;&#26469;&#35757;&#32451;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36825;&#26679;&#30340;&#23398;&#20064;&#38754;&#20020;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#20043;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#20063;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#22686;&#37327;&#31867;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21517;&#20026;&#26799;&#24230;&#30456;&#20851;&#23376;&#31354;&#38388;&#23398;&#20064;&#65288;GCSL&#65289;&#12290;&#35813;&#26041;&#27861;&#26816;&#27979;&#21040;&#26368;&#19981;&#21463;&#20197;&#21069;&#20219;&#21153;&#24433;&#21709;&#30340;&#26435;&#37325;&#23376;&#31354;&#38388;&#65292;&#24182;&#23558;&#26435;&#37325;&#25237;&#24433;&#21040;&#35813;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32473;&#23450;&#32593;&#32476;&#26550;&#26500;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#23618;&#65292;&#24182;&#19988;&#25152;&#20351;&#29992;&#30340;&#23376;&#31354;&#38388;&#22823;&#23567;&#21487;&#20197;&#20174;&#23618;&#21040;&#23618;&#12289;&#20219;&#21153;&#21040;&#20219;&#21153;&#36827;&#34892;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02334v1 Announce Type: cross  Abstract: Efficient continual learning techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#26694;&#26550;COMMIT&#65292;&#20197;&#30830;&#20445;&#20854;&#38024;&#23545;&#35821;&#20041;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02329</link><description>&lt;p&gt;
COMMIT: &#38024;&#23545;&#35821;&#20041;&#25915;&#20987;&#23545;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02329
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#26694;&#26550;COMMIT&#65292;&#20197;&#30830;&#20445;&#20854;&#38024;&#23545;&#35821;&#20041;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#65288;MSFs&#65289;&#22312;&#29616;&#20195;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AVs&#65289;&#20013;&#20316;&#20026;&#24863;&#30693;&#27169;&#22359;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#23427;&#20204;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#19988;&#36924;&#30495;&#30340;&#25932;&#23545;&#35821;&#20041;&#36716;&#25442;&#65288;&#22914;&#26059;&#36716;&#21644;&#20301;&#31227;&#65289;&#30340;&#31283;&#20581;&#24615;&#23545;&#20110;AVs&#30340;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#26694;&#26550;COMMIT&#65292;&#20197;&#23545;&#25239;&#35821;&#20041;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02329v1 Announce Type: new  Abstract: Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs). Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs. While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations. Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks. So far, there is no certified defense proposed for MSFs. In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks. In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with multi-modal data and perfo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#20351;&#27169;&#22411;&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#24182;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02325</link><description>&lt;p&gt;
&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65306;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#23450;&#20301;&#20934;&#30830;&#24615;&#32780;&#26080;&#38656;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#20351;&#27169;&#22411;&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#24182;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#22270;&#20687;&#20013;&#29305;&#21035;&#30456;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#23548;&#27169;&#22411;&#26356;&#23494;&#20999;&#22320;&#20851;&#27880;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#24320;&#28304;&#30340;VLMs&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;VL&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02325v1 Announce Type: cross  Abstract: Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a "visual prompt", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#36890;&#36807;&#20998;&#22359;&#39044;&#35013;&#22635;&#25216;&#26415;&#24179;&#34913;&#20102;GPU&#35745;&#31639;&#39281;&#21644;&#21644;&#21333;&#20010;&#26631;&#35760;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.02310</link><description>&lt;p&gt;
&#22312;LLM&#25512;&#29702;&#20013;&#24179;&#34913;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#26435;&#34913;&#30340;&#30740;&#31350;&#65306;Sarathi-Serve&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02310
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#36890;&#36807;&#20998;&#22359;&#39044;&#35013;&#22635;&#25216;&#26415;&#24179;&#34913;&#20102;GPU&#35745;&#31639;&#39281;&#21644;&#21644;&#21333;&#20010;&#26631;&#35760;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#20010;LLM&#26381;&#21153;&#35831;&#27714;&#32463;&#21382;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#26159;prefill&#38454;&#27573;&#65292;&#22788;&#29702;&#25972;&#20010;&#36755;&#20837;&#25552;&#31034;&#20197;&#29983;&#25104;&#19968;&#20010;&#36755;&#20986;&#26631;&#35760;&#65307;&#31532;&#20108;&#20010;&#26159;decode&#38454;&#27573;&#65292;&#36880;&#20010;&#29983;&#25104;&#20854;&#20313;&#30340;&#36755;&#20986;&#26631;&#35760;&#12290;Prefill&#36845;&#20195;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#24182;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#20351;GPU&#35745;&#31639;&#39281;&#21644;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;decode&#36845;&#20195;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#65292;&#20294;&#20063;&#20165;&#20351;&#29992;&#36739;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#27599;&#20010;&#35831;&#27714;&#21482;&#22788;&#29702;&#19968;&#20010;&#26631;&#35760;&#12290;&#36825;&#20351;&#24471;&#23545;&#35299;&#30721;&#26469;&#35828;&#25209;&#22788;&#29702;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#27492;&#23545;&#25972;&#20307;&#21534;&#21520;&#37327;&#20063;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25209;&#37327;&#22788;&#29702;&#22810;&#20010;&#35831;&#27714;&#20250;&#23548;&#33268;prefill&#21644;decode&#36845;&#20195;&#20132;&#38169;&#36827;&#34892;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#20043;&#38388;&#30340;&#24179;&#34913;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#28789;&#24863;&#26469;&#33258;&#25105;&#20204;&#26368;&#21021;&#20026;&#20248;&#21270;Sarathi&#30340;&#21534;&#21520;&#37327;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;Sarathi-Serve&#21033;&#29992;&#20102;&#20174;Sarathi&#20013;&#24341;&#20837;&#30340;&#20998;&#22359;prefill&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02310v1 Announce Type: new  Abstract: Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#23545;&#20110;&#23398;&#20064;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20010;&#32479;&#35745;&#26597;&#35810;&#19979;&#30028;&#65292;&#34920;&#26126;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23384;&#22312;&#30528;&#36229;&#22810;&#39033;&#24335;&#20449;&#24687;-&#35745;&#31639;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.02300</link><description>&lt;p&gt;
&#23545;&#23398;&#20064;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#20272;&#35745;&#30340;&#32479;&#35745;&#26597;&#35810;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Statistical Query Lower Bounds for Learning Truncated Gaussians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#23545;&#20110;&#23398;&#20064;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20010;&#32479;&#35745;&#26597;&#35810;&#19979;&#30028;&#65292;&#34920;&#26126;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23384;&#22312;&#30528;&#36229;&#22810;&#39033;&#24335;&#20449;&#24687;-&#35745;&#31639;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25130;&#26029;&#35774;&#32622;&#20013;&#20272;&#35745;&#22343;&#20540;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25130;&#26029;&#38598;&#26469;&#33258;&#19968;&#20010;&#20302;&#22797;&#26434;&#24230;&#38598;&#21512;$\mathcal{C}$&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#19968;&#20010;&#22266;&#23450;&#20294;&#26410;&#30693;&#30340;&#25130;&#26029;&#38598;$S \subseteq \mathbb{R}^d$&#65292;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#20174;&#20998;&#24067;$\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$&#25130;&#26029;&#21040;&#38598;&#21512;$S$&#30340;&#26679;&#26412;&#12290;&#30446;&#26631;&#26159;&#22312;$\ell_2$-&#33539;&#25968;&#20869;&#20197;&#31934;&#24230;$\epsilon&gt;0$&#20272;&#35745;$\boldsymbol\mu$&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#19979;&#30028;&#65292;&#34920;&#26126;&#20102;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23384;&#22312;&#30528;&#36229;&#22810;&#39033;&#24335;&#20449;&#24687;-&#35745;&#31639;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#20219;&#20309;SQ&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#22343;&#20026;$d^{\mathrm{poly}(1/\epsilon)}$&#65292;&#21363;&#20351;&#31867;$\mathcal{C}$&#26159;&#31616;&#21333;&#30340;&#65292;&#25152;&#20197;&#20449;&#24687;&#35770;&#19978;$\mathrm{poly}(d/\epsilon)$&#20010;&#26679;&#26412;&#36275;&#22815;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;SQ&#19979;&#30028;&#36866;&#29992;&#20110;&#24403;$\mathcal{C}$&#26159;&#26377;&#30028;&#20010;&#25968;&#30340;&#24182;&#38598;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02300v1 Announce Type: cross  Abstract: We study the problem of estimating the mean of an identity covariance Gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to samples from the distribution $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$ truncated to the set $S$. The goal is to estimate $\boldsymbol\mu$ within accuracy $\epsilon&gt;0$ in $\ell_2$-norm. Our main result is a Statistical Query (SQ) lower bound suggesting a super-polynomial information-computation gap for this task. In more detail, we show that the complexity of any SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class $\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice. Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union of a bounded num
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65292;&#30740;&#31350;&#20102;&#21313;&#24180;&#38388;&#38544;&#31169;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#38544;&#31169;&#35780;&#35770;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#65292;&#25506;&#35752;&#20102;&#28909;&#38376;&#21644;&#36880;&#28176;&#20943;&#23569;&#30340;&#38544;&#31169;&#35805;&#39064;&#65292;&#20197;&#21450;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30475;&#27861;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.02292</link><description>&lt;p&gt;
Android&#24212;&#29992;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#30340;&#21313;&#24180;&#22823;&#35268;&#27169;&#36235;&#21183;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02292
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65292;&#30740;&#31350;&#20102;&#21313;&#24180;&#38388;&#38544;&#31169;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#38544;&#31169;&#35780;&#35770;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#65292;&#25506;&#35752;&#20102;&#28909;&#38376;&#21644;&#36880;&#28176;&#20943;&#23569;&#30340;&#38544;&#31169;&#35805;&#39064;&#65292;&#20197;&#21450;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30475;&#27861;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#36825;&#20123;&#35780;&#35770;&#36328;&#36234;&#20102;10&#24180;&#26102;&#38388;&#12290;&#36890;&#36807;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#26102;&#38388;&#12289;&#22269;&#23478;&#12289;&#24212;&#29992;&#31867;&#22411;&#12289;&#19981;&#21516;&#38544;&#31169;&#20027;&#39064;&#20197;&#21450;&#22810;&#31181;&#24773;&#24863;&#32500;&#24230;&#19978;&#26816;&#35270;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#25345;&#32493;&#22686;&#38271;&#65292;&#24182;&#25506;&#31350;&#20102;&#19968;&#20123;&#28909;&#38376;&#35805;&#39064;&#65288;&#22914;&#25968;&#25454;&#21024;&#38500;&#21644;&#25968;&#25454;&#31363;&#21462;&#65289;&#65292;&#20197;&#21450;&#19968;&#20123;&#36880;&#28176;&#20943;&#23569;&#30340;&#35805;&#39064;&#65288;&#22914;&#28041;&#21450;&#25935;&#24863;&#26435;&#38480;&#30340;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65289;&#12290;&#23613;&#31649;&#38544;&#31169;&#35780;&#35770;&#26469;&#33258;200&#22810;&#20010;&#22269;&#23478;&#65292;&#20294;&#26377;33&#20010;&#22269;&#23478;&#25552;&#20379;&#20102;90%&#30340;&#38544;&#31169;&#35780;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#27599;&#20010;&#22269;&#23478;&#29992;&#25143;&#35780;&#35770;&#30340;&#38544;&#31169;&#20027;&#39064;&#20998;&#24067;&#26469;&#36827;&#34892;&#36328;&#22269;&#23478;&#27604;&#36739;&#65292;&#21457;&#29616;&#22320;&#29702;&#25509;&#36817;&#24182;&#19981;&#24847;&#21619;&#30528;&#38468;&#36817;&#22269;&#23478;&#26377;&#31867;&#20284;&#30340;&#38544;&#31169;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02292v1 Announce Type: new  Abstract: We present an analysis of 12 million instances of privacy-relevant reviews publicly visible on the Google Play Store that span a 10 year period. By leveraging state of the art NLP techniques, we can examine what users have been writing about privacy along multiple dimensions: time, countries, app types, diverse privacy topics, and even across a spectrum of emotions. We find consistent growth of privacy-relevant reviews, and explore topics that are trending (such as Data Deletion and Data Theft), as well as those on the decline (such as privacy-relevant reviews on sensitive permissions). We find that although privacy reviews come from more than 200 countries, 33 countries provide 90% of privacy reviews. We conduct a comparison across countries by examining the distribution of privacy topics a country's users write about, and find that geographic proximity is not a reliable indicator that nearby countries have similar privacy perspectives.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;Koopman&#31639;&#23376;&#25216;&#26415;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25552;&#21319;&#21040;&#26032;&#22352;&#26631;&#31995;&#65292;&#22312;&#20854;&#20013;&#21160;&#21147;&#23398;&#21464;&#24471;&#36817;&#20284;&#32447;&#24615;&#65292;&#20174;&#32780;&#26500;&#24314;&#20004;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#29366;&#24577;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#20256;&#32479;&#26041;&#31243;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02290</link><description>&lt;p&gt;
Koopman&#36741;&#21161;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Koopman-Assisted Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;Koopman&#31639;&#23376;&#25216;&#26415;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25552;&#21319;&#21040;&#26032;&#22352;&#26631;&#31995;&#65292;&#22312;&#20854;&#20013;&#21160;&#21147;&#23398;&#21464;&#24471;&#36817;&#20284;&#32447;&#24615;&#65292;&#20174;&#32780;&#26500;&#24314;&#20004;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#29366;&#24577;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#20256;&#32479;&#26041;&#31243;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40077;&#26364;&#26041;&#31243;&#21450;&#20854;&#36830;&#32493;&#24418;&#24335;&#65292;&#21363;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#65288;HJB&#65289;&#26041;&#31243;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#29366;&#24577;&#21644;&#38750;&#32447;&#24615;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#26041;&#31243;&#24456;&#24555;&#21464;&#24471;&#38590;&#20197;&#35299;&#20915;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;Koopman&#31639;&#23376;&#19982;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#24320;&#21457;&#20986;&#20004;&#31181;&#26032;&#30340;RL&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#21033;&#29992;Koopman&#31639;&#23376;&#25216;&#26415;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25552;&#21319;&#21040;&#26032;&#22352;&#26631;&#31995;&#65292;&#20854;&#20013;&#21160;&#21147;&#23398;&#21464;&#24471;&#36817;&#20284;&#32447;&#24615;&#65292;HJB&#26041;&#27861;&#26356;&#26131;&#22788;&#29702;&#12290;&#29305;&#21035;&#22320;&#65292;Koopman&#31639;&#23376;&#33021;&#22815;&#36890;&#36807;&#25552;&#21319;&#21040;&#30340;&#22352;&#26631;&#31995;&#20013;&#30340;&#32447;&#24615;&#21160;&#24577;&#26469;&#25429;&#33719;&#32473;&#23450;&#31995;&#32479;&#20540;&#20989;&#25968;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#26399;&#26395;&#12290;&#36890;&#36807;&#29992;&#25511;&#21046;&#21160;&#20316;&#21442;&#25968;&#21270;Koopman&#31639;&#23376;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;Koopman&#24352;&#37327;&#8221;&#65292;&#20197;&#20415;&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02290v1 Announce Type: new  Abstract: The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning (RL) and control theory. However, these equations quickly become intractable for systems with high-dimensional states and nonlinearity. This paper explores the connection between the data-driven Koopman operator and Markov Decision Processes (MDPs), resulting in the development of two new RL algorithms to address these limitations. We leverage Koopman operator techniques to lift a nonlinear system into new coordinates where the dynamics become approximately linear, and where HJB-based methods are more tractable. In particular, the Koopman operator is able to capture the expectation of the time evolution of the value function of a given system via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with the control actions, we construct a ``Koopman tensor'' that facilitates the es
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;PINC&#32593;&#32476;&#20013;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#24182;&#23436;&#21892;ODE&#20013;&#30340;&#26576;&#20123;&#39033;&#65292;&#35813;&#30740;&#31350;&#25913;&#36827;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#27668;&#20030;&#27833;&#20117;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.02289</link><description>&lt;p&gt;
&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27169;&#25311;&#19982;&#25511;&#21046;&#27668;&#20030;&#27833;&#20117;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks with Skip Connections for Modeling and Control of Gas-Lifted Oil Wells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02289
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;PINC&#32593;&#32476;&#20013;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#24182;&#23436;&#21892;ODE&#20013;&#30340;&#26576;&#20123;&#39033;&#65292;&#35813;&#30740;&#31350;&#25913;&#36827;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#27668;&#20030;&#27833;&#20117;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;PINC&#26694;&#26550;&#23558;PINNs&#25193;&#23637;&#21040;&#25511;&#21046;&#24212;&#29992;&#65292;&#20801;&#35768;&#23545;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#24320;&#25918;&#24335;&#38271;&#36317;&#31163;&#39044;&#27979;&#21644;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;PINC&#29992;&#20110;&#27169;&#25311;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#22914;&#27668;&#20030;&#27833;&#20117;&#12290;&#36890;&#36807;&#22312;PINC&#32593;&#32476;&#20013;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#24182;&#23436;&#21892;ODE&#20013;&#30340;&#26576;&#20123;&#39033;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#20026;&#27833;&#20117;&#31995;&#32479;&#30340;&#26377;&#25928;&#24314;&#27169;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#29256;PINC&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;&#27833;&#20117;&#24212;&#29992;&#20013;&#24179;&#22343;&#23558;&#39564;&#35777;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;&#20102;67&#65285;&#65292;&#24182;&#26174;&#33879;&#22686;&#24378;&#20102;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02289v1 Announce Type: new  Abstract: Neural networks, while powerful, often lack interpretability. Physics-Informed Neural Networks (PINNs) address this limitation by incorporating physics laws into the loss function, making them applicable to solving Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs). The recently introduced PINC framework extends PINNs to control applications, allowing for open-ended long-range prediction and control of dynamic systems. In this work, we enhance PINC for modeling highly nonlinear systems such as gas-lifted oil wells. By introducing skip connections in the PINC network and refining certain terms in the ODE, we achieve more accurate gradients during training, resulting in an effective modeling process for the oil well system. Our proposed improved PINC demonstrates superior performance, reducing the validation prediction error by an average of 67% in the oil well application and significantly enhancing gradient 
&lt;/p&gt;</description></item><item><title>NatSGD&#26159;&#19968;&#20010;&#21253;&#21547;&#35821;&#38899;&#12289;&#25163;&#21183;&#21644;&#28436;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#26426;&#22120;&#20154;&#36890;&#36807;&#22810;&#27169;&#24335;&#20154;&#31867;&#21629;&#20196;&#29702;&#35299;&#20219;&#21153;&#65292;&#24182;&#24378;&#35843;&#20102;&#20849;&#21516;&#32771;&#34385;&#35821;&#38899;&#21644;&#25163;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02274</link><description>&lt;p&gt;
NatSGD: &#19968;&#20010;&#21253;&#21547;&#35821;&#38899;&#12289;&#25163;&#21183;&#21644;&#28436;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#33258;&#28982;&#20154;&#26426;&#20132;&#20114;&#20013;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02274
&lt;/p&gt;
&lt;p&gt;
NatSGD&#26159;&#19968;&#20010;&#21253;&#21547;&#35821;&#38899;&#12289;&#25163;&#21183;&#21644;&#28436;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#26426;&#22120;&#20154;&#36890;&#36807;&#22810;&#27169;&#24335;&#20154;&#31867;&#21629;&#20196;&#29702;&#35299;&#20219;&#21153;&#65292;&#24182;&#24378;&#35843;&#20102;&#20849;&#21516;&#32771;&#34385;&#35821;&#38899;&#21644;&#25163;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24335;&#20154;&#26426;&#20132;&#20114;(HRI)&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#31361;&#20986;&#20102;&#35821;&#38899;&#21644;&#25163;&#21183;&#30340;&#34701;&#21512;&#65292;&#25193;&#23637;&#20102;&#26426;&#22120;&#20154;&#21560;&#25910;&#26174;&#24335;&#21644;&#38544;&#24335;&#20154;&#26426;&#20132;&#20114;&#27934;&#35265;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;-&#25163;&#21183;HRI&#25968;&#25454;&#38598;&#36890;&#24120;&#19987;&#27880;&#20110;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#29289;&#20307;&#25351;&#21521;&#21644;&#25512;&#21160;&#65292;&#25581;&#31034;&#20102;&#22312;&#25193;&#23637;&#21040;&#22797;&#26434;&#39046;&#22495;&#21644;&#20248;&#20808;&#32771;&#34385;&#20154;&#31867;&#21629;&#20196;&#25968;&#25454;&#32780;&#19981;&#26159;&#26426;&#22120;&#20154;&#34892;&#20026;&#35760;&#24405;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NatSGD&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;HRI&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#36890;&#36807;&#35821;&#38899;&#21644;&#25163;&#21183;&#20256;&#36798;&#30340;&#33258;&#28982;&#20154;&#31867;&#21629;&#20196;&#65292;&#36825;&#20123;&#21629;&#20196;&#19982;&#26426;&#22120;&#20154;&#34892;&#20026;&#28436;&#31034;&#21516;&#27493;&#12290;NatSGD&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;HRI&#30740;&#31350;&#20132;&#21449;&#28857;&#19978;&#30340;&#22522;&#30784;&#36164;&#28304;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#26426;&#22120;&#20154;&#36890;&#36807;&#22810;&#27169;&#24335;&#20154;&#31867;&#21629;&#20196;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24378;&#35843;&#20102;&#20849;&#21516;&#32771;&#34385;&#35821;&#38899;&#21644;&#25163;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24050;&#21457;&#24067;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#25311;&#22120;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02274v1 Announce Type: cross  Abstract: Recent advancements in multimodal Human-Robot Interaction (HRI) datasets have highlighted the fusion of speech and gesture, expanding robots' capabilities to absorb explicit and implicit HRI insights. However, existing speech-gesture HRI datasets often focus on elementary tasks, like object pointing and pushing, revealing limitations in scaling to intricate domains and prioritizing human command data over robot behavior records. To bridge these gaps, we introduce NatSGD, a multimodal HRI dataset encompassing human commands through speech and gestures that are natural, synchronized with robot behavior demonstrations. NatSGD serves as a foundational resource at the intersection of machine learning and HRI research, and we demonstrate its effectiveness in training robots to understand tasks through multimodal human commands, emphasizing the significance of jointly considering speech and gestures. We have released our dataset, simulator, a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02271</link><description>&lt;p&gt;
RIFF: &#23398;&#20064;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#25913;&#20889;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#31934;&#30830;&#22320;&#20026;&#19979;&#28216;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20248;&#21270;&#36755;&#20837;&#25552;&#31034;&#25110;&#35843;&#25972;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914; LoRA&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21464;&#21407;&#22987;&#20219;&#21153;&#30340;&#36755;&#20837;&#25991;&#26412;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#26368;&#26377;&#25928;&#22320;&#37325;&#20889;&#36755;&#20837;&#25991;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#36229;&#20986;&#20102;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02271v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.02253</link><description>&lt;p&gt;
KnowPhish&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20197;&#22686;&#24378;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24050;&#32473;&#20010;&#20154;&#21644;&#20225;&#19994;&#36896;&#25104;&#20102;&#37325;&#22823;&#25439;&#22833;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#39640;&#25928;&#30340;&#33258;&#21160;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#65288;RBPDs&#65289;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#27604;&#36739;&#30446;&#26631;&#32593;&#39029;&#19978;&#30340;&#26631;&#24535;&#19982;&#24050;&#30693;&#26631;&#24535;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;RBPDs&#30340;&#20027;&#35201;&#23616;&#38480;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#25163;&#21160;&#26500;&#24314;&#30340;&#21697;&#29260;&#30693;&#35782;&#24211;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#37327;&#21697;&#29260;&#65292;&#23548;&#33268;&#30001;&#20110;&#30693;&#35782;&#24211;&#20013;&#21697;&#29260;&#35206;&#30422;&#19981;&#36275;&#32780;&#20986;&#29616;&#20551;&#38452;&#24615;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#37319;&#29992;&#35813;&#27969;&#27700;&#32447;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21253;&#21547;20k&#20010;&#21697;&#29260;&#21644;&#27599;&#20010;&#21697;&#29260;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;KnowPhish&#21487;&#20197;&#29992;&#26469;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#25552;&#21319;&#29616;&#26377;RBPDs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02253v1 Announce Type: cross  Abstract: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#20915;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#8220;&#39044;&#27979;&#21018;&#24615;&#8221;&#20316;&#20026;&#19968;&#31181;&#33719;&#24471;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#22238;&#24402;&#22120;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#26041;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#22810;&#31181;&#22238;&#24402;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02251</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#20302;&#25104;&#26412;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#21018;&#24615;&#24418;&#24335;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
A prediction rigidity formalism for low-cost uncertainties in trained neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02251
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#20915;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#8220;&#39044;&#27979;&#21018;&#24615;&#8221;&#20316;&#20026;&#19968;&#31181;&#33719;&#24471;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#22238;&#24402;&#22120;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#26041;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#22810;&#31181;&#22238;&#24402;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26041;&#27861;&#23545;&#31185;&#23398;&#21644;&#25216;&#26415;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25311;&#21512;&#27169;&#22411;&#22312;&#20854;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#21487;&#33021;&#26497;&#19981;&#21487;&#38752;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#8220;&#39044;&#27979;&#21018;&#24615;&#8221;&#20316;&#20026;&#19968;&#31181;&#33719;&#24471;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#22238;&#24402;&#22120;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#36125;&#21494;&#26031;&#25512;&#26029;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20801;&#35768;&#26032;&#26041;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#36924;&#36817;&#12290;&#36825;&#31181;&#25193;&#23637;&#25552;&#20379;&#20102;&#19981;&#38656;&#35201;&#23545;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#25110;&#20854;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#20302;&#25104;&#26412;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#31616;&#21333;&#29609;&#20855;&#27169;&#22411;&#21040;&#21270;&#23398;&#21644;&#27668;&#35937;&#23398;&#24212;&#29992;&#30340;&#24191;&#27867;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02251v1 Announce Type: cross  Abstract: Regression methods are fundamental for scientific and technological applications. However, fitted models can be highly unreliable outside of their training domain, and hence the quantification of their uncertainty is crucial in many of their applications. Based on the solution of a constrained optimization problem, we propose "prediction rigidities" as a method to obtain uncertainties of arbitrary pre-trained regressors. We establish a strong connection between our framework and Bayesian inference, and we develop a last-layer approximation that allows the new method to be applied to neural networks. This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure. We show the effectiveness of our method on a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#29992;&#20110;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#24490;&#29615;&#31934;&#24230;&#35757;&#32451;&#35843;&#24230;&#30340;&#26356;&#22909;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.02243</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#26356;&#22909;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Better Schedules for Low Precision Training of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#29992;&#20110;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#24490;&#29615;&#31934;&#24230;&#35757;&#32451;&#35843;&#24230;&#30340;&#26356;&#22909;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31934;&#24230;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#25216;&#26415;&#65292;&#20294;&#24490;&#29615;&#31934;&#24230;&#35757;&#32451;(CPT)&#65292;&#26681;&#25454;&#24490;&#29615;&#35843;&#24230;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31934;&#24230;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;DNN&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;CPT&#23454;&#29616;&#37319;&#29992;&#24120;&#35265;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#65288;&#20363;&#22914;&#65292;&#21608;&#26399;&#20313;&#24358;&#35843;&#24230;&#65289;&#65292;&#24182;&#22312;&#20302;&#31934;&#24230;&#35757;&#32451;&#20013;&#20351;&#29992;&#23427;&#20204;&#65292;&#20294;&#26410;&#19982;&#20854;&#20182;&#35843;&#24230;&#36873;&#39033;&#36827;&#34892;&#20805;&#20998;&#27604;&#36739;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#22871;&#22810;&#26679;&#21270;&#30340;CPT&#35843;&#24230;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#21508;&#31181;DNN&#35757;&#32451;&#26041;&#26696;&#20013;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#20302;&#31934;&#24230;&#35757;&#32451;&#25991;&#29486;&#20013;&#23578;&#26410;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65289;&#12290;&#36890;&#36807;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#25552;&#20379;&#36827;&#19968;&#27493;&#25552;&#21319;&#35757;&#32451;&#25928;&#29575;&#30340;&#26367;&#20195;CPT&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02243v1 Announce Type: cross  Abstract: Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout train- ing according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine sched- ules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks). From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02241</link><description>&lt;p&gt;
&#31070;&#32463;&#32418;&#31227;&#65306;&#38543;&#26426;&#32593;&#32476;&#24182;&#38750;&#38543;&#26426;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Redshift: Random Networks are not Random Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#19981;&#23436;&#25972;&#12290;&#30446;&#21069;&#30340;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#38544;&#21547;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#35299;&#37322;&#26799;&#24230;&#33258;&#30001;&#26041;&#27861;&#20013;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20063;&#26080;&#27861;&#35299;&#37322;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#26410;&#32463;&#35757;&#32451;&#32593;&#32476;&#30340;&#31616;&#21333;&#20559;&#35265;&#12290;&#26412;&#25991;&#23547;&#25214;NNs&#20013;&#30340;&#20854;&#20182;&#27867;&#21270;&#28304;&#12290;&#20026;&#20102;&#29420;&#31435;&#20110;GD&#29702;&#35299;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#25105;&#20204;&#30740;&#31350;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;MLPs&#20063;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65306;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#20250;&#20135;&#29983;&#19968;&#20010;&#38750;&#24120;&#20559;&#21521;&#20110;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#20998;&#24067;&#12290;&#20294;&#19982;&#24120;&#35268;&#26234;&#24935;&#19981;&#21516;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#12290;&#36825;&#19968;&#29305;&#24615;&#21462;&#20915;&#20110;&#32452;&#20214;&#65292;&#22914;ReLU&#12289;&#27531;&#24046;&#36830;&#25509;&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;&#21487;&#21033;&#29992;&#26367;&#20195;&#20307;&#31995;&#32467;&#26500;&#26500;&#24314;&#20559;&#21521;&#20110;&#20219;&#20309;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20559;&#35265;&#12290;Transformers&#20063;&#20855;&#26377;&#36825;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;</title><link>https://arxiv.org/abs/2403.02233</link><description>&lt;p&gt;
Transformers&#22312;Masked Image Modeling&#20013;&#33021;&#22815;&#35777;&#26126;&#23398;&#20064;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Masked image modeling (MIM)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20174;&#26410;&#23631;&#34109;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#38543;&#26426;&#23631;&#34109;&#30340;&#34917;&#19969;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;transformers&#30340;MIM&#30340;&#29702;&#35770;&#29702;&#35299;&#30456;&#24403;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#19968;&#23618;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;&#30340;&#29702;&#35770;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02233v1 Announce Type: new  Abstract: Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02232</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#23545;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25506;&#35752;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;&#20351;&#29992;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#12290;&#26088;&#22312;&#36890;&#36807;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#32531;&#35299;&#23041;&#32961;&#26469;&#25512;&#36827;&#32593;&#32476;&#23433;&#20840;&#33021;&#21147;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#38598;&#25104;&#21644;&#38750;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;TF-IDF&#34920;&#31034;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#38598;&#25104;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#38480;&#21046;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#19981;&#26029;&#36866;&#24212;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02232v1 Announce Type: cross  Abstract: This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation t
&lt;/p&gt;</description></item><item><title>TPLLM&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26377;&#38480;&#30340;&#22320;&#21306;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02221</link><description>&lt;p&gt;
TPLLM: &#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02221
&lt;/p&gt;
&lt;p&gt;
TPLLM&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26377;&#38480;&#30340;&#22320;&#21306;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02221v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20132;&#36890;&#39044;&#27979;&#26500;&#25104;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#33539;&#22260;&#20869;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#39640;&#31934;&#24230;&#39044;&#27979;&#23545;&#20110;&#26377;&#25928;&#20132;&#36890;&#31649;&#29702;&#20855;&#26377;&#28145;&#36828;&#24847;&#20041;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#24230;&#36890;&#24120;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#32780;&#21576;&#19978;&#21319;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#26102;&#31354;&#25968;&#25454;&#24448;&#24448;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#20445;&#23384;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22312;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26377;&#38480;&#30340;&#22320;&#21306;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#24320;&#21457;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#36805;&#36895;&#21457;&#23637;&#30340;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36328;&#27169;&#24577;&#30693;&#35782;&#20256;&#36755;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02221v1 Announce Type: new  Abstract: Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#32852;&#21512;&#20272;&#35745;&#21644;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#22312;&#32447;&#35757;&#32451;&#21644;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.02215</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#23454;&#29616;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32852;&#21512;&#21442;&#25968;&#21644;&#21442;&#25968;&#21270;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02215
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#32852;&#21512;&#20272;&#35745;&#21644;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#22312;&#32447;&#35757;&#32451;&#21644;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#34920;&#31034;&#25968;&#20540;&#27169;&#25311;&#20013;&#26410;&#30693;&#21644;&#20122;&#32593;&#26684;&#29289;&#29702;&#36807;&#31243;&#30340;&#21442;&#25968;&#21270;(&#25110;&#38381;&#21512;)&#24182;&#23545;&#20854;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#23545;&#20110;&#35299;&#26512;&#35768;&#22810;&#38382;&#39064;&#30340;&#31895;&#31890;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#20197;&#21450;&#28237;&#27969;&#27169;&#25311;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#30475;&#21040;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23545;&#36825;&#20123;&#20122;&#32593;&#26684;&#36807;&#31243;&#24314;&#27169;&#65292;&#23548;&#33268;&#20102;&#36890;&#36807;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#38598;&#25104;&#24320;&#21457;&#28151;&#21512;&#29289;&#29702;-ML&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#12290;&#36890;&#36807;&#22312;&#32447;&#35757;&#32451;&#21644;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#20511;&#21161;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#33021;&#21147;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02215v1 Announce Type: new  Abstract: Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical simulations with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence simulations. Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers. In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming. Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming. This proof 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02187</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mutual Information Estimation via Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20272;&#35745;&#38382;&#39064;&#65292;&#21363;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#20855;&#26377;&#24050;&#30693;&#20114;&#20449;&#24687;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#36890;&#36807;&#39640;&#32500;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#20272;&#35745;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
&lt;/p&gt;</description></item><item><title>ChatGPT&#34987;&#24212;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#36801;&#31227;&#23398;&#20064;&#34701;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29983;&#25104;&#36731;&#37327;&#32423;&#20027;&#39064;&#21644;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#37327;&#21270;&#25237;&#36164;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.02185</link><description>&lt;p&gt;
ChatGPT&#20027;&#39064;&#21644;&#24773;&#24863;&#24314;&#27169;&#22312;&#37329;&#34701;&#20013;&#30340;&#24212;&#29992;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distilled ChatGPT Topic &amp; Sentiment Modeling with Applications in Finance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02185
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#34987;&#24212;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#36801;&#31227;&#23398;&#20064;&#34701;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29983;&#25104;&#36731;&#37327;&#32423;&#20027;&#39064;&#21644;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#37327;&#21270;&#25237;&#36164;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;ChatGPT&#21019;&#24314;&#20102;&#31616;&#21270;&#27169;&#22411;&#65292;&#29983;&#25104;&#26131;&#20110;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#35780;&#20272;&#20174;&#36130;&#25253;&#30005;&#35805;&#20250;&#35758;&#20013;&#24471;&#20986;&#30340;&#36130;&#21153;&#32467;&#26524;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#30693;&#35782;&#33976;&#39311;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#36731;&#37327;&#32423;&#30340;&#20027;&#39064;&#21644;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#32780;&#20934;&#30830;&#29575;&#25439;&#22833;&#19981;&#22823;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#12290;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#20004;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#31361;&#26174;&#20102;&#29983;&#25104;&#30340;&#29305;&#24449;&#22914;&#20309;&#22312;&#37327;&#21270;&#25237;&#36164;&#22330;&#26223;&#20013;&#26377;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02185v1 Announce Type: cross  Abstract: In this study, ChatGPT is utilized to create streamlined models that generate easily interpretable features. These features are then used to evaluate financial outcomes from earnings calls. We detail a training approach that merges knowledge distillation and transfer learning, resulting in lightweight topic and sentiment classification models without significant loss in accuracy. These models are assessed through a dataset annotated by experts. The paper also delves into two practical case studies, highlighting how the generated features can be effectively utilized in quantitative investing scenarios.
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.02178</link><description>&lt;p&gt;
&#25513;&#38754;&#24605;&#24819;:&#31616;&#21333;&#22320;&#25513;&#30422;&#37096;&#20998;&#25512;&#29702;&#27493;&#39588;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02178
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#36731;&#24494;&#30340;&#38169;&#35823;&#20063;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#38543;&#26426;&#25513;&#30422;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#36825;&#31181;&#25216;&#26415;&#23545;&#25512;&#29702;&#20219;&#21153;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02178v1 Announce Type: cross  Abstract: In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% im
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#23431;&#23449;&#23398;&#27169;&#25311;&#20013;&#23581;&#35797;&#39044;&#27979;&#32467;&#26500;&#28436;&#21270;&#65292;&#21457;&#29616;&#22312;2D&#27169;&#25311;&#20013;&#33021;&#22815;&#24456;&#22909;&#22320;&#39044;&#27979;&#26263;&#29289;&#36136;&#22330;&#30340;&#32467;&#26500;&#28436;&#21270;&#65292;&#20294;&#22312;3D&#27169;&#25311;&#20013;&#34920;&#29616;&#26356;&#24046;&#65292;&#25552;&#20379;&#36895;&#24230;&#22330;&#20316;&#20026;&#36755;&#20837;&#21518;&#32467;&#26524;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2403.02171</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#39044;&#27979;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Predicting large scale cosmological structure evolution with GAN-based autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02171
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#23431;&#23449;&#23398;&#27169;&#25311;&#20013;&#23581;&#35797;&#39044;&#27979;&#32467;&#26500;&#28436;&#21270;&#65292;&#21457;&#29616;&#22312;2D&#27169;&#25311;&#20013;&#33021;&#22815;&#24456;&#22909;&#22320;&#39044;&#27979;&#26263;&#29289;&#36136;&#22330;&#30340;&#32467;&#26500;&#28436;&#21270;&#65292;&#20294;&#22312;3D&#27169;&#25311;&#20013;&#34920;&#29616;&#26356;&#24046;&#65292;&#25552;&#20379;&#36895;&#24230;&#22330;&#20316;&#20026;&#36755;&#20837;&#21518;&#32467;&#26524;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23431;&#23449;&#23398;&#27169;&#25311;&#22312;&#20174;&#21021;&#22987;&#26465;&#20214;&#39044;&#27979;&#21644;&#29702;&#35299;&#22823;&#23610;&#24230;&#32467;&#26500;&#24418;&#25104;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;GAN&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#23581;&#35797;&#39044;&#27979;&#27169;&#25311;&#20013;&#30340;&#32467;&#26500;&#28436;&#21270;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;&#22312;&#25551;&#36848;&#26263;&#29289;&#36136;&#22330;&#28436;&#21270;&#30340;2D&#21644;3D N&#20307;&#27169;&#25311;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#31435;&#26041;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#24456;&#22909;&#22320;&#39044;&#27979;2D&#27169;&#25311;&#26263;&#29289;&#36136;&#22330;&#30340;&#32467;&#26500;&#28436;&#21270;&#65292;&#20294;&#22312;&#31867;&#20284;&#26465;&#20214;&#19979;&#65292;&#20165;&#20351;&#29992;&#23494;&#24230;&#22330;&#20316;&#20026;&#36755;&#20837;&#24773;&#20917;&#19979;&#65292;&#22312;3D&#27169;&#25311;&#20013;&#34920;&#29616;&#26126;&#26174;&#26356;&#24046;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#36895;&#24230;&#22330;&#20316;&#20026;&#36755;&#20837;&#33021;&#22815;&#22823;&#22823;&#25913;&#21892;&#32467;&#26524;&#65292;&#39044;&#27979;&#31867;&#20284;&#65292;&#32780;&#26080;&#35770;&#36755;&#20837;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#26102;&#38388;&#24046;&#24322;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02171v1 Announce Type: cross  Abstract: Cosmological simulations play a key role in the prediction and understanding of large scale structure formation from initial conditions. We make use of GAN-based Autoencoders (AEs) in an attempt to predict structure evolution within simulations. The AEs are trained on images and cubes issued from respectively 2D and 3D N-body simulations describing the evolution of the dark matter (DM) field. We find that while the AEs can predict structure evolution for 2D simulations of DM fields well, using only the density fields as input, they perform significantly more poorly in similar conditions for 3D simulations. However, additionally providing velocity fields as inputs greatly improves results, with similar predictions regardless of time-difference between input and target.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Recency-Weighted Temporally-Segmented&#65288;ReWTS&#65289;&#38598;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#22359;&#29366;&#26041;&#27861;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#65292;&#21487;&#20197;&#19987;&#38376;&#21270;&#27169;&#22411;&#24182;&#20248;&#21270;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02150</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#26102;&#38388;&#20998;&#27573;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Recency-Weighted Temporally-Segmented Ensemble for Time-Series Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02150
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Recency-Weighted Temporally-Segmented&#65288;ReWTS&#65289;&#38598;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#22359;&#29366;&#26041;&#27861;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#65292;&#21487;&#20197;&#19987;&#38376;&#21270;&#27169;&#22411;&#24182;&#20248;&#21270;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#33402;&#34892;&#19994;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#38754;&#23545;&#22788;&#29702;&#22797;&#26434;&#12289;&#22810;&#26041;&#38754;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#25968;&#25454;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#22810;&#26679;&#21160;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23548;&#33268;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Recency-Weighted Temporally-Segmented&#65288;ReWTS&#65292;&#21457;&#38899;&#20026;`roots'&#65289;&#38598;&#25104;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;&#22810;&#27493;&#39044;&#27979;&#26041;&#27861;&#12290;ReWTS&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#24449;&#26377;&#20004;&#20010;&#65306;1&#65289;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#21010;&#20998;&#20026;&#25968;&#25454;&#22359;&#24182;&#23545;&#27599;&#20010;&#22359;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#19987;&#38376;&#21270;&#20026;&#19981;&#21516;&#30340;&#21160;&#24577;&#12290;2&#65289;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#19968;&#20010;&#20248;&#21270;&#36807;&#31243;&#35780;&#20272;&#26368;&#36817;&#30340;&#36807;&#21435;&#20013;&#30340;&#27599;&#20010;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#27963;&#21160;&#27169;&#22411;&#65292;&#20197;&#20415;&#33021;&#22815;&#21484;&#22238;&#20197;&#21069;&#23398;&#20064;&#30340;&#21160;&#24577;&#30340;&#36866;&#24403;&#28151;&#21512;&#26469;&#39044;&#27979;&#26410;&#26469;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25429;&#25417;&#20102;&#27599;&#20010;&#26102;&#26399;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#32780;&#19988;&#33021;&#22815;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02150v1 Announce Type: cross  Abstract: Time-series modeling in process industries faces the challenge of dealing with complex, multi-faceted, and evolving data characteristics. Conventional single model approaches often struggle to capture the interplay of diverse dynamics, resulting in suboptimal forecasts. Addressing this, we introduce the Recency-Weighted Temporally-Segmented (ReWTS, pronounced `roots') ensemble model, a novel chunk-based approach for multi-step forecasting. The key characteristics of the ReWTS model are twofold: 1) It facilitates specialization of models into different dynamics by segmenting the training data into `chunks' of data and training one model per chunk. 2) During inference, an optimization procedure assesses each model on the recent past and selects the active models, such that the appropriate mixture of previously learned dynamics can be recalled to forecast the future. This method not only captures the nuances of each period, but also adapt
&lt;/p&gt;</description></item><item><title>Inf2Guard&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#38450;&#24481;&#26694;&#26550;&#65292;&#24212;&#23545;&#19977;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#25512;&#26029;&#25915;&#20987;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#30456;&#20114;&#20449;&#24687;&#30446;&#26631;&#65292;&#20998;&#21035;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;&#25928;&#29992;&#20445;&#30041;&#12290;</title><link>https://arxiv.org/abs/2403.02116</link><description>&lt;p&gt;
Inf2Guard&#65306;&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#38450;&#27490;&#25512;&#26029;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02116
&lt;/p&gt;
&lt;p&gt;
Inf2Guard&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#38450;&#24481;&#26694;&#26550;&#65292;&#24212;&#23545;&#19977;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#25512;&#26029;&#25915;&#20987;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#30456;&#20114;&#20449;&#24687;&#30446;&#26631;&#65292;&#20998;&#21035;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;&#25928;&#29992;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23481;&#26131;&#21463;&#21040;&#25512;&#26029;&#25915;&#20987;&#65288;&#20363;&#22914;&#25104;&#21592;&#25512;&#26029;&#12289;&#23646;&#24615;&#25512;&#26029;&#21644;&#25968;&#25454;&#37325;&#26500;&#31561;&#65289;&#30340;&#23041;&#32961;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#25512;&#26029;&#35757;&#32451;&#25968;&#25454;&#25110;&#25968;&#25454;&#38598;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#21482;&#38024;&#23545;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#25915;&#20987;&#35774;&#35745;&#65292;&#19988;&#20250;&#29306;&#29298;&#36739;&#22823;&#30340;&#25928;&#29992;&#65292;&#25110;&#24456;&#24555;&#34987;&#33258;&#36866;&#24212;&#25915;&#20987;&#20987;&#30772;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#38450;&#24481;&#26694;&#26550;&#65292;&#21517;&#20026;Inf2Guard&#65292;&#29992;&#20110;&#25269;&#24481;&#19977;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#25512;&#26029;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21463;&#21040;&#34920;&#31034;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#35748;&#20026;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#19981;&#20165;&#33410;&#30465;&#26102;&#38388;/&#25104;&#26412;&#65292;&#32780;&#19988;&#26377;&#30410;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;Inf2Guard&#21253;&#25324;&#20004;&#20010;&#30456;&#20114;&#20449;&#24687;&#30446;&#26631;&#65292;&#20998;&#21035;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;&#25928;&#29992;&#20445;&#30041;&#12290;Inf2Guard&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65306;&#23427;&#20419;&#36827;&#20102;&#38024;&#23545;&#29305;&#23450;&#25512;&#26029;&#25915;&#20987;&#30340;&#23450;&#21046;&#30446;&#26631;&#30340;&#35774;&#35745;&#65307;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02116v1 Announce Type: new  Abstract: Machine learning (ML) is vulnerable to inference (e.g., membership inference, property inference, and data reconstruction) attacks that aim to infer the private information of training data or dataset. Existing defenses are only designed for one specific type of attack and sacrifice significant utility or are soon broken by adaptive attacks. We address these limitations by proposing an information-theoretic defense framework, called Inf2Guard, against the three major types of inference attacks. Our framework, inspired by the success of representation learning, posits that learning shared representations not only saves time/costs but also benefits numerous downstream tasks. Generally, Inf2Guard involves two mutual information objectives, for privacy protection and utility preservation, respectively. Inf2Guard exhibits many merits: it facilitates the design of customized objectives against the specific inference attack; it provides a gener
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.02090</link><description>&lt;p&gt;
&#24314;&#27169;&#22810;&#27169;&#24577;&#31038;&#20132;&#20114;&#21160;&#65306;&#20855;&#26377;&#23494;&#38598;&#23545;&#40784;&#34920;&#31034;&#30340;&#26032;&#25361;&#25112;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28041;&#21450;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#32447;&#32034;&#30340;&#31038;&#20132;&#20114;&#21160;&#23545;&#26377;&#25928;&#35299;&#37322;&#31038;&#20132;&#24773;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#22810;&#27169;&#24577;&#31038;&#20132;&#32447;&#32034;&#30340;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20154;&#34892;&#20026;&#19978;&#65292;&#25110;&#20381;&#36182;&#20110;&#19982;&#22810;&#26041;&#29615;&#22659;&#20013;&#30340;&#35805;&#35821;&#23494;&#20999;&#23545;&#40784;&#30340;&#25972;&#20307;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#20204;&#22312;&#24314;&#27169;&#22810;&#26041;&#20114;&#21160;&#30340;&#22797;&#26434;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#24314;&#27169;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65306;&#35805;&#35821;&#30446;&#26631;&#35782;&#21035;&#12289;&#20195;&#35789;&#25351;&#20195;&#28040;&#35299;&#21644;&#25552;&#21450;&#29609;&#23478;&#39044;&#27979;&#12290;&#25105;&#20204;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#20013;&#30340;&#36825;&#20123;&#26032;&#25361;&#25112;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#19982;&#20854;&#23545;&#24212;&#30340;&#35805;&#35821;&#21516;&#27493;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#65292;&#36825;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
&lt;/p&gt;</description></item><item><title>&#24403;&#20449;&#22122;&#27604;&#36739;&#20302;&#26102;&#65292;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#38647;&#36798;&#26816;&#27979;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02080</link><description>&lt;p&gt;
&#20302;&#20449;&#22122;&#27604;&#19979;&#20197;&#38647;&#36798;&#20026;&#22522;&#30784;&#30340;&#26080;&#20154;&#26426;&#26816;&#27979;&#19982;&#20998;&#31867;&#30340;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection and Classification in Low Signal-to-Noise Ratio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02080
&lt;/p&gt;
&lt;p&gt;
&#24403;&#20449;&#22122;&#27604;&#36739;&#20302;&#26102;&#65292;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#38647;&#36798;&#26816;&#27979;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(HQNN)&#21644;&#21487;&#27604;&#36739;&#30340;&#32463;&#20856;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#35299;&#20915;&#38647;&#36798;&#26816;&#27979;&#21644;&#20998;&#31867;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20174;&#30005;&#30913;&#29702;&#35770;&#20013;&#24471;&#20986;&#30340;&#30456;&#24403;&#22797;&#26434;&#30340;&#38647;&#36798;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21363;Martin-Mulgrew&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#34987;&#29992;&#26469;&#27169;&#25311;&#24102;&#26377;&#26059;&#36716;&#21494;&#29255;&#30340;&#29289;&#20307;&#65288;&#22914;&#26080;&#20154;&#26426;&#65289;&#30340;&#38647;&#36798;&#36820;&#22238;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#36739;&#39640;&#26102;&#65292;CNN&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#38754;&#20248;&#20110;HQNN&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;SNR&#33539;&#22260;&#65288;&#36825;&#22312;&#23454;&#36341;&#20013;&#26368;&#20026;&#20851;&#27880;&#65289;&#20013;&#65292;HQNN&#30340;&#24615;&#33021;&#34987;&#21457;&#29616;&#20248;&#20110;&#31867;&#20284;&#26550;&#26500;&#30340;CNN&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02080v1 Announce Type: cross  Abstract: In this paper, we investigate the performance of a Hybrid Quantum Neural Network (HQNN) and a comparable classical Convolution Neural Network (CNN) for detection and classification problem using a radar. Specifically, we take a fairly complex radar time-series model derived from electromagnetic theory, namely the Martin-Mulgrew model, that is used to simulate radar returns of objects with rotating blades, such as drones. We find that when that signal-to-noise ratio (SNR) is high, CNN outperforms the HQNN for detection and classification. However, in the low SNR regime (which is of greatest interest in practice) the performance of HQNN is found to be superior to that of the CNN of a similar architecture.
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#23614;&#25200;&#21160;&#19979;&#65292;&#22122;&#22768;SGD&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#31867;&#65292;&#29305;&#21035;&#26159;&#38750;&#20984;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.02051</link><description>&lt;p&gt;
&#22312;&#37325;&#23614;&#25200;&#21160;&#19979;&#22122;&#22768;(S)GD&#30340;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02051
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#23614;&#25200;&#21160;&#19979;&#65292;&#22122;&#22768;SGD&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#31867;&#65292;&#29305;&#21035;&#26159;&#38750;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#23614;&#22122;&#22768;&#27880;&#20837;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#36845;&#20195;&#20013;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#23545;&#23548;&#33268;&#30340;&#31639;&#27861;&#30340;&#21508;&#31181;&#29702;&#35770;&#24615;&#36136;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20027;&#35201;&#26469;&#33258;&#23398;&#20064;&#29702;&#35770;&#21644;&#20248;&#21270;&#35270;&#35282;&#65292;&#20294;&#23427;&#20204;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#23578;&#26410;&#24314;&#31435;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#20026;&#22122;&#22768;SGD&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;(DP)&#20445;&#35777;&#65292;&#24403;&#27880;&#20837;&#30340;&#22122;&#22768;&#36981;&#24490;$\alpha$-&#31283;&#23450;&#20998;&#24067;&#26102;&#65292;&#35813;&#20998;&#24067;&#21253;&#25324;&#19968;&#31995;&#21015;&#37325;&#23614;&#20998;&#24067;(&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;)&#20197;&#21450;&#39640;&#26031;&#20998;&#24067;&#12290;&#32771;&#34385;$(\epsilon,\delta)$-DP&#26694;&#26550;&#65292;&#25105;&#20204;&#34920;&#26126;&#24102;&#26377;&#37325;&#23614;&#25200;&#21160;&#30340;SGD&#23454;&#29616;&#20102;$(0,\tilde{\mathcal{O}}(1/n))$-DP&#30340;&#24191;&#27867;&#25439;&#22833;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#26159;&#38750;&#20984;&#30340;&#65292;&#36825;&#37324;$n$&#26159;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#12290;&#20316;&#20026;&#19968;&#39033;&#26174;&#30528;&#30340;&#21103;&#20135;&#21697;&#65292;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#35813;&#24037;&#20316;&#35201;&#27714;&#26377;&#30028;se
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02051v1 Announce Type: cross  Abstract: Injecting heavy-tailed noise to the iterates of stochastic gradient descent (SGD) has received increasing attention over the past few years. While various theoretical properties of the resulting algorithm have been analyzed mainly from learning theory and optimization perspectives, their privacy preservation properties have not yet been established. Aiming to bridge this gap, we provide differential privacy (DP) guarantees for noisy SGD, when the injected noise follows an $\alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the Gaussian distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that SGD with heavy-tailed perturbations achieves $(0, \tilde{\mathcal{O}}(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded se
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.02042</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#36523;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32422;&#26463;&#33719;&#21462;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network for Constraint Acquisition through Tailored Loss Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02042
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32422;&#26463;&#30340;&#37325;&#35201;&#24615;&#34987;&#20854;&#22312;&#23454;&#38469;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#25152;&#24378;&#35843;&#12290;&#23613;&#31649;&#32422;&#26463;&#22312;&#24314;&#27169;&#21644;&#35299;&#20915;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32422;&#26463;&#30340;&#26041;&#27861;&#20173;&#28982;&#30456;&#23545;&#31232;&#32570;&#12290;&#27492;&#22806;&#65292;&#24314;&#27169;&#30340;&#22797;&#26434;&#20219;&#21153;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#23481;&#26131;&#20986;&#38169;&#65292;&#22240;&#27492;&#32422;&#26463;&#33719;&#21462;&#26041;&#27861;&#36890;&#36807;&#20174;&#31034;&#20363;&#25110;&#35299;&#20915;&#26041;&#26696;&#21644;&#38750;&#35299;&#20915;&#26041;&#26696;&#30340;&#34892;&#20026;&#20013;&#23398;&#21040;&#30340;&#32422;&#26463;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#32422;&#26463;&#12290;&#21033;&#29992;&#24403;&#21069;&#26041;&#27861;&#65292;&#30452;&#25509;&#21046;&#23450;&#32422;&#26463;&#24471;&#20197;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;DNN&#30340;&#24191;&#27867;&#39044;&#20808;&#24320;&#21457;&#30340;&#26550;&#26500;&#21644;&#21151;&#33021;&#65292;&#19982;&#20854;&#20182;&#26694;&#26550;&#30340;&#36830;&#25509;&#21644;&#25193;&#23637;&#21487;&#20197;&#39044;&#35265;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02042v1 Announce Type: new  Abstract: The significance of learning constraints from data is underscored by its potential applications in real-world problem-solving. While constraints are popular for modeling and solving, the approaches to learning constraints from data remain relatively scarce. Furthermore, the intricate task of modeling demands expertise and is prone to errors, thus constraint acquisition methods offer a solution by automating this process through learnt constraints from examples or behaviours of solutions and non-solutions. This work introduces a novel approach grounded in Deep Neural Network (DNN) based on Symbolic Regression that, by setting suitable loss functions, constraints can be extracted directly from datasets. Using the present approach, direct formulation of constraints was achieved. Furthermore, given the broad pre-developed architectures and functionalities of DNN, connections and extensions with other frameworks could be foreseen.
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;Gevrey&#31867;&#38382;&#39064;&#19978;&#20855;&#26377;&#25351;&#25968;&#34920;&#36798;&#33021;&#21147;&#65292;&#20013;&#38388;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;ReLU&#21644;ReLU$^2$&#28608;&#27963;&#32452;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#27169;&#25311;&#39640;&#38454;&#26377;&#38480;&#20803;&#12290;</title><link>https://arxiv.org/abs/2403.02035</link><description>&lt;p&gt;
ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;Gevrey&#31867;&#38382;&#39064;&#19978;&#30340;&#25351;&#25968;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes with Point Singularities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02035
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;Gevrey&#31867;&#38382;&#39064;&#19978;&#20855;&#26377;&#25351;&#25968;&#34920;&#36798;&#33021;&#21147;&#65292;&#20013;&#38388;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;ReLU&#21644;ReLU$^2$&#28608;&#27963;&#32452;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#27169;&#25311;&#39640;&#38454;&#26377;&#38480;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26377;&#30028;&#30340;&#22810;&#38754;&#20307;&#22495;$\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$&#20013;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;&#24179;&#28369;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#36895;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Sobolev&#31354;&#38388;&#20013;&#30340;&#25351;&#25968;&#27169;&#25311;&#36895;&#29575;&#65292;&#36825;&#19982;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#20197;&#21450;Gevrey&#27491;&#21017;&#35299;&#31867;&#20869;&#30340;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#26377;&#20851;&#65292;&#36825;&#20123;&#35299;&#31867;&#26159;&#26681;&#25454;$\mathrm{D}$&#20013;&#21152;&#26435;Sobolev&#26631;&#24230;&#23450;&#20041;&#30340;&#65292;&#21253;&#25324;I.M. Babu\v{s}ka&#21644;B.Q. Guo&#30340;&#21487;&#25968;&#33539;&#25968;&#31354;&#38388;&#12290;&#20316;&#20026;&#20013;&#38388;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20219;&#24847;&#27491;&#21017;&#30340;&#31616;&#21333;&#24418;&#22495;$\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$&#19978;&#30340;&#36830;&#32493;&#12289;&#20998;&#27573;&#22810;&#39033;&#24335;&#39640;&#38454;(``$p$-version'')&#26377;&#38480;&#20803;&#65292;&#20854;&#20803;&#32032;&#22810;&#39033;&#24335;&#24230;&#20026;$p\in\mathbb{N}$&#65292;&#21487;&#20197;&#34987;ReLU&#21644;ReLU$^2$&#28608;&#27963;&#32452;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#27169;&#25311;&#12290;&#22312;&#24418;&#29366;&#27491;&#21017;&#30340;&#22810;&#38754;&#20307;&#22495;$\mathrm{D}$&#30340;&#31616;&#21333;&#24418;&#22495;&#21010;&#20998;&#19978;&#65292;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#20197;&#21450;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02035v1 Announce Type: cross  Abstract: We analyze deep Neural Network emulation rates of smooth functions with point singularities in bounded, polytopal domains $\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the number of neurons and in terms of the number of nonzero coefficients for Gevrey-regular solution classes defined in terms of weighted Sobolev scales in $\mathrm{D}$, comprising the countably-normed spaces of I.M. Babu\v{s}ka and B.Q. Guo.   As intermediate result, we prove that continuous, piecewise polynomial high order (``$p$-version'') finite elements with elementwise polynomial degree $p\in\mathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral domains $\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$ can be exactly emulated by neural networks combining ReLU and ReLU$^2$ activations. On shape-regular, simplicial partitions of polytopal domains $\mathrm{D}$, both the number of neurons and t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#31639;&#27861;&#65292;&#22312;&#23454;&#29616;&#19978;&#26126;&#26174;&#27604;&#24050;&#26377;&#31639;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02019</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Mealy Machines with Timers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#31639;&#27861;&#65292;&#22312;&#23454;&#29616;&#19978;&#26126;&#26174;&#27604;&#24050;&#26377;&#31639;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#19968;&#33324;&#31867;&#21035;&#30340;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#65288;MMTs&#65289;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;Vaandrager&#31561;&#20154;&#30340;L&#65283;&#31639;&#27861;&#23545;&#23450;&#26102;&#35774;&#32622;&#30340;&#25193;&#23637;&#12290;&#31867;&#20284;&#20110;Waga&#25552;&#20986;&#30340;&#29992;&#20110;&#23398;&#20064;&#23450;&#26102;&#33258;&#21160;&#26426;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;Maler&#65286;Pnueli&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;Waga&#30340;&#31639;&#27861;&#37117;&#20351;&#29992;&#31526;&#21495;&#26597;&#35810;&#36827;&#34892;&#22522;&#30784;&#35821;&#35328;&#23398;&#20064;&#65292;&#28982;&#21518;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#20855;&#20307;&#26597;&#35810;&#36827;&#34892;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;Waga&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#20855;&#20307;&#26597;&#35810;&#26469;&#23454;&#29616;&#21333;&#20010;&#31526;&#21495;&#26597;&#35810;&#65292;&#32780;&#25105;&#20204;&#21482;&#38656;&#35201;&#22810;&#39033;&#24335;&#25968;&#37327;&#12290;&#36825;&#26159;&#22240;&#20026;&#35201;&#23398;&#20064;&#23450;&#26102;&#33258;&#21160;&#26426;&#65292;&#23398;&#20064;&#32773;&#38656;&#35201;&#30830;&#23450;&#27599;&#20010;&#36716;&#25442;&#30340;&#30830;&#20999;&#21355;&#20853;&#21644;&#37325;&#32622;&#65288;&#26377;&#25351;&#25968;&#22810;&#31181;&#21487;&#33021;&#24615;&#65289;&#65292;&#32780;&#35201;&#23398;&#20064;MMT&#65292;&#23398;&#20064;&#32773;&#21482;&#38656;&#35201;&#24324;&#28165;&#26970;&#21738;&#20123;&#20808;&#21069;&#30340;&#36716;&#25442;&#23548;&#33268;&#36229;&#26102;&#12290;&#27491;&#22914;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02019v1 Announce Type: cross  Abstract: We present the first algorithm for query learning of a general class of Mealy machines with timers (MMTs) in a black-box context. Our algorithm is an extension of the L# algorithm of Vaandrager et al. to a timed setting. Like the algorithm for learning timed automata proposed by Waga, our algorithm is inspired by ideas of Maler &amp; Pnueli. Based on the elementary languages of, both Waga's and our algorithm use symbolic queries, which are then implemented using finitely many concrete queries. However, whereas Waga needs exponentially many concrete queries to implement a single symbolic query, we only need a polynomial number. This is because in order to learn a timed automaton, a learner needs to determine the exact guard and reset for each transition (out of exponentially many possibilities), whereas for learning an MMT a learner only needs to figure out which of the preceding transitions caused a timeout. As shown in our previous work, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;HSIC&#24809;&#32602;&#39033;&#65292;&#30830;&#20445;&#20102;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02011</link><description>&lt;p&gt;
&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;HSIC&#24809;&#32602;&#39033;&#65292;&#30830;&#20445;&#20102;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#23884;&#20837;&#26469;&#34920;&#31034;&#20108;&#20998;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#30740;&#31350;&#29983;&#24577;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#27604;&#22914;&#36830;&#25509;&#26893;&#29289;&#21644;&#20256;&#31881;&#32773;&#31561;&#32593;&#32476;&#65292;&#38656;&#32771;&#34385;&#35768;&#22810;&#21327;&#21464;&#37327;&#65292;&#23588;&#20854;&#35201;&#25511;&#21046;&#25277;&#26679;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#35843;&#25972;&#20026;&#20108;&#20998;&#24773;&#20917;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#23884;&#20837;&#65292;&#20854;&#20013;&#20004;&#32452;&#33410;&#28857;&#30340;&#20301;&#32622;&#22522;&#20110;&#23427;&#20204;&#30340;&#36830;&#25509;&#27010;&#29575;&#12290;&#25105;&#20204;&#23558;&#22312;&#31038;&#20250;&#23398;&#20013;&#24120;&#32771;&#34385;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#36716;&#21270;&#20026;&#29983;&#24577;&#23398;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65288;HSIC&#65289;&#20316;&#20026;&#39069;&#22806;&#24809;&#32602;&#39033;&#65292;&#25105;&#20204;&#30830;&#20445;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#65288;&#19982;&#25277;&#26679;&#36807;&#31243;&#30456;&#20851;&#65289;&#26080;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25913;&#21464;&#25105;&#20204;&#23545;&#29983;&#24577;&#32593;&#32476;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02011v1 Announce Type: cross  Abstract: We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological n
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20110;&#19968;&#33324;&#21270;&#30340;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.02004</link><description>&lt;p&gt;
&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#20197;&#21450;log-Sobolev&#21644;Talagrand&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02004
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20110;&#19968;&#33324;&#21270;&#30340;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;(PGD)~(Kuntz&#31561;&#20154;&#65292;2023)&#30340;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#21270;&#33258;&#30001;&#33021;&#26799;&#24230;&#27969;&#33719;&#24471;&#30340;&#22823;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23545;&#20110;&#28385;&#36275;&#19968;&#33324;&#21270;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#65288;LSI&#21644;PLI&#65289;&#30340;&#27169;&#22411;&#65292;&#27969;&#20197;&#25351;&#25968;&#36895;&#24230;&#25910;&#25947;&#21040;&#33258;&#30001;&#33021;&#30340;&#26497;&#23567;&#21270;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26368;&#20248;&#36755;&#36816;&#25991;&#29486;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26524;&#65288;LSI&#24847;&#21619;&#30528;Talagrand&#19981;&#31561;&#24335;&#65289;&#21450;&#20854;&#22312;&#20248;&#21270;&#25991;&#29486;&#20013;&#30340;&#23545;&#24212;&#29289;&#65288;PLI&#24847;&#21619;&#30528;&#25152;&#35859;&#30340;&#20108;&#27425;&#22686;&#38271;&#26465;&#20214;&#65289;&#25193;&#23637;&#24182;&#24212;&#29992;&#21040;&#25105;&#20204;&#30340;&#26032;&#35774;&#32622;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#24378;&#20985;&#23545;&#25968;&#20284;&#28982;&#30340;&#27169;&#22411;&#65292;LSI/PLI&#30340;&#27010;&#25324;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\'Emery Theorem and show that the LSI/P{\L}I generalization holds for models with strongly concave log-likelihoods. For such m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#30452;&#25509;&#20272;&#35745;&#20998;&#25968;&#30697;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.01948</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#20013;&#30340;&#20998;&#25968;&#30697;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On Fractional Moment Estimation from Polynomial Chaos Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#30452;&#25509;&#20272;&#35745;&#20998;&#25968;&#30697;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#32479;&#35745;&#30697;&#34987;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#27010;&#29575;&#20998;&#24067;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#32479;&#35745;&#25277;&#26679;&#20272;&#35745;&#26114;&#36149;&#25968;&#23398;&#27169;&#22411;&#30340;&#20998;&#25968;&#32479;&#35745;&#30697;&#26159;&#20855;&#26377;&#25361;&#25112;&#30340;&#65292;&#22240;&#20026;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#36890;&#24120;&#26080;&#27861;&#21019;&#24314;&#22823;&#22411;&#23454;&#39564;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#30452;&#25509;&#20272;&#35745;&#20998;&#25968;&#30697;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#30830;&#23450;&#24615;PCE&#31995;&#25968;&#33719;&#24471;&#30340;&#21069;&#22235;&#20010;&#32479;&#35745;&#30697;&#34987;&#29992;&#20110;&#36890;&#36807;H\"{o}lder&#19981;&#31561;&#24335;&#20272;&#35745;&#20219;&#24847;&#20998;&#25968;&#30697;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#19977;&#20010;&#19981;&#26029;&#22797;&#26434;&#30340;&#25968;&#20540;&#31034;&#20363;&#20013;&#30340;&#32479;&#35745;&#30697;&#21644;&#27010;&#29575;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01948v1 Announce Type: cross  Abstract: Fractional statistical moments are utilized for various tasks of uncertainty quantification, including the estimation of probability distributions. However, an estimation of fractional statistical moments of costly mathematical models by statistical sampling is challenging since it is typically not possible to create a large experimental design due to limitations in computing capacity. This paper presents a novel approach for the analytical estimation of fractional moments, directly from polynomial chaos expansions. Specifically, the first four statistical moments obtained from the deterministic PCE coefficients are used for an estimation of arbitrary fractional moments via H\"{o}lder's inequality. The proposed approach is utilized for an estimation of statistical moments and probability distributions in three numerical examples of increasing complexity. Obtained results show that the proposed approach achieves a superior performance i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#21464;&#25442;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01946</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#31216;&#21464;&#25442;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Generative Model of Symmetry Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#21464;&#25442;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#25429;&#25417;&#25968;&#25454;&#30340;&#23545;&#31216;&#21464;&#25442;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#25928;&#27169;&#22411;&#65292;&#23613;&#31649;&#28041;&#21450;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#26368;&#36817;&#22312;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36825;&#20123;&#23545;&#31216;&#24615;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#21028;&#21035;&#35774;&#32622;&#19978;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#26126;&#30830;&#26088;&#22312;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#20197;&#21487;&#35299;&#37322;&#26041;&#24335;&#23398;&#20064;&#25968;&#25454;&#20013;&#23384;&#22312;&#21738;&#20123;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#23398;&#20064;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20223;&#23556;&#21644;&#39068;&#33394;&#21464;&#25442;&#19979;&#25429;&#25417;&#23545;&#31216;&#24615;&#30340;&#33021;&#21147;&#12290;&#23558;&#25105;&#20204;&#30340;&#23545;&#31216;&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#36793;&#38469;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#21644;&#23545;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01946v1 Announce Type: new  Abstract: Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we construct a generative model that explicitly aims to capture symmetries in the data, resulting in a model that learns which symmetries are present in an interpretable way. We provide a simple algorithm for efficiently learning our generative model and demonstrate its ability to capture symmetries under affine and color transformations. Combining our symmetry model with existing generative models results in higher marginal test-log-likelihoods and robustness to data sparsification.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36741;&#21161;&#20613;&#31435;&#21494;&#22522;&#22686;&#24378;&#65288;AFA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#22686;&#24378;&#65292;&#22635;&#34917;&#20102;&#35270;&#35273;&#22686;&#24378;&#36951;&#30041;&#30340;&#22686;&#24378;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01944</link><description>&lt;p&gt;
&#29992;&#20613;&#31435;&#21494;&#22522;&#20989;&#25968;&#24357;&#21512;&#22686;&#24378;&#24046;&#36317;&#65306;&#37325;&#26032;&#24605;&#32771;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#39057;&#29575;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01944
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36741;&#21161;&#20613;&#31435;&#21494;&#22522;&#22686;&#24378;&#65288;AFA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#22686;&#24378;&#65292;&#22635;&#34917;&#20102;&#35270;&#35273;&#22686;&#24378;&#36951;&#30041;&#30340;&#22686;&#24378;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#22312;&#37096;&#32626;&#21040;&#29616;&#23454;&#22330;&#26223;&#20013;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#32771;&#34385;&#21040;&#30340;&#36755;&#20837;&#20986;&#29616;&#20102;&#24847;&#22806;&#21464;&#21270;&#12290;&#25968;&#25454;&#22686;&#24378;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#22686;&#21152;&#25968;&#25454;&#21464;&#21270;&#24615;&#24182;&#20943;&#23569;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#35270;&#35273;&#22686;&#24378;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24191;&#27867;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36741;&#21161;&#20613;&#31435;&#21494;&#22522;&#22686;&#24378;&#65288;AFA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39057;&#29575;&#22495;&#22686;&#24378;&#30340;&#34917;&#20805;&#25216;&#26415;&#65292;&#22635;&#34917;&#20102;&#35270;&#35273;&#22686;&#24378;&#36951;&#30041;&#30340;&#22686;&#24378;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#39640;&#25928;&#30340;&#23545;&#25239;&#35774;&#32622;&#23637;&#31034;&#20102;&#20613;&#31435;&#21494;&#22522;&#21152;&#24615;&#22122;&#22768;&#22686;&#24378;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AFA&#26377;&#21161;&#20110;&#27169;&#22411;&#23545;&#24120;&#35265;&#25439;&#22351;&#12289;OOD&#27867;&#21270;&#20197;&#21450;&#27169;&#22411;&#24615;&#33021;&#38543;&#30528;&#24615;&#33021;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01944v1 Announce Type: cross  Abstract: Computer vision models normally witness degraded performance when deployed in real-world scenarios, due to unexpected changes in inputs that were not accounted for during training. Data augmentation is commonly used to address this issue, as it aims to increase data variety and reduce the distribution gap between training and test data. However, common visual augmentations might not guarantee extensive robustness of computer vision models. In this paper, we propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique targeting augmentation in the frequency domain and filling the augmentation gap left by visual augmentations. We demonstrate the utility of augmentation via Fourier-basis additive noise in a straightforward and efficient adversarial setting. Our results show that AFA benefits the robustness of models against common corruptions, OOD generalization, and consistency of performance of models against increasing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01942</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;&#20943;&#36731;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#38899;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Noise on Graph via Topological Sample Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31934;&#24515;&#27880;&#37322;&#30340;&#22522;&#20934;&#27979;&#35797;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24403;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#26102;&#65292;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26377;&#25928;&#24615;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#21463;&#21040;&#30456;&#24403;&#22823;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#22312;&#26679;&#26412;&#36873;&#25321;&#26041;&#38754;&#30340;&#25506;&#32034;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24212;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#20391;&#37325;&#20110;i.i.d&#25968;&#25454;&#65292;&#24403;&#36716;&#21521;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#21644;GNNs&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#25361;&#25112;&#65306;(1) &#20301;&#20110;&#25299;&#25169;&#31867;&#36793;&#30028;&#38468;&#36817;&#30340;&#33410;&#28857;&#23545;&#20998;&#31867;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#26080;&#27861;&#36890;&#36807;&#21551;&#21457;&#24335;&#26679;&#26412;&#36873;&#25321;&#25104;&#21151;&#21306;&#20998;&#12290;(2) &#27809;&#26377;&#21487;&#29992;&#30340;&#34913;&#37327;&#26631;&#20934;&#32771;&#34385;&#22270;&#30340;&#25299;&#25169;&#20449;&#24687;&#20197;&#20419;&#36827;&#22270;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#65288;TSS&#65289;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#25552;&#21319;&#22270;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01942v1 Announce Type: new  Abstract: Despite the success of the carefully-annotated benchmarks, the effectiveness of existing graph neural networks (GNNs) can be considerably impaired in practice when the real-world graph data is noisily labeled. Previous explorations in sample selection have been demonstrated as an effective way for robust learning with noisy labels, however, the conventional studies focus on i.i.d data, and when moving to non-iid graph data and GNNs, two notable challenges remain: (1) nodes located near topological class boundaries are very informative for classification but cannot be successfully distinguished by the heuristic sample selection. (2) there is no available measure that considers the graph topological information to promote sample selection in a graph. To address this dilemma, we propose a $\textit{Topological Sample Selection}$ (TSS) method that boosts the informative sample selection process in a graph by utilising topological information.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#32447;&#24615;&#37327;&#21270;&#24212;&#29992;&#20110;FPGA-based soft sensors&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31934;&#24230;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#21644;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#65292;&#20026;&#23454;&#26102;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;&#25552;&#20379;&#20102;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#26367;&#20195;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.01922</link><description>&lt;p&gt;
FlowPrecision: &#21033;&#29992;&#32447;&#24615;&#37327;&#21270;&#25512;&#36827;&#22522;&#20110;FPGA&#30340;&#23454;&#26102;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#32447;&#24615;&#37327;&#21270;&#24212;&#29992;&#20110;FPGA-based soft sensors&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31934;&#24230;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#21644;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#65292;&#20026;&#23454;&#26102;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;&#25552;&#20379;&#20102;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21644;&#29615;&#22659;&#30417;&#27979;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21644;&#31934;&#30830;&#30340;&#27969;&#20307;&#27969;&#37327;&#27979;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23558;&#32447;&#24615;&#37327;&#21270;&#24212;&#29992;&#20110;&#22522;&#20110;FPGA&#30340;&#36719;&#20256;&#24863;&#22120;&#20013;&#65292;&#29992;&#20110;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;&#65292;&#36890;&#36807;&#20811;&#26381;&#20256;&#32479;&#23450;&#28857;&#37327;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38024;&#23545;&#24615;&#30340;&#30828;&#20214;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22343;&#26041;&#35823;&#24046;&#30340;&#26368;&#22810;&#20943;&#23569;10.10&#65285;&#65292;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#20102;9.39&#65285;&#12290;&#32463;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#21518;&#30340;&#22522;&#20110;FPGA&#30340;&#37327;&#21270;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#23454;&#26102;&#25512;&#29702;&#65292;&#20026;&#26222;&#36941;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;&#20113;&#22788;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01922v1 Announce Type: new  Abstract: In industrial and environmental monitoring, achieving real-time and precise fluid flow measurement remains a critical challenge. This study applies linear quantization in FPGA-based soft sensors for fluid flow estimation, significantly enhancing Neural Network model precision by overcoming the limitations of traditional fixed-point quantization. Our approach achieves up to a 10.10% reduction in Mean Squared Error and a notable 9.39% improvement in inference speed through targeted hardware optimizations. Validated across multiple data sets, our findings demonstrate that the optimized FPGA-based quantized models can provide efficient, accurate real-time inference, offering a viable alternative to cloud-based processing in pervasive autonomous systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01919</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20248;&#21270;&#21644;&#21015;&#23376;&#38598;&#36873;&#25321;&#30340;&#30697;&#38453;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion with Convex Optimization and Column Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27599;&#19968;&#27493;&#20013;&#35299;&#20915;&#19968;&#20010;&#20984;&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#25105;&#20204;&#30340;&#21015;&#36873;&#25321;&#30697;&#38453;&#23436;&#25104;&#65288;CSMC&#65289;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#27599;&#31181;&#31639;&#27861;&#38024;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#65292;&#22312;&#20998;&#26512;&#20013;&#25105;&#20204;&#38416;&#26126;&#20102;&#24517;&#35201;&#30340;&#20551;&#35774;&#21644;&#25214;&#21040;&#27491;&#30830;&#35299;&#30340;&#27010;&#29575;&#12290;&#22312;&#35770;&#25991;&#30340;&#31532;&#20108;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#24037;&#20316;&#30340;&#32467;&#26524;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;&#30697;&#38453;&#22823;&#23567;&#12289;&#31209;&#21644;&#32570;&#22833;&#20803;&#32032;&#27604;&#20363;&#23545;&#35299;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01919v1 Announce Type: new  Abstract: We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-li
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35777;&#25454;&#24037;&#20855;&#24635;&#32447;&#30340;&#33258;&#36866;&#24212;&#39550;&#39542;&#31995;&#32479;&#36830;&#32493;&#20445;&#38556;&#26696;&#20363;&#21019;&#24314;&#65292;&#23454;&#29616;&#20102;&#20174;&#39044;&#23450;&#20041;&#27169;&#24335;&#26500;&#24314;&#21644;&#25345;&#32493;&#32500;&#25252;&#20445;&#38556;&#26696;&#20363;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31995;&#32479;&#38656;&#27714;&#21464;&#21270;&#21644;&#32452;&#20214;&#24694;&#21270;&#24102;&#26469;&#30340;&#20445;&#38556;&#26696;&#20363;&#32500;&#25252;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01918</link><description>&lt;p&gt;
&#22522;&#20110;&#35777;&#25454;&#24037;&#20855;&#24635;&#32447;&#30340;&#33258;&#36866;&#24212;&#39550;&#39542;&#31995;&#32479;&#36830;&#32493;&#20445;&#38556;&#26696;&#20363;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
Towards Continuous Assurance Case Creation for ADS with the Evidential Tool Bus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01918
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35777;&#25454;&#24037;&#20855;&#24635;&#32447;&#30340;&#33258;&#36866;&#24212;&#39550;&#39542;&#31995;&#32479;&#36830;&#32493;&#20445;&#38556;&#26696;&#20363;&#21019;&#24314;&#65292;&#23454;&#29616;&#20102;&#20174;&#39044;&#23450;&#20041;&#27169;&#24335;&#26500;&#24314;&#21644;&#25345;&#32493;&#32500;&#25252;&#20445;&#38556;&#26696;&#20363;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31995;&#32479;&#38656;&#27714;&#21464;&#21270;&#21644;&#32452;&#20214;&#24694;&#21270;&#24102;&#26469;&#30340;&#20445;&#38556;&#26696;&#20363;&#32500;&#25252;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#38556;&#26696;&#20363;&#24050;&#25104;&#20026;&#35748;&#35777;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25163;&#21160;&#23450;&#20041;&#20445;&#38556;&#26696;&#20363;&#27169;&#24335;&#21487;&#33021;&#26080;&#27861;&#36991;&#20813;&#65292;&#20445;&#38556;&#26696;&#20363;&#27169;&#24335;&#30340;&#31995;&#32479;&#29305;&#23450;&#23454;&#20363;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#24403;&#31995;&#32479;&#38656;&#20445;&#38556;&#30340;&#31995;&#32479;&#38656;&#27714;&#21457;&#29983;&#21464;&#21270;&#65292;&#25110;&#32773;&#22240;&#31995;&#32479;&#32452;&#20214;&#24694;&#21270;&#23548;&#33268;&#20445;&#38556;&#22768;&#26126;&#26080;&#25928;&#26102;&#65292;&#21363;&#20351;&#26159;&#25191;&#34892;&#23398;&#20064;&#21551;&#29992;&#32452;&#20214;&#26102;&#65292;&#32500;&#25252;&#31995;&#32479;&#30340;&#20445;&#38556;&#26696;&#20363;&#21464;&#24471;&#23588;&#20026;&#22797;&#26434;&#12290; &#26412;&#25991;&#25253;&#21578;&#20102;&#25105;&#20204;&#21033;&#29992;&#24037;&#20855;&#38598;&#25104;&#26694;&#26550;Evidential Tool Bus (ETB)&#20174;&#39044;&#23450;&#20041;&#30340;&#20445;&#38556;&#26696;&#20363;&#27169;&#24335;&#26500;&#24314;&#21644;&#25345;&#32493;&#32500;&#25252;&#20445;&#38556;&#26696;&#20363;&#30340;&#21021;&#27493;&#32463;&#39564;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;&#27773;&#36710;&#39046;&#22495;&#30340;&#24037;&#19994;&#33258;&#21160;&#27850;&#36710;&#31995;&#32479;&#19978;&#28436;&#31034;&#20102;&#20445;&#38556;&#36807;&#31243;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25152;&#25552;&#20379;&#30340;&#20445;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01918v1 Announce Type: cross  Abstract: An assurance case has become an integral component for the certification of safety-critical systems. While manually defining assurance case patterns can be not avoided, system-specific instantiations of assurance case patterns are both costly and time-consuming. It becomes especially complex to maintain an assurance case for a system when the requirements of the System-Under-Assurance change, or an assurance claim becomes invalid due to, e.g., degradation of a systems component, as common when deploying learning-enabled components. In this paper, we report on our preliminary experience leveraging the tool integration framework Evidential Tool Bus (ETB) for the construction and continuous maintenance of an assurance case from a predefined assurance case pattern. Specifically, we demonstrate the assurance process on an industrial Automated Valet Parking system from the automotive domain. We present the formalization of the provided assur
&lt;/p&gt;</description></item><item><title>Hopfield&#25552;&#20986;&#20102;&#19968;&#31181;Hebbian&#23398;&#20064;&#35268;&#21017;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20851;&#32852;&#35760;&#24518;&#30340;&#23481;&#37327;&#65292;&#25351;&#20986;&#32593;&#32476;&#30340;&#23481;&#37327;&#19982;&#27169;&#24335;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#23481;&#37327;&#39044;&#27979;&#20540;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33879;&#21517;&#27169;&#24335;&#30340;&#21560;&#24341;&#30406;&#22320;&#26469;&#25506;&#35752;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01907</link><description>&lt;p&gt;
Hebbian-Hopfield&#32593;&#32476;&#20851;&#32852;&#35760;&#24518;&#30340;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Capacity of the Hebbian-Hopfield network associative memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01907
&lt;/p&gt;
&lt;p&gt;
Hopfield&#25552;&#20986;&#20102;&#19968;&#31181;Hebbian&#23398;&#20064;&#35268;&#21017;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20851;&#32852;&#35760;&#24518;&#30340;&#23481;&#37327;&#65292;&#25351;&#20986;&#32593;&#32476;&#30340;&#23481;&#37327;&#19982;&#27169;&#24335;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#23481;&#37327;&#39044;&#27979;&#20540;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33879;&#21517;&#27169;&#24335;&#30340;&#21560;&#24341;&#30406;&#22320;&#26469;&#25506;&#35752;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Hopfield&#30340;&#35770;&#25991;&#20013;&#65292;&#20182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hebbian&#23398;&#20064;&#35268;&#21017;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#39640;&#25928;&#22320;&#20316;&#20026;&#19968;&#20010;&#20851;&#32852;&#35760;&#24518;&#12290;&#22312;&#30740;&#31350;&#38543;&#26426;&#20108;&#36827;&#21046;&#27169;&#24335;&#26102;&#65292;&#20182;&#36824;&#21457;&#29616;&#65292;&#22914;&#26524;&#23384;&#20648;&#27169;&#24335;&#26816;&#32034;&#20013;&#23481;&#24525;&#19968;&#23567;&#37096;&#20998;&#38169;&#35823;&#65292;&#32593;&#32476;&#30340;&#23481;&#37327;&#65288;&#26368;&#22823;&#35760;&#24518;&#27169;&#24335;&#25968;&#65292;$m$&#65289;&#19982;&#27599;&#20010;&#27169;&#24335;&#30340;&#22823;&#23567;$n$&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20182;&#33879;&#21517;&#22320;&#39044;&#27979;&#20102;$\alpha_c=\lim_{n\rightarrow\infty}\frac{m}{n}\approx 0.14$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38750;&#24120;&#30456;&#21516;&#30340;&#24773;&#26223;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#33879;&#21517;&#27169;&#24335;&#30340;&#21560;&#24341;&#30406;&#22320;&#65306;\textbf{\emph{(i)}}&#26469;&#33258;\cite{AmiGutSom85}&#30340;AGS&#65307;&#20197;&#21450;\textbf{\emph{(ii)}}&#26469;&#33258;\cite{Newman88,Louk94,Louk94a,Louk97,Tal98}&#30340;NLT&#12290;&#20381;&#36182;&#20110;&#26469;&#33258;\cite{Stojnicflrdt23}&#30340;\emph{&#23436;&#20840;&#25552;&#21319;&#30340;&#38543;&#26426;&#23545;&#20598;&#29702;&#35770;}&#65288;fl RDT&#65289;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#22312;&#31532;&#19968;&#23618;&#25552;&#21319;&#19978;&#30340;&#20197;&#19979;&#26126;&#30830;&#23481;&#37327;&#29305;&#24615;&#25551;&#36848;&#65306;\begin{equation} \alpha ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01907v1 Announce Type: cross  Abstract: In \cite{Hop82}, Hopfield introduced a \emph{Hebbian} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, $m$) scales linearly with each pattern's size, $n$. Moreover, he famously predicted $\alpha_c=\lim_{n\rightarrow\infty}\frac{m}{n}\approx 0.14$. We study this very same scenario with two famous pattern's basins of attraction: \textbf{\emph{(i)}} The AGS one from \cite{AmiGutSom85}; and \textbf{\emph{(ii)}} The NLT one from \cite{Newman88,Louk94,Louk94a,Louk97,Tal98}. Relying on the \emph{fully lifted random duality theory} (fl RDT) from \cite{Stojnicflrdt23}, we obtain the following explicit capacity characterizations on the first level of lifting:   \begin{equation}   \alpha
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#23618;&#31995;&#32479;&#30340;&#36924;&#36817;&#33021;&#21147;&#21644;&#32479;&#19968;&#24378;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#20018;&#32852;RNN&#20648;&#23618;&#26500;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#31995;&#32479;</title><link>https://arxiv.org/abs/2403.01900</link><description>&lt;p&gt;
&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#23618;&#31995;&#32479;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Universality of reservoir systems with recurrent neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01900
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#23618;&#31995;&#32479;&#30340;&#36924;&#36817;&#33021;&#21147;&#21644;&#32479;&#19968;&#24378;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#20018;&#32852;RNN&#20648;&#23618;&#26500;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#20648;&#23618;&#31995;&#32479;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#20854;&#20013;&#20648;&#23618;&#26159;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#38382;&#39064;&#35774;&#23450;&#20013;&#65292;&#20648;&#23618;&#31995;&#32479;&#36890;&#36807;&#35843;&#25972;&#20854;&#32447;&#24615;&#36755;&#20986;&#26469;&#36924;&#36817;&#19968;&#32452;&#20989;&#25968;&#65292;&#32780;&#20648;&#23618;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;&#25105;&#20204;&#25152;&#31216;&#30340;&#19968;&#31867;&#20989;&#25968;&#30340;RNN&#20648;&#23618;&#31995;&#32479;&#30340;&#32479;&#19968;&#24378;&#26222;&#36866;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#23545;&#20110;&#20219;&#24847;&#27491;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;RNN&#20648;&#23618;&#31995;&#32479;&#65292;&#20854;&#23545;&#35813;&#31867;&#20989;&#25968;&#20013;&#27599;&#20010;&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#37117;&#34987;&#27491;&#25968;&#20174;&#19978;&#26041;&#38480;&#23450;&#12290;&#36825;&#26679;&#30340;RNN&#20648;&#23618;&#31995;&#32479;&#26159;&#36890;&#36807;&#24182;&#34892;&#20018;&#32852;RNN&#20648;&#23618;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01900v1 Announce Type: cross  Abstract: Approximation capability of reservoir systems whose reservoir is a recurrent neural network (RNN) is discussed. In our problem setting, a reservoir system approximates a set of functions just by adjusting its linear readout while the reservoir is fixed. We will show what we call uniform strong universality of a family of RNN reservoir systems for a certain class of functions to be approximated. This means that, for any positive number, we can construct a sufficiently large RNN reservoir system whose approximation error for each function in the class of functions to be approximated is bounded from above by the positive number. Such RNN reservoir systems are constructed via parallel concatenation of RNN reservoirs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#27010;&#29575;&#19978;&#38480;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#21462;&#20915;&#20110;&#25200;&#21160;&#33539;&#25968;&#12289;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#25509;&#36817;&#30340;&#19981;&#21516;&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01896</link><description>&lt;p&gt;
&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#30340;&#24378;&#40065;&#26834;&#24615;&#30028;&#38480;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Robustness Bounds on the Successful Adversarial Examples: Theory and Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#27010;&#29575;&#19978;&#38480;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#21462;&#20915;&#20110;&#25200;&#21160;&#33539;&#25968;&#12289;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#25509;&#36817;&#30340;&#19981;&#21516;&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#65288;AE&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#28155;&#21152;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#26469;&#35825;&#20351;&#38169;&#20998;&#12290;&#26412;&#25991;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#31867;&#65292;&#30740;&#31350;&#20102;&#25104;&#21151;AE&#30340;&#27010;&#29575;&#19978;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#65292;&#21462;&#20915;&#20110;AE&#30340;&#25200;&#21160;&#33539;&#25968;&#12289;GP&#20013;&#20351;&#29992;&#30340;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26368;&#25509;&#36817;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#19978;&#38480;&#19981;&#21463;&#26679;&#26412;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;ImageNet&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#21464;&#26680;&#20989;&#25968;&#21442;&#25968;&#20250;&#23548;&#33268;&#25104;&#21151;AE&#27010;&#29575;&#19978;&#38480;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01896v1 Announce Type: new  Abstract: Adversarial example (AE) is an attack method for machine learning, which is crafted by adding imperceptible perturbation to the data inducing misclassification. In the current paper, we investigated the upper bound of the probability of successful AEs based on the Gaussian Process (GP) classification. We proved a new upper bound that depends on AE's perturbation norm, the kernel function used in GP, and the distance of the closest pair with different labels in the training dataset. Surprisingly, the upper bound is determined regardless of the distribution of the sample dataset. We showed that our theoretical result was confirmed through the experiment using ImageNet. In addition, we showed that changing the parameters of the kernel function induces a change of the upper bound of the probability of successful AEs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;FCM-wDTW&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#32534;&#30721;&#25104;&#28508;&#22312;&#31354;&#38388;&#24182;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#25581;&#31034;&#27491;&#24120;&#32500;&#24230;&#20851;&#31995;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01895</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01895
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;FCM-wDTW&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#32534;&#30721;&#25104;&#28508;&#22312;&#31354;&#38388;&#24182;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#25581;&#31034;&#27491;&#24120;&#32500;&#24230;&#20851;&#31995;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20854;&#30456;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#29305;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#24191;&#27867;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#23545;&#22122;&#22768;&#25935;&#24863;&#12290;&#34429;&#28982;&#29616;&#26377;&#24037;&#20316;&#24050;&#32463;&#25506;&#35752;&#20102;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#20197;&#22686;&#24378;&#20854;&#31283;&#20581;&#24615;&#65292;&#20294;&#23427;&#20204;&#20165;&#25903;&#25345;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#19978;&#30340;&#30417;&#30563;&#20219;&#21153;&#65292;&#32570;&#20047;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FCM-wDTW&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#21407;&#22987;&#25968;&#25454;&#32534;&#30721;&#25104;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#25581;&#31034;&#27491;&#24120;&#32500;&#24230;&#20851;&#31995;&#12290;FCM-wDTW&#23558;&#23616;&#37096;&#21152;&#26435;DTW&#24341;&#20837;&#21040;&#27169;&#31946;C&#22343;&#20540;&#32858;&#31867;&#20013;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#26368;&#20339;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#36890;&#36807;&#25968;&#25454;&#37325;&#24314;&#23454;&#29616;&#24322;&#24120;&#35782;&#21035;&#12290;&#38024;&#23545;11&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01895v1 Announce Type: cross  Abstract: Distance-based time series anomaly detection methods are prevalent due to their relative non-parametric nature and interpretability. However, the commonly used Euclidean distance is sensitive to noise. While existing works have explored dynamic time warping (DTW) for its robustness, they only support supervised tasks over multivariate time series (MTS), leaving a scarcity of unsupervised methods. In this work, we propose FCM-wDTW, an unsupervised distance metric learning method for anomaly detection over MTS, which encodes raw data into latent space and reveals normal dimension relationships through cluster centers. FCM-wDTW introduces locally weighted DTW into fuzzy C-means clustering and learns the optimal latent space efficiently, enabling anomaly identification via data reconstruction. Experiments with 11 different types of benchmarks demonstrate our method's competitive accuracy and efficiency.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01888</link><description>&lt;p&gt;
&#38646;&#25104;&#26412;&#22522;&#20934;&#19978;&#24322;&#27493;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#30340;&#24555;&#36895;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01888
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#32467;&#26524;&#24448;&#24448;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#31934;&#24515;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#32791;&#26102;&#24615;&#20351;&#24471;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#25302;&#24930;&#20102;&#39640;&#25928;HPO&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20419;&#36827;&#38646;&#25104;&#26412;&#22522;&#20934;&#19979;&#39640;&#25928;&#30340;&#24182;&#34892;HPO&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#23384;&#20648;&#22312;&#25991;&#20214;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#35745;&#31639;&#31934;&#30830;&#30340;&#36820;&#22238;&#39034;&#24207;&#65292;&#28040;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#31561;&#24453;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;HPO&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.01875</link><description>&lt;p&gt;
ICLN&#65306;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#29992;&#20110;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ICLN: Input Convex Loss Network for Decision Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#36890;&#24120;&#34987;&#35748;&#20026;&#19982;&#20248;&#21270;&#37096;&#20998;&#26080;&#20851;&#12290;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#20197;&#20026;&#30456;&#24212;&#20219;&#21153;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#26469;&#25972;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#30340;DFL&#33539;&#24335;&#20013;&#23454;&#29616;&#12290;ICLN&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#24050;&#32463;&#34987;&#20445;&#35777;&#20026;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#38024;&#23545;&#22806;&#37096;&#20998;&#24067;&#27867;&#21270;&#35780;&#20272;&#36827;&#34892;&#30340;&#39318;&#27425;&#21162;&#21147;&#65292;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#27867;&#21270;&#30340;&#34920;&#29616;&#22909;&#22351;&#12290;</title><link>https://arxiv.org/abs/2403.01874</link><description>&lt;p&gt;
&#23545;&#22806;&#20998;&#24067;&#27867;&#21270;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#38024;&#23545;&#22806;&#37096;&#20998;&#24067;&#27867;&#21270;&#35780;&#20272;&#36827;&#34892;&#30340;&#39318;&#27425;&#21162;&#21147;&#65292;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#27867;&#21270;&#30340;&#34920;&#29616;&#22909;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34429;&#28982;&#36234;&#26469;&#36234;&#20808;&#36827;&#65292;&#20294;&#20005;&#37325;&#20381;&#36182;IID&#20551;&#35774;&#65292;&#32780;&#30001;&#20110;&#19981;&#21487;&#36991;&#20813;&#30340;&#20998;&#24067;&#36716;&#25442;&#65292;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#27492;&#20551;&#35774;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#39118;&#38505;&#25935;&#24863;&#24212;&#29992;&#30340;&#37096;&#32626;&#20013;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#24182;&#19988;&#19981;&#21487;&#20449;&#12290;&#36825;&#31181;&#37325;&#35201;&#38382;&#39064;&#22240;&#27492;&#34893;&#29983;&#20986;&#21508;&#31181;&#33268;&#21147;&#20110;&#24320;&#21457;&#33021;&#22815;&#36827;&#34892;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#30340;&#31639;&#27861;&#30340;&#20998;&#25903;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#20294;&#23545;OOD&#27867;&#21270;&#30340;&#35780;&#20272;&#21364;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#65292;&#36825;&#20063;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#22522;&#26412;&#30340;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#19981;&#20165;&#22312;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#33021;&#21147;&#26159;&#21542;&#24378;&#22823;&#65292;&#36824;&#35201;&#35780;&#20272;&#27169;&#22411;&#27867;&#21270;&#33391;&#22909;&#25110;&#19981;&#20339;&#20043;&#22788;&#12290;&#36825;&#28041;&#21450;&#25551;&#36848;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#30340;&#20998;&#24067;&#36716;&#25442;&#31867;&#22411;&#65292;&#24182;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#36755;&#20837;&#21306;&#22495;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#39318;&#27425;&#21162;&#21147;&#36827;&#34892;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01874v1 Announce Type: new  Abstract: Machine learning models, while progressively advanced, rely heavily on the IID assumption, which is often unfulfilled in practice due to inevitable distribution shifts. This renders them susceptible and untrustworthy for deployment in risk-sensitive applications. Such a significant problem has consequently spawned various branches of works dedicated to developing algorithms capable of Out-of-Distribution (OOD) generalization. Despite these efforts, much less attention has been paid to the evaluation of OOD generalization, which is also a complex and fundamental problem. Its goal is not only to assess whether a model's OOD generalization capability is strong or not, but also to evaluate where a model generalizes well or poorly. This entails characterizing the types of distribution shifts that a model can effectively address, and identifying the safe and risky input regions given a model. This paper serves as the first effort to conduct a 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.01865</link><description>&lt;p&gt;
&#36890;&#36807;&#38170;&#22810;&#20803;&#20998;&#26512;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalisation via anchor multivariate analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01865
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#65292;&#20197;&#25913;&#21892;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#65288;MVA&#65289;&#31639;&#27861;&#65292;&#22914;&#65288;&#27491;&#20132;&#21270;&#65289;PLS&#12289;RRR&#21644;MLR&#65292;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#31616;&#21333;&#30340;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#27668;&#20505;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;&#20026;&#25152;&#36873;&#31639;&#27861;&#25552;&#20379;&#20102;&#20272;&#35745;&#22120;&#65292;&#23637;&#31034;&#20102;&#20854;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32463;&#39564;&#39564;&#35777;&#31361;&#26174;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24378;&#35843;&#20854;&#19982;MVA&#26041;&#27861;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#22686;&#24378;&#21487;&#22797;&#21046;&#24615;&#30340;&#21516;&#26102;&#25269;&#24481;&#20998;&#24067;&#36716;&#31227;&#20013;&#30340;&#20316;&#29992;&#12290;&#25193;&#23637;&#30340;AR&#26694;&#26550;&#25512;&#36827;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#65292;&#35299;&#20915;&#20102;&#21487;&#38752;OOD&#27867;&#21270;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
&lt;/p&gt;</description></item><item><title>&#22312;&#20256;&#32479;&#27431;&#27663;&#31354;&#38388;&#20043;&#22806;&#30340;&#40654;&#26364;&#31354;&#38388;&#20013;&#65292;&#38024;&#23545;&#22810;&#37325;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#23545;&#27604;&#38598;&#20307;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01864</link><description>&lt;p&gt;
RCoCo&#65306;&#40654;&#26364;&#31354;&#38388;&#20013;&#22810;&#37325;&#32593;&#32476;&#20013;&#30340;&#23545;&#27604;&#38598;&#20307;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RCoCo: Contrastive Collective Link Prediction across Multiplex Network in Riemannian Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01864
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#27431;&#27663;&#31354;&#38388;&#20043;&#22806;&#30340;&#40654;&#26364;&#31354;&#38388;&#20013;&#65292;&#38024;&#23545;&#22810;&#37325;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#23545;&#27604;&#38598;&#20307;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#36890;&#24120;&#30740;&#31350;&#30340;&#26159;&#22312;&#21333;&#20010;&#31038;&#20132;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#33410;&#28857;&#20043;&#38388;&#26410;&#26469;&#30456;&#20114;&#36830;&#25509;&#30340;&#27010;&#29575;&#12290;&#29616;&#23454;&#22330;&#26223;&#24448;&#24448;&#21576;&#29616;&#20026;&#20855;&#26377;&#22312;&#22810;&#20010;&#31038;&#20132;&#32593;&#32476;&#20013;&#27963;&#36291;&#30340;&#20849;&#21516;&#65288;&#38170;&#23450;&#65289;&#29992;&#25143;&#30340;&#22810;&#37325;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#30740;&#31350;&#21333;&#20010;&#32593;&#32476;&#20013;&#30340;&#20869;&#37096;&#38142;&#25509;&#39044;&#27979;&#65292;&#35201;&#20040;&#30740;&#31350;&#32593;&#32476;&#20043;&#38388;&#30340;&#22806;&#37096;&#38142;&#25509;&#39044;&#27979;&#65288;&#21363;&#32593;&#32476;&#23545;&#40784;&#65289;&#65292;&#24182;&#35748;&#20026;&#36825;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#26159;&#29420;&#31435;&#30340;&#65292;&#36825;&#20173;&#28982;&#36828;&#31163;&#20107;&#23454;&#12290;&#22312;&#34920;&#31034;&#31354;&#38388;&#19978;&#65292;&#32477;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#24314;&#31435;&#22312;&#20256;&#32479;&#30340;&#27431;&#27663;&#31354;&#38388;&#20043;&#19978;&#65292;&#24573;&#35270;&#20102;&#31038;&#20132;&#32593;&#32476;&#30340;&#22266;&#26377;&#20960;&#20309;&#24615;&#12290;&#31532;&#19977;&#20010;&#38382;&#39064;&#26159;&#31232;&#32570;&#30340;&#38170;&#23450;&#29992;&#25143;&#12290;&#27880;&#37322;&#38170;&#23450;&#29992;&#25143;&#26159;&#36153;&#26102;&#36153;&#21147;&#19988;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#22823;&#37327;&#38170;&#23450;&#29992;&#25143;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#37492;&#20110;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#35758;&#30740;&#31350;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01864v1 Announce Type: cross  Abstract: Link prediction typically studies the probability of future interconnection among nodes with the observation in a single social network. More often than not, real scenario is presented as a multiplex network with common (anchor) users active in multiple social networks. In the literature, most existing works study either the intra-link prediction in a single network or inter-link prediction among networks (a.k.a. network alignment), and consider two learning tasks are independent from each other, which is still away from the fact. On the representation space, the vast majority of existing methods are built upon the traditional Euclidean space, unaware of the inherent geometry of social networks. The third issue is on the scarce anchor users. Annotating anchor users is laborious and expensive, and thus it is impractical to work with quantities of anchor users. Herein, in light of the issues above, we propose to study a challenging yet p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#65288;RLHF&#65289;&#33539;&#24335;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#33539;&#24335;&#65292;&#23545;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#25512;&#23548;&#20986;&#20102;&#27425;&#20248;&#24046;&#36317;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.01857</link><description>&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#19982;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#65306;&#20174;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#65288;RLHF&#65289;&#33539;&#24335;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#33539;&#24335;&#65292;&#23545;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#25512;&#23548;&#20986;&#20102;&#27425;&#20248;&#24046;&#36317;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#27604;&#36739;&#20174;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#33539;&#24335;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#33539;&#24335;&#65292;&#36808;&#21521;&#23545;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#20197;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#21644;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#30340;&#31867;&#20026;&#37325;&#28857;&#12290;&#20026;&#20102;&#27604;&#36739;&#36825;&#20004;&#31181;&#33539;&#24335;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#30001;RLHF&#21644;DPO&#24341;&#36215;&#30340;&#27425;&#20248;&#24046;&#36317;&#25512;&#23548;&#20986;&#26497;&#23567;-&#26368;&#22823;&#32479;&#35745;&#30028;&#38480;&#65292;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#30830;&#20999;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#39044;&#35328;&#12290;&#25105;&#20204;&#23601;&#30456;&#23545;&#27604;&#36739;&#20004;&#31181;&#33539;&#24335;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#65292;&#21516;&#26102;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#12289;&#31574;&#30053;&#21644;&#22870;&#21169;&#31867;&#32500;&#25968;&#20197;&#21450;&#27491;&#21017;&#21270;&#28201;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#36817;&#20284;&#20248;&#21270;&#35774;&#32622;&#65292;&#24182;&#20026;RLHF&#21644;DPO&#20998;&#21035;&#25512;&#23548;&#20986;&#25351;&#25968;&#34928;&#20943;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01857v1 Announce Type: new  Abstract: In this paper, we take a step towards a deeper understanding of learning from human preferences by systematically comparing the paradigm of reinforcement learning from human feedback (RLHF) with the recently proposed paradigm of direct preference optimization (DPO). We focus our attention on the class of loglinear policy parametrization and linear reward functions. In order to compare the two paradigms, we first derive minimax statistical bounds on the suboptimality gap induced by both RLHF and DPO, assuming access to an oracle that exactly solves the optimization problems. We provide a detailed discussion on the relative comparison between the two paradigms, simultaneously taking into account the sample size, policy and reward class dimensions, and the regularization temperature. Moreover, we extend our analysis to the approximate optimization setting and derive exponentially decaying convergence rates for both RLHF and DPO. Next, we an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#24378;&#38887;&#25991;&#26412;&#25552;&#31034;&#26469;&#25913;&#21892;&#23545;&#25239;&#25915;&#20987;&#38887;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Prompt Tuning&#65288;APT&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31232;&#30095;&#26041;&#26696;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01849</link><description>&lt;p&gt;
&#19968;&#20010;&#25552;&#31034;&#35789;&#23601;&#36275;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#24378;&#38887;&#25991;&#26412;&#25552;&#31034;&#26469;&#25913;&#21892;&#23545;&#25239;&#25915;&#20987;&#38887;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Prompt Tuning&#65288;APT&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31232;&#30095;&#26041;&#26696;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;CLIP&#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#23613;&#31649;&#20855;&#26377;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#38750;&#24120;&#33030;&#24369;&#12290;&#26412;&#25991;&#20174;&#25991;&#26412;&#25552;&#31034;&#30340;&#26032;&#39062;&#35270;&#35282;&#32780;&#38750;&#20256;&#32479;&#30740;&#31350;&#30340;&#27169;&#22411;&#26435;&#37325;&#65288;&#22312;&#26412;&#25991;&#20013;&#20923;&#32467;&#65289;&#30740;&#31350;&#20102;VLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#23545;&#20351;&#29992;&#30340;&#25991;&#26412;&#25552;&#31034;&#25935;&#24863;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;VLMs&#30340;&#24378;&#38887;&#25991;&#26412;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Prompt Tuning&#65288;APT&#65289;&#65292;&#22312;&#35745;&#31639;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#37117;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#22312;15&#20010;&#25968;&#25454;&#38598;&#21644;4&#31181;&#25968;&#25454;&#31232;&#30095;&#26041;&#26696;&#65288;&#20174;1-shot&#21040;&#23436;&#20840;&#35757;&#32451;&#25968;&#25454;&#35774;&#32622;&#65289;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;APT&#30456;&#23545;&#20110;&#25163;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;APT&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01849v1 Announce Type: cross  Abstract: Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent 
&lt;/p&gt;</description></item><item><title>NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01845</link><description>&lt;p&gt;
NASH&#65306;&#29992;&#20110;&#30828;&#20214;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01845
&lt;/p&gt;
&lt;p&gt;
NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22312;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NASH&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#12290;&#20351;&#29992;NASH&#65292;&#30828;&#20214;&#35774;&#35745;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#29256;&#26412;&#30340;NASH&#31574;&#30053;&#65292;&#25152;&#26377;&#36825;&#20123;&#31574;&#30053;&#26174;&#31034;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#20247;&#22810;&#27169;&#22411;&#25805;&#20316;&#20013;&#36873;&#25321;&#29305;&#23450;&#25805;&#20316;&#65292;&#24341;&#23548;&#35757;&#32451;&#36807;&#31243;&#26397;&#21521;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312; ResNet18 &#25110; ResNet34 &#19978;&#24212;&#29992;NASH&#65292;&#19982;&#38750;NASH&#29256;&#26412;&#30456;&#27604;&#65292;&#21487;&#20351;Top1&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;3.1%&#65292;Top5&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;2.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.01841</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#39044;&#27979;&#19978;&#20248;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models Great on Tabular Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#31181;DNN&#30340;&#20248;&#21183;&#22312;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#65288;&#20363;&#22914;&#22238;&#24402;&#25110;&#20998;&#31867;&#20219;&#21153;&#65289;&#19978;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TP-BERTa&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#23558;&#26631;&#37327;&#25968;&#20540;&#29305;&#24449;&#20540;&#36716;&#25442;&#20026;&#31163;&#25955;&#24230;&#39640;&#12289;&#39640;&#32500;&#24230;&#30340;&#26631;&#35760;&#65292;&#24182;&#19988;&#19968;&#31181;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#25972;&#21512;&#20102;&#29305;&#24449;&#21517;&#31216;&#21644;&#25968;&#20540;&#29305;&#24449;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23439;&#35266;&#36741;&#21161;&#28176;&#36817;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#65288;MA-APNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#23610;&#24230;&#24615;&#36136;&#21644;&#39640;&#32500;&#24230;&#30340;&#26102;&#21464;&#32447;&#24615;&#36752;&#23556;&#20256;&#36882;&#26041;&#31243;&#65292;&#21033;&#29992;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#21152;&#26435;&#28176;&#36817;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#25968;&#20540;&#31034;&#20363;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01820</link><description>&lt;p&gt;
&#23439;&#35266;&#36741;&#21161;&#28176;&#36817;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32447;&#24615;&#36752;&#23556;&#20256;&#36882;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Macroscopic auxiliary asymptotic preserving neural networks for the linear radiative transfer equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23439;&#35266;&#36741;&#21161;&#28176;&#36817;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#65288;MA-APNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#23610;&#24230;&#24615;&#36136;&#21644;&#39640;&#32500;&#24230;&#30340;&#26102;&#21464;&#32447;&#24615;&#36752;&#23556;&#20256;&#36882;&#26041;&#31243;&#65292;&#21033;&#29992;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#21152;&#26435;&#28176;&#36817;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#25968;&#20540;&#31034;&#20363;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23439;&#35266;&#36741;&#21161;&#28176;&#36817;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#65288;MA-APNN&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#23610;&#24230;&#24615;&#36136;&#21644;&#39640;&#32500;&#24230;&#30340;&#26102;&#21464;&#32447;&#24615;&#36752;&#23556;&#20256;&#36882;&#26041;&#31243;&#65288;LRTEs&#65289;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#21152;&#26435;&#28176;&#36817;&#20445;&#25345;&#65288;AP&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21253;&#21547;&#20102;&#30452;&#25509;&#20174;&#21407;&#22987;&#20256;&#36882;&#26041;&#31243;&#23548;&#20986;&#30340;&#23439;&#35266;&#36741;&#21161;&#26041;&#31243;&#65292;&#24182;&#26126;&#30830;&#22320;&#21253;&#21547;&#25193;&#25955;&#26497;&#38480;&#26041;&#31243;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#23610;&#24230;&#21442;&#25968;&#36235;&#21521;&#20110;&#38646;&#65292;&#25439;&#22833;&#20989;&#25968;&#36880;&#28176;&#20174;&#36755;&#36816;&#24577;&#36807;&#28193;&#21040;&#25193;&#25955;&#26497;&#38480;&#24577;&#12290;&#27492;&#22806;&#65292;&#21021;&#22987;&#25968;&#25454;&#12289;&#36793;&#30028;&#26465;&#20214;&#21644;&#23432;&#24658;&#23450;&#24459;&#20316;&#20026;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#25968;&#20540;&#31034;&#20363;&#26469;&#23637;&#31034;MA-APNN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01820v1 Announce Type: cross  Abstract: We develop a Macroscopic Auxiliary Asymptotic-Preserving Neural Network (MA-APNN) method to solve the time-dependent linear radiative transfer equations (LRTEs), which have a multi-scale nature and high dimensionality. To achieve this, we utilize the Physics-Informed Neural Networks (PINNs) framework and design a new adaptive exponentially weighted Asymptotic-Preserving (AP) loss function, which incorporates the macroscopic auxiliary equation that is derived from the original transfer equation directly and explicitly contains the information of the diffusion limit equation. Thus, as the scale parameter tends to zero, the loss function gradually transitions from the transport state to the diffusion limit state. In addition, the initial data, boundary conditions, and conservation laws serve as the regularization terms for the loss. We present several numerical examples to demonstrate the effectiveness of MA-APNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Tsallis&#29109;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#21487;&#35299;MDP&#21644;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#65292;&#33021;&#22815;&#24179;&#34913;&#25506;&#32034;&#21644;&#33719;&#24471;&#30340;&#25511;&#21046;&#27861;&#21017;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01805</link><description>&lt;p&gt;
Tsallis&#29109;&#27491;&#21017;&#21270;&#29992;&#20110;&#32447;&#24615;&#21487;&#35299;MDP&#21644;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Tsallis&#29109;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#21487;&#35299;MDP&#21644;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#65292;&#33021;&#22815;&#24179;&#34913;&#25506;&#32034;&#21644;&#33719;&#24471;&#30340;&#25511;&#21046;&#27861;&#21017;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shannon&#29109;&#27491;&#21017;&#21270;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26368;&#20248;&#25511;&#21046;&#20013;&#65292;&#22240;&#20026;&#23427;&#33021;&#20419;&#36827;&#25506;&#32034;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#65292;Soft Actor-Critic&#20013;&#37319;&#29992;&#30340;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#20351;&#29992;Tsallis&#29109;&#65288;Shannon&#29109;&#30340;&#21333;&#21442;&#25968;&#25193;&#23637;&#65289;&#26469;&#27491;&#21017;&#21270;&#32447;&#24615;&#21487;&#35299;MDP&#21644;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#24179;&#34913;&#25506;&#32034;&#21644;&#33719;&#24471;&#30340;&#25511;&#21046;&#27861;&#21017;&#30340;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01805v1 Announce Type: cross  Abstract: Shannon entropy regularization is widely adopted in optimal control due to its ability to promote exploration and enhance robustness, e.g., maximum entropy reinforcement learning known as Soft Actor-Critic. In this paper, Tsallis entropy, which is a one-parameter extension of Shannon entropy, is used for the regularization of linearly solvable MDP and linear quadratic regulators. We derive the solution for these problems and demonstrate its usefulness in balancing between exploration and sparsity of the obtained control law.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;Transformer&#27169;&#22411;&#19982;&#22806;&#37096;&#31227;&#21160;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#36328;&#22478;&#24066;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36716;&#31227;&#20013;&#30340;&#36866;&#24212;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01801</link><description>&lt;p&gt;
COLA: &#36328;&#22478;&#24066;&#31227;&#21160;&#24615;&#36716;&#25442;&#22120;&#29992;&#20110;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
COLA: Cross-city Mobility Transformer for Human Trajectory Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01801
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;Transformer&#27169;&#22411;&#19982;&#22806;&#37096;&#31227;&#21160;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#36328;&#22478;&#24066;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36716;&#31227;&#20013;&#30340;&#36866;&#24212;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#26085;&#24120;&#31227;&#21160;&#35774;&#22791;&#20135;&#29983;&#30340;&#20154;&#31867;&#36712;&#36857;&#25968;&#25454;&#22312;&#22478;&#24066;&#35268;&#21010;&#21644;&#30123;&#24773;&#38450;&#25511;&#31561;&#37325;&#35201;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#38024;&#23545;&#20010;&#20154;&#38544;&#31169;&#38382;&#39064;&#65292;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#22823;&#37327;&#36924;&#30495;&#30340;&#31227;&#21160;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31232;&#32570;&#30340;&#26222;&#36941;&#38382;&#39064;&#26080;&#30097;&#38477;&#20302;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#36328;&#22478;&#24066;&#31227;&#21160;&#24615;&#36716;&#31227;&#30340;&#38382;&#39064;&#65292;&#25226;&#25569;&#20154;&#31867;&#36712;&#36857;&#30340;&#26222;&#36941;&#27169;&#24335;&#65292;&#20026;Transformer&#27169;&#22411;&#22686;&#21152;&#22806;&#37096;&#31227;&#21160;&#25968;&#25454;&#12290;&#22312;&#36328;&#22478;&#24066;&#30693;&#35782;&#36716;&#31227;&#20013;&#20986;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#20351;Transformer&#36866;&#24212;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65307;2&#65289;&#22914;&#20309;&#26657;&#20934;Transformer&#20197;&#36866;&#24212;&#32454;&#24494;&#19981;&#21516;&#30340;&#38271;&#23614;&#39057;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01801v1 Announce Type: cross  Abstract: Human trajectory data produced by daily mobile devices has proven its usefulness in various substantial fields such as urban planning and epidemic prevention. In terms of the individual privacy concern, human trajectory simulation has attracted increasing attention from researchers, targeting at offering numerous realistic mobility data for downstream tasks. Nevertheless, the prevalent issue of data scarcity undoubtedly degrades the reliability of existing deep learning models. In this paper, we are motivated to explore the intriguing problem of mobility transfer across cities, grasping the universal patterns of human trajectories to augment the powerful Transformer with external mobility data. There are two crucial challenges arising in the knowledge transfer across cities: 1) how to transfer the Transformer to adapt for domain heterogeneity; 2) how to calibrate the Transformer to adapt for subtly different long-tail frequency distrib
&lt;/p&gt;</description></item><item><title>Astraea&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22411;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#21040;&#20844;&#24179;&#24615;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01798</link><description>&lt;p&gt;
&#26397;&#30528;&#20844;&#24179;&#39640;&#25928;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25317;&#22622;&#25511;&#21046;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Fair and Efficient Learning-based Congestion Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01798
&lt;/p&gt;
&lt;p&gt;
Astraea&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22411;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#21040;&#20844;&#24179;&#24615;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#22522;&#20110;&#23398;&#20064;&#30340;&#25317;&#22622;&#25511;&#21046;&#65288;CC&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;TCP&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#33021;&#25552;&#20379;&#19968;&#33268;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#21253;&#25324;&#20844;&#24179;&#24615;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#31283;&#23450;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#23427;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#19982;&#36825;&#20123;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#23613;&#31649;&#30452;&#35273;&#19978;&#36825;&#20123;&#24615;&#33021;&#22312;&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;CC&#20013;&#36827;&#34892;&#25972;&#21512;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#65306;1&#65289;&#23427;&#20204;&#30340;&#35757;&#32451;&#29615;&#22659;&#26159;&#20026;&#20102;&#21333;&#27969;&#24615;&#33021;&#20248;&#21270;&#32780;&#35774;&#35745;&#30340;&#65292;&#26080;&#27861;&#36827;&#34892;&#21512;&#20316;&#22810;&#27969;&#20248;&#21270;&#65307;2&#65289;&#32570;&#20047;&#30452;&#25509;&#21487;&#24230;&#37327;&#30340;&#25351;&#26631;&#26469;&#23558;&#36825;&#20123;&#24615;&#33021;&#34920;&#31034;&#21040;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Astraea&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25317;&#22622;&#25511;&#21046;&#65292;&#30830;&#20445;&#20197;&#31283;&#23450;&#24615;&#24555;&#36895;&#25910;&#25947;&#21040;&#20844;&#24179;&#24615;&#12290;Astraea&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01798v1 Announce Type: cross  Abstract: Recent years have witnessed a plethora of learning-based solutions for congestion control (CC) that demonstrate better performance over traditional TCP schemes. However, they fail to provide consistently good convergence properties, including {\em fairness}, {\em fast convergence} and {\em stability}, due to the mismatch between their objective functions and these properties. Despite being intuitive, integrating these properties into existing learning-based CC is challenging, because: 1) their training environments are designed for the performance optimization of single flow but incapable of cooperative multi-flow optimization, and 2) there is no directly measurable metric to represent these properties into the training objective function.   We present Astraea, a new learning-based congestion control that ensures fast convergence to fairness with stability. At the heart of Astraea is a multi-agent deep reinforcement learning framework 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#20449;&#24687;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#24490;&#29615;&#22609;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#19988;&#31616;&#21270;&#39640;&#25928;&#20102;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.01776</link><description>&lt;p&gt;
&#21033;&#29992;&#28151;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#20449;&#24687;&#35268;&#33539;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#24490;&#29615;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hybrid data-driven and physics-informed regularized learning of cyclic plasticity with Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01776
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#20449;&#24687;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#24490;&#29615;&#22609;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#19988;&#31616;&#21270;&#39640;&#25928;&#20102;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20197;&#34920;&#31034;&#24490;&#29615;&#22609;&#24615;&#24182;&#26367;&#20195;&#22522;&#20110;&#24452;&#21521;&#22238;&#24402;&#26144;&#23556;&#31639;&#27861;&#30340;&#20256;&#32479;&#26448;&#26009;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#29616;&#29289;&#29702;&#20449;&#24687;&#35268;&#33539;&#21644;&#32972;&#24212;&#21147;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#21368;&#36733;&#34987;&#26368;&#22823;&#31243;&#24230;&#22320;&#24212;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#26356;&#31616;&#21333;&#12289;&#26356;&#39640;&#25928;&#65292;&#21516;&#26102;&#20195;&#34920;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#19977;&#32500;&#26448;&#26009;&#27169;&#22411;&#12290;&#36890;&#36807;&#29992;Armstrong-Frederick&#36816;&#21160;&#30828;&#21270;&#27169;&#22411;&#33719;&#21462;&#30340;&#20195;&#29702;&#25968;&#25454;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#37319;&#29992;&#22343;&#26041;&#35823;&#24046;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#35268;&#23450;&#20102;&#20960;&#39033;&#38480;&#21046;&#26465;&#20214;&#65306;&#20869;&#37096;&#21464;&#37327;&#30340;&#20559;&#24046;&#29305;&#24615;&#12289;&#31526;&#21512;&#27969;&#21160;&#35268;&#21017;&#12289;&#24377;&#24615;&#21306;&#22495;&#30340;&#21306;&#20998;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01776v1 Announce Type: cross  Abstract: An extendable, efficient and explainable Machine Learning approach is proposed to represent cyclic plasticity and replace conventional material models based on the Radial Return Mapping algorithm. High accuracy and stability by means of a limited amount of training data is achieved by implementing physics-informed regularizations and the back stress information. The off-loading of the Neural Network is applied to the maximal extent. The proposed model architecture is simpler and more efficient compared to existing solutions from the literature, while representing a complete three-dimensional material model. The validation of the approach is carried out by means of surrogate data obtained with the Armstrong-Frederick kinematic hardening model. The Mean Squared Error is assumed as the loss function which stipulates several restrictions: deviatoric character of internal variables, compliance with the flow rule, the differentiation of elas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.01773</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#25913;&#21892;&#22270;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving out-of-distribution generalization in graphs via hierarchical semantic environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01773
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20998;&#24067;&#36716;&#31227;&#21644;&#32570;&#20047;&#29615;&#22659;&#32972;&#26223;&#65292;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#29983;&#25104;&#24179;&#38754;&#29615;&#22659;&#26469;&#22686;&#24378;&#22270;&#30340;OOD&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24179;&#38754;&#29615;&#22659;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#21253;&#21547;&#21508;&#31181;&#35757;&#32451;&#29615;&#22659;&#65288;&#22914;&#39592;&#26550;&#12289;&#22823;&#23567;&#31561;&#65289;&#30340;DrugOOD&#25968;&#25454;&#38598;&#65292;&#24179;&#38754;&#29615;&#22659;&#26080;&#27861;&#20805;&#20998;&#35299;&#20915;&#20854;&#39640;&#24322;&#36136;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#29983;&#25104;&#26356;&#20855;&#35821;&#20041;&#20016;&#23500;&#30340;&#29615;&#22659;&#65292;&#20197;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#22270;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#36755;&#20837;&#22270;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#25552;&#21462;&#36755;&#20837;&#22270;&#20013;&#30340;&#21464;&#20307;&#23376;&#22270;&#65292;&#20197;&#22312;&#26412;&#22320;&#29615;&#22659;&#19978;&#29983;&#25104;&#20195;&#29702;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#38543;&#26426;&#27880;&#24847;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01773v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;&#30340;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21069;&#31579;&#36873;&#20986;&#19981;&#27963;&#36291;&#26679;&#26412;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01769</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Safe Screening Rule with Bi-level Optimization of $\nu$ Support Vector Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;&#30340;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21069;&#31579;&#36873;&#20986;&#19981;&#27963;&#36291;&#26679;&#26412;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#22312;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#12290;&#20316;&#20026;&#20256;&#32479;SVM&#30340;&#19968;&#20010;&#33879;&#21517;&#25193;&#23637;&#65292;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;$\nu$-SVM&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#32780;&#34920;&#29616;&#21331;&#36234;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#23427;&#20173;&#38754;&#20020;&#35757;&#32451;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;$\nu$-SVM&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;&#65288;SRBO-$\nu$-SVM&#65289;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#31579;&#36873;&#20986;&#19981;&#27963;&#36291;&#30340;&#26679;&#26412;&#65292;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;SRBO-$\nu$-SVM&#20005;&#26684;&#22320;&#36890;&#36807;&#25972;&#21512;KKT&#26465;&#20214;&#12289;&#20984;&#38382;&#39064;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;$\nu$&#23646;&#24615;&#25512;&#23548;&#32780;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#20598;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#65288;DCDM&#65289;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;SRBO&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01769v1 Announce Type: cross  Abstract: Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the $\nu$ support vector machine ($\nu$-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for $\nu$-SVM (SRBO-$\nu$-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-$\nu$-SVM is strictly deduced by integrating the Karush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex problems and the $\nu$-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to ac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.01759</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#65306;&#22238;&#39038;&#19982;&#26032;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Open-world Machine Learning: A Review and New Outlooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#65292;&#21363;&#20551;&#23450;&#29615;&#22659;&#26159;&#38745;&#24577;&#30340;&#65292;&#27169;&#22411;&#19968;&#26086;&#37096;&#32626;&#23601;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#22522;&#26412;&#19988;&#30456;&#24403;&#24188;&#31258;&#30340;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#24320;&#25918;&#29615;&#22659;&#22797;&#26434;&#12289;&#21160;&#24577;&#19988;&#20805;&#28385;&#26410;&#30693;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25298;&#32477;&#26410;&#30693;&#12289;&#21457;&#29616;&#26032;&#22855;&#28857;&#65292;&#28982;&#21518;&#36880;&#27493;&#23398;&#20064;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#20687;&#29983;&#29289;&#31995;&#32479;&#19968;&#26679;&#23433;&#20840;&#22320;&#24182;&#25345;&#32493;&#36827;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#22312;&#32479;&#19968;&#33539;&#24335;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25361;&#25112;&#12289;&#21407;&#21017;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01759v1 Announce Type: new  Abstract: Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then incrementally learning them, could enable models to be safe and evolve continually as biological systems do. This paper provides a holistic view of open-world machine learning by investigating unknown rejection, novel class discovery, and class-incremental learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Finally, we discuss several potential directions for future research. This paper aims to provide a compr
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26500;&#24314;&#30340; AFBT GAN &#22686;&#24378;&#20102;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.01758</link><description>&lt;p&gt;
AFBT GAN: &#36890;&#36807;&#21453;&#20107;&#23454;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22686;&#24378;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01758
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26500;&#24314;&#30340; AFBT GAN &#22686;&#24378;&#20102;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21151;&#33021;&#36830;&#25509;&#65288;FC&#65289;&#30340;&#35299;&#37322;&#32467;&#26524;&#36890;&#24120;&#26159;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#32467;&#26524;&#26631;&#31614;&#21644;&#35832;&#22914;Pearson&#30456;&#20851;&#24615;&#25110;&#26799;&#24230;&#21453;&#25512;&#31561;&#30456;&#20851;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#27169;&#22411;&#20173;&#28982;&#26159;&#22312;&#40657;&#30418;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#32570;&#20047;&#23545;&#37325;&#35201;&#21306;&#22495;FC&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#35786;&#26029;&#24615;&#33021;&#65292;&#22312;&#35786;&#26029;&#27169;&#22411;&#20013;&#25552;&#20379;&#20851;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30456;&#20851;&#21306;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#24403;&#20581;&#24247;&#21463;&#35797;&#32773;&#65288;HC&#65289;&#21457;&#23637;&#20026;&#20027;&#35266;&#35748;&#30693;&#34928;&#36864;&#65288;SCD&#65289;&#21644;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#26102;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#30830;&#23450;&#31070;&#32463;&#36864;&#34892;&#24615;&#30456;&#20851;&#21306;&#22495;&#65292;&#25105;&#20204;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#28304;&#26631;&#31614;FC&#27966;&#29983;&#30340;&#30446;&#26631;&#26631;&#31614;FC&#30697;&#38453;&#65292;&#28982;&#21518;&#23558;&#28304;&#26631;&#31614;FC&#20943;&#21435;&#30446;&#26631;&#26631;&#31614;FC&#12290;&#33258;&#36866;&#24212;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#25442;&#26500;&#25104;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01758v1 Announce Type: cross  Abstract: Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson's correlation or gradient backward. However, the diagnostic model is still trained on the black box model and might lack the attention of FCs in important regions during the training. To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step. To better determine the neurodegeneration-related regions, we employ counterfactual reasoning to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC. The counterfactual reasoning architecture is constructed by adaptive forward and backward transfo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#36827;&#34892;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.01757</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#38598;&#25104;&#22914;&#20309;&#25552;&#21319;LLM&#22312;&#20248;&#21270;&#20013;&#30340;&#24615;&#33021;&#65306;&#20197;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01757
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#36827;&#34892;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#26126;&#26174;&#22320;&#23558;&#23427;&#20204;&#23450;&#20301;&#20026;&#35299;&#20915;&#22797;&#26434;&#20248;&#21270;&#25361;&#25112;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#34987;&#35748;&#21487;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#20248;&#21270;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#65292;&#22312;&#20165;&#20381;&#36182;&#20110;&#25968;&#23383;&#25991;&#26412;&#25552;&#31034;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#65292;&#38590;&#20197;&#25429;&#25417;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#26469;&#22686;&#24378;&#20248;&#21270;&#24615;&#33021;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#28145;&#20837;&#20102;&#35299;&#22788;&#29702;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#38598;&#25104;&#20801;&#35768;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;LLM&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#25193;&#23637;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01757v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have notably positioned them as capable tools for addressing complex optimization challenges. Despite this recognition, a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems. Keeping this in mind, we first propose to enhance the optimization performance using multimodal LLM capable of processing both textual and visual prompts for deeper insights of the processed optimization problem. This integration allows for a more comprehensive understanding of optimization problems, akin to human cognitive processes. We have developed a multimodal LLM-based optimization framework that simulates human problem-solving workflows, thereby offering a more nuanced and effective analysis. The efficacy of this method is evaluated through exten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01742</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#29992;&#20110;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-TS: Interpretable Diffusion for General Time Series Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs)&#27491;&#36880;&#28176;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#26368;&#36817;&#24050;&#22312;&#38899;&#39057;&#21512;&#25104;&#12289;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#21644;&#39044;&#27979;&#31561;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-TS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#20854;&#20013;&#20998;&#35299;&#25216;&#26415;&#25351;&#23548;Diffusion-TS&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#32780;&#21464;&#21387;&#22120;&#20174;&#22024;&#26434;&#30340;&#27169;&#22411;&#36755;&#20837;&#20013;&#25366;&#25496;&#35814;&#32454;&#30340;&#24207;&#21015;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#37325;&#24314;&#22122;&#22768;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;Fourier&#30340;&#25439;&#22833;&#39033;&#12290;&#39044;&#26399;Diffusion-TS&#21487;&#20197;&#29983;&#25104;&#26082;&#20855;&#26377;&#35299;&#37322;&#24615;&#21448;&#30495;&#23454;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36824;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;Diffusion
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01742v1 Announce Type: cross  Abstract: Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-T
&lt;/p&gt;</description></item><item><title>ComS2T&#26159;&#19968;&#31181;&#20114;&#34917;&#30340;&#26102;&#31354;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32467;&#26500;&#20998;&#20026;&#31283;&#23450;&#30340;&#26032;&#30382;&#36136;&#21644;&#21160;&#24577;&#28023;&#39532;&#20307;&#65292;&#23454;&#29616;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.01738</link><description>&lt;p&gt;
ComS2T&#65306;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#33258;&#36866;&#24212;&#27169;&#22411;&#28436;&#21270;&#30340;&#20114;&#34917;&#26102;&#31354;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01738
&lt;/p&gt;
&lt;p&gt;
ComS2T&#26159;&#19968;&#31181;&#20114;&#34917;&#30340;&#26102;&#31354;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32467;&#26500;&#20998;&#20026;&#31283;&#23450;&#30340;&#26032;&#30382;&#36136;&#21644;&#21160;&#24577;&#28023;&#39532;&#20307;&#65292;&#23454;&#29616;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01738v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#26102;&#31354;&#65288;ST&#65289;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#23454;&#29616;&#26234;&#33021;&#22478;&#24066;&#21644;&#21487;&#25345;&#32493;&#22478;&#24066;&#21457;&#23637;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#24403;&#21069;&#30340;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#21508;&#31181;&#31354;&#38388;&#21367;&#31215;&#21644;&#26102;&#38388;&#28436;&#21270;&#22359;&#25429;&#33719;&#20102;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#24555;&#36895;&#30340;&#22478;&#24066;&#21270;&#23548;&#33268;&#22478;&#24066;&#25968;&#25454;&#21644;&#22478;&#24066;&#32467;&#26500;&#22312;&#30701;&#26399;&#20869;&#21576;&#27874;&#21160;&#20998;&#24067;&#65292;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#21644;&#25968;&#25454;&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#26032;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#22312;&#37325;&#22797;&#35757;&#32451;&#26041;&#38754;&#20063;&#23384;&#22312;&#23616;&#38480;&#12290;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#20114;&#34917;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ComS2T&#30340;&#21450;&#26102;&#20114;&#34917;&#26102;&#31354;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#28436;&#21270;&#65292;&#23454;&#29616;&#25968;&#25454;&#36866;&#24212;&#12290;ComS2T&#23558;&#31070;&#32463;&#32467;&#26500;&#20998;&#25104;&#31283;&#23450;&#30340;&#26032;&#30382;&#36136;&#29992;&#20110;&#24041;&#22266;&#21382;&#21490;&#35760;&#24518;&#20197;&#21450;&#21160;&#24577;&#28023;&#39532;&#20307;&#29992;&#20110;&#26356;&#26032;&#26032;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01738v1 Announce Type: new  Abstract: Spatiotemporal (ST) learning has become a crucial technique to enable smart cities and sustainable urban development. Current ST learning models capture the heterogeneity via various spatial convolution and temporal evolution blocks. However, rapid urbanization leads to fluctuating distributions in urban data and city structures over short periods, resulting in existing methods suffering generalization and data adaptation issues. Despite efforts, existing methods fail to deal with newly arrived observations and those methods with generalization capacity are limited in repeated training. Motivated by complementary learning in neuroscience, we introduce a prompt-based complementary spatiotemporal learning termed ComS2T, to empower the evolution of models for data adaptation. ComS2T partitions the neural architecture into a stable neocortex for consolidating historical memory and a dynamic hippocampus for new knowledge update. We first dise
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RbSL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#26679;&#32422;&#26463;&#26102;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.01734</link><description>&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#22312;&#20855;&#26377;&#24674;&#22797;&#31574;&#30053;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01734
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RbSL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#26679;&#32422;&#26463;&#26102;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#26088;&#22312;&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#38598;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#30446;&#26631;&#36798;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#19979;&#30340;&#31163;&#32447;GCRL&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#24674;&#22797;&#30340;&#30417;&#30563;&#23398;&#20064;&#65288;RbSL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23436;&#25104;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01734v1 Announce Type: cross  Abstract: Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GC
&lt;/p&gt;</description></item><item><title>&#32479;&#35745;&#21147;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#31232;&#30095;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#32423;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#26469;&#24179;&#34913;&#25968;&#25454;&#25311;&#21512;&#21644;&#31616;&#27905;&#24615;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#26497;&#38480;&#20013;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01723</link><description>&lt;p&gt;
&#21160;&#21147;&#23398;&#31995;&#32479;&#35782;&#21035;&#30340;&#32479;&#35745;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Statistical Mechanics of Dynamical System Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01723
&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#21147;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#31232;&#30095;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#32423;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#26469;&#24179;&#34913;&#25968;&#25454;&#25311;&#21512;&#21644;&#31616;&#27905;&#24615;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#26497;&#38480;&#20013;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#21040;&#30340;&#22122;&#22768;&#25968;&#25454;&#20013;&#24674;&#22797;&#21160;&#21147;&#23398;&#26041;&#31243;&#26159;&#31995;&#32479;&#35782;&#21035;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#26469;&#20998;&#26512;&#31232;&#30095;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#36229;&#21442;&#25968;&#30340;&#35797;&#38169;&#36873;&#25321;&#24179;&#34913;&#25968;&#25454;&#25311;&#21512;&#21644;&#31616;&#27905;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#32479;&#35745;&#21147;&#23398;&#25552;&#20379;&#20102;&#20998;&#26512;&#22797;&#26434;&#24615;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#24037;&#20855;&#65292;&#31867;&#20284;&#20110;&#29109;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#31181;&#31867;&#27604;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#36807;&#31243;&#23450;&#20041;&#20026;&#19968;&#20010;&#23558;&#21464;&#37327;&#36873;&#25321;&#19982;&#31995;&#25968;&#20540;&#20998;&#24320;&#30340;&#20004;&#32423;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#20351;&#24471;&#21518;&#39564;&#21442;&#25968;&#20998;&#24067;&#21487;&#20197;&#20197;&#38381;&#24335;&#24418;&#24335;&#35745;&#31639;&#12290;&#37319;&#29992;&#32479;&#35745;&#21147;&#23398;&#27010;&#24565;&#65288;&#22914;&#33258;&#30001;&#33021;&#21644;&#37197;&#20998;&#20989;&#25968;&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#22312;&#20302;&#25968;&#25454;&#26497;&#38480;&#20013;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01723v1 Announce Type: cross  Abstract: Recovering dynamical equations from observed noisy data is the central challenge of system identification. We develop a statistical mechanical approach to analyze sparse equation discovery algorithms, which typically balance data fit and parsimony through a trial-and-error selection of hyperparameters. In this framework, statistical mechanics offers tools to analyze the interplay between complexity and fitness, in analogy to that done between entropy and energy. To establish this analogy, we define the optimization procedure as a two-level Bayesian inference problem that separates variable selection from coefficient values and enables the computation of the posterior parameter distribution in closed form. A key advantage of employing statistical mechanical concepts, such as free energy and the partition function, is in the quantification of uncertainty, especially in in the low-data limit; frequently encountered in real-world applicati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20234;&#36763;&#27169;&#22411;&#20316;&#20026;L0&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22330;&#24863;&#30693;&#22240;&#23376;&#20998;&#35299;&#26426;&#65288;FFM&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#33021;&#21516;&#26102;&#30830;&#23450;&#22810;&#20010;&#32676;&#32452;&#30340;&#26368;&#20339;&#29305;&#24449;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.01718</link><description>&lt;p&gt;
&#36890;&#36807;&#20234;&#36763;&#27169;&#22411;&#36827;&#34892;&#22330;&#24863;&#30693;&#22240;&#23376;&#20998;&#35299;&#26426;&#30340;L0&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
L0 Regularization of Field-Aware Factorization Machine through Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01718
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20234;&#36763;&#27169;&#22411;&#20316;&#20026;L0&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22330;&#24863;&#30693;&#22240;&#23376;&#20998;&#35299;&#26426;&#65288;FFM&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#33021;&#21516;&#26102;&#30830;&#23450;&#22810;&#20010;&#32676;&#32452;&#30340;&#26368;&#20339;&#29305;&#24449;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20234;&#36763;&#27169;&#22411;&#20316;&#20026;&#22330;&#24863;&#30693;&#22240;&#23376;&#20998;&#35299;&#26426;&#65288;FFM&#65289;&#30340;L0&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#21516;&#26102;&#30830;&#23450;&#27599;&#20010;&#32676;&#32452;&#30340;&#26368;&#20339;&#29305;&#24449;&#32452;&#21512;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27599;&#20010;&#32676;&#32452;&#36873;&#25321;&#30340;&#29305;&#24449;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#21152;&#28145;&#23545;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01718v1 Announce Type: new  Abstract: We examined the use of the Ising model as an L0 regularization method for field-aware factorization machines (FFM). This approach improves generalization performance and has the advantage of simultaneously determining the best feature combinations for each of several groups. We can deepen the interpretation and understanding of the model from the similarities and differences in the features selected in each group.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;(SSB)&#25511;&#21046;&#38382;&#39064;&#65292;&#22312;&#20801;&#35768;&#32456;&#31471;&#20998;&#24067;&#19982;&#39044;&#20808;&#25351;&#23450;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#24809;&#32602;&#20004;&#32773;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#12290;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;SSB&#35299;&#65292;&#26174;&#31034;&#26368;&#20248;&#25511;&#21046;&#36807;&#31243;&#30340;&#32456;&#31471;&#20998;&#24067;&#26159;&#956;T&#21644;&#20854;&#20182;&#20998;&#24067;&#30340;&#20960;&#20309;&#28151;&#21512;&#65292;&#24182;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26102;&#38388;&#24207;&#21015;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.01717</link><description>&lt;p&gt;
&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;&#65306;&#19968;&#31181;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Soft-constrained Schrodinger Bridge: a Stochastic Control Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01717
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;(SSB)&#25511;&#21046;&#38382;&#39064;&#65292;&#22312;&#20801;&#35768;&#32456;&#31471;&#20998;&#24067;&#19982;&#39044;&#20808;&#25351;&#23450;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#24809;&#32602;&#20004;&#32773;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#12290;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;SSB&#35299;&#65292;&#26174;&#31034;&#26368;&#20248;&#25511;&#21046;&#36807;&#31243;&#30340;&#32456;&#31471;&#20998;&#24067;&#26159;&#956;T&#21644;&#20854;&#20182;&#20998;&#24067;&#30340;&#20960;&#20309;&#28151;&#21512;&#65292;&#24182;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26102;&#38388;&#24207;&#21015;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34203;&#23450;&#35860;&#26725;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20855;&#26377;&#39044;&#20808;&#25351;&#23450;&#30340;&#32456;&#31471;&#20998;&#24067;&#956;T&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20801;&#35768;&#32456;&#31471;&#20998;&#24067;&#19982;&#956;T&#19981;&#21516;&#20294;&#24809;&#32602;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#26469;&#27867;&#21270;&#36825;&#20010;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#25511;&#21046;&#38382;&#39064;&#31216;&#20026;&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;(SSB)&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;SSB&#35299;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#34920;&#26126;&#26368;&#20248;&#25511;&#21046;&#36807;&#31243;&#30340;&#32456;&#31471;&#20998;&#24067;&#26159;&#956;T&#21644;&#21478;&#19968;&#20123;&#20998;&#24067;&#30340;&#20960;&#20309;&#28151;&#21512;&#12290;&#36825;&#20010;&#32467;&#26524;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#26102;&#38388;&#24207;&#21015;&#35774;&#32622;&#12290;SSB&#30340;&#19968;&#20010;&#24212;&#29992;&#26159;&#40065;&#26834;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#31639;&#27861;&#26469;&#20174;&#20960;&#20309;&#28151;&#21512;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#29992;&#36884;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01717v1 Announce Type: cross  Abstract: Schr\"{o}dinger bridge can be viewed as a continuous-time stochastic control problem where the goal is to find an optimally controlled diffusion process with a pre-specified terminal distribution $\mu_T$. We propose to generalize this stochastic control problem by allowing the terminal distribution to differ from $\mu_T$ but penalizing the Kullback-Leibler divergence between the two distributions. We call this new control problem soft-constrained Schr\"{o}dinger bridge (SSB). The main contribution of this work is a theoretical derivation of the solution to SSB, which shows that the terminal distribution of the optimally controlled process is a geometric mixture of $\mu_T$ and some other distribution. This result is further extended to a time series setting. One application of SSB is the development of robust generative diffusion models. We propose a score matching-based algorithm for sampling from geometric mixtures and showcase its us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#31569;&#35774;&#35745;&#20915;&#31574;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23581;&#35797;&#24212;&#29992;&#20110;&#26550;&#26500;&#20915;&#31574;&#35760;&#24405;&#65288;ADR&#65289;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.01709</link><description>&lt;p&gt;
LLMs&#33021;&#21542;&#29983;&#25104;&#24314;&#31569;&#35774;&#35745;&#20915;&#31574;&#65311;-&#19968;&#39033;&#25506;&#32034;&#24615;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#31569;&#35774;&#35745;&#20915;&#31574;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23581;&#35797;&#24212;&#29992;&#20110;&#26550;&#26500;&#20915;&#31574;&#35760;&#24405;&#65288;ADR&#65289;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#30693;&#35782;&#31649;&#29702;&#65288;AKM&#65289;&#28041;&#21450;&#23545;&#39033;&#30446;&#25110;&#32452;&#32455;&#20013;&#19982;&#24314;&#31569;&#20915;&#31574;&#21644;&#35774;&#35745;&#30456;&#20851;&#20449;&#24687;&#30340;&#26377;&#32452;&#32455;&#22788;&#29702;&#12290;AKM&#30340;&#19968;&#20010;&#37325;&#35201;&#20135;&#29289;&#26159;&#26550;&#26500;&#20915;&#31574;&#35760;&#24405;&#65288;ADR&#65289;&#65292;&#23427;&#35760;&#24405;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#12290; ADR&#26159;&#25429;&#25417;&#20915;&#31574;&#32972;&#26223;&#12289;&#24050;&#20570;&#20986;&#30340;&#20915;&#31574;&#20197;&#21450;&#19982;&#35774;&#35745;&#20915;&#31574;&#30456;&#20851;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#25991;&#20214;&#65292;&#20174;&#32780;&#20419;&#36827;&#36879;&#26126;&#24230;&#12289;&#21327;&#20316;&#21644;&#29702;&#35299;&#12290; &#23613;&#31649;&#23427;&#20204;&#26377;&#30410;&#22788;&#65292;&#20294;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21442;&#19982;&#24230;&#19981;&#19968;&#33268;&#31561;&#25361;&#25112;&#65292;ADR&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#37319;&#29992;&#36895;&#24230;&#36739;&#24930;&#12290; &#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#33021;&#26377;&#21161;&#20110;&#24357;&#21512;&#36825;&#31181;&#37319;&#29992;&#24046;&#36317;&#65292;&#36890;&#36807;&#20419;&#36827;ADR&#30340;&#29983;&#25104;&#12290; &#20294;&#26159;&#65292;LLM&#29992;&#20110;ADR&#29983;&#25104;&#25110;&#29702;&#35299;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290; &#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#26088;&#22312;&#35843;&#26597;&#20351;&#29992;LLM&#36827;&#34892;&#30340;&#21487;&#34892;&#24615;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01709v1 Announce Type: cross  Abstract: Architectural Knowledge Management (AKM) involves the organized handling of information related to architectural decisions and design within a project or organization. An essential artifact of AKM is the Architecture Decision Records (ADR), which documents key design decisions. ADRs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. Despite their benefits, ADR adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. Recent advancements in Large Language Models (LLMs) may help bridge this adoption gap by facilitating ADR generation. However, the effectiveness of LLM for ADR generation or understanding is something that has not been explored. To this end, in this work, we perform an exploratory study that aims to investigate the feasibility of using LLM for the 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;</title><link>https://arxiv.org/abs/2403.01695</link><description>&lt;p&gt;
DyCE&#65306;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#21644;&#25193;&#23637;&#30340;&#21160;&#24577;&#21487;&#37197;&#32622;&#36864;&#20986;
&lt;/p&gt;
&lt;p&gt;
DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01695
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#38656;&#35201;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#26377;&#25928;&#37096;&#32626;&#26102;&#65292;&#20351;&#29992;&#32553;&#25918;&#21644;&#21387;&#32553;&#25216;&#26415;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#21644;&#37327;&#21270;&#65292;&#36890;&#24120;&#26159;&#38745;&#24577;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21160;&#24577;&#21387;&#32553;&#26041;&#27861;&#65288;&#22914;&#25552;&#21069;&#36864;&#20986;&#65289;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#26679;&#26412;&#30340;&#22256;&#38590;&#31243;&#24230;&#24182;&#26681;&#25454;&#38656;&#35201;&#20998;&#37197;&#35745;&#31639;&#26469;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;&#21160;&#24577;&#26041;&#27861;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#19982;&#38745;&#24577;&#26041;&#27861;&#20849;&#23384;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#29616;&#19978;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#21160;&#24577;&#37096;&#20998;&#30340;&#20219;&#20309;&#21464;&#21270;&#37117;&#20250;&#24433;&#21709;&#21518;&#32493;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#21160;&#24577;&#21387;&#32553;&#35774;&#35745;&#37117;&#26159;&#21333;&#29255;&#30340;&#65292;&#19982;&#22522;&#30784;&#27169;&#22411;&#32039;&#23494;&#38598;&#25104;&#65292;&#20174;&#32780;&#20351;&#20854;&#38590;&#20197;&#36866;&#24212;&#26032;&#39062;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#31181;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#35774;&#35745;&#32771;&#34385;&#30456;&#20114;&#35299;&#32806;&#20197;&#21450;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01695v1 Announce Type: cross  Abstract: Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as pruning and quantization are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the 
&lt;/p&gt;</description></item><item><title>CATS&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#22806;&#29983;&#21464;&#37327;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#25972;&#21512;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#21644;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.01673</link><description>&lt;p&gt;
CATS&#65306;&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01673
&lt;/p&gt;
&lt;p&gt;
CATS&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#22806;&#29983;&#21464;&#37327;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#25972;&#21512;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;MTSF&#65289;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#26174;&#31034;&#65292;&#21333;&#21464;&#37327;&#27169;&#22411;&#32463;&#24120;&#20248;&#20110;&#22810;&#20803;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#20803;&#27169;&#22411;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#65288;CATS&#65289;&#65292;&#23427;&#31867;&#20284;&#20110;2D&#26102;&#38388;&#19978;&#19979;&#25991;&#20851;&#27880;&#26426;&#21046;&#65292;&#20174;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#65288;OTS&#65289;&#29983;&#25104;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#65288;ATS&#65289;&#65292;&#20197;&#26377;&#25928;&#34920;&#31034;&#21644;&#25972;&#21512;&#31995;&#21015;&#38388;&#20851;&#31995;&#29992;&#20110;&#39044;&#27979;&#12290;ATS&#30340;&#20851;&#38190;&#21407;&#21017;-&#36830;&#32493;&#24615;&#65292;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;-&#36890;&#36807;&#19981;&#21516;&#27169;&#22359;&#36827;&#34892;&#35782;&#21035;&#21644;&#23454;&#29616;&#12290;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;2&#23618;MLP&#20316;&#20026;&#26680;&#24515;&#39044;&#27979;&#22120;&#65292;CATS&#20063;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#21644;&#21442;&#25968;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#25928;&#19988;&#21487;&#36716;&#31227;&#30340;MTSF&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01673v1 Announce Type: cross  Abstract: For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the difficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS - continuity, sparsity, and variability - are identified and implemented through different modules. Even with a basic 2-layer MLP as core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it an efficient and transferable MTSF solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.01671</link><description>&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#65306;&#32479;&#35745;&#26816;&#39564;&#12289;&#24230;&#37327;&#29109;&#20013;&#30340;&#38477;&#32500;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#20197;&#21033;&#29992;&#26469;&#31616;&#21270;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#30340;&#23545;&#31216;&#24615;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#20851;&#20110;&#26500;&#24314;&#25490;&#21015;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#30740;&#31350;&#27963;&#21160;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#30340;&#21464;&#37327;&#22914;&#20309;&#32479;&#35745;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#20854;&#20013;&#26679;&#26412;&#37327;&#20801;&#35768;&#38543;&#30528;&#32500;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#22312;&#32479;&#35745;&#29702;&#35770;&#26041;&#38754;&#65292;&#20851;&#20110;&#25490;&#21015;&#19981;&#21464;&#24615;&#22914;&#20309;&#24110;&#21161;&#20272;&#35745;&#20013;&#38477;&#32500;&#30340;&#30693;&#35782;&#29978;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20960;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22238;&#39038;&#24182;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27979;&#35797;&#22810;&#20803;&#20998;&#24067;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#65307;&#65288;ii&#65289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#65307;&#65288;iii&#65289;&#20998;&#26512;&#20809;&#28369;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#24182;&#23558;&#20854;&#19982;&#26410;&#24378;&#21152;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#23545;&#24212;&#20989;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01671v1 Announce Type: new  Abstract: Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the sample size. Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without impos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#28789;&#27963;&#24615;&#25351;&#26631;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#20303;&#23429;&#24314;&#31569;&#22312;&#19981;&#21516;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#20013;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.01669</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37327;&#21270;&#21644;&#39044;&#27979;&#20303;&#23429;&#24314;&#31569;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Predicting Residential Building Flexibility Using Machine Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#28789;&#27963;&#24615;&#25351;&#26631;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#20303;&#23429;&#24314;&#31569;&#22312;&#19981;&#21516;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#20013;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20303;&#23429;&#24314;&#31569;&#21344;&#21040;2022&#24180;&#32654;&#22269;&#24635;&#29992;&#30005;&#37327;&#30340;35\%&#65292;&#38543;&#30528;&#24314;&#31569;&#20013;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#25552;&#20379;&#28789;&#27963;&#24615;&#32473;&#30005;&#32593;&#30340;&#28508;&#21147;&#20063;&#22312;&#22686;&#21152;&#12290;&#20026;&#20102;&#21033;&#29992;&#24314;&#31569;&#25152;&#25552;&#20379;&#30340;&#28789;&#27963;&#24615;&#65292;&#32858;&#21512;&#21830;&#25110;&#31995;&#32479;&#25805;&#20316;&#32773;&#38656;&#35201;&#37327;&#21270;&#21644;&#39044;&#27979;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#28789;&#27963;&#24615;&#25351;&#26631;&#65288;&#21363;&#21151;&#29575;&#21644;&#33021;&#37327;&#28789;&#27963;&#24615;&#65289;&#65292;&#24182;&#25506;&#35752;&#20102;&#20960;&#31181;&#20027;&#27969;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22235;&#23567;&#26102;&#21644;24&#23567;&#26102;&#39044;&#27979;&#26102;&#27573;&#19979;&#20303;&#23429;&#24314;&#31569;&#30340;&#26102;&#38388;&#21464;&#21270;&#21644;&#38646;&#26143;&#28789;&#27963;&#24615;&#12290;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#39044;&#27979;24&#23567;&#26102;&#20869;&#30340;&#21151;&#29575;&#28789;&#27963;&#24615;&#65292;&#26368;&#22810;&#21487;&#25552;&#21069;24&#23567;&#26102;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01669v1 Announce Type: new  Abstract: Residential buildings account for a significant portion (35\%) of the total electricity consumption in the U.S. as of 2022. As more distributed energy resources are installed in buildings, their potential to provide flexibility to the grid increases. To tap into that flexibility provided by buildings, aggregators or system operators need to quantify and forecast flexibility. Previous works in this area primarily focused on commercial buildings, with little work on residential buildings. To address the gap, this paper first proposes two complementary flexibility metrics (i.e., power and energy flexibility) and then investigates several mainstream machine learning-based models for predicting the time-variant and sporadic flexibility of residential buildings at four-hour and 24-hour forecast horizons. The long-short-term-memory (LSTM) model achieves the best performance and can predict power flexibility for up to 24 hours ahead with the ave
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;EBMs&#23884;&#20837;&#21040;&#25193;&#25955;&#27493;&#39588;&#20013;&#24182;&#24341;&#20837;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#25239;&#24615;&#33021;&#37327;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01666</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#25913;&#36827;&#23545;&#25239;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Energy-Based Model via Diffusion Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01666
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;EBMs&#23884;&#20837;&#21040;&#25193;&#25955;&#27493;&#39588;&#20013;&#24182;&#24341;&#20837;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#23545;&#25239;&#24615;&#33021;&#37327;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#32780;&#39640;&#25928;&#30340;&#20284;&#28982;&#24230;&#20272;&#35745;&#21364;&#40092;&#20026;&#20154;&#30693;&#12290;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#22320;&#21442;&#25968;&#21270;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#65292;&#20294;&#35757;&#32451;&#38590;&#24230;&#24456;&#22823;&#12290;&#23545;&#25239;&#24615;&#30340;EBMs&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#22120;&#24418;&#25104;&#19968;&#20010;&#26497;&#23567;&#26497;&#22823;&#35757;&#32451;&#28216;&#25103;&#65292;&#20197;&#36991;&#20813;&#20256;&#32479;EBMs&#20013;&#20351;&#29992;&#26114;&#36149;&#30340;MCMC&#37319;&#26679;&#65292;&#20294;&#23545;&#25239;&#24615;&#30340;EBMs&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;EBMs&#23884;&#20837;&#21040;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#65292;&#23558;&#19968;&#20010;&#38271;&#29983;&#25104;&#36807;&#31243;&#20998;&#25104;&#20960;&#20010;&#36739;&#23567;&#30340;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#31216;&#30340;Jeffrey&#25955;&#24230;&#24182;&#24341;&#20837;&#19968;&#20010;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#26469;&#35757;&#32451;&#29983;&#25104;&#22120;&#65292;&#20197;&#35299;&#20915;&#23545;&#25239;&#24615;EBMs&#20013;&#23384;&#22312;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;EBMs&#30456;&#27604;&#65292;&#22312;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01666v1 Announce Type: new  Abstract: Generative models have shown strong generation ability while efficient likelihood estimation is less explored. Energy-based models~(EBMs) define a flexible energy function to parameterize unnormalized densities efficiently but are notorious for being difficult to train. Adversarial EBMs introduce a generator to form a minimax training game to avoid expensive MCMC sampling used in traditional EBMs, but a noticeable gap between adversarial EBMs and other strong generative models still exists. Inspired by diffusion-based models, we embedded EBMs into each denoising step to split a long-generated process into several smaller steps. Besides, we employ a symmetric Jeffrey divergence and introduce a variational posterior distribution for the generator's training to address the main challenges that exist in adversarial EBMs. Our experiments show significant improvement in generation compared to existing adversarial EBMs, while also providing a u
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20043;&#38388;&#30340;&#39118;&#38505;&#36317;&#31163;&#27010;&#24565;&#65292;&#36890;&#36807;&#39118;&#38505;&#36317;&#31163;&#21487;&#20197;&#37327;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21464;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.01660</link><description>&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#20960;&#20309;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Geometry and Stability of Supervised Learning Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01660
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20043;&#38388;&#30340;&#39118;&#38505;&#36317;&#31163;&#27010;&#24565;&#65292;&#36890;&#36807;&#39118;&#38505;&#36317;&#31163;&#21487;&#20197;&#37327;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21464;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20043;&#38388;&#30340;&#36317;&#31163;&#27010;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39118;&#38505;&#36317;&#31163;&#12290;&#36825;&#31181;&#20197;&#26368;&#20248;&#20256;&#36755;&#20026;&#28789;&#24863;&#30340;&#36317;&#31163;&#20419;&#36827;&#20102;&#31283;&#23450;&#24615;&#32467;&#26524;&#65307;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20462;&#25913;&#21487;&#20197;&#23558;&#38382;&#39064;&#31227;&#21160;&#22810;&#23569;&#26469;&#37327;&#21270;&#35832;&#22914;&#37319;&#26679;&#20559;&#24046;&#12289;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#21644;&#36924;&#36817;&#31561;&#38382;&#39064;&#22312;&#39118;&#38505;&#36317;&#31163;&#19979;&#22914;&#20309;&#25913;&#21464;&#32473;&#23450;&#38382;&#39064;&#12290;&#22312;&#24314;&#31435;&#20102;&#36317;&#31163;&#20043;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20135;&#29983;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#27979;&#22320;&#32447;&#24182;&#35777;&#26126;&#20998;&#31867;&#38382;&#39064;&#38598;&#22312;&#26356;&#22823;&#31867;&#30340;&#38382;&#39064;&#20013;&#26159;&#23494;&#38598;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#39118;&#38505;&#36317;&#31163;&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#38382;&#39064;&#30340;&#39044;&#27979;&#21464;&#37327;&#19978;&#32467;&#21512;&#20102;&#25351;&#23450;&#30340;&#26435;&#37325;&#65292;&#21478;&#19968;&#20010;&#23545;&#38382;&#39064;&#30340;&#39118;&#38505;&#26223;&#35266;&#36718;&#24275;&#26356;&#20026;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01660v1 Announce Type: new  Abstract: We introduce a notion of distance between supervised learning problems, which we call the Risk distance. This optimal-transport-inspired distance facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the geometry of the resulting space of supervised learning problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem's predictors, and one that is more sensitive to the contours of a problem's risk landscape.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#21382;&#21490;&#21457;&#30005;&#37327;&#21644;&#22825;&#27668;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#20197;&#24448;&#22312;&#21306;&#22495;&#39044;&#27979;&#20013;&#24573;&#35270;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#25110;&#29983;&#25104;&#22823;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.01653</link><description>&lt;p&gt;
&#22522;&#20110;&#21382;&#21490;&#21457;&#30005;&#37327;&#21644;&#22825;&#27668;&#25968;&#25454;&#30340;&#20998;&#23618;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#21069;&#21306;&#22495;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01653
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#21382;&#21490;&#21457;&#30005;&#37327;&#21644;&#22825;&#27668;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#20197;&#24448;&#22312;&#21306;&#22495;&#39044;&#27979;&#20013;&#24573;&#35270;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#25110;&#29983;&#25104;&#22823;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22495;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#19968;&#20010;&#22320;&#21306;&#25152;&#26377;&#23627;&#39030;&#20809;&#20239;&#31995;&#32479;&#30340;&#24635;&#21457;&#30005;&#37327;&#65292;&#23545;&#20110;&#33021;&#28304;&#34892;&#19994;&#30340;&#21508;&#26041;&#21033;&#30410;&#30456;&#20851;&#32773;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#32771;&#34385;&#26469;&#33258;&#22320;&#29702;&#19978;&#20998;&#25955;&#20301;&#32622;&#30340;&#22823;&#37327;&#22826;&#38451;&#33021;&#21457;&#30005;&#21644;&#22825;&#27668;&#26102;&#38388;&#24207;&#21015;&#65292;&#36825;&#20351;&#24471;&#20934;&#30830;&#30340;&#21306;&#22495;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#23558;&#37325;&#28857;&#38480;&#21046;&#22312;&#39044;&#27979;&#21333;&#20010;&#26102;&#38388;&#24207;&#21015;&#65288;&#21363;&#32858;&#21512;&#26102;&#38388;&#24207;&#21015;&#65289;&#65292;&#21363;&#19968;&#20010;&#22320;&#21306;&#25152;&#26377;&#22826;&#38451;&#33021;&#21457;&#30005;&#26102;&#38388;&#24207;&#21015;&#30340;&#24635;&#21644;&#65292;&#24573;&#30053;&#20102;&#29305;&#23450;&#20301;&#32622;&#30340;&#22825;&#27668;&#25928;&#24212;&#65292;&#35201;&#20040;&#29420;&#31435;&#20351;&#29992;&#29305;&#23450;&#20301;&#32622;&#30340;&#22825;&#27668;&#25968;&#25454;&#26469;&#39044;&#27979;&#27599;&#20010;&#20809;&#20239;&#31449;&#28857;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#26102;&#38388;&#24207;&#21015;&#65288;&#21363;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#65289;&#65292;&#23548;&#33268;&#25968;&#37327;&#24222;&#22823;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01653v1 Announce Type: new  Abstract: Regional solar power forecasting, which involves predicting the total power generation from all rooftop photovoltaic systems in a region holds significant importance for various stakeholders in the energy sector. However, the vast amount of solar power generation and weather time series from geographically dispersed locations that need to be considered in the forecasting process makes accurate regional forecasting challenging. Therefore, previous work has limited the focus to either forecasting a single time series (i.e., aggregated time series) which is the addition of all solar generation time series in a region, disregarding the location-specific weather effects or forecasting solar generation time series of each PV site (i.e., individual time series) independently using location-specific weather data, resulting in a large number of forecasting models. In this work, we propose two deep-learning-based regional forecasting methods that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#20248;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#34013;&#33394;&#21644;&#32511;&#33394;&#20004;&#31181;&#24037;&#20316;&#27169;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#28608;&#27963;&#20851;&#38190;&#20256;&#24863;&#22120;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#32780;&#19981;&#24433;&#21709;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01642</link><description>&lt;p&gt;
&#36890;&#36807;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#23454;&#29616;&#30340;&#34013;&#32511;&#27169;&#24335;&#39640;&#33021;&#25928;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;
&lt;/p&gt;
&lt;p&gt;
Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#20248;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#34013;&#33394;&#21644;&#32511;&#33394;&#20004;&#31181;&#24037;&#20316;&#27169;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#28608;&#27963;&#20851;&#38190;&#20256;&#24863;&#22120;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#32780;&#19981;&#24433;&#21709;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01642v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#29289;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#38656;&#35201;&#24320;&#21457;&#26082;&#39640;&#25928;&#21448;&#33021;&#32988;&#20219;&#30340;&#20248;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#37319;&#29992;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#22996;&#21592;&#20250;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#21033;&#29992;&#24377;&#24615;&#32593;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#31574;&#30053;&#35782;&#21035;&#20986;&#22312;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#20013;&#23545;&#20934;&#30830;&#20998;&#31867;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#20256;&#24863;&#22120;&#65306;&#24341;&#20837;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#32858;&#21512;&#20256;&#24863;&#22120;&#36873;&#25321;&#20013;&#30340;&#27169;&#22411;&#24847;&#35265;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24037;&#20316;&#27169;&#24335;&#65292;&#31216;&#20026;&#8220;&#34013;&#33394;&#8221;&#21644;&#8220;&#32511;&#33394;&#8221;&#12290;&#34013;&#33394;&#27169;&#24335;&#21033;&#29992;&#25152;&#26377;&#20256;&#24863;&#22120;&#36827;&#34892;&#26368;&#22823;&#26816;&#27979;&#33021;&#21147;&#65292;&#32780;&#32511;&#33394;&#27169;&#24335;&#20165;&#36873;&#25321;&#24615;&#28608;&#27963;&#20851;&#38190;&#20256;&#24863;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#32780;&#19981;&#24433;&#21709;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#29702;&#35770;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01642v1 Announce Type: new  Abstract: The rapid advancement of Internet of Things (IoT) necessitates the development of optimized Chemiresistive Sensor (CRS) arrays that are both energy-efficient and capable. This study introduces a novel optimization strategy that employs a rapid ensemble learning-based model committee approach to achieve these goals. Utilizing machine learning models such as Elastic Net Regression, Random Forests, and XGBoost, among others, the strategy identifies the most impactful sensors in a CRS array for accurate classification: A weighted voting mechanism is introduced to aggregate the models' opinions in sensor selection, thereby setting up wo distinct working modes, termed "Blue" and "Green". The Blue mode operates with all sensors for maximum detection capability, while the Green mode selectively activates only key sensors, significantly reducing energy consumption without compromising detection accuracy. The strategy is validated through theoreti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32972;&#26223;&#19979;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#23558;&#25193;&#25955;&#25351;&#23548;&#32435;&#20837;&#19981;&#20165;&#25552;&#21319;&#20102;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#32780;&#19988;&#20943;&#23569;&#20102;&#20998;&#24067;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01639</link><description>&lt;p&gt;
Gaussian Mixture Models&#30340;&#25193;&#25955;&#25351;&#23548;&#30340;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32972;&#26223;&#19979;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#23558;&#25193;&#25955;&#25351;&#23548;&#32435;&#20837;&#19981;&#20165;&#25552;&#21319;&#20102;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#32780;&#19988;&#20943;&#23569;&#20102;&#20998;&#24067;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21463;&#30410;&#20110;&#23558;&#29305;&#23450;&#20219;&#21153;&#20449;&#24687;&#27880;&#20837;&#35780;&#20998;&#20989;&#25968;&#20197;&#24341;&#23548;&#26679;&#26412;&#29983;&#25104;&#26397;&#21521;&#25152;&#38656;&#23646;&#24615;&#12290;&#36825;&#31181;&#20449;&#24687;&#34987;&#31216;&#20026;&#25351;&#23548;&#12290;&#20363;&#22914;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25991;&#26412;&#36755;&#20837;&#34987;&#32534;&#30721;&#20026;&#25351;&#23548;&#20197;&#29983;&#25104;&#35821;&#20041;&#23545;&#40784;&#30340;&#22270;&#20687;&#12290;&#36866;&#24403;&#30340;&#25351;&#23548;&#36755;&#20837;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35266;&#23519;&#26159;&#65292;&#24378;&#26377;&#21147;&#30340;&#25351;&#23548;&#20419;&#36827;&#20102;&#19982;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#30340;&#32039;&#23494;&#23545;&#40784;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32972;&#26223;&#19979;&#25552;&#20379;&#20102;&#23545;&#29702;&#35299;&#25351;&#23548;&#23545;&#25193;&#25955;&#27169;&#22411;&#24433;&#21709;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#25193;&#25955;&#25351;&#23548;&#32435;&#20837;&#19981;&#20165;&#25552;&#21319;&#20102;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#32780;&#19988;&#20943;&#23569;&#20102;&#20998;&#24067;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#36755;&#20986;&#20998;&#24067;&#30340;&#24046;&#24322;&#29109;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01639v1 Announce Type: new  Abstract: Diffusion models benefit from instillation of task-specific information into the score function to steer the sample generation towards desired properties. Such information is coined as guidance. For example, in text-to-image synthesis, text input is encoded as guidance to generate semantically aligned images. Proper guidance inputs are closely tied to the performance of diffusion models. A common observation is that strong guidance promotes a tight alignment to the task-specific information, while reducing the diversity of the generated samples. In this paper, we provide the first theoretical study towards understanding the influence of guidance on diffusion models in the context of Gaussian mixture models. Under mild conditions, we prove that incorporating diffusion guidance not only boosts classification confidence but also diminishes distribution diversity, leading to a reduction in the differential entropy of the output distribution.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#20195;&#29702;&#22312;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30701;&#35270;&#25506;&#32034;&#35774;&#35745;&#30340;&#36890;&#29992;&#31574;&#30053;&#20849;&#20139;&#31639;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01636</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#30701;&#35270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01636
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#20195;&#29702;&#22312;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30701;&#35270;&#25506;&#32034;&#35774;&#35745;&#30340;&#36890;&#29992;&#31574;&#30053;&#20849;&#20139;&#31639;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#65288;MTRL&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#36817;&#26399;MTRL&#29702;&#35770;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20551;&#35774;&#20219;&#21153;&#38388;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#32479;&#35745;&#25928;&#29575;&#65292;&#23545;&#20110;RL&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25506;&#32034;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#21364;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#65292;&#24403;&#20195;&#29702;&#22312;&#36275;&#22815;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30701;&#35270;&#25506;&#32034;&#35774;&#35745;&#65288;&#22914;$\epsilon$-&#36138;&#24515;&#65289;&#30340;&#36890;&#29992;&#31574;&#30053;&#20849;&#20139;&#31639;&#27861;&#21487;&#20197;&#22312;MTRL&#20013;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20174;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#8220;&#25506;&#32034;&#25910;&#30410;&#8221;&#22312;MTRL&#20013;&#30340;&#39318;&#27425;&#29702;&#35770;&#35777;&#26126;&#65292;&#20063;&#26377;&#21161;&#20110;&#35299;&#37322;&#30701;&#35270;&#25506;&#32034;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#20026;&#20102;&#39564;&#35777;&#22810;&#26679;&#24615;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01636v1 Announce Type: cross  Abstract: Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#26469;&#27169;&#25311;&#25176;&#21345;&#39532;&#20811;&#31561;&#31163;&#23376;&#20307;&#20869;&#37096;&#30340;&#33021;&#37327;&#36716;&#31227;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#23548;&#25193;&#25955;&#24615;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#33021;&#37327;&#30456;&#20114;&#20316;&#29992;&#27169;&#25311;&#65292;&#20026;&#25552;&#39640;&#25176;&#21345;&#39532;&#20811;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.01635</link><description>&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22312;&#25176;&#21345;&#39532;&#20811;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Neural Ordinary Differential Equations for Tokamak Plasma Dynamics Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#26469;&#27169;&#25311;&#25176;&#21345;&#39532;&#20811;&#31561;&#31163;&#23376;&#20307;&#20869;&#37096;&#30340;&#33021;&#37327;&#36716;&#31227;&#36807;&#31243;&#65292;&#36890;&#36807;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#23548;&#25193;&#25955;&#24615;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#33021;&#37327;&#30456;&#20114;&#20316;&#29992;&#27169;&#25311;&#65292;&#20026;&#25552;&#39640;&#25176;&#21345;&#39532;&#20811;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#21463;&#25511;&#28909;&#26680;&#32858;&#21464;&#30340;&#36807;&#31243;&#20013;&#65292;&#25176;&#21345;&#39532;&#20811;&#22312;&#29702;&#35299;&#29123;&#28903;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#26041;&#38754;&#38754;&#20020;&#30528;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#21306;&#22495;&#22810;&#26102;&#38388;&#23610;&#24230;&#20256;&#36755;&#27169;&#22411;&#65292;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#26469;&#27169;&#25311;&#25176;&#21345;&#39532;&#20811;&#20869;&#37096;&#22797;&#26434;&#30340;&#33021;&#37327;&#36716;&#31227;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20174;DIII-D&#25176;&#21345;&#39532;&#20811;&#30340;&#23454;&#39564;&#25968;&#25454;&#20013;&#25968;&#20540;&#25512;&#23548;&#25193;&#25955;&#24615;&#21442;&#25968;&#65292;&#23454;&#29616;&#23545;&#30005;&#23376;&#21644;&#31163;&#23376;&#22312;&#26680;&#24515;&#12289;&#36793;&#32536;&#21644;&#21038;&#21066;&#23618;&#31561;&#21508;&#20010;&#21306;&#22495;&#20043;&#38388;&#33021;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#31934;&#30830;&#24314;&#27169;&#12290;&#36825;&#20123;&#21306;&#22495;&#34987;&#27010;&#24565;&#21270;&#20026;&#19981;&#21516;&#30340;&#33410;&#28857;&#65292;&#25429;&#25417;&#36752;&#23556;&#21644;&#20256;&#36755;&#36807;&#31243;&#30340;&#20851;&#38190;&#26102;&#38388;&#23610;&#24230;&#65292;&#20174;&#32780;&#23545;&#25176;&#21345;&#39532;&#20811;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#36741;&#21161;&#21152;&#28909;&#26465;&#20214;&#19979;&#39564;&#35777;&#27169;&#22411;&#19982;DIII-D&#31561;&#31163;&#23376;&#20307;&#30340;&#21305;&#37197;&#24615;&#65292;&#26368;&#32456;&#25581;&#31034;&#20102;&#22686;&#24378;&#25176;&#21345;&#39532;&#20811;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01635v1 Announce Type: cross  Abstract: In the quest for controlled thermonuclear fusion, tokamaks present complex challenges in understanding burning plasma dynamics. This study introduces a multi-region multi-timescale transport model, employing Neural Ordinary Differential Equations (Neural ODEs) to simulate the intricate energy transfer processes within tokamaks. Our methodology leverages Neural ODEs for the numerical derivation of diffusivity parameters from DIII-D tokamak experimental data, enabling the precise modeling of energy interactions between electrons and ions across various regions, including the core, edge, and scrape-off layer. These regions are conceptualized as distinct nodes, capturing the critical timescales of radiation and transport processes essential for efficient tokamak operation. Validation against DIII-D plasmas under various auxiliary heating conditions demonstrates the model's effectiveness, ultimately shedding light on ways to enhance tokamak
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#8220;&#25209;&#21028;&#24615;&#31383;&#21475;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#38024;&#23545;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#28151;&#21512;&#25968;&#25454;&#65292;&#36825;&#20123;&#31383;&#21475;&#26159;&#21487;&#20197;&#26126;&#30830;&#22320;&#21463;&#21040;&#19968;&#23450;&#30340;&#20998;&#31163;&#24230;&#37327;&#32422;&#26463;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.01633</link><description>&lt;p&gt;
&#25209;&#21028;&#24615;&#31383;&#21475;&#65306;&#25193;&#25955;&#27169;&#22411;&#20013;&#29305;&#24449;&#20986;&#29616;&#30340;&#38750;&#28176;&#36827;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Critical windows: non-asymptotic theory for feature emergence in diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01633
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#8220;&#25209;&#21028;&#24615;&#31383;&#21475;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#38024;&#23545;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#28151;&#21512;&#25968;&#25454;&#65292;&#36825;&#20123;&#31383;&#21475;&#26159;&#21487;&#20197;&#26126;&#30830;&#22320;&#21463;&#21040;&#19968;&#23450;&#30340;&#20998;&#31163;&#24230;&#37327;&#32422;&#26463;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#23637;&#29702;&#35770;&#26469;&#29702;&#35299;&#22270;&#20687;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#20013;&#19968;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25209;&#21028;&#24615;&#31383;&#21475;&#12290;&#23454;&#35777;&#19978;&#35266;&#23519;&#21040;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#23384;&#22312;&#29421;&#31364;&#30340;&#26102;&#38388;&#38388;&#38548;&#65292;&#22312;&#27492;&#26399;&#38388;&#20250;&#20986;&#29616;&#26368;&#32456;&#22270;&#20687;&#30340;&#29305;&#23450;&#29305;&#24449;&#65292;&#20363;&#22914;&#22270;&#20687;&#31867;&#21035;&#25110;&#32972;&#26223;&#39068;&#33394;&#12290;&#32780;&#36825;&#31181;&#29305;&#24615;&#23545;&#20110;&#35299;&#37322;&#24615;&#26159;&#26377;&#21033;&#30340;&#65292;&#22240;&#20026;&#24847;&#21619;&#30528;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#29305;&#24615;&#23450;&#20301;&#21040;&#36712;&#36857;&#30340;&#19968;&#20010;&#23567;&#29255;&#27573;&#65292;&#20294;&#36825;&#20284;&#20046;&#19982;&#25193;&#25955;&#30340;&#36830;&#32493;&#24615;&#36136;&#30456;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#31383;&#21475;&#65292;&#24182;&#34920;&#26126;&#23545;&#20110;&#26469;&#33258;&#28151;&#21512;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#31383;&#21475;&#21487;&#20197;&#29992;&#19968;&#23450;&#30340;&#36328;&#32452;&#21644;&#32452;&#20869;&#20998;&#31163;&#24230;&#37327;&#26469;&#26174;&#24335;&#22320;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#20026;&#35832;&#22914;&#33391;&#26465;&#20214;G&#30340;&#20855;&#20307;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#36825;&#20123;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01633v1 Announce Type: new  Abstract: We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya &amp; Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned G
&lt;/p&gt;</description></item><item><title>SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#27861;&#22686;&#24378;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving LLM Code Generation with Grammar Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01632
&lt;/p&gt;
&lt;p&gt;
SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SynCode&#65292;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#21644;&#36890;&#29992;&#22320;&#35299;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#30721;&#30340;&#26032;&#26694;&#26550;&#12290;SynCode&#21033;&#29992;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#65292;&#21033;&#29992;&#31163;&#32447;&#26500;&#24314;&#30340;&#22522;&#20110;&#35821;&#35328;&#35821;&#27861;&#32456;&#32467;&#31526;&#30340;&#39640;&#25928;&#26597;&#25214;&#34920;DFA mask store&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SynCode&#22312;&#32473;&#23450;&#32534;&#31243;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#23436;&#22791;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#23637;&#31034;&#20854;&#22312;&#20445;&#30041;&#35821;&#20041;&#19978;&#26377;&#25928;&#20196;&#29260;&#30340;&#21516;&#26102;&#25298;&#32477;&#26080;&#25928;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19982;&#30001;CFG&#23450;&#20041;&#30340;&#20219;&#20309;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#39564;&#35777;&#20102;&#38024;&#23545;Python&#21644;Go&#30340;CFG&#23454;&#39564;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#24403;SynCode&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#32467;&#21512;&#26102;&#65292;&#35821;&#27861;&#38169;&#35823;&#20943;&#23569;96.07%&#65292;&#24432;&#26174;&#20102;&#20854;&#23545;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01632v1 Announce Type: new  Abstract: We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs). SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode's soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art LLMs, showcasing its substantial impact on enhancing syntactical precision in code generation.   Our code is available at https://github.com/uiuc-focal-lab/syncode.
&lt;/p&gt;</description></item><item><title>ML4H 2023&#30740;&#35752;&#20250;&#36890;&#36807;&#30740;&#31350;&#22278;&#26700;&#35752;&#35770;&#20250;&#20419;&#36827;&#20102;&#19982;&#20250;&#32773;&#21644;&#36164;&#28145;&#30740;&#31350;&#20154;&#21592;&#23601;&#20581;&#24247;&#39046;&#22495;&#30340;&#26102;&#20107;&#35805;&#39064;&#23637;&#24320;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01628</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20581;&#24247;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#25361;&#25112;&#65306;&#26469;&#33258;ML4H 2023&#30740;&#35752;&#20250;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Recent Advances, Applications, and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2023 Symposium
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01628
&lt;/p&gt;
&lt;p&gt;
ML4H 2023&#30740;&#35752;&#20250;&#36890;&#36807;&#30740;&#31350;&#22278;&#26700;&#35752;&#35770;&#20250;&#20419;&#36827;&#20102;&#19982;&#20250;&#32773;&#21644;&#36164;&#28145;&#30740;&#31350;&#20154;&#21592;&#23601;&#20581;&#24247;&#39046;&#22495;&#30340;&#26102;&#20107;&#35805;&#39064;&#23637;&#24320;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19977;&#23626;ML4H&#30740;&#35752;&#20250;&#20110;2023&#24180;12&#26376;10&#26085;&#22312;&#32654;&#22269;&#36335;&#26131;&#26031;&#23433;&#37027;&#24030;&#26032;&#22885;&#23572;&#33391;&#20030;&#34892;&#12290;&#30740;&#35752;&#20250;&#21253;&#25324;&#30740;&#31350;&#22278;&#26700;&#35752;&#35770;&#20250;&#65292;&#26088;&#22312;&#20419;&#36827;&#19982;ML4H&#31038;&#21306;&#30456;&#20851;&#20027;&#39064;&#30340;&#21442;&#19982;&#32773;&#21644;&#36164;&#28145;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#35752;&#35770;&#12290;&#20250;&#35758;&#32452;&#32455;&#20102;11&#20010;&#29616;&#22330;&#22278;&#26700;&#21644;4&#20010;&#34394;&#25311;&#22278;&#26700;&#65292;&#28041;&#21450;&#20102;17&#20301;&#36164;&#28145;&#20027;&#24109;&#21644;19&#20301;&#21021;&#32423;&#20027;&#24109;&#12290;&#27599;&#20010;&#22278;&#26700;&#35752;&#35770;&#20250;&#37117;&#21253;&#25324;&#21463;&#36992;&#30340;&#36164;&#28145;&#20027;&#24109;&#65288;&#22312;&#35813;&#39046;&#22495;&#25317;&#26377;&#20016;&#23500;&#32463;&#39564;&#65289;&#12289;&#21021;&#32423;&#20027;&#24109;&#65288;&#36127;&#36131;&#20419;&#36827;&#35752;&#35770;&#65289;&#20197;&#21450;&#23545;&#35813;&#20027;&#39064;&#24863;&#20852;&#36259;&#30340;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#19982;&#20250;&#32773;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20250;&#35758;&#30340;&#32452;&#32455;&#36807;&#31243;&#65292;&#24182;&#25972;&#29702;&#20102;&#36825;&#20123;&#30740;&#35752;&#20250;&#35752;&#35770;&#30340;&#25910;&#33719;&#65292;&#21253;&#25324;&#26368;&#26032;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01628v1 Announce Type: new  Abstract: The third ML4H symposium was held in person on December 10, 2023, in New Orleans, Louisiana, USA. The symposium included research roundtable sessions to foster discussions between participants and senior researchers on timely and relevant topics for the \ac{ML4H} community. Encouraged by the successful virtual roundtables in the previous year, we organized eleven in-person roundtables and four virtual roundtables at ML4H 2022. The organization of the research roundtables at the conference involved 17 Senior Chairs and 19 Junior Chairs across 11 tables. Each roundtable session included invited senior chairs (with substantial experience in the field), junior chairs (responsible for facilitating the discussion), and attendees from diverse backgrounds with interest in the session's topic. Herein we detail the organization process and compile takeaways from these roundtable discussions, including recent advances, applications, and open challe
&lt;/p&gt;</description></item><item><title>&#26412;&#31454;&#36187;&#26088;&#22312;&#25512;&#21160;&#21457;&#23637;&#26032;&#30340;ML&#25216;&#26415;&#65292;&#20351;&#29992;&#19968;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#65292;&#39318;&#20010;&#31454;&#36187;&#35299;&#20915;&#20102;&#21033;&#29992;ML-based&#20195;&#29702;&#26041;&#27861;&#25913;&#21892;&#35745;&#31639;&#25928;&#29575;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01623</link><description>&lt;p&gt;
ML4PhySim&#65306;&#29289;&#29702;&#27169;&#25311;&#25361;&#25112;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;&#32764;&#22411;&#35774;&#35745;&#65289;
&lt;/p&gt;
&lt;p&gt;
ML4PhySim : Machine Learning for Physical Simulations Challenge (The airfoil design)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31454;&#36187;&#26088;&#22312;&#25512;&#21160;&#21457;&#23637;&#26032;&#30340;ML&#25216;&#26415;&#65292;&#20351;&#29992;&#19968;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#65292;&#39318;&#20010;&#31454;&#36187;&#35299;&#20915;&#20102;&#21033;&#29992;ML-based&#20195;&#29702;&#26041;&#27861;&#25913;&#21892;&#35745;&#31639;&#25928;&#29575;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#23398;&#20064;&#24471;&#21040;&#30340;&#29289;&#29702;&#27169;&#22411;&#20173;&#28982;&#26159;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#27425;&#27604;&#36187;&#30340;&#30446;&#30340;&#26159;&#40723;&#21169;&#24320;&#21457;&#26032;&#30340;ML&#25216;&#26415;&#65292;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#33268;&#24615;&#35780;&#20272;&#26694;&#26550;Learning Industrial Physical Simulations (LIPS) &#26469;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19968;&#20010;&#21517;&#20026;AirfRANS&#30340;&#25968;&#25454;&#38598;&#23398;&#20064;&#20195;&#34920;&#30528;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#29289;&#29702;&#29992;&#20363;&#65306;&#32764;&#22411;&#35774;&#35745;&#27169;&#25311;&#12290;&#23545;&#20110;&#27599;&#19968;&#20010;&#25552;&#20132;&#30340;&#35299;&#20915;&#26041;&#26696;&#35745;&#31639;&#30340;&#20840;&#23616;&#35780;&#20998;&#22522;&#20110;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#26631;&#20934;&#65292;&#28085;&#30422;&#19981;&#21516;&#26041;&#38754;&#65292;&#21363;&#65306;&#19982;ML&#30456;&#20851;&#30340;&#26631;&#20934;&#65292;&#20998;&#24067;&#22806;&#26631;&#20934;&#21644;&#29289;&#29702;&#31526;&#21512;&#24615;&#26631;&#20934;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#21033;&#29992;ML-based&#20195;&#29702;&#26041;&#27861;&#25913;&#21892;&#35745;&#31639;&#25928;&#29575;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#31454;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01623v1 Announce Type: new  Abstract: The use of machine learning (ML) techniques to solve complex physical problems has been considered recently as a promising approach. However, the evaluation of such learned physical models remains an important issue for industrial use. The aim of this competition is to encourage the development of new ML techniques to solve physical problems using a unified evaluation framework proposed recently, called Learning Industrial Physical Simulations (LIPS). We propose learning a task representing a well-known physical use case: the airfoil design simulation, using a dataset called AirfRANS. The global score calculated for each submitted solution is based on three main categories of criteria covering different aspects, namely: ML-related, Out-Of-Distribution, and physical compliance criteria. To the best of our knowledge, this is the first competition addressing the use of ML-based surrogate approaches to improve the trade-off computational cos
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#27867;&#21270;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.01621</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#27867;&#21270;&#38382;&#39064;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning vs Deep Learning: The Generalization Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01621
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#27867;&#21270;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#19982;&#27169;&#22411;&#30340;&#25928;&#29992;&#21644;&#40065;&#26834;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#22312;&#22806;&#25512;&#26041;&#38754;&#30340;&#27604;&#36739;&#33021;&#21147;&#8212;&#8212;&#36825;&#26159;&#27867;&#21270;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#23545;&#19981;&#22312;&#20854;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01621v1 Announce Type: cross  Abstract: The capacity to generalize beyond the range of training data is a pivotal challenge, often synonymous with a model's utility and robustness. This study investigates the comparative abilities of traditional machine learning (ML) models and deep learning (DL) algorithms in terms of extrapolation -- a more challenging aspect of generalization because it requires the model to make inferences about data points that lie outside the domain it has been trained on. We present an empirical analysis where both ML and DL models are trained on an exponentially growing function and then tested on values outside the training domain. The choice of this function allows us to distinctly showcase the divergence in performance when models are required to predict beyond the scope of their training data. Our findings suggest that deep learning models possess inherent capabilities to generalize beyond the training scope, an essential feature for real-world a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#37096;&#20998;&#32852;&#37030;&#23398;&#20064;(PartialFL)&#65292;&#20801;&#35768;&#23558;&#25968;&#25454;&#27169;&#24577;&#25110;&#20854;&#20013;&#38388;&#34920;&#31034;&#30340;&#23376;&#38598;&#25552;&#20379;&#32473;&#26381;&#21153;&#22120;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#25968;&#25454;&#38480;&#21046;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01615</link><description>&lt;p&gt;
&#37096;&#20998;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Partial Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01615
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#37096;&#20998;&#32852;&#37030;&#23398;&#20064;(PartialFL)&#65292;&#20801;&#35768;&#23558;&#25968;&#25454;&#27169;&#24577;&#25110;&#20854;&#20013;&#38388;&#34920;&#31034;&#30340;&#23376;&#38598;&#25552;&#20379;&#32473;&#26381;&#21153;&#22120;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#25968;&#25454;&#38480;&#21046;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29992;&#25143;&#25968;&#25454;&#21463;&#38480;&#20110;&#36793;&#32536;&#35774;&#22791;(&#20363;&#22914;&#65292;&#31227;&#21160;&#30005;&#35805;)&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#26159;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;FL&#22312;&#35757;&#32451;&#26102;&#20551;&#35774;&#29992;&#25143;&#25968;&#25454;&#30340;&#20219;&#20309;&#37096;&#20998;&#37117;&#19981;&#33021;&#20174;&#36793;&#32536;&#27969;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29983;&#20135;&#29615;&#22659;&#20013;&#65292;&#29305;&#23450;&#30340;&#25968;&#25454;&#27169;&#24577;/&#20803;&#25968;&#25454;&#34987;&#38480;&#21046;&#22312;&#35774;&#22791;&#19978;&#65292;&#32780;&#20854;&#20182;&#25968;&#25454;&#19981;&#21463;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#22312;&#21830;&#19994;SLU&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#24076;&#26395;&#38450;&#27490;&#23558;&#29983;&#29289;&#35782;&#21035;&#20449;&#21495;(&#20363;&#22914;&#36755;&#20837;&#25552;&#31034;&#30340;&#38899;&#39057;&#35760;&#24405;)&#20256;&#36755;&#21040;&#20113;&#31471;&#65292;&#20294;&#21487;&#33021;&#23558;&#26412;&#22320;(&#21363;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;)&#36716;&#24405;&#30340;&#25991;&#26412;&#20256;&#36755;&#21040;&#20113;&#31471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37096;&#20998;&#32852;&#37030;&#23398;&#20064;(PartialFL)&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#25968;&#25454;&#27169;&#24577;&#30340;&#23376;&#38598;&#25110;&#20854;&#20013;&#38388;&#34920;&#31034;&#21487;&#20197;&#21521;&#26381;&#21153;&#22120;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01615v1 Announce Type: new  Abstract: Federated Learning (FL) is a popular algorithm to train machine learning models on user data constrained to edge devices (for example, mobile phones) due to privacy concerns. Typically, FL is trained with the assumption that no part of the user data can be egressed from the edge. However, in many production settings, specific data-modalities/meta-data are limited to be on device while others are not. For example, in commercial SLU systems, it is typically desired to prevent transmission of biometric signals (such as audio recordings of the input prompt) to the cloud, but egress of locally (i.e. on the edge device) transcribed text to the cloud may be possible. In this work, we propose a new algorithm called Partial Federated Learning (PartialFL), where a machine learning model is trained using data where a subset of data modalities or their intermediate representations can be made available to the server. We further restrict our model tr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#22312;&#32447;RNN&#31639;&#27861;&#22312;&#25918;&#30103;&#27835;&#30103;&#36807;&#31243;&#20013;&#20934;&#30830;&#39044;&#27979;&#21628;&#21560;&#36816;&#21160;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01607</link><description>&lt;p&gt;
&#38024;&#23545;&#22806;&#37096;&#23548;&#21521;&#25918;&#30103;&#20013;&#23433;&#20840;&#22686;&#24378;&#30340;&#21628;&#21560;&#36816;&#21160;&#39044;&#27979;&#19982;&#22312;&#32447;&#23398;&#20064;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#22312;&#32447;RNN&#31639;&#27861;&#22312;&#25918;&#30103;&#27835;&#30103;&#36807;&#31243;&#20013;&#20934;&#30830;&#39044;&#27979;&#21628;&#21560;&#36816;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32954;&#37096;&#25918;&#30103;&#20013;&#65292;&#32418;&#22806;&#25668;&#20687;&#22836;&#21487;&#20197;&#35760;&#24405;&#33016;&#37096;&#21453;&#23556;&#29289;&#20307;&#30340;&#20301;&#32622;&#65292;&#20197;&#25512;&#26029;&#30001;&#20110;&#21628;&#21560;&#32780;&#31227;&#21160;&#30340;&#32959;&#30244;&#20301;&#32622;&#65292;&#20294;&#27835;&#30103;&#31995;&#32479;&#30340;&#24310;&#36831;&#24433;&#21709;&#20102;&#25918;&#23556;&#26463;&#31934;&#24230;&#12290;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#65288;RTRL&#65289;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23398;&#20064;&#38750;&#24179;&#31283;&#21628;&#21560;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#20294;&#20855;&#26377;&#36739;&#39640;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#22312;&#32447;RNN&#31639;&#27861;&#65292;&#21363;&#26080;&#20559;&#22312;&#32447;&#24490;&#29615;&#20248;&#21270;&#65288;UORO&#65289;&#12289;&#31232;&#30095;-1&#27493;&#36924;&#36817;&#65288;SnAp-1&#65289;&#21644;&#35299;&#32806;&#31070;&#32463;&#25509;&#21475;&#65288;DNI&#65289;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#25918;&#30103;&#27835;&#30103;&#36807;&#31243;&#20013;&#30340;&#21628;&#21560;&#36816;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20581;&#24247;&#21463;&#35797;&#32773;&#33016;&#37096;&#22806;&#37096;&#26631;&#35760;&#29289;&#30340;&#21253;&#21547;3D&#20301;&#32622;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24433;&#21709;&#21644;&#21363;&#26102;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#21387;&#32553;&#20197;&#21450;&#32447;&#24615;&#31995;&#25968;&#30340;&#20934;&#30830;&#26356;&#26032;&#29992;&#20110;&#20449;&#29992;&#20998;&#37197;&#30340;SnAp-1&#21644;DNI&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01607v1 Announce Type: new  Abstract: In lung radiotherapy, infrared cameras can record the location of reflective objects on the chest to infer the position of the tumor moving due to breathing, but treatment system latencies hinder radiation beam precision. Real-time recurrent learning (RTRL), is a potential solution as it can learn patterns within non-stationary respiratory data but has high complexity. This study assesses the capabilities of resource-efficient online RNN algorithms, namely unbiased online recurrent optimization (UORO), sparse-1 step approximation (SnAp-1), and decoupled neural interfaces (DNI) to forecast respiratory motion during radiotherapy treatment accurately. We use time series containing the 3D position of external markers on the chest of healthy subjects. We propose efficient implementations for SnAp-1 and DNI based on compression of the influence and immediate Jacobian matrices and an accurate update of the linear coefficients used in credit ass
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26041;&#27861;&#26469;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#65292;&#20462;&#27491;&#27531;&#24046;&#35823;&#24046;&#65292;&#26377;&#26395;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.01605</link><description>&lt;p&gt;
&#26397;&#21521;&#21487;&#35777;&#26126;&#30340;&#23545;&#25968;&#23494;&#24230;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Provable Log Density Policy Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26041;&#27861;&#26469;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#65292;&#20462;&#27491;&#27531;&#24046;&#35823;&#24046;&#65292;&#26377;&#26395;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26159;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#29616;&#20195;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#34429;&#28982;&#25104;&#21151;&#65292;&#20294;&#22312;&#26799;&#24230;&#20272;&#35745;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27531;&#24046;&#35823;&#24046;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#20010;&#27531;&#24046;&#39033;&#24456;&#37325;&#35201;&#65292;&#32416;&#27491;&#23427;&#26377;&#21487;&#33021;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26469;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#65292;&#21487;&#20197;&#32416;&#27491;&#36825;&#20010;&#27531;&#24046;&#35823;&#24046;&#39033;&#12290;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29366;&#24577;-&#21160;&#20316;&#25240;&#25187;&#20998;&#24067;&#24418;&#24335;&#26469;&#35745;&#31639;&#31574;&#30053;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#20934;&#30830;&#25214;&#21040;&#26631;&#31614;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#25152;&#38656;&#30340;&#26041;&#31243;&#24335;&#12290;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21518;&#21521;&#21363;&#26102;&#65288;TD&#65289;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21518;&#21521;&#30340;&#21516;&#31574;&#30053;&#26679;&#26412;&#12290;&#30001;&#20110;&#20174;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#36827;&#34892;&#21518;&#21521;&#37319;&#26679;&#26159;&#39640;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01605v1 Announce Type: cross  Abstract: Policy gradient methods are a vital ingredient behind the success of modern reinforcement learning. Modern policy gradient methods, although successful, introduce a residual error in gradient estimation. In this work, we argue that this residual term is significant and correcting for it could potentially improve sample-complexity of reinforcement learning methods. To that end, we propose log density gradient to estimate the policy gradient, which corrects for this residual error term. Log density gradient method computes policy gradient by utilising the state-action discounted distributional formulation. We first present the equations needed to exactly find the log density gradient for a tabular Markov Decision Processes (MDPs). For more complex environments, we propose a temporal difference (TD) method that approximates log density gradient by utilizing backward on-policy samples. Since backward sampling from a Markov chain is highly 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SCHEMA&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36861;&#36394;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.01599</link><description>&lt;p&gt;
SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SCHEMA&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36861;&#36394;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#32473;&#20986;&#26681;&#25454;&#37096;&#20998;&#35270;&#35273;&#29366;&#24577;&#35266;&#23519;&#29983;&#25104;&#30446;&#26631;&#23548;&#21521;&#30340;&#21160;&#20316;&#27493;&#39588;&#24207;&#21015;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#32467;&#26500;&#21270;&#19988;&#21487;&#35268;&#21010;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#29366;&#24577;&#21464;&#21270;&#23545;&#20110;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#24456;&#37325;&#35201;&#65292;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#31435;&#26356;&#20026;&#32467;&#26500;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#22320;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36319;&#36394;&#31243;&#24207;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01599v1 Announce Type: cross  Abstract: We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thoug
&lt;/p&gt;</description></item><item><title>Mamba&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#20851;&#27880;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#36825;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01590</link><description>&lt;p&gt;
Mamba&#27169;&#22411;&#30340;&#38544;&#34255;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
The Hidden Attention of Mamba Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01590
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#20851;&#27880;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#36825;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mamba&#23618;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#22312;&#24314;&#27169;&#22810;&#20010;&#39046;&#22495;&#21253;&#25324;NLP&#12289;&#38271;&#36317;&#31163;&#24207;&#21015;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36873;&#25321;&#24615;SSMs&#34987;&#35270;&#20026;&#21452;&#37325;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#36807;IO-aware&#24182;&#34892;&#25195;&#25551;&#22312;&#25972;&#20010;&#24207;&#21015;&#19978;&#36827;&#34892;&#24182;&#34892;&#35757;&#32451;&#65292;&#24182;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#37096;&#32626;&#12290;&#25105;&#20204;&#28155;&#21152;&#20102;&#31532;&#19977;&#20010;&#35270;&#35282;&#65292;&#24182;&#23637;&#31034;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#20851;&#27880;&#39537;&#21160;&#30340;&#27169;&#22411;&#12290;&#36825;&#19968;&#26032;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24213;&#23618;&#26426;&#21046;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35753;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#31397;&#25506;Mamba&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01590v1 Announce Type: new  Abstract: The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains including NLP, long-range sequences processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MMDA&#65289;&#35774;&#32622;&#65292;&#20801;&#35768;&#22810;&#26679;&#21270;&#30340;&#28304;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28304;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01582</link><description>&lt;p&gt;
&#22312;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19978;
&lt;/p&gt;
&lt;p&gt;
On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MMDA&#65289;&#35774;&#32622;&#65292;&#20801;&#35768;&#22810;&#26679;&#21270;&#30340;&#28304;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28304;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSFDA&#65289;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#28304;&#27169;&#22411;&#32780;&#38750;&#28304;&#25968;&#25454;&#65292;&#20174;&#22810;&#20010;&#33391;&#22909;&#26631;&#35760;&#30340;&#28304;&#22495;&#20256;&#36882;&#30693;&#35782;&#21040;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#12290;&#29616;&#26377;&#30340;MSFDA&#26041;&#27861;&#23616;&#38480;&#20110;&#27599;&#20010;&#28304;&#22495;&#20165;&#25552;&#20379;&#21333;&#19968;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#32479;&#19968;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;MSFDA&#35774;&#32622;&#65306;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MMDA&#65289;&#65292;&#20801;&#35768;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#26679;&#21270;&#28304;&#27169;&#22411;&#65292;&#32780;&#19981;&#21463;&#23450;&#37327;&#38480;&#21046;&#12290;&#34429;&#28982;MMDA&#20855;&#26377;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#21512;&#24182;&#22823;&#37327;&#28304;&#27169;&#22411;&#20250;&#22686;&#21152;&#21253;&#21547;&#19981;&#24819;&#35201;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#31361;&#26174;&#20986;&#28304;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20004;&#20010;&#22522;&#26412;&#36873;&#25321;&#21407;&#21017;&#65306;&#21487;&#36716;&#31227;&#24615;&#21407;&#21017;&#21644;&#22810;&#26679;&#24615;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#21512;&#23427;&#20204;&#30340;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01582v1 Announce Type: new  Abstract: Multi-Source-Free Unsupervised Domain Adaptation (MSFDA) aims to transfer knowledge from multiple well-labeled source domains to an unlabeled target domain, using source models instead of source data. Existing MSFDA methods limited that each source domain provides only a single model, with a uniform structure. This paper introduces a new MSFDA setting: Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation (MMDA), allowing diverse source models with varying architectures, without quantitative restrictions. While MMDA holds promising potential, incorporating numerous source models poses a high risk of including undesired models, which highlights the source model selection problem. To address it, we first provide a theoretical analysis of this problem. We reveal two fundamental selection principles: transferability principle and diversity principle, and introduce a selection algorithm to integrate them. Then, considering the measu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;Kullback-Leibler&#25955;&#24230;&#19982;Cohen's Kappa&#30456;&#20851;&#32852;&#65292;&#38480;&#21046;&#20102;&#20998;&#31867;&#24615;&#33021;&#30340;&#26368;&#22823;&#38480;&#24230;</title><link>https://arxiv.org/abs/2403.01571</link><description>&lt;p&gt;
&#23558;Kullback-Leibler&#25955;&#24230;&#19982;Cohen's Kappa&#30456;&#20851;&#32852;&#65292;&#38480;&#21046;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01571
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;Kullback-Leibler&#25955;&#24230;&#19982;Cohen's Kappa&#30456;&#20851;&#32852;&#65292;&#38480;&#21046;&#20102;&#20998;&#31867;&#24615;&#33021;&#30340;&#26368;&#22823;&#38480;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#20272;&#35745;&#25351;&#26631;&#26469;&#35780;&#20272;&#30340;&#65292;&#36890;&#24120;&#26159;&#20174;&#28151;&#28102;&#30697;&#38453;&#20013;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#21644;&#20132;&#21449;&#39564;&#35777;&#24471;&#20986;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24182;&#19981;&#35777;&#26126;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#21487;&#20197;&#20351;&#29992;&#20449;&#24687;&#36317;&#31163;&#24230;&#37327;&#26469;&#20272;&#35745;&#38169;&#35823;&#29575;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#28151;&#28102;&#30697;&#38453;&#24050;&#34987;&#21046;&#23450;&#20026;&#31526;&#21512;Chernoff-Stein&#24341;&#29702;&#12290;&#36825;&#23558;&#38169;&#35823;&#29575;&#19982;&#25551;&#36848;&#20004;&#20010;&#31867;&#21035;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30456;&#20851;&#32852;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#65292;&#23558;Cohen's Kappa&#19982;&#30005;&#38459;&#22120;&#24179;&#22343;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#36825;&#26159;&#20004;&#20010;Kullback-Leibler&#25955;&#24230;&#30340;&#24182;&#32852;&#30005;&#38459;&#22120;&#32452;&#21512;&#12290;&#30005;&#38459;&#22120;&#24179;&#22343;&#36317;&#31163;&#20855;&#26377;&#27604;&#29305;&#21333;&#20301;&#65292;&#21487;&#20197;&#20174;&#21516;&#19968;&#35757;&#32451;&#25968;&#25454;&#20013;&#20351;&#29992;&#20998;&#31867;&#31639;&#27861;&#20272;&#35745;&#30340;KullBack-Leibler&#25955;&#24230;&#30340;kNN&#20272;&#35745;&#20013;&#24471;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01571v1 Announce Type: cross  Abstract: The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01570</link><description>&lt;p&gt;
SERVAL&#65306;&#22402;&#30452;&#27169;&#22411;&#21644;LLM&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#23454;&#29616;&#38646;-shot&#32423;&#21035;&#30340;&#21307;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20986;&#23545;&#36890;&#29992;&#21644;&#24120;&#35782;&#38382;&#39064;&#21331;&#36234;&#30340;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22402;&#30452;&#30693;&#35782;&#26041;&#38754;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#22402;&#30452;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#19987;&#23478;&#21442;&#19982;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#22686;&#24378;&#27169;&#22411;&#22402;&#30452;&#33021;&#21147;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23545;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#36827;&#34892;&#26080;&#30417;&#30563;&#24320;&#21457;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SERVAL&#21033;&#29992;LLMs&#30340;&#38646;-shot&#36755;&#20986;&#20316;&#20026;&#27880;&#37322;&#65292;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#26469;&#20174;&#22836;&#24320;&#22987;&#25945;&#25480;&#19968;&#20010;&#24378;&#22823;&#30340;&#22402;&#30452;&#27169;&#22411;&#12290;&#21453;&#36807;&#26469;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#22402;&#30452;&#27169;&#22411;&#24341;&#23548;LLM&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#38646;-shot&#33021;&#21147;&#65292;&#36880;&#27493;&#25913;&#36827;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#23427;&#20204;&#19982;&#20803;&#23398;&#20064;&#30340;&#20851;&#32852;&#65292;&#29992;&#20110;&#30417;&#30563;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.01554</link><description>&lt;p&gt;
&#29992;&#20110;&#30417;&#30563;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Supervised Online Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#23427;&#20204;&#19982;&#20803;&#23398;&#20064;&#30340;&#20851;&#32852;&#65292;&#29992;&#20110;&#30417;&#30563;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#24050;&#25104;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25110;&#38899;&#39057;&#22788;&#29702;&#65289;&#30340;&#20027;&#23548;&#26550;&#26500;&#65292;&#29978;&#33267;&#34987;&#32771;&#34385;&#29992;&#20110;&#38750;&#33258;&#28982;&#39034;&#24207;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#23427;&#20204;&#33021;&#22815;&#20851;&#27880;&#21644;&#22788;&#29702;&#19968;&#32452;&#26631;&#35760;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#20351;&#20854;&#33021;&#22815;&#21457;&#23637;&#20986;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#31350;&#12290;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#36866;&#24212;&#38750;&#38745;&#24577;&#25968;&#25454;&#27969;&#65292;&#26368;&#23567;&#21270;&#32047;&#31215;&#30340;&#19979;&#19968;&#27493;&#39044;&#27979;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01554v1 Announce Type: new  Abstract: Transformers have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification. Their ability to attend to and to process a set of tokens as context enables them to develop in-context few-shot learning abilities. However, their potential for online continual learning remains relatively unexplored. In online continual learning, a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss. We focus on the supervised online continual learning setting, where we learn a predictor $x_t \rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the in-context learning capabilities of transformers and their connection to meta-learning, we propose a method that leverages these strengths for online continual learning. Our approach e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#39640;&#32500;&#24230;&#21644;&#26377;&#38480;&#31354;&#38388;&#20998;&#36776;&#29575;&#25361;&#25112;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#25968;&#25454;&#65292;&#24212;&#29992;&#23545;&#25239;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#20811;&#26381;&#39046;&#22495;&#24046;&#24322;&#21644;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#23450;&#21046;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36866;&#37197;HSI&#25968;&#25454;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.01546</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21333;&#27169;&#21644;&#22810;&#27169;&#35774;&#32622;&#19979;&#36827;&#34892;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Image Analysis in Single-Modal and Multimodal setting using Deep Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#39640;&#32500;&#24230;&#21644;&#26377;&#38480;&#31354;&#38388;&#20998;&#36776;&#29575;&#25361;&#25112;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#25968;&#25454;&#65292;&#24212;&#29992;&#23545;&#25239;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#20811;&#26381;&#39046;&#22495;&#24046;&#24322;&#21644;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#23450;&#21046;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36866;&#37197;HSI&#25968;&#25454;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#25104;&#20687;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#20809;&#35889;&#20998;&#36776;&#29575;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#21644;&#35206;&#30422;&#25552;&#20379;&#20102;&#31934;&#20934;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#39640;&#32500;&#24230;&#21644;&#26377;&#38480;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#31561;&#25361;&#25112;&#24433;&#21709;&#20102;&#20854;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#38598;&#25104;&#21270;&#26041;&#24335;&#39640;&#25928;&#22788;&#29702;&#12289;&#25552;&#21462;&#29305;&#24449;&#21644;&#20998;&#31867;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#20102;&#22686;&#24378;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;&#25972;&#21512;&#26469;&#33258;LiDAR&#21644;SAR&#25968;&#25454;&#31561;&#20114;&#34917;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#23545;&#25239;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#20811;&#26381;&#22240;&#39046;&#22495;&#24046;&#24322;&#21644;&#32570;&#22833;&#27169;&#24577;&#32780;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;HSI&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#37327;&#36523;&#23450;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32500;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#20854;&#36830;&#32493;&#20809;&#35889;&#32500;&#24230;&#12290;&#26550;&#26500;&#20869;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#21644;&#21453;&#39304;&#36830;&#25509;&#31561;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01546v1 Announce Type: cross  Abstract: Hyperspectral imaging provides precise classification for land use and cover due to its exceptional spectral resolution. However, the challenges of high dimensionality and limited spatial resolution hinder its effectiveness. This study addresses these challenges by employing deep learning techniques to efficiently process, extract features, and classify data in an integrated manner. To enhance spatial resolution, we integrate information from complementary modalities such as LiDAR and SAR data through multimodal learning. Moreover, adversarial learning and knowledge distillation are utilized to overcome issues stemming from domain disparities and missing modalities. We also tailor deep learning architectures to suit the unique characteristics of HSI data, utilizing 1D convolutional and recurrent neural networks to handle its continuous spectral dimension. Techniques like visual attention and feedback connections within the architecture
&lt;/p&gt;</description></item><item><title>&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#21644;&#27169;&#22411;&#32858;&#21512;&#65292;&#36890;&#36807;&#37327;&#21270;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#34920;&#29616;&#20986;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01540</link><description>&lt;p&gt;
&#20998;&#23618;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01540
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#21644;&#27169;&#22411;&#32858;&#21512;&#65292;&#36890;&#36807;&#37327;&#21270;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#34920;&#29616;&#20986;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#38598;&#21512;&#20013;&#32467;&#21512;&#20102;&#37327;&#21270;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#24377;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38598;&#21512;&#20869;&#36845;&#20195;&#20013;&#32467;&#21512;&#20102;&#26799;&#24230;&#32858;&#21512;&#21644;&#38598;&#21512;&#38388;&#36845;&#20195;&#20013;&#30340;&#27169;&#22411;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#26512;&#26694;&#26550;&#26469;&#35780;&#20272;&#20854;&#26368;&#20248;&#24615;&#24046;&#36317;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23558;&#36825;&#20123;&#26041;&#38754;&#19982;&#20256;&#32479;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#34920;&#36848;&#65292;&#20197;&#23548;&#20986;&#23553;&#38381;&#24418;&#24335;&#30340;&#26368;&#20248;&#31995;&#32479;&#21442;&#25968;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#21442;&#25968;&#19978;&#22987;&#32456;&#23454;&#29616;&#39640;&#23398;&#20064;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#22330;&#26223;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20998;&#23618;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01540v1 Announce Type: new  Abstract: This paper presents a novel hierarchical federated learning algorithm within multiple sets that incorporates quantization for communication-efficiency and demonstrates resilience to statistical heterogeneity. Unlike conventional hierarchical federated learning algorithms, our approach combines gradient aggregation in intra-set iterations with model aggregation in inter-set iterations. We offer a comprehensive analytical framework to evaluate its optimality gap and convergence rate, comparing these aspects with those of conventional algorithms. Additionally, we develop a problem formulation to derive optimal system parameters in a closed-form solution. Our findings reveal that our algorithm consistently achieves high learning accuracy over a range of parameters and significantly outperforms other hierarchical algorithms, particularly in scenarios with heterogeneous data distributions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01537</link><description>&lt;p&gt;
&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#29992;&#20110;&#20154;&#32676;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Mixed-Strategy Nash Equilibrium for Crowd Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#20154;&#32676;&#23548;&#33322;&#25214;&#21040;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20154;&#32676;&#20013;&#19981;&#30830;&#23450;&#20294;&#21512;&#20316;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#22826;&#39640;&#65292;&#26080;&#27861;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#23454;&#26102;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#25910;&#25947;&#20110;&#28151;&#21512;&#31574;&#30053;&#31038;&#20132;&#23548;&#33322;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#31574;&#30053;&#21021;&#22987;&#21270;&#20026;&#20174;&#20154;&#31867;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#26469;&#26500;&#24314;&#35813;&#28216;&#25103;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20154;&#32676;&#23548;&#33322;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#23548;&#33322;&#26041;&#27861;&#20013;&#65292;&#24182;&#21487;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041; CPU &#19978;&#23454;&#26102;&#36816;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20154;&#31867;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#20989;&#25968;&#30340;&#24555;&#36895;&#36941;&#21382;&#25628;&#32034;&#26041;&#27861;&#65292;&#20854;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26446;&#32676;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27979;&#35797;&#23637;&#31034;&#27604;&#29616;&#26377;&#31639;&#27861;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2403.01536</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#20989;&#25968;&#30340;&#24555;&#36895;&#36941;&#21382;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fast Ergodic Search with Kernel Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01536
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#20989;&#25968;&#30340;&#24555;&#36895;&#36941;&#21382;&#25628;&#32034;&#26041;&#27861;&#65292;&#20854;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26446;&#32676;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27979;&#35797;&#23637;&#31034;&#27604;&#29616;&#26377;&#31639;&#27861;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36941;&#21382;&#25628;&#32034;&#20351;&#24471;&#23545;&#20449;&#24687;&#20998;&#24067;&#36827;&#34892;&#26368;&#20339;&#25506;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#23545;&#25628;&#32034;&#31354;&#38388;&#30340;&#28176;&#36817;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#25351;&#25968;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#36941;&#21382;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#26680;&#30340;&#36941;&#21382;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#20174;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25512;&#24191;&#21040;&#26446;&#32676;&#19978;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25152;&#24314;&#35758;&#30340;&#24230;&#37327;&#19982;&#26631;&#20934;&#36941;&#21382;&#24230;&#37327;&#19968;&#33268;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26680;&#36941;&#21382;&#24230;&#37327;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#36825;&#20351;&#24471;&#36712;&#36857;&#20248;&#21270;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;&#20840;&#38754;&#30340;&#25968;&#20540;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33267;&#23569;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01536v1 Announce Type: cross  Abstract: Ergodic search enables optimal exploration of an information distribution while guaranteeing the asymptotic coverage of the search space. However, current methods typically have exponential computation complexity in the search space dimension and are restricted to Euclidean space. We introduce a computationally efficient ergodic search method. Our contributions are two-fold. First, we develop a kernel-based ergodic metric and generalize it from Euclidean space to Lie groups. We formally prove the proposed metric is consistent with the standard ergodic metric while guaranteeing linear complexity in the search space dimension. Secondly, we derive the first-order optimality condition of the kernel ergodic metric for nonlinear systems, which enables efficient trajectory optimization. Comprehensive numerical benchmarks show that the proposed method is at least two orders of magnitude faster than the state-of-the-art algorithm. Finally, we d
&lt;/p&gt;</description></item><item><title>NGG&#26159;&#19968;&#20010;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;&#65292;&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#22270;&#27169;&#24335;&#21644;&#25511;&#21046;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01535</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65306;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01535
&lt;/p&gt;
&lt;p&gt;
NGG&#26159;&#19968;&#20010;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;&#65292;&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#22270;&#27169;&#24335;&#21644;&#25511;&#21046;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#38754;&#20020;&#30528;&#29983;&#25104;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#29305;&#23450;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65288;NGG&#65289;&#36825;&#19968;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#29983;&#25104;&#12290;NGG&#23637;&#31034;&#20102;&#23545;&#22797;&#26434;&#22270;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#23545;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;NGG&#21033;&#29992;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#21387;&#32553;&#65292;&#21033;&#29992;&#22312;&#28508;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#30001;&#24635;&#32467;&#22270;&#32479;&#35745;&#20449;&#24687;&#30340;&#21521;&#37327;&#25351;&#23548;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NGG&#22312;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#25429;&#25417;&#26399;&#26395;&#22270;&#23646;&#24615;&#24182;&#25512;&#24191;&#21040;&#26410;&#35265;&#22270;&#24418;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01535v1 Announce Type: new  Abstract: Graph generation has emerged as a crucial task in machine learning, with significant challenges in generating graphs that accurately reflect specific properties. Existing methods often fall short in efficiently addressing this need as they struggle with the high-dimensional complexity and varied nature of graph properties. In this paper, we introduce the Neural Graph Generator (NGG), a novel approach which utilizes conditioned latent diffusion models for graph generation. NGG demonstrates a remarkable capacity to model complex graph patterns, offering control over the graph generation process. NGG employs a variational graph autoencoder for graph compression and a diffusion process in the latent vector space, guided by vectors summarizing graph statistics. We demonstrate NGG's versatility across various graph generation tasks, showing its capability to capture desired graph properties and generalize to unseen graphs. This work signifies 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#25506;&#32034;&#38271;&#26399;&#27515;&#20129;&#29575;&#39044;&#27979;&#65292;&#20026;&#24515;&#33039;&#30149;&#24739;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.01533</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#25910;&#32553;&#26102;&#38388;&#38388;&#38548;&#21644;&#26085;&#24120;&#25910;&#38598;&#30340;&#20020;&#24202;&#25968;&#25454;&#39044;&#27979;&#24613;&#24615;&#24515;&#32908;&#26775;&#27515;&#21518;&#30340;&#38271;&#26399;&#27515;&#20129;&#29575;
&lt;/p&gt;
&lt;p&gt;
Machine learning predicts long-term mortality after acute myocardial infarction using systolic time intervals and routinely collected clinical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#25506;&#32034;&#38271;&#26399;&#27515;&#20129;&#29575;&#39044;&#27979;&#65292;&#20026;&#24515;&#33039;&#30149;&#24739;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#20272;&#35745;&#24515;&#33039;&#24739;&#32773;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#21512;&#24182;&#30151;&#26159;&#20248;&#20808;&#32771;&#34385;&#36830;&#32493;&#29983;&#29702;&#30417;&#27979;&#21644;&#26032;&#30103;&#27861;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#24739;&#26377;&#24515;&#33039;&#30149;&#30340;&#24739;&#32773;&#30701;&#26399;&#27515;&#20129;&#29575;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#29992;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38271;&#26399;&#27515;&#20129;&#29575;&#39044;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#20004;&#20010;&#26368;&#36817;&#24341;&#20837;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#23545;&#38271;&#26399;&#27515;&#20129;&#29575;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#21488;&#28286;&#20013;&#22269;&#21355;&#29983;&#31119;&#21033;&#37096;CCHIA&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#12290;&#21307;&#30103;&#35760;&#24405;&#29992;&#20110;&#25910;&#38598;&#21253;&#25324;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;BMI&#12289;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#65288;PCI&#65289;&#29366;&#24577;&#21644;&#39640;&#34880;&#21387;&#12289;&#34880;&#33026;&#24322;&#24120;&#12289;ST&#27573;&#25260;&#39640;&#22411;&#24515;&#32908;&#26775;&#27515;&#65288;STEMI&#65289;&#21644;&#38750;STEMI&#31561;&#21512;&#24182;&#30151;&#22312;&#20869;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01533v1 Announce Type: cross  Abstract: Precise estimation of cardiac patients' current and future comorbidities is an important factor in prioritizing continuous physiological monitoring and new therapies. ML models have shown satisfactory performance in short-term mortality prediction of patients with heart disease, while their utility in long-term predictions is limited. This study aims to investigate the performance of tree-based ML models on long-term mortality prediction and the effect of two recently introduced biomarkers on long-term mortality. This study utilized publicly available data from CCHIA at the Ministry of Health and Welfare, Taiwan, China. Medical records were used to gather demographic and clinical data, including age, gender, BMI, percutaneous coronary intervention (PCI) status, and comorbidities such as hypertension, dyslipidemia, ST-segment elevation myocardial infarction (STEMI), and non-STEMI. Using medical and demographic records as well as two rec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#23616;&#37096;&#31639;&#23376;&#26597;&#25214;&#31639;&#27861;Phi Method&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#23398;&#20064;&#21442;&#25968;&#21160;&#21147;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#27979;&#31995;&#32479;&#22312;&#26410;&#35265;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.01532</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23616;&#37096;&#31639;&#23376;&#26597;&#25214;&#29992;&#20110;&#31561;&#31163;&#23376;&#20307;&#31995;&#32479;&#31616;&#21270;&#24314;&#27169;&#65306;II. &#24212;&#29992;&#20110;&#21442;&#25968;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Data-driven local operator finding for reduced-order modelling of plasma systems: II. Application to parametric dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#23616;&#37096;&#31639;&#23376;&#26597;&#25214;&#31639;&#27861;Phi Method&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#23398;&#20064;&#21442;&#25968;&#21160;&#21147;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#27979;&#31995;&#32479;&#22312;&#26410;&#35265;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#36890;&#24120;&#34920;&#29616;&#20986;&#21463;&#21508;&#31181;&#21442;&#25968;&#24433;&#21709;&#30340;&#21160;&#24577;&#65292;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#26159;&#22266;&#26377;&#30340;&#20063;&#21487;&#20197;&#26159;&#22806;&#37096;&#21487;&#25511;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#33021;&#22815;&#21487;&#38752;&#25429;&#25417;&#36825;&#20123;&#21442;&#25968;&#34892;&#20026;&#30340;&#27169;&#22411;&#12290;&#31561;&#31163;&#23376;&#20307;&#25216;&#26415;&#26159;&#36825;&#31181;&#31995;&#32479;&#30340;&#20856;&#22411;&#20363;&#23376;&#12290;&#20363;&#22914;&#65292;&#20027;&#23548;&#38669;&#23572;&#25512;&#36827;&#22120;&#65288;&#19968;&#31181;&#33322;&#22825;&#22120;&#25512;&#36827;&#25216;&#26415;&#65289;&#20840;&#23616;&#21160;&#24577;&#30340;&#29616;&#35937;&#20250;&#38543;&#30528;&#21508;&#31181;&#21442;&#25968;&#21464;&#21270;&#65292;&#27604;&#22914;&#8220;&#33258;&#32500;&#25345;&#30005;&#22330;&#8221;&#12290;&#22312;&#26412;&#25991;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#32487;&#32493;&#22312;&#31532;&#19968;&#37096;&#20998;&#20013;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26032;&#39062;&#25968;&#25454;&#39537;&#21160;&#23616;&#37096;&#31639;&#23376;&#26597;&#25214;&#31639;&#27861;Phi Method&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#21442;&#25968;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#39044;&#27979;&#31995;&#32479;&#22312;&#26410;&#35265;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#65306;&#21442;&#25968;&#21270;Phi Method&#21644;&#38598;&#21512;Phi Method&#65292;&#24182;&#36890;&#36807;&#20108;&#32500;&#27969;&#20307;-&#31359;&#36807;&#22278;&#26609;&#21644;&#19968;&#32500;&#38669;&#23572;&#25512;&#36827;&#22120;&#31561;&#31163;&#23376;&#25918;&#30005;&#38382;&#39064;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#22312;&#27969;&#20307;&#26696;&#20363;&#20013;&#19982;&#21442;&#25968;&#21270;OPT-DMD&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01532v1 Announce Type: cross  Abstract: Real-world systems often exhibit dynamics influenced by various parameters, either inherent or externally controllable, necessitating models capable of reliably capturing these parametric behaviors. Plasma technologies exemplify such systems. For example, phenomena governing global dynamics in Hall thrusters (a spacecraft propulsion technology) vary with various parameters, such as the "self-sustained electric field". In this Part II, following on the introduction of our novel data-driven local operator finding algorithm, Phi Method, in Part I, we showcase the method's effectiveness in learning parametric dynamics to predict system behavior across unseen parameter spaces. We present two adaptations: the "parametric Phi Method" and the "ensemble Phi Method", which are demonstrated through 2D fluid-flow-past-a-cylinder and 1D Hall-thruster-plasma-discharge problems. Comparative evaluation against parametric OPT-DMD in the fluid case demo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;"Phi Method"&#65292;&#36890;&#36807;&#32422;&#26463;&#22238;&#24402;&#21644;&#20505;&#36873;&#39033;&#24211;&#21457;&#29616;&#31163;&#25955;&#21270;&#30340;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#25104;&#21151;&#23548;&#20986;&#20102;&#21487;&#38752;&#19988;&#31283;&#20581;&#30340;&#31616;&#21270;&#31561;&#31163;&#23376;&#20307;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.01523</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31561;&#31163;&#23376;&#20307;&#31995;&#32479;&#31616;&#21270;&#24314;&#27169;&#20013;&#30340;&#26412;&#22320;&#31639;&#23376;&#26597;&#25214;&#65306;I. &#27010;&#24565;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Data-driven local operator finding for reduced-order modelling of plasma systems: I. Concept and verifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01523
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;"Phi Method"&#65292;&#36890;&#36807;&#32422;&#26463;&#22238;&#24402;&#21644;&#20505;&#36873;&#39033;&#24211;&#21457;&#29616;&#31163;&#25955;&#21270;&#30340;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#25104;&#21151;&#23548;&#20986;&#20102;&#21487;&#38752;&#19988;&#31283;&#20581;&#30340;&#31616;&#21270;&#31561;&#31163;&#23376;&#20307;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#24191;&#27867;&#36861;&#27714;&#20294;&#38590;&#20197;&#25214;&#21040;&#30340;&#31616;&#21270;&#31561;&#31163;&#23376;&#20307;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#19981;&#21516;&#35774;&#32622;&#21644;&#37197;&#32622;&#19979;&#30340;&#31561;&#31163;&#23376;&#20307;&#34892;&#20026;&#12290;&#30001;&#20110;&#20854;&#28508;&#21147;&#20419;&#36827;&#31185;&#23398;&#30740;&#31350;&#24182;&#21152;&#24555;&#31561;&#31163;&#23376;&#20307;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36807;&#21435;&#21313;&#24180;&#23545;&#36825;&#31867;&#27169;&#22411;&#30340;&#38656;&#27714;&#24613;&#21095;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;Phi Method&#8221;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#35745;&#31639;&#33021;&#21147;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#31532;&#19968;&#37096;&#20998;&#20171;&#32461;&#20102;&#36825;&#19968;&#26032;&#39062;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21463;&#25968;&#20540;&#31163;&#25955;&#26041;&#26696;&#21551;&#21457;&#30340;&#20505;&#36873;&#39033;&#24211;&#19978;&#30340;&#32422;&#26463;&#22238;&#24402;&#26469;&#21457;&#29616;&#31163;&#25955;&#21270;&#30340;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Phi Method&#22312;&#19977;&#20010;&#27979;&#35797;&#26696;&#20363;&#20013;&#65288;Lorenz&#21560;&#24341;&#23376;&#12289;&#22278;&#26609;&#20307;&#27969;&#21160;&#20197;&#21450;&#20195;&#34920;&#24615;&#30340;&#19968;&#32500;Hall&#25512;&#36827;&#22120;&#31561;&#31163;&#23376;&#20307;&#65289;&#23548;&#20986;&#21487;&#38752;&#19988;&#31283;&#20581;&#30340;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#30340;&#26377;&#25928;&#24615;&#12290;&#31532;&#20108;&#37096;&#20998;&#23558;&#28145;&#20837;&#25506;&#35752;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#21160;&#24577;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01523v1 Announce Type: cross  Abstract: Reduced-order plasma models that can efficiently predict plasma behavior across various settings and configurations are highly sought after yet elusive. The demand for such models has surged in the past decade due to their potential to facilitate scientific research and expedite the development of plasma technologies. In line with the advancements in computational power and data-driven methods, we introduce the "Phi Method" in this two-part article. Part I presents this novel algorithm, which employs constrained regression on a candidate term library informed by numerical discretization schemes to discover discretized systems of differential equations. We demonstrate Phi Method's efficacy in deriving reliable and robust reduced-order models (ROMs) for three test cases: the Lorenz attractor, flow past a cylinder, and a 1D Hall-thruster-representative plasma. Part II will delve into the method's application for parametric dynamics discov
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#36866;&#24212;&#21487;&#20197;&#23558;&#21442;&#25968;&#36716;&#21464;&#20026;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#25552;&#20379;&#19968;&#31181;&#20855;&#26377;&#20869;&#23384;&#26435;&#37325;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#24418;&#24335;&#65292;&#26356;&#31526;&#21512;&#31070;&#32463;&#31185;&#23398;&#20013;&#35760;&#24518;&#27010;&#24565;&#65292;&#19988;&#22312;&#25552;&#21319;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#26102;&#23588;&#20854;&#26377;&#36259;&#12290;</title><link>https://arxiv.org/abs/2403.01518</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21160;&#24577;&#35780;&#20272;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01518
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36866;&#24212;&#21487;&#20197;&#23558;&#21442;&#25968;&#36716;&#21464;&#20026;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#25552;&#20379;&#19968;&#31181;&#20855;&#26377;&#20869;&#23384;&#26435;&#37325;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#24418;&#24335;&#65292;&#26356;&#31526;&#21512;&#31070;&#32463;&#31185;&#23398;&#20013;&#35760;&#24518;&#27010;&#24565;&#65292;&#19988;&#22312;&#25552;&#21319;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#26102;&#23588;&#20854;&#26377;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#20063;&#21363;&#31216;&#20026;&#21160;&#24577;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33324;&#35748;&#20026;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24378;&#35843;&#22312;&#32447;&#35843;&#25972;&#23558;&#21442;&#25968;&#36716;&#21464;&#20026;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#20869;&#23384;&#26435;&#37325;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#24418;&#24335;&#65292;&#26356;&#31526;&#21512;&#31070;&#32463;&#31185;&#23398;&#20013;&#35760;&#24518;&#27010;&#24565;&#30340;&#24605;&#36335;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36866;&#24212;&#36895;&#24230;&#65288;&#20197;&#26679;&#26412;&#25928;&#29575;&#34913;&#37327;&#65289;&#12289;&#23545;&#25972;&#20307;&#20998;&#24067;&#24615;&#28418;&#31227;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#25191;&#34892;&#26799;&#24230;&#35745;&#31639;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;&#20309;&#26102;&#22312;&#32447;&#35843;&#25972;&#23588;&#20026;&#26377;&#36259;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;&#32447;&#35843;&#25972;&#20351;&#24471;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#20869;&#23384;&#22312;&#27010;&#24565;&#19978;&#30340;&#21306;&#20998;&#27169;&#31946;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01518v1 Announce Type: new  Abstract: We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;GNNs&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26041;&#24335;&#35782;&#21035;&#29305;&#23450;&#31867;&#22411;&#32593;&#32476;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#24341;&#20837;&#20102;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;&#26469;&#33719;&#21462;&#22270;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2403.01501</link><description>&lt;p&gt;
&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32593;&#32476;&#27969;&#37327;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;GNNs&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26041;&#24335;&#35782;&#21035;&#29305;&#23450;&#31867;&#22411;&#32593;&#32476;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#24341;&#20837;&#20102;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;&#26469;&#33719;&#21462;&#22270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#20854;&#36866;&#21512;&#34920;&#31034;&#32593;&#32476;&#27969;&#37327;&#32780;&#24341;&#36215;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;NIDS&#26041;&#27861;&#26159;&#26377;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#30340;&#12290;&#32593;&#32476;&#27969;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#20026;&#30417;&#30563;&#26631;&#31614;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#65292;&#23548;&#33268;NIDS&#38590;&#20197;&#36866;&#24212;&#28508;&#22312;&#22797;&#26434;&#30340;&#25915;&#20987;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;&#29616;&#26377;&#22522;&#20110;GNN&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#30528;&#37325;&#20110;&#23558;&#32593;&#32476;&#27969;&#20998;&#31867;&#20026;&#33391;&#24615;&#25110;&#38750;&#33391;&#24615;&#65292;&#38590;&#20197;&#25581;&#31034;&#23454;&#38469;&#25915;&#20987;&#31867;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#24212;&#29992;GNNs&#35782;&#21035;&#29305;&#23450;&#31867;&#22411;&#32593;&#32476;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#33719;&#21462;&#22270;&#23884;&#20837;&#65292;&#24341;&#20837;&#20102;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#23558;&#36793;&#20449;&#24687;&#35270;&#20026;&#21807;&#19968;&#22522;&#26412;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01501v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have garnered intensive attention for Network Intrusion Detection System (NIDS) due to their suitability for representing the network traffic flows. However, most present GNN-based methods for NIDS are supervised or semi-supervised. Network flows need to be manually annotated as supervisory labels, a process that is time-consuming or even impossible, making NIDS difficult to adapt to potentially complex attacks, especially in large-scale real-world scenarios. The existing GNN-based self-supervised methods focus on the binary classification of network flow as benign or not, and thus fail to reveal the types of attack in practice. This paper studies the application of GNNs to identify the specific types of network flows in an unsupervised manner. We first design an encoder to obtain graph embedding, that introduces the graph attention mechanism and considers the edge information as the only essential factor. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#28789;&#27963;&#23398;&#20064;&#36825;&#20123;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2403.01499</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Normalising Flow-based Differentiable Particle Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#28789;&#27963;&#23398;&#20064;&#36825;&#20123;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#23558;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#31890;&#23376;&#28388;&#27874;&#22120;&#20013;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#20363;&#22914;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23545;&#38750;&#32447;&#24615;&#38750;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#32852;&#21512;&#39034;&#24207;&#29366;&#24577;&#20272;&#35745;&#21644;&#27169;&#22411;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#20027;&#35201;&#30001;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#19981;&#20801;&#35768;&#23494;&#24230;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#35201;&#20040;&#21463;&#38480;&#20110;&#33258;&#20030;&#31890;&#23376;&#28388;&#27874;&#26694;&#26550;&#65292;&#35201;&#20040;&#37319;&#29992;&#39044;&#23450;&#20041;&#30340;&#20998;&#24067;&#31995;&#21015;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#65288;&#26465;&#20214;&#65289;&#27491;&#35268;&#21270;&#27969;&#26500;&#24314;&#21160;&#24577;&#27169;&#22411;&#12289;&#25552;&#35758;&#20998;&#24067;&#21644;&#27979;&#37327;&#27169;&#22411;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#26694;&#26550;&#12290;&#36825;&#19981;&#20165;&#20351;&#20854;&#33021;&#22815;&#20135;&#29983;&#26377;&#25928;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#36824;&#20351;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#23398;&#20064;&#36825;&#20123;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01499v1 Announce Type: new  Abstract: Recently, there has been a surge of interest in incorporating neural networks into particle filters, e.g. differentiable particle filters, to perform joint sequential state estimation and model learning for non-linear non-Gaussian state-space models in complex environments. Existing differentiable particle filters are mostly constructed with vanilla neural networks that do not allow density estimation. As a result, they are either restricted to a bootstrap particle filtering framework or employ predefined distribution families (e.g. Gaussian distributions), limiting their performance in more complex real-world scenarios. In this paper we present a differentiable particle filtering framework that uses (conditional) normalising flows to build its dynamic model, proposal distribution, and measurement model. This not only enables valid probability densities but also allows the proposed method to adaptively learn these modules in a flexible w
&lt;/p&gt;</description></item><item><title>ConvTimeNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#27573;&#21644;&#20840;&#21367;&#31215;&#22359;&#35774;&#35745;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#20840;&#23616;&#24207;&#21015;&#21644;&#36328;&#21464;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01493</link><description>&lt;p&gt;
ConvTimeNet: &#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01493
&lt;/p&gt;
&lt;p&gt;
ConvTimeNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#27573;&#21644;&#20840;&#21367;&#31215;&#22359;&#35774;&#35745;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#20840;&#23616;&#24207;&#21015;&#21644;&#36328;&#21464;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ConvTimeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#32593;&#32476;&#65292;&#26088;&#22312;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#32593;&#32476;&#30340;&#20851;&#38190;&#35774;&#35745;&#26159;&#20026;&#20102;&#20811;&#26381;&#20256;&#32479;&#21367;&#31215;&#32593;&#32476;&#30340;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26102;&#38388;&#24207;&#21015;&#21010;&#20998;&#20026;&#23376;&#24207;&#21015;&#32423;&#34917;&#19969;&#30340;&#33258;&#36866;&#24212;&#20998;&#27573;&#65292;&#23558;&#20854;&#35270;&#20026;&#22522;&#26412;&#24314;&#27169;&#21333;&#20803;&#12290;&#36825;&#31181;&#35774;&#32622;&#36991;&#20813;&#20102;&#19982;&#21407;&#22987;&#28857;&#32423;&#26102;&#38388;&#27493;&#38271;&#30456;&#20851;&#32852;&#30340;&#31232;&#30095;&#35821;&#20041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#21367;&#31215;&#22359;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#38598;&#25104;&#28145;&#24230;&#21644;&#28857;&#21367;&#31215;&#25805;&#20316;&#65292;&#36981;&#24490;Transformer&#32534;&#30721;&#22120;&#20013;&#37319;&#29992;&#30340;&#20808;&#36827;&#26500;&#24314;&#22359;&#39118;&#26684;&#12290;&#36825;&#20010;&#39592;&#24178;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#24207;&#21015;&#21644;&#36328;&#21464;&#37327;&#20381;&#36182;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#34701;&#21512;&#20102;Transformer&#26550;&#26500;&#30340;&#20808;&#36827;&#24615;&#65292;&#36824;&#32487;&#25215;&#20102;&#21367;&#31215;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01493v1 Announce Type: new  Abstract: This paper introduces ConvTimeNet, a novel deep hierarchical fully convolutional network designed to serve as a general-purpose model for time series analysis. The key design of this network is twofold, designed to overcome the limitations of traditional convolutional networks. Firstly, we propose an adaptive segmentation of time series into sub-series level patches, treating these as fundamental modeling units. This setting avoids the sparsity semantics associated with raw point-level time steps. Secondly, we design a fully convolutional block by skillfully integrating deepwise and pointwise convolution operations, following the advanced building block style employed in Transformer encoders. This backbone network allows for the effective capture of both global sequence and cross-variable dependence, as it not only incorporates the advancements of Transformer architecture but also inherits the inherent properties of convolution. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#28857;&#20851;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;OOD&#25968;&#25454;&#24212;&#20855;&#26377;&#26356;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#31616;&#21333;&#30452;&#35273;&#65292;&#36890;&#36807;&#36817;&#20284;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#23454;&#29616;&#35813;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.01485</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#36827;&#34892;&#36817;&#20284;&#29992;&#20110;&#26816;&#27979;&#31163;&#32676;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#28857;&#20851;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;OOD&#25968;&#25454;&#24212;&#20855;&#26377;&#26356;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#31616;&#21333;&#30452;&#35273;&#65292;&#36890;&#36807;&#36817;&#20284;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#23454;&#29616;&#35813;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20284;&#28982;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#26159;&#36817;&#24180;&#26469;&#29992;&#20110;&#25311;&#21512;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#38899;&#39057;&#65289;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#12290;&#23427;&#20204;&#21487;&#20197;&#33258;&#28982;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20043;&#19968;&#65292;&#21363;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;Nalisnick&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#34920;&#26126;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22987;&#32456;&#20026;OOD&#25968;&#25454;&#25512;&#26029;&#20986;&#27604;&#23427;&#20204;&#35757;&#32451;&#36807;&#30340;&#25968;&#25454;&#26356;&#39640;&#30340;&#23545;&#25968;&#20284;&#28982;&#65292;&#26631;&#24535;&#30528;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#25968;&#25454;&#28857;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21442;&#25968;&#26799;&#24230;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#31616;&#21333;&#30452;&#35273;&#65292;&#21363;OOD&#25968;&#25454;&#30340;&#26799;&#24230;&#33539;&#25968;&#24212;&#35813;&#22823;&#20110;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#22320;&#23558;&#26799;&#24230;&#22823;&#23567;&#30340;&#24230;&#37327;&#37327;&#21270;&#20026;&#36817;&#20284;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#20855;&#26377;&#36739;&#22823;&#30340;&#32477;&#23545;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#26041;&#21521;&#24615;&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;Directional Graph Attention Network&#65288;DGAT&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#32467;&#21512;&#29305;&#24449;&#27880;&#24847;&#21147;&#21644;&#20840;&#23616;&#26041;&#21521;&#20449;&#24687;&#65292;&#36890;&#36807;&#26032;&#22411;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#20943;&#23569;&#33410;&#28857;&#20043;&#38388;&#30340;&#25193;&#25955;&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#25299;&#25169;&#24341;&#23548;&#30340;&#37051;&#22495;&#20462;&#21098;&#21644;&#36793;&#28155;&#21152;&#26426;&#21046;&#26469;&#25552;&#21319;&#24322;&#36136;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01475</link><description>&lt;p&gt;
&#20855;&#26377;&#26041;&#21521;&#24615;&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#24322;&#36136;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Heterophilic Graph with Directional Neighborhood Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01475
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#26041;&#21521;&#24615;&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;Directional Graph Attention Network&#65288;DGAT&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#32467;&#21512;&#29305;&#24449;&#27880;&#24847;&#21147;&#21644;&#20840;&#23616;&#26041;&#21521;&#20449;&#24687;&#65292;&#36890;&#36807;&#26032;&#22411;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#20943;&#23569;&#33410;&#28857;&#20043;&#38388;&#30340;&#25193;&#25955;&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#25299;&#25169;&#24341;&#23548;&#30340;&#37051;&#22495;&#20462;&#21098;&#21644;&#36793;&#28155;&#21152;&#26426;&#21046;&#26469;&#25552;&#21319;&#24322;&#36136;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20043;&#19968;&#65292;&#23427;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23398;&#20064;&#36793;&#32536;&#26435;&#37325;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#21482;&#21253;&#21547;&#20102;&#26469;&#33258;&#21363;&#26102;&#37051;&#22495;&#30340;&#20449;&#24687;&#65292;&#32570;&#20047;&#25429;&#33719;&#36828;&#31243;&#21644;&#20840;&#23616;&#22270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#24322;&#36136;&#22270;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#26041;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;DGAT&#65289;&#12290;DGAT&#33021;&#22815;&#23558;&#22522;&#20110;&#29305;&#24449;&#30340;&#27880;&#24847;&#21147;&#19982;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#30340;&#20840;&#23616;&#26041;&#21521;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#21487;&#20197;&#26126;&#26174;&#20943;&#23569;&#33410;&#28857;&#20043;&#38388;&#30340;&#25193;&#25955;&#36317;&#31163;&#12290;&#22522;&#20110;&#26032;&#30340;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#24341;&#23548;&#30340;&#37051;&#22495;&#20462;&#21098;&#21644;&#36793;&#28155;&#21152;&#26426;&#21046;&#65292;&#20197;&#28040;&#38500;&#22122;&#22768;&#21644;&#38480;&#21046;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01475v1 Announce Type: cross  Abstract: Graph Attention Network (GAT) is one of the most popular Graph Neural Network (GNN) architecture, which employs the attention mechanism to learn edge weights and has demonstrated promising performance in various applications. However, since it only incorporates information from immediate neighborhood, it lacks the ability to capture long-range and global graph information, leading to unsatisfactory performance on some datasets, particularly on heterophilic graphs. To address this limitation, we propose the Directional Graph Attention Network (DGAT) in this paper. DGAT is able to combine the feature-based attention with the global directional information extracted from the graph topology. To this end, a new class of Laplacian matrices is proposed which can provably reduce the diffusion distance between nodes. Based on the new Laplacian, topology-guided neighbour pruning and edge adding mechanisms are proposed to remove the noisy and cap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.01472</link><description>&lt;p&gt;
WARDEN&#65306;&#22810;&#26041;&#21521;&#32972;&#38376;&#27700;&#21360;&#29992;&#20110;Embedding-as-a-Service&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Embedding as a Service&#65288;EaaS&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;EaaS&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#65307;&#28982;&#32780;&#65292;&#36890;&#36807;&#21521;&#25991;&#26412;&#23884;&#20837;&#28155;&#21152;&#32972;&#38376;&#27700;&#21360;&#65292;&#24182;&#38543;&#21518;&#39564;&#35777;&#25915;&#20987;&#27169;&#22411;&#30340;&#21457;&#24067;&#21518;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#26368;&#36817;&#29992;&#20110;EaaS&#30340;&#27700;&#21360;&#31574;&#30053;EmbMarker&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CSE&#65288;Cluster&#12289;Selection&#12289;Elimination&#65289;&#25915;&#20987;&#65292;&#23427;&#33021;&#22815;&#31227;&#38500;&#32972;&#38376;&#27700;&#21360;&#21516;&#26102;&#20445;&#25345;&#23884;&#20837;&#30340;&#39640;&#25928;&#24615;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#27700;&#21360;&#26041;&#27861;&#26159;&#21487;&#20197;&#34987;&#31361;&#30772;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#23041;&#32961;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#20351;&#27700;&#21360;&#30340;&#31227;&#38500;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;WARDEN&#26174;&#33879;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01472v1 Announce Type: cross  Abstract: Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the ste
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35843;&#25972;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.01471</link><description>&lt;p&gt;
&#20445;&#25345;&#30456;&#20851;&#24615;&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preserving correlations: A statistical method for generating synthetic data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01471
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35843;&#25972;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#20010;&#33298;&#36866;&#30340;&#38544;&#31169;&#32423;&#21035;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#23458;&#25143;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#29992;&#20110;&#20998;&#26512;&#21407;&#22987;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22823;&#22411;&#33021;&#28304;&#30456;&#20851;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#22312;&#23450;&#24615;&#65288;&#20363;&#22914;&#36890;&#36807;&#21487;&#35270;&#21270;&#30456;&#20851;&#24615;&#22270;&#65289;&#21644;&#23450;&#37327;&#65288;&#20197;&#36866;&#24403;&#30340;$\ell^1$&#31867;&#22411;&#35823;&#24046;&#33539;&#25968;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#65289;&#26041;&#38754;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#26159;&#19968;&#33324;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#20351;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26399;&#26395;&#23427;&#21487;&#36866;&#29992;&#20110;&#27604;&#27492;&#22788;&#25351;&#31034;&#30340;&#26356;&#24191;&#27867;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01471v1 Announce Type: new  Abstract: We propose a method to generate statistically representative synthetic data. The main goal is to be able to maintain in the synthetic dataset the correlations of the features present in the original one, while offering a comfortable privacy level that can be eventually tailored on specific customer demands.   We describe in detail our algorithm used both for the analysis of the original dataset and for the generation of the synthetic data points. The approach is tested using a large energy-related dataset. We obtain good results both qualitatively (e.g. via vizualizing correlation maps) and quantitatively (in terms of suitable $\ell^1$-type error norms used as evaluation metrics).   The proposed methodology is general in the sense that it does not rely on the used test dataset. We expect it to be applicable in a much broader context than indicated here.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCTA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#30340;&#27169;&#22411;&#36866;&#24212;&#21644;&#22270;&#36866;&#24212;&#26469;&#35299;&#20915;&#26080;&#28304;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01467</link><description>&lt;p&gt;
&#21327;&#20316;&#36866;&#24212;&#65306;&#36890;&#36807;&#21452;&#21521;&#36866;&#24212;&#23454;&#29616;&#26080;&#28304;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCTA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#30340;&#27169;&#22411;&#36866;&#24212;&#21644;&#22270;&#36866;&#24212;&#26469;&#35299;&#20915;&#26080;&#28304;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UGDA&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#26631;&#35760;&#20016;&#23500;&#30340;&#28304;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#23436;&#20840;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22270;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#26631;&#35760;&#20016;&#23500;&#30340;&#28304;&#22270;&#25552;&#20379;&#30417;&#30563;&#20449;&#21495;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#65292;&#21407;&#22240;&#26159;&#35268;&#23450;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26080;&#28304;&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22330;&#26223;&#65292;&#35797;&#22270;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#32780;&#19981;&#20351;&#29992;&#26631;&#35760;&#30340;&#28304;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GraphCTA&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#31243;&#24207;&#21327;&#20316;&#22320;&#25191;&#34892;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#22270;&#33258;&#36866;&#24212;&#65306;&#65288;1&#65289;&#22522;&#20110;&#30446;&#26631;&#22270;&#20013;&#33410;&#28857;&#37051;&#22495;&#39044;&#27979;&#36827;&#34892;&#27169;&#22411;&#33258;&#36866;&#24212;&#65292;&#32771;&#34385;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65307;&#65288;2&#65289;&#36890;&#36807;&#37051;&#22495;&#23545;&#27604;&#24615;&#22320;&#26356;&#26032;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#26469;&#25191;&#34892;&#22270;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01467v1 Announce Type: cross  Abstract: Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph. However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph. Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#27010;&#29575;&#30340;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#38170;&#28857;&#21040;&#31867;&#21035;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#20197;&#21450;&#20174;&#26679;&#26412;&#21040;&#31867;&#21035;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#33719;&#24471;&#26679;&#26412;&#21644;&#38170;&#28857;&#30340;&#36719;&#26631;&#31614;&#30697;&#38453;&#65292;&#22686;&#24378;&#20102;&#32858;&#31867;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01460</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#27010;&#29575;&#30340;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
One-Step Multi-View Clustering Based on Transition Probability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#27010;&#29575;&#30340;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#38170;&#28857;&#21040;&#31867;&#21035;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#20197;&#21450;&#20174;&#26679;&#26412;&#21040;&#31867;&#21035;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#33719;&#24471;&#26679;&#26412;&#21644;&#38170;&#28857;&#30340;&#36719;&#26631;&#31614;&#30697;&#38453;&#65292;&#22686;&#24378;&#20102;&#32858;&#31867;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#38170;&#22270;&#65292;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24403;&#21069;&#26041;&#27861;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#19988;&#26410;&#20805;&#20998;&#32771;&#34385;&#36328;&#19981;&#21516;&#35270;&#35282;&#30340;&#34917;&#20805;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#36716;&#31227;&#27010;&#29575;&#30340;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;OSMVC-TP&#65289;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#27010;&#29575;&#26041;&#27861;&#65292;&#21033;&#29992;&#38170;&#22270;&#65292;&#34920;&#31034;&#26679;&#26412;&#21040;&#38170;&#28857;&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#20174;&#38170;&#28857;&#21040;&#31867;&#21035;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#24182;&#35745;&#31639;&#20174;&#26679;&#26412;&#21040;&#31867;&#21035;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#20174;&#32780;&#33719;&#24471;&#26679;&#26412;&#21644;&#38170;&#28857;&#30340;&#36719;&#26631;&#31614;&#30697;&#38453;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01460v1 Announce Type: new  Abstract: The large-scale multi-view clustering algorithms, based on the anchor graph, have shown promising performance and efficiency and have been extensively explored in recent years. Despite their successes, current methods lack interpretability in the clustering process and do not sufficiently consider the complementary information across different views. To address these shortcomings, we introduce the One-Step Multi-View Clustering Based on Transition Probability (OSMVC-TP). This method adopts a probabilistic approach, which leverages the anchor graph, representing the transition probabilities from samples to anchor points. Our method directly learns the transition probabilities from anchor points to categories, and calculates the transition probabilities from samples to categories, thus obtaining soft label matrices for samples and anchor points, enhancing the interpretability of clustering. Furthermore, to maintain consistency in labels ac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#25968;&#25454;&#28335;&#28304;&#21644;&#27169;&#22411;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#23494;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#31649;&#29702;&#26469;&#36861;&#36394;&#25968;&#25454;&#21464;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.01451</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#22686;&#24378;&#25968;&#25454;&#28335;&#28304;&#21644;&#27169;&#22411;&#36879;&#26126;&#24615; - &#19968;&#20010;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Data Provenance and Model Transparency in Federated Learning Systems - A Database Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01451
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#25968;&#25454;&#28335;&#28304;&#21644;&#27169;&#22411;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#23494;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#31649;&#29702;&#26469;&#36861;&#36394;&#25968;&#25454;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30830;&#20445;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#25968;&#25454;&#28335;&#28304;&#21644;&#27169;&#22411;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#31995;&#21015;&#21152;&#23494;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#31649;&#29702;&#26469;&#36319;&#36394;FL&#36807;&#31243;&#20013;&#25968;&#25454;&#30340;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01451v1 Announce Type: cross  Abstract: Federated Learning (FL) presents a promising paradigm for training machine learning models across decentralized edge devices while preserving data privacy. Ensuring the integrity and traceability of data across these distributed environments, however, remains a critical challenge. The ability to create transparent artificial intelligence, such as detailing the training process of a machine learning model, has become an increasingly prominent concern due to the large number of sensitive (hyper)parameters it utilizes; thus, it is imperative to strike a reasonable balance between openness and the need to protect sensitive information.   In this paper, we propose one of the first approaches to enhance data provenance and model transparency in federated learning systems. Our methodology leverages a combination of cryptographic techniques and efficient model management to track the transformation of data throughout the FL process, and seeks 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#35010;&#20026;Grid Station&#65288;GS&#65289;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#37096;&#20998;&#65292;&#23454;&#29616;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#36127;&#36733;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.01438</link><description>&lt;p&gt;
&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01438
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#35010;&#20026;Grid Station&#65288;GS&#65289;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#37096;&#20998;&#65292;&#23454;&#29616;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#36127;&#36733;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#23545;&#33021;&#28304;&#31649;&#29702;&#12289;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#21644;&#20379;&#38656;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#12290;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23548;&#33268;&#20102;&#20256;&#24863;&#22120;&#25968;&#25454;&#39537;&#21160;&#30340;&#36127;&#33655;&#39044;&#27979;&#38656;&#27714;&#12290;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#26234;&#33021;&#30005;&#34920;&#30340;&#25968;&#25454;&#35757;&#32451;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#38656;&#35201;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#24341;&#21457;&#20102;&#23545;&#32593;&#32476;&#35201;&#27714;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35010;&#23398;&#20064;&#30340;&#36127;&#33655;&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#19968;&#20010;&#29992;&#20110;&#27599;&#20010;Grid Station&#65288;GS&#65289;&#65292;&#36127;&#36131;&#19968;&#20010;&#25972;&#20010;&#31038;&#21306;&#30340;&#26234;&#33021;&#30005;&#34920;&#65307;&#21478;&#19968;&#20010;&#29992;&#20110;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#12290;&#23458;&#25143;&#26234;&#33021;&#30005;&#34920;&#19981;&#20849;&#20139;&#20854;&#25968;&#25454;&#65292;&#32780;&#26159;&#20351;&#29992;&#21508;&#33258;&#30340;GS&#27169;&#22411;&#25286;&#20998;&#36827;&#34892;&#21069;&#21521;&#20256;&#36882;&#65292;&#21482;&#23558;&#20854;&#28608;&#27963;&#19982;GS&#20849;&#20139;&#12290;&#22312;&#36825;&#19968;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;GS&#36127;&#36131;&#20026;&#20854;&#21508;&#33258;&#30340;&#31038;&#21306;&#35757;&#32451;&#20010;&#24615;&#21270;&#27169;&#22411;&#20998;&#35010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01438v1 Announce Type: new  Abstract: Accurate load forecasting is crucial for energy management, infrastructure planning, and demand-supply balancing. Smart meter data availability has led to the demand for sensor-based load forecasting. Conventional ML allows training a single global model using data from multiple smart meters requiring data transfer to a central server, raising concerns for network requirements, privacy, and security. We propose a split learning-based framework for load forecasting to alleviate this issue. We split a deep neural network model into two parts, one for each Grid Station (GS) responsible for an entire neighbourhood's smart meters and the other for the Service Provider (SP). Instead of sharing their data, client smart meters use their respective GSs' model split for forward pass and only share their activations with the GS. Under this framework, each GS is responsible for training a personalized model split for their respective neighbourhoods,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#25581;&#31034;&#20102;&#22312;SE(3)-&#19981;&#21464;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#19988;&#26080;&#38656;&#25237;&#24433;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#36335;&#24452;&#30340;&#24615;&#33021;&#21644;&#36895;&#24230;&#65292;&#24182;&#20026;&#20854;&#20182;&#31995;&#32479;&#30340;SE(3)-&#19981;&#21464;&#24615;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.01430</link><description>&lt;p&gt;
&#22312;SE(3)-&#19981;&#21464;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On Diffusion Process in SE(3)-invariant Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#25581;&#31034;&#20102;&#22312;SE(3)-&#19981;&#21464;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#19988;&#26080;&#38656;&#25237;&#24433;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#36335;&#24452;&#30340;&#24615;&#33021;&#21644;&#36895;&#24230;&#65292;&#24182;&#20026;&#20854;&#20182;&#31995;&#32479;&#30340;SE(3)-&#19981;&#21464;&#24615;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#23545;&#20855;&#26377;SE(3)-&#19981;&#21464;&#24615;&#30340;&#21487;&#34892;3D&#32467;&#26500;&#65288;&#20363;&#22914;&#20998;&#23376;&#21644;&#28857;&#20113;&#65289;&#36827;&#34892;&#37319;&#26679;&#65292;&#22312;&#21508;&#31181;&#29616;&#23454;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#21069;&#26223;&#65292;&#20854;&#20013;SE(3)-&#19981;&#21464;&#24615;&#23646;&#24615;&#21487;&#20197;&#36890;&#36807;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#27969;&#24418;&#33258;&#28982;&#22320;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#24179;&#20961;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#36825;&#31181;SE(3)-&#19981;&#21464;&#31354;&#38388;&#20869;&#25193;&#25955;&#26426;&#21046;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25968;&#23398;&#19978;&#25551;&#32472;&#22312;SE(3)-&#19981;&#21464;&#24615;&#19979;&#30340;&#25193;&#25955;&#26426;&#21046;&#65292;&#36890;&#36807;&#24494;&#20998;&#20960;&#20309;&#30340;&#35270;&#35282;&#28145;&#20837;&#20998;&#26512;&#22352;&#26631;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#21644;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#27969;&#24418;&#12290;&#22312;&#36825;&#19968;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#20934;&#30830;&#19988;&#26080;&#38656;&#25237;&#24433;&#30340;&#25193;&#25955;SDE&#21644;ODE&#12290;&#36825;&#20123;&#20844;&#24335;&#19981;&#20165;&#33021;&#22815;&#25552;&#39640;&#29983;&#25104;&#36335;&#24452;&#30340;&#24615;&#33021;&#21644;&#36895;&#24230;&#65292;&#21516;&#26102;&#36824;&#20026;&#21253;&#21547;SE(3)-&#19981;&#21464;&#24615;&#30340;&#20854;&#20182;&#31995;&#32479;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01430v1 Announce Type: new  Abstract: Sampling viable 3D structures (e.g., molecules and point clouds) with SE(3)-invariance using diffusion-based models proved promising in a variety of real-world applications, wherein SE(3)-invariant properties can be naturally characterized by the inter-point distance manifold. However, due to the non-trivial geometry, we still lack a comprehensive understanding of the diffusion mechanism within such SE(3)-invariant space. This study addresses this gap by mathematically delineating the diffusion mechanism under SE(3)-invariance, via zooming into the interaction behavior between coordinates and the inter-point distance manifold through the lens of differential geometry. Upon this analysis, we propose accurate and projection-free diffusion SDE and ODE accordingly. Such formulations enable enhancing the performance and the speed of generation pathways; meanwhile offering valuable insights into other systems incorporating SE(3)-invariance.
&lt;/p&gt;</description></item><item><title>Algogens&#26159;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#22797;&#26434;&#25361;&#25112;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01426</link><description>&lt;p&gt;
Algogens&#27010;&#35770;
&lt;/p&gt;
&lt;p&gt;
Introduction to Algogens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01426
&lt;/p&gt;
&lt;p&gt;
Algogens&#26159;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#22797;&#26434;&#25361;&#25112;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#20070;&#20171;&#32461;&#20102;Algogens&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#19968;&#31181;&#21069;&#26223;&#24191;&#38420;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#36328;&#21508;&#39046;&#22495;&#30340;&#38382;&#39064;&#35299;&#20915;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#20110;&#29702;&#35299;&#30340;&#27010;&#36848;&#65292;&#38416;&#36848;&#20102;Algogens&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#28508;&#21147;&#19982;&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#30456;&#32467;&#21512;&#65292;&#20197;&#27604;&#21333;&#29420;&#20351;&#29992;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#22797;&#26434;&#25361;&#25112;&#12290;&#35813;&#25991;&#25506;&#35752;&#20102;Algogens&#30340;&#22522;&#30784;&#30693;&#35782;&#12289;&#21457;&#23637;&#12289;&#24212;&#29992;&#21644;&#20248;&#21183;&#65292;&#20363;&#22914;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#20363;&#23376;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35835;&#32773;&#23558;&#20102;&#35299;&#21040;Algogens&#22312;&#20170;&#22825;&#30340;&#23454;&#38469;&#29992;&#36884;&#20197;&#21450;&#23427;&#20204;&#22312;&#26410;&#26469;&#32593;&#32476;&#23433;&#20840;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#31185;&#23398;&#21019;&#26032;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01426v1 Announce Type: new  Abstract: This book introduces the concept of Algogens, a promising integration of generative AI with traditional algorithms aimed at improving problem-solving techniques across various fields. It provides an accessible overview of how Algogens combine AI's innovative potential with algorithms' reliability to tackle complex challenges more effectively than either could alone.   The text explores the basics of Algogens, their development, applications, and advantages, such as better adaptability and efficiency. Through examples and case studies, readers will learn about Algogens' practical uses today and their potential for future cybersecurity, healthcare, and environmental science innovation.   Acknowledging new technologies' challenges and ethical considerations, the book offers a balanced look at the prospects and obstacles facing Algogens. It invites a broad audience, including experts and newcomers, to engage with the topic and consider Algog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#27880;&#20837;&#25915;&#20987;&#30340;GNN&#30340;&#38598;&#20307;&#35777;&#20070;&#65292;&#36890;&#36807;&#20108;&#36827;&#21046;&#25972;&#25968;&#20108;&#27425;&#32422;&#26463;&#32447;&#24615;&#35268;&#21010;&#26500;&#24314;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#32447;&#24615;&#21270;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#35748;&#35777;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01423</link><description>&lt;p&gt;
&#38598;&#20307;&#35748;&#35777;&#23545;&#25239;&#22270;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Collective Certified Robustness against Graph Injection Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#27880;&#20837;&#25915;&#20987;&#30340;GNN&#30340;&#38598;&#20307;&#35777;&#20070;&#65292;&#36890;&#36807;&#20108;&#36827;&#21046;&#25972;&#25968;&#20108;&#27425;&#32422;&#26463;&#32447;&#24615;&#35268;&#21010;&#26500;&#24314;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#32447;&#24615;&#21270;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#35748;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22270;&#27880;&#20837;&#25915;&#20987;&#19979;&#30340;GNN&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#20165;&#36890;&#36807;&#39564;&#35777;&#27599;&#20010;&#33410;&#28857;&#29420;&#31435;&#25552;&#20379;&#26679;&#26412;&#32423;&#35777;&#20070;&#65292;&#23548;&#33268;&#35748;&#35777;&#24615;&#33021;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#38598;&#20307;&#35777;&#20070;&#65292;&#21487;&#20197;&#21516;&#26102;&#35748;&#35777;&#19968;&#32452;&#30446;&#26631;&#33410;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#25972;&#25968;&#20108;&#27425;&#32422;&#26463;&#32447;&#24615;&#35268;&#21010;&#65288;BQCLP&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#32447;&#24615;&#21270;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;BQCLP&#26494;&#24347;&#20026;&#21487;&#20197;&#39640;&#25928;&#27714;&#35299;&#30340;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38598;&#20307;&#35748;&#35777;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#35748;&#35777;&#24615;&#33021;&#65292;&#32780;&#35745;&#31639;&#24320;&#38144;&#21364;&#24456;&#23567;&#12290;&#20363;&#22914;&#65292;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#22312;1&#20998;&#38047;&#20869;&#35299;&#20915;LP&#65292;&#25105;&#20204;&#23558;&#35748;&#35777;&#27604;&#29575;&#20174;0.0%&#26174;&#33879;&#25552;&#39640;&#21040;81.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01423v1 Announce Type: cross  Abstract: We investigate certified robustness for GNNs under graph injection attacks. Existing research only provides sample-wise certificates by verifying each node independently, leading to very limited certifying performance. In this paper, we present the first collective certificate, which certifies a set of target nodes simultaneously. To achieve it, we formulate the problem as a binary integer quadratic constrained linear programming (BQCLP). We further develop a customized linearization technique that allows us to relax the BQCLP into linear programming (LP) that can be efficiently solved. Through comprehensive experiments, we demonstrate that our collective certification scheme significantly improves certification performance with minimal computational overhead. For instance, by solving the LP within 1 minute on the Citeseer dataset, we achieve a significant increase in the certified ratio from 0.0% to 81.2% when the injected node number
&lt;/p&gt;</description></item><item><title>&#24322;&#36136;&#24615;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#30340;&#36129;&#29486;&#35299;&#37322;&#20102;&#20026;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#20851;&#32852;&#24615;&#35757;&#32451;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.01420</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#23545;&#19981;&#21464;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#38544;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Heterogeneity towards Invariance and Causality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01420
&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#24615;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#30340;&#36129;&#29486;&#35299;&#37322;&#20102;&#20026;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#20851;&#32852;&#24615;&#35757;&#32451;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20351;&#29992;&#19968;&#31181;&#21464;&#20307;&#22238;&#24402;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;&#36825;&#19982;&#20256;&#32479;&#26234;&#24935;&#8220;&#20851;&#32852;&#19981;&#26159;&#22240;&#26524;&#8221;&#20197;&#21450;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#33539;&#24335;&#30456;&#21453;&#65292;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#33539;&#24335;&#35748;&#20026;&#20808;&#21069;&#30340;&#22240;&#26524;&#30693;&#35782;&#24212;&#35880;&#24910;&#22320;&#32435;&#20837;&#21040;&#26041;&#27861;&#35774;&#35745;&#20013;&#12290;&#20196;&#20154;&#22256;&#24785;&#30340;&#26159;&#65292;&#20026;&#20309;&#22312;&#36861;&#27714;&#20851;&#32852;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#33021;&#22815;&#20174;&#26356;&#39640;&#23618;&#27425;&#30340;&#29702;&#35299;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#12290;&#26412;&#25991;&#22768;&#31216;&#20174;&#38754;&#21521;&#20851;&#32852;&#30340;&#35757;&#32451;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#21487;&#20197;&#24402;&#22240;&#20110;&#28304;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#35757;&#32451;&#31639;&#27861;&#30340;&#38543;&#26426;&#24615;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21270;&#30340;&#32806;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#35265;&#22320;&#30340;&#27169;&#22411;&#26469;&#38416;&#37322;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#22238;&#24402;&#25439;&#22833;&#23398;&#20064;&#19981;&#21464;&#24615;&#65292;&#19968;&#31181;&#20934;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01420v1 Announce Type: new  Abstract: It is observed empirically that the large language models (LLM), trained with a variant of regression loss using numerous corpus from the Internet, can unveil causal associations to some extent. This is contrary to the traditional wisdom that ``association is not causation'' and the paradigm of traditional causal inference in which prior causal knowledge should be carefully incorporated into the design of methods. It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations. In this paper, we claim the emergence of causality from association-oriented training can be attributed to the coupling effects from the heterogeneity of the source data, stochasticity of training algorithms, and over-parameterization of the learning models. We illustrate such an intuition using a simple but insightful model that learns invariance, a quasi-causality, using regression loss. To be spec
&lt;/p&gt;</description></item><item><title>Asyn2F&#26159;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#21521;&#27169;&#22411;&#32858;&#21512;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#24322;&#27493;&#32858;&#21512;&#22810;&#20010;&#26412;&#22320;&#27169;&#22411;&#24182;&#24471;&#21040;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#35757;&#32451;&#24037;&#20316;&#32773;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#20840;&#23616;&#27169;&#22411;&#30340;&#26032;&#29256;&#26412;&#32858;&#21512;&#21040;&#26412;&#22320;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#36807;&#26102;&#20449;&#24687;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01417</link><description>&lt;p&gt;
Asyn2F&#65306;&#19968;&#31181;&#20855;&#26377;&#21452;&#21521;&#27169;&#22411;&#32858;&#21512;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional Model Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01417
&lt;/p&gt;
&lt;p&gt;
Asyn2F&#26159;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#21521;&#27169;&#22411;&#32858;&#21512;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#24322;&#27493;&#32858;&#21512;&#22810;&#20010;&#26412;&#22320;&#27169;&#22411;&#24182;&#24471;&#21040;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#35757;&#32451;&#24037;&#20316;&#32773;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#20840;&#23616;&#27169;&#22411;&#30340;&#26032;&#29256;&#26412;&#32858;&#21512;&#21040;&#26412;&#22320;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#36807;&#26102;&#20449;&#24687;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#21516;&#27493;&#25110;&#24322;&#27493;&#35757;&#32451;&#12290;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#19987;&#27880;&#20110;&#20026;&#26381;&#21153;&#22120;&#24320;&#21457;&#32858;&#21512;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#20182;&#20204;&#24573;&#30053;&#20102;&#35757;&#32451;&#24037;&#20316;&#32773;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#23548;&#33268;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#30340;&#24310;&#36831;&#65292;&#36827;&#32780;&#23548;&#33268;&#36807;&#26102;&#20449;&#24687;&#38382;&#39064;&#12290;&#26412;&#25991;&#35774;&#35745;&#24182;&#24320;&#21457;&#20102;Asyn2F&#65292;&#19968;&#31181;&#20855;&#26377;&#21452;&#21521;&#27169;&#22411;&#32858;&#21512;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#21452;&#21521;&#27169;&#22411;&#32858;&#21512;&#65292;Asyn2F&#19968;&#26041;&#38754;&#20801;&#35768;&#26381;&#21153;&#22120;&#24322;&#27493;&#32858;&#21512;&#22810;&#20010;&#26412;&#22320;&#27169;&#22411;&#24182;&#24471;&#21040;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20801;&#35768;&#35757;&#32451;&#24037;&#20316;&#32773;&#23558;&#20840;&#23616;&#27169;&#22411;&#30340;&#26032;&#29256;&#26412;&#32858;&#21512;&#21040;&#26412;&#22320;&#27169;&#22411;&#20013;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26102;&#26399;&#20013;&#36884;&#20063;&#21487;&#20197;&#36827;&#34892;&#12290;&#25105;&#20204;&#24320;&#21457;Asyn2F&#26102;&#32771;&#34385;&#20102;&#23454;&#38469;&#23454;&#26045;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01417v1 Announce Type: new  Abstract: In federated learning, the models can be trained synchronously or asynchronously. Many research works have focused on developing an aggregation method for the server to aggregate multiple local models into the global model with improved performance. They ignore the heterogeneity of the training workers, which causes the delay in the training of the local models, leading to the obsolete information issue. In this paper, we design and develop Asyn2F, an Asynchronous Federated learning Framework with bidirectional model aggregation. By bidirectional model aggregation, Asyn2F, on one hand, allows the server to asynchronously aggregate multiple local models and results in a new global model. On the other hand, it allows the training workers to aggregate the new version of the global model into the local model, which is being trained even in the middle of a training epoch. We develop Asyn2F considering the practical implementation requirements
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26032;&#39062;&#30340;&#23454;&#20363;&#32423;&#26694;&#26550;Weigh And Select&#65288;WAS&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#35299;&#32806;&#30340;&#36830;&#20307;&#32593;&#32476;&#32452;&#21512;&#20102;&#26435;&#34913;&#21644;&#36873;&#25321;&#36825;&#20004;&#20010;&#21327;&#20316;&#36807;&#31243;</title><link>https://arxiv.org/abs/2403.01400</link><description>&lt;p&gt;
&#35299;&#32806;&#26435;&#34913;&#21644;&#36873;&#25321;&#65306;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26032;&#39062;&#30340;&#23454;&#20363;&#32423;&#26694;&#26550;Weigh And Select&#65288;WAS&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#35299;&#32806;&#30340;&#36830;&#20307;&#32593;&#32476;&#32452;&#21512;&#20102;&#26435;&#34913;&#21644;&#36873;&#25321;&#36825;&#20004;&#20010;&#21327;&#20316;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#39044;&#35757;&#32451;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20276;&#38543;&#30528;&#25968;&#30334;&#31181;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#25552;&#20986;&#65292;&#25972;&#21512;&#20174;&#22810;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#27492;&#20027;&#39064;&#30340;&#20004;&#20010;&#37325;&#35201;&#21327;&#20316;&#36807;&#31243;&#65306;&#65288;1&#65289;&#36873;&#25321;&#65306;&#22914;&#20309;&#22522;&#20110;&#23427;&#20204;&#30340;&#20860;&#23481;&#24615;&#20174;&#32473;&#23450;&#20219;&#21153;&#27744;&#20013;&#36873;&#25321;&#26368;&#20339;&#20219;&#21153;&#32452;&#21512;&#65292;&#21644;&#65288;2&#65289;&#26435;&#34913;&#65306;&#22914;&#20309;&#22522;&#20110;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#26435;&#34913;&#25152;&#36873;&#20219;&#21153;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#24456;&#22810;&#24037;&#20316;&#38598;&#20013;&#22312;&#26435;&#34913;&#19978;&#65292;&#20294;&#30456;&#27604;&#20043;&#19979;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26032;&#39062;&#30340;&#23454;&#20363;&#32423;&#26694;&#26550;Weigh And Select&#65288;WAS&#65289;&#65292;&#20854;&#20013;&#26435;&#34913;&#21644;&#36873;&#25321;&#36825;&#20004;&#20010;&#21327;&#20316;&#36807;&#31243;&#36890;&#36807;&#35299;&#32806;&#30340;&#36830;&#20307;&#32593;&#32476;&#36827;&#34892;&#32452;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#39318;&#20808;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20339;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01400v1 Announce Type: cross  Abstract: Recent years have witnessed the great success of graph pre-training for graph representation learning. With hundreds of graph pre-training tasks proposed, integrating knowledge acquired from multiple pre-training tasks has become a popular research topic. In this paper, we identify two important collaborative processes for this topic: (1) select: how to select an optimal task combination from a given task pool based on their compatibility, and (2) weigh: how to weigh the selected tasks based on their importance. While there currently has been a lot of work focused on weighing, comparatively little effort has been devoted to selecting. This paper proposes a novel instance-level framework for integrating multiple graph pre-training tasks, Weigh And Select (WAS), where the two collaborative processes, weighing and selecting, are combined by decoupled siamese networks. Specifically, it first adaptively learns an optimal combination of task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#36827;&#34892;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01389</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#19982;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fusion of Gaussian Processes Predictions with Monte Carlo Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#36827;&#34892;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#25105;&#20204;&#32463;&#24120;&#20351;&#29992;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#27169;&#22411;&#12290;&#35748;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#26159;&#23545;&#29616;&#23454;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#24076;&#26395;&#23558;&#22810;&#20010;&#27169;&#22411;&#24212;&#29992;&#20110;&#30456;&#21516;&#30340;&#25968;&#25454;&#24182;&#25972;&#21512;&#23427;&#20204;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#22312;&#36125;&#21494;&#26031;&#33539;&#24335;&#20869;&#36816;&#34892;&#65292;&#20381;&#36182;&#20110;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#39044;&#27979;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;pdf&#65289;&#65292;&#30446;&#26631;&#26159;&#31995;&#32479;&#22320;&#25972;&#21512;&#23427;&#20204;&#65292;&#20351;&#29992;&#32447;&#24615;&#21644;&#23545;&#25968;&#32447;&#24615;&#27719;&#24635;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#20110;&#23545;&#25968;&#32447;&#24615;&#27719;&#24635;&#30340;&#26032;&#26041;&#27861;&#65292;&#30830;&#23450;&#20102;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;pdf&#30340;&#36755;&#20837;&#30456;&#20851;&#26435;&#37325;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#20174;&#20854;&#21518;&#39564;&#20013;&#25277;&#21462;&#26435;&#37325;&#26679;&#26412;&#23454;&#29616;pdf&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#22522;&#20110;&#32447;&#24615;&#27719;&#24635;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01389v1 Announce Type: new  Abstract: In science and engineering, we often work with models designed for accurate prediction of variables of interest. Recognizing that these models are approximations of reality, it becomes desirable to apply multiple models to the same data and integrate their outcomes. In this paper, we operate within the Bayesian paradigm, relying on Gaussian processes as our models. These models generate predictive probability density functions (pdfs), and the objective is to integrate them systematically, employing both linear and log-linear pooling. We introduce novel approaches for log-linear pooling, determining input-dependent weights for the predictive pdfs of the Gaussian processes. The aggregation of the pdfs is realized through Monte Carlo sampling, drawing samples of weights from their posterior. The performance of these methods, as well as those based on linear pooling, is demonstrated using a synthetic dataset.
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#21017;&#23558;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#21442;&#19982;&#32773;&#25968;&#25454;&#29305;&#24449;&#31354;&#38388;&#21644;&#20998;&#24067;&#30340;&#24046;&#24322;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01387</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;&#65306;&#25361;&#25112;&#12289;&#26041;&#27861;&#19982;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Federated Transfer Learning: Challenges, Methods and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01387
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#21017;&#23558;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#21442;&#19982;&#32773;&#25968;&#25454;&#29305;&#24449;&#31354;&#38388;&#21644;&#20998;&#24067;&#30340;&#24046;&#24322;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01387v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20849;&#20139;&#30340;&#35201;&#27714;&#65292;&#20351;&#21442;&#19982;&#32773;&#33021;&#22815;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#24102;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#38598;&#20013;&#27169;&#22411;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;FL&#36890;&#24120;&#28041;&#21450;&#22810;&#20010;&#21442;&#19982;&#32773;&#65292;&#24182;&#19988;&#38656;&#35201;&#31532;&#19977;&#26041;&#32858;&#21512;&#20840;&#23616;&#20449;&#24687;&#20197;&#25351;&#23548;&#30446;&#26631;&#21442;&#19982;&#32773;&#30340;&#26356;&#26032;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#27599;&#20010;&#21442;&#19982;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#21487;&#33021;&#19981;&#26159;&#20174;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#30456;&#21516;&#30340;&#22522;&#30784;&#20998;&#24067;&#20013;&#25277;&#26679;&#24471;&#26469;&#65292;&#24456;&#22810;FL&#26041;&#27861;&#24448;&#24448;&#25928;&#26524;&#19981;&#20339;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#26412;&#22320;&#35774;&#22791;&#30340;&#24046;&#24322;&#65288;&#31995;&#32479;&#24322;&#26500;&#24615;&#65289;&#12289;&#22312;&#32447;&#25968;&#25454;&#30340;&#25345;&#32493;&#28044;&#20837;&#65288;&#22686;&#37327;&#25968;&#25454;&#65289;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#21487;&#33021;&#36827;&#19968;&#27493;&#24433;&#21709;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#38598;&#25104;&#21040;FL&#20013;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01387v1 Announce Type: new  Abstract: Federated learning (FL) is a novel distributed machine learning paradigm that enables participants to collaboratively train a centralized model with privacy preservation by eliminating the requirement of data sharing. In practice, FL often involves multiple participants and requires the third party to aggregate global information to guide the update of the target participant. Therefore, many FL methods do not work well due to the training and test data of each participant may not be sampled from the same feature space and the same underlying distribution. Meanwhile, the differences in their local devices (system heterogeneity), the continuous influx of online data (incremental data), and labeled data scarcity may further influence the performance of these methods. To solve this problem, federated transfer learning (FTL), which integrates transfer learning (TL) into FL, has attracted the attention of numerous researchers. However, since F
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01384</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Compressibility of Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#36793;&#32536;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#22914;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#20943;&#23569;&#25968;&#25454;&#31227;&#21160;&#65292;&#20174;&#32780;&#21152;&#36895;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#37327;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21464;&#20998;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;ELBO&#21644;&#33719;&#21462;&#20302;&#26041;&#24046;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#21644;&#25512;&#26029;&#32593;&#32476;&#30340;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#65292;&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#21270;&#20026;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01371</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21464;&#20998;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large-scale variational Gaussian state-space models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01371
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21464;&#20998;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;ELBO&#21644;&#33719;&#21462;&#20302;&#26041;&#24046;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#21644;&#25512;&#26029;&#32593;&#32476;&#30340;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#65292;&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#21270;&#20026;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23884;&#22871;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#21464;&#20998;&#36924;&#36817;&#26041;&#27861;&#65292;&#20854;&#20013;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30001;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#27809;&#26377;&#37319;&#29992;&#23545;&#35282;&#39640;&#26031;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35780;&#20272;ELBO&#21644;&#20302;&#26041;&#24046;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#65288;i&#65289;&#36890;&#36807;&#21160;&#21147;&#23398;&#23545;&#38544;&#29366;&#24577;&#36827;&#34892;&#36793;&#32536;&#21270;&#30340;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#30340;&#20302;&#31209;&#32467;&#26500;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#25512;&#26029;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#20302;&#31209;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#26469;&#36817;&#20284;&#26356;&#26032;&#27493;&#39588;&#65292;&#65288;iii&#65289;&#23558;&#24403;&#21069;&#21644;&#26410;&#26469;&#35266;&#27979;&#32534;&#30721;&#20026;&#20266;&#35266;&#27979;--&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#25442;&#20026;&#65288;&#26356;&#31616;&#21333;&#30340;&#65289;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#24517;&#35201;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;ELBO&#21487;&#20197;&#22312;$O&#65288;TL&#65288;Sr+S^2+r^2&#65289;&#65289;$&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#20854;&#20013;$T$&#26159;&#31995;&#21015;&#38271;&#24230;&#65292;$L$&#26159;&#29366;&#24577;&#31354;&#38388;&#32500;&#25968;&#65292;$S$&#26159;&#29992;&#20110;&#36924;&#36817;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01371v1 Announce Type: cross  Abstract: We introduce an amortized variational inference algorithm and structured variational approximation for state-space models with nonlinear dynamics driven by Gaussian noise. Importantly, the proposed framework allows for efficient evaluation of the ELBO and low-variance stochastic gradient estimates without resorting to diagonal Gaussian approximations by exploiting (i) the low-rank structure of Monte-Carlo approximations to marginalize the latent state through the dynamics (ii) an inference network that approximates the update step with low-rank precision matrix updates (iii) encoding current and future observations into pseudo observations -- transforming the approximate smoothing problem into an (easier) approximate filtering problem. Overall, the necessary statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$ is the series length, $L$ is the state-space dimensionality, $S$ are the number of samples used to app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Wav2Vec2&#23884;&#20837;&#22312;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;SSL&#34920;&#31034;&#23545;&#22686;&#24378;&#20219;&#21153;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.01369</link><description>&lt;p&gt;
Wav2Vec2&#23884;&#20837;&#22312;&#35774;&#22791;&#31471;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Wav2Vec2&#23884;&#20837;&#22312;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;SSL&#34920;&#31034;&#23545;&#22686;&#24378;&#20219;&#21153;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34987;&#21457;&#29616;&#22312;&#26576;&#20123;&#35821;&#38899;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#27604;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#31561;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;SSL&#34920;&#31034;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#29992;&#36884;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#23545;&#22686;&#24378;&#20219;&#21153;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01369v1 Announce Type: cross  Abstract: Self-supervised learned models have been found to be very effective for certain speech tasks such as automatic speech recognition, speaker identification, keyword spotting and others. While the features are undeniably useful in speech recognition and associated tasks, their utility in speech enhancement systems is yet to be firmly established, and perhaps not properly understood. In this paper, we investigate the uses of SSL representations for single-channel speech enhancement in challenging conditions and find that they add very little value for the enhancement task. Our constraints are designed around on-device real-time speech enhancement -- model is causal, the compute footprint is small. Additionally, we focus on low SNR conditions where such models struggle to provide good enhancement. In order to systematically examine how SSL representations impact performance of such enhancement models, we propose a variety of techniques to u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#30446;&#26631;&#33829;&#38144;&#30340;&#24378;&#30423;&#21033;&#28070;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#25932;&#23545;&#24378;&#30423;&#24773;&#22659;&#19979;&#30340;&#36817;&#20046;&#26368;&#20248;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01361</link><description>&lt;p&gt;
&#38024;&#23545;&#30446;&#26631;&#33829;&#38144;&#30340;&#24378;&#30423;&#21033;&#28070;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bandit Profit-maximization for Targeted Marketing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#30446;&#26631;&#33829;&#38144;&#30340;&#24378;&#30423;&#21033;&#28070;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#25932;&#23545;&#24378;&#30423;&#24773;&#22659;&#19979;&#30340;&#36817;&#20046;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#39034;&#24207;&#21033;&#28070;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20248;&#21270;&#20215;&#26684;&#21644;&#20687;&#33829;&#38144;&#25903;&#20986;&#36825;&#26679;&#30340;&#36741;&#21161;&#21464;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#19968;&#20010;&#20219;&#24847;&#24207;&#21015;&#30340;&#22810;&#20010;&#38656;&#27714;&#26354;&#32447;&#19978;&#26368;&#22823;&#21270;&#21033;&#28070;&#65292;&#27599;&#20010;&#26354;&#32447;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21516;&#30340;&#36741;&#21161;&#21464;&#37327;&#65292;&#20294;&#20849;&#20139;&#30456;&#21516;&#30340;&#20215;&#26684;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#38024;&#23545;&#33829;&#38144;&#65292;&#20854;&#20013;&#19968;&#23478;&#20844;&#21496;&#65288;&#21334;&#26041;&#65289;&#24076;&#26395;&#22312;&#22810;&#20010;&#24066;&#22330;&#19978;&#38144;&#21806;&#20135;&#21697;&#12290;&#20844;&#21496;&#21487;&#20197;&#20026;&#19981;&#21516;&#24066;&#22330;&#25237;&#20837;&#19981;&#21516;&#30340;&#33829;&#38144;&#25903;&#20986;&#20197;&#20248;&#21270;&#23458;&#25143;&#33719;&#21462;&#65292;&#20294;&#24517;&#39035;&#22312;&#25152;&#26377;&#24066;&#22330;&#19978;&#20445;&#25345;&#30456;&#21516;&#30340;&#20215;&#26684;&#12290;&#27492;&#22806;&#65292;&#24066;&#22330;&#21487;&#33021;&#20855;&#26377;&#24322;&#36136;&#30340;&#38656;&#27714;&#26354;&#32447;&#65292;&#27599;&#20010;&#38656;&#27714;&#26354;&#32447;&#23545;&#20215;&#26684;&#21644;&#33829;&#38144;&#25903;&#20986;&#30340;&#21709;&#24212;&#26041;&#24335;&#19981;&#21516;&#12290;&#20844;&#21496;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#27611;&#21033;&#28070;&#65292;&#21363;&#24635;&#25910;&#20837;&#20943;&#21435;&#33829;&#38144;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01361v1 Announce Type: new  Abstract: We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm's objective is to maximize its gross profit, the total revenue minus marketing costs.   Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive seque
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#26080;&#20851;&#30340;&#26816;&#27979;&#25104;&#26412;&#20989;&#25968;&#65288;a-DCF&#65289;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#25269;&#24481;&#27450;&#39575;&#25915;&#20987;&#30340;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.01355</link><description>&lt;p&gt;
a-DCF&#65306;&#19968;&#31181;&#19982;&#26550;&#26500;&#26080;&#20851;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#25269;&#24481;&#27450;&#39575;&#25915;&#20987;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#26080;&#20851;&#30340;&#26816;&#27979;&#25104;&#26412;&#20989;&#25968;&#65288;a-DCF&#65289;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#25269;&#24481;&#27450;&#39575;&#25915;&#20987;&#30340;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#39575;&#26816;&#27979;&#30446;&#21069;&#26159;&#19968;&#20010;&#20027;&#27969;&#30740;&#31350;&#35838;&#39064;&#12290;&#26631;&#20934;&#24230;&#37327;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#23396;&#31435;&#27450;&#39575;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#65292;&#20063;&#26377;&#19968;&#20123;&#25552;&#20986;&#26469;&#25903;&#25345;&#23427;&#20204;&#22312;&#19982;&#35828;&#35805;&#20154;&#26816;&#27979;&#32467;&#21512;&#26102;&#30340;&#35780;&#20272;&#65292;&#20294;&#23384;&#22312;&#24050;&#30693;&#30340;&#32570;&#38519;&#25110;&#32773;&#38480;&#21046;&#20102;&#32467;&#21512;&#35828;&#35805;&#20154;&#21644;&#27450;&#39575;&#26816;&#27979;&#22120;&#30340;&#26550;&#26500;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#26080;&#20851;&#30340;&#26816;&#27979;&#25104;&#26412;&#20989;&#25968;&#65288;a-DCF&#65289;&#12290;&#20316;&#20026;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#24615;&#33021;&#30340;&#21407;&#22987;DCF&#30340;&#25512;&#24191;&#65292;a-DCF&#26088;&#22312;&#29992;&#20110;&#35780;&#20272;&#25269;&#24481;&#27450;&#39575;&#25915;&#20987;&#30340;ASV&#12290;&#19982;DCF&#31867;&#20284;&#65292;a-DCF&#20174;Bayes&#39118;&#38505;&#30340;&#35282;&#24230;&#21453;&#26144;&#20102;&#20915;&#31574;&#30340;&#20195;&#20215;&#65292;&#20854;&#20013;&#26126;&#30830;&#23450;&#20041;&#20102;&#31867;&#20808;&#39564;&#21644;&#26816;&#27979;&#25104;&#26412;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26550;&#26500;&#24322;&#26500;&#30340;&#25269;&#24481;&#27450;&#39575;&#25915;&#20987;&#30340;ASV&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#22522;&#20934;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;a-DCF&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01355v1 Announce Type: cross  Abstract: Spoofing detection is today a mainstream research topic. Standard metrics can be applied to evaluate the performance of isolated spoofing detection solutions and others have been proposed to support their evaluation when they are combined with speaker detection. These either have well-known deficiencies or restrict the architectural approach to combine speaker and spoof detectors. In this paper, we propose an architecture-agnostic detection cost function (a-DCF). A generalisation of the original DCF used widely for the assessment of automatic speaker verification (ASV), the a-DCF is designed for the evaluation of spoofing-robust ASV. Like the DCF, the a-DCF reflects the cost of decisions in a Bayes risk sense, with explicitly defined class priors and detection cost model. We demonstrate the merit of the a-DCF through the benchmarking evaluation of architecturally-heterogeneous spoofing-robust ASV solutions.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38047;&#24418;&#26354;&#32447;&#26435;&#37325;&#20989;&#25968;&#25913;&#36827;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#30417;&#30563;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01352</link><description>&lt;p&gt;
&#20351;&#29992;&#38047;&#24418;&#26354;&#32447;&#26435;&#37325;&#20989;&#25968;&#25913;&#36827;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improving Uncertainty Sampling with Bell Curve Weight Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01352
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38047;&#24418;&#26354;&#32447;&#26435;&#37325;&#20989;&#25968;&#25913;&#36827;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#30417;&#30563;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#26410;&#26631;&#35760;&#23454;&#20363;&#36827;&#34892;&#27880;&#37322;&#26469;&#35757;&#32451;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23398;&#20064;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#33719;&#21462;&#26631;&#35760;&#23454;&#20363;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#38047;&#24418;&#26354;&#32447;&#26435;&#37325;&#20989;&#25968;&#25913;&#36827;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#20197;&#25552;&#39640;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01352v1 Announce Type: new  Abstract: Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection. Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model. Nonetheless, the performance 
&lt;/p&gt;</description></item><item><title>SANGRIA&#26159;&#19968;&#20010;&#22522;&#20110;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#19982;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#23460;&#20869;&#23450;&#20301;&#26694;&#26550;&#65292;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24179;&#22343;&#23450;&#20301;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.01348</link><description>&lt;p&gt;
SANGRIA&#65306;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01348
&lt;/p&gt;
&lt;p&gt;
SANGRIA&#26159;&#19968;&#20010;&#22522;&#20110;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#19982;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#23460;&#20869;&#23450;&#20301;&#26694;&#26550;&#65292;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24179;&#22343;&#23450;&#20301;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#26159;&#35768;&#22810;&#23884;&#20837;&#24335;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20363;&#22914;&#36164;&#20135;&#36319;&#36394;&#12289;&#24212;&#24613;&#21709;&#24212;&#21644;&#23454;&#26102;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SANGRIA&#30340;&#22522;&#20110;&#25351;&#32441;&#30340;&#23460;&#20869;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#19982;&#26799;&#24230;&#25552;&#21319;&#26641;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#35774;&#22791;&#24322;&#26500;&#24615;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#21487;&#33021;&#23548;&#33268;&#29992;&#20110;&#23450;&#20301;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20043;&#38388;&#30340;&#26080;&#32447;&#20449;&#21495;&#27979;&#37327;&#20986;&#29616;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;SANGRIA&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#19981;&#21516;&#23460;&#20869;&#22330;&#25152;&#21644;&#24322;&#26500;&#35774;&#22791;&#19978;&#23637;&#31034;&#20102;42.96%&#36739;&#20302;&#30340;&#24179;&#22343;&#23450;&#20301;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01348v1 Announce Type: cross  Abstract: Indoor localization is a critical task in many embedded applications, such as asset tracking, emergency response, and realtime navigation. In this article, we propose a novel fingerprintingbased framework for indoor localization called SANGRIA that uses stacked autoencoder neural networks with gradient boosted trees. Our approach is designed to overcome the device heterogeneity challenge that can create uncertainty in wireless signal measurements across embedded devices used for localization. We compare SANGRIA to several state-of-the-art frameworks and demonstrate 42.96% lower average localization error across diverse indoor locales and heterogeneous devices.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#20869;&#36827;&#34892;&#37319;&#26679;&#65292;&#38480;&#21046;&#20102;&#22024;&#26434;&#21644;&#27491;&#26631;&#31614;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01346</link><description>&lt;p&gt;
&#25552;&#39640;&#22312;&#22024;&#26434;&#25968;&#25454;&#38598;&#19978;&#20027;&#21160;&#23398;&#20064;&#30340;&#25104;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improve Cost Efficiency of Active Learning over Noisy Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#20869;&#36827;&#34892;&#37319;&#26679;&#65292;&#38480;&#21046;&#20102;&#22024;&#26434;&#21644;&#27491;&#26631;&#31614;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#31574;&#30053;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20027;&#21160;&#35782;&#21035;&#21644;&#26631;&#35760;&#25968;&#25454;&#28857;&#20197;&#20248;&#21270;&#20854;&#23398;&#20064;&#36807;&#31243;&#12290;&#35813;&#31574;&#30053;&#22312;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#20294;&#26631;&#35760;&#36825;&#20123;&#25968;&#25454;&#28857;&#30340;&#25104;&#26412;&#36807;&#39640;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20108;&#20803;&#20998;&#31867;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#33719;&#21462;&#27491;&#23454;&#20363;&#30340;&#25104;&#26412;&#26126;&#26174;&#39640;&#20110;&#36127;&#23454;&#20363;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01346v1 Announce Type: new  Abstract: Active learning is a learning strategy whereby the machine learning algorithm actively identifies and labels data points to optimize its learning. This strategy is particularly effective in domains where an abundance of unlabeled data exists, but the cost of labeling these data points is prohibitively expensive. In this paper, we consider cases of binary classification, where acquiring a positive instance incurs a significantly higher cost compared to that of negative instances. For example, in the financial industry, such as in money-lending businesses, a defaulted loan constitutes a positive event leading to substantial financial loss. To address this issue, we propose a shifted normal distribution sampling function that samples from a wider range than typical uncertainty sampling. Our simulation underscores that our proposed sampling function limits both noisy and positive label selection, delivering between 20% and 32% improved cost 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21046;&#23450;&#31867;&#21035;&#29305;&#23450;&#30340;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#30446;&#26631;&#21407;&#22411;&#26469;&#25913;&#21892;CTA&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01344</link><description>&lt;p&gt;
&#32531;&#35299;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Bias in the Model for Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21046;&#23450;&#31867;&#21035;&#29305;&#23450;&#30340;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#30446;&#26631;&#21407;&#22411;&#26469;&#25913;&#21892;CTA&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#30446;&#26631;&#22495;&#12290;&#22312;CTA&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#19981;&#30693;&#36947;&#30446;&#26631;&#39046;&#22495;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#22312;&#27979;&#35797;&#26102;&#38388;&#38754;&#20020;&#27969;&#36755;&#20837;&#20998;&#24067;&#30340;&#21095;&#28872;&#21464;&#21270;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#20197;&#22312;&#32447;&#26041;&#24335;&#25345;&#32493;&#22320;&#35843;&#25972;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#19981;&#26029;&#36866;&#24212;&#30446;&#26631;&#25968;&#25454;&#30340;&#20998;&#24067;&#26102;&#20250;&#21576;&#29616;&#39640;&#24230;&#20559;&#20506;&#30340;&#39044;&#27979;&#12290;&#23427;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#26576;&#20123;&#31867;&#21035;&#65292;&#32780;&#36825;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#26469;&#25552;&#39640;CTA&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#38752;&#30340;&#30446;&#26631;&#26679;&#26412;&#21046;&#23450;&#20197;&#31867;&#21035;&#20026;&#22522;&#30784;&#30340;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#30446;&#26631;&#21407;&#22411;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20197;&#31867;&#21035;&#26041;&#24335;&#23545;&#30446;&#26631;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26088;&#22312;&#23545;&#40784;&#30446;&#26631;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01344v1 Announce Type: new  Abstract: Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains. In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time. The key challenge is to keep adapting the model to the continually changing target domains in an online manner. We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data. It predicts certain classes more often than other classes, making inaccurate over-confident predictions. This paper mitigates this issue to improve performance in the CTA scenario. To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely. Moreover, we aim to align the target d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;$G$&#19981;&#21464;&#22810;&#39033;&#24335;&#19968;&#33268;&#36924;&#36817;$G$&#19981;&#21464;&#20989;&#25968;&#65292;&#21516;&#26102;&#33021;&#22815;&#33719;&#24471;&#21453;&#23545;&#31216;&#20989;&#25968;&#30340;&#32479;&#19968;$\mathcal{C}^k$&#36924;&#36817;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24212;&#29992;&#20110;&#23436;&#20840;&#23545;&#31216;&#20989;&#25968;&#65292;&#36824;&#21487;&#20197;&#23558;&#23884;&#20837;&#32500;&#24230;&#29420;&#31435;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#12289;&#36924;&#36817;&#20934;&#30830;&#24230;&#21644;$k$&#12290;</title><link>https://arxiv.org/abs/2403.01339</link><description>&lt;p&gt;
$\mathcal{C}^k$&#19968;&#33268;&#36924;&#36817;$G$-&#19981;&#21464;&#21644;&#21453;&#23545;&#31216;&#20989;&#25968;&#65292;&#23884;&#20837;&#32500;&#24230;&#21644;&#22810;&#39033;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Uniform $\mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric Functions, Embedding Dimensions, and Polynomial Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;$G$&#19981;&#21464;&#22810;&#39033;&#24335;&#19968;&#33268;&#36924;&#36817;$G$&#19981;&#21464;&#20989;&#25968;&#65292;&#21516;&#26102;&#33021;&#22815;&#33719;&#24471;&#21453;&#23545;&#31216;&#20989;&#25968;&#30340;&#32479;&#19968;$\mathcal{C}^k$&#36924;&#36817;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#24212;&#29992;&#20110;&#23436;&#20840;&#23545;&#31216;&#20989;&#25968;&#65292;&#36824;&#21487;&#20197;&#23558;&#23884;&#20837;&#32500;&#24230;&#29420;&#31435;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#12289;&#36924;&#36817;&#20934;&#30830;&#24230;&#21644;$k$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23545;&#31216;&#32676;$\mathcal{S}_n$&#30340;&#20219;&#24847;&#23376;&#32676;$G$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#30001;$G$&#19981;&#21464;&#22810;&#39033;&#24335;&#36924;&#36817;$G$&#19981;&#21464;&#20989;&#25968;&#30340;$\mathcal{C}^k$&#19968;&#33268;&#36924;&#36817;&#32467;&#26524;&#12290;&#23545;&#20110;&#23436;&#20840;&#23545;&#31216;&#20989;&#25968;&#30340;&#24773;&#20917;($G = \mathcal{S}_n$)&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#20102;Zaheer&#31561;&#20154;(2018)&#30340;sum-decomposition Deep Sets&#26041;&#27861;&#65292;&#20854;&#20013;&#20869;&#37096;&#21644;&#22806;&#37096;&#20989;&#25968;&#37117;&#21487;&#20197;&#36873;&#25321;&#20026;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#20869;&#37096;&#20989;&#25968;&#21487;&#20197;&#36873;&#25321;&#19982;&#34987;&#36924;&#36817;&#30340;&#30446;&#26631;&#20989;&#25968;&#26080;&#20851;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#38656;&#30340;&#23884;&#20837;&#32500;&#24230;&#19982;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#12289;&#26399;&#26395;&#36924;&#36817;&#30340;&#20934;&#30830;&#24615;&#20197;&#21450;$k$&#26080;&#20851;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#36807;&#31243;&#21487;&#20197;&#35753;&#25105;&#20204;&#24471;&#21040;&#21453;&#23545;&#31216;&#20989;&#25968;&#30340;$\mathcal{C}^k$&#19968;&#33268;&#36924;&#36817;&#20316;&#20026;$K$&#39033;&#30340;&#21644;&#65292;&#20854;&#20013;&#27599;&#19968;&#39033;&#22343;&#20026;&#20809;&#28369;&#23436;&#20840;&#23545;&#31216;&#20989;&#25968;&#21644;&#20809;&#28369;&#21453;&#23545;&#31216;&#20989;&#25968;&#20043;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01339v1 Announce Type: new  Abstract: For any subgroup $G$ of the symmetric group $\mathcal{S}_n$ on $n$ symbols, we present results for the uniform $\mathcal{C}^k$ approximation of $G$-invariant functions by $G$-invariant polynomials. For the case of totally symmetric functions ($G = \mathcal{S}_n$), we show that this gives rise to the sum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both the inner and outer functions can be chosen to be smooth, and moreover, the inner function can be chosen to be independent of the target function being approximated. In particular, we show that the embedding dimension required is independent of the regularity of the target function, the accuracy of the desired approximation, as well as $k$. Next, we show that a similar procedure allows us to obtain a uniform $\mathcal{C}^k$ approximation of antisymmetric functions as a sum of $K$ terms, where each term is a product of a smooth totally symmetric function and a smooth antisy
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;chatGPT 3.5-turbo&#30340;&#24494;&#35843;&#65292;&#22312;&#23398;&#20064;DNA&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;&#26041;&#38754;&#26174;&#31034;&#20986;&#26032;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01332</link><description>&lt;p&gt;
&#23558;&#24605;&#32500;&#21644;LLMs&#20018;&#32852;&#36215;&#26469;&#23398;&#20064;DNA&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Chaining thoughts and LLMs to learn DNA structural biophysics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;chatGPT 3.5-turbo&#30340;&#24494;&#35843;&#65292;&#22312;&#23398;&#20064;DNA&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;&#26041;&#38754;&#26174;&#31034;&#20986;&#26032;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#21457;&#23637;AI&#31185;&#23398;&#23478;&#30340;&#19968;&#39033;&#37325;&#35201;&#21457;&#23637;&#26159;&#65292;&#19968;&#20010;&#33021;&#22815;&#25972;&#21512;&#21508;&#31181;&#23454;&#39564;&#25968;&#25454;&#24182;&#29983;&#25104;&#21487;&#39564;&#35777;&#20551;&#35774;&#30340;&#24037;&#20855;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23450;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#21019;&#24314;&#29992;&#20110;&#19987;&#38376;&#20174;&#20107;&#21333;&#19968;&#31185;&#23398;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#36890;&#29992;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;chatGPT 3.5-turbo&#65292;&#21487;&#20197;&#34987;&#24494;&#35843;&#26469;&#23398;&#20064;DNA&#30340;&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#27169;&#22411;&#24494;&#35843;&#20026;&#36820;&#22238;&#24605;&#32500;&#38142;&#24335;&#21709;&#24212;&#20197;&#21450;&#20018;&#32852;&#24494;&#35843;&#29992;&#20110;&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#33021;&#21147;&#26469;&#20998;&#26512;&#21644;&#35774;&#35745;DNA&#24207;&#21015;&#21450;&#20854;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01332v1 Announce Type: cross  Abstract: The future development of an AI scientist, a tool that is capable of integrating a variety of experimental data and generating testable hypotheses, holds immense potential. So far, bespoke machine learning models have been created to specialize in singular scientific tasks, but otherwise lack the flexibility of a general purpose model. Here, we show that a general purpose large language model, chatGPT 3.5-turbo, can be fine-tuned to learn the structural biophysics of DNA. We find that both fine-tuning models to return chain-of-thought responses and chaining together models fine-tuned for subtasks have an enhanced ability to analyze and design DNA sequences and their structures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#65288;BNS&#65289;&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#20855;&#26377;&#23567;&#21442;&#25968;&#31354;&#38388;&#12289;&#24555;&#36895;&#20248;&#21270;&#12289;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#22312;&#20302;&#20013; NFE &#33539;&#22260;&#20869;&#25509;&#36817;&#26631;&#20934;&#31934;&#28860;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01329</link><description>&lt;p&gt;
&#20026;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#25552;&#20379;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#65288;BNS&#65289;&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#20855;&#26377;&#23567;&#21442;&#25968;&#31354;&#38388;&#12289;&#24555;&#36895;&#20248;&#21270;&#12289;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#22312;&#20302;&#20013; NFE &#33539;&#22260;&#20869;&#25509;&#36817;&#26631;&#20934;&#31934;&#28860;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#65288;BNS&#65289;&#27714;&#35299;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#31639;&#31934;&#39635;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;BNS &#27714;&#35299;&#22120;&#22522;&#20110;&#19968;&#31995;&#21015;&#21487;&#35777;&#26126;&#21253;&#21547;&#29616;&#26377;&#25968;&#20540; ODE &#27714;&#35299;&#22120;&#30340;&#38750;&#24179;&#31283;&#27714;&#35299;&#22120;&#23478;&#26063;&#65292;&#38543;&#20043;&#26174;&#33879;&#25913;&#36827;&#26679;&#26412;&#36924;&#36817;&#24230;&#65288;PSNR&#65289;&#36229;&#36807;&#36825;&#20123;&#22522;&#32447;&#12290;&#19982;&#27169;&#22411;&#31934;&#28860;&#30456;&#27604;&#65292;BNS &#27714;&#35299;&#22120;&#20855;&#26377;&#24494;&#23567;&#21442;&#25968;&#31354;&#38388;&#65288;&lt;200 &#21442;&#25968;&#65289;&#12289;&#24555;&#36895;&#20248;&#21270;&#65288;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#65289;&#12289;&#20445;&#25345;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#19982;&#20197;&#21069;&#30340;&#27714;&#35299;&#22120;&#31934;&#28860;&#26041;&#27861;&#30456;&#21453;&#65292;&#20960;&#20046;&#33021;&#22312;&#20302;&#20013; NFE &#33539;&#22260;&#20869;&#25509;&#36817;&#26631;&#20934;&#31934;&#28860;&#26041;&#27861;&#65292;&#22914; Progressive Distillation&#12290;&#20363;&#22914;&#65292;BNS &#27714;&#35299;&#22120;&#22312; class-conditional ImageNet-64 &#20013;&#20351;&#29992; 16 NFE &#23454;&#29616; 45 PSNR / 1.76 FID&#12290;&#25105;&#20204;&#23581;&#35797;&#20102; BNS &#27714;&#35299;&#22120;&#26469;&#36827;&#34892;&#26377;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01329v1 Announce Type: cross  Abstract: This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models. BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to model distillation, BNS solvers benefit from a tiny parameter space ($&lt;$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#20272;&#35745;&#21644;&#21435;&#20559;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#65292;&#25903;&#25345;&#29702;&#35770;&#30340;&#20223;&#30495;&#30740;&#31350;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#25991;&#26412;&#20998;&#26512;&#20013;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.01318</link><description>&lt;p&gt;
&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#65306;&#20197;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#25991;&#26412;&#20998;&#26512;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#20272;&#35745;&#21644;&#21435;&#20559;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#65292;&#25903;&#25345;&#29702;&#35770;&#30340;&#20223;&#30495;&#30740;&#31350;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#25991;&#26412;&#20998;&#26512;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#30340;&#28857;&#36190;&#20998;&#24067;&#65288;&#22914;&#28857;&#36190;&#25968;&#37327;&#65289;&#32463;&#39564;&#24615;&#24130;&#24459;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#21450;&#20854;&#21442;&#25968;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#20559;&#27491;&#21017;&#21270;&#20272;&#35745;&#65292;&#35777;&#26126;&#20102;&#21435;&#20559;&#20272;&#35745;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#20223;&#30495;&#30740;&#31350;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#23545;&#28041;&#21450; LGBTQ+ &#35805;&#39064;&#30340; X&#65288;&#21407; Twitter&#65289;&#30149;&#27602;&#24086;&#23376;&#30340;&#25991;&#26412;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01318v1 Announce Type: cross  Abstract: Motivated by the empirical power law of the distributions of credits (e.g., the number of "likes") of viral posts in social media, we introduce the high-dimensional tail index regression and methods of estimation and inference for its parameters. We propose a regularized estimator, establish its consistency, and derive its convergence rate. To conduct inference, we propose to debias the regularized estimate, and establish the asymptotic normality of the debiased estimator. Simulation studies support our theory. These methods are applied to text analyses of viral posts in X (formerly Twitter) concerning LGBTQ+.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.01317</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#23398;&#20064;&#30340;&#36339;&#25968;&#22270;&#27880;&#24847;&#21147;&#22312;&#30005;&#36335;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#20219;&#21153;&#20013;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#22823;&#22270;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#26032;&#35774;&#35745;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#36825;&#20123;&#38480;&#21046;&#20351;&#23427;&#20204;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22797;&#26434;&#30005;&#36335;&#38382;&#39064;&#26102;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOGA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#12290;HOGA&#39318;&#20808;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#35745;&#31639;&#36339;&#25968;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#36339;&#25968;&#29305;&#24449;&#20165;&#29992;&#20110;&#36890;&#36807;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#29983;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#27169;&#22359;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19981;&#21516;&#36339;&#25968;&#20043;&#38388;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#32780;&#19981;&#28041;&#21450;&#22270;&#25299;&#25169;&#12290;&#22240;&#27492;&#65292;HOGA&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30005;&#36335;&#20043;&#38388;&#30340;&#21508;&#31181;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#30340;&#25509;&#36817;&#26368;&#20248;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;&#65292;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#65292;&#20351;&#29992;Generalized EXP3&#12289;EXP3-IX&#21644;Tsallis entropy&#19979;&#30340;FTRL&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#36739;&#20043;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.01315</link><description>&lt;p&gt;
&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#20013;&#25509;&#36817;&#26368;&#20248;&#30340;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Near-optimal Per-Action Regret Bounds for Sleeping Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01315
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#30340;&#25509;&#36817;&#26368;&#20248;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;&#65292;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#65292;&#20351;&#29992;Generalized EXP3&#12289;EXP3-IX&#21644;Tsallis entropy&#19979;&#30340;FTRL&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#36739;&#20043;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#38024;&#23545;&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#30340;&#25509;&#36817;&#26368;&#20248;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;&#25932;&#25163;&#36873;&#25321;&#27599;&#36718;&#21487;&#29992;&#33218;&#30340;&#38598;&#21512;&#21644;&#23427;&#20204;&#30340;&#25439;&#22833;&#12290;&#22312;&#27599;&#36718;&#33267;&#22810;&#26377; $A$ &#20010;&#21487;&#29992;&#33218;&#30340; $K$ &#20010;&#24635;&#33218;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#30693;&#30340;&#26368;&#22909;&#19978;&#30028;&#20026; $O(K\sqrt{TA\ln{K}})$&#65292;&#36890;&#36807;&#38388;&#25509;&#26368;&#23567;&#21270;&#20869;&#37096;&#30561;&#30496;&#36951;&#25022;&#33719;&#24471;&#12290;&#19982;&#26497;&#23567;&#20540; $\Omega(\sqrt{TA})$ &#19979;&#30028;&#30456;&#27604;&#65292;&#36825;&#20010;&#19978;&#30028;&#21253;&#21547;&#39069;&#22806;&#30340;&#20056;&#25968;&#22240;&#23376; $K\ln{K}$&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#65292;&#20351;&#29992;EXP3&#12289;EXP3-IX&#21644;&#24102;&#26377;Tsallis&#29109;&#30340;FTRL&#30340;&#25512;&#24191;&#29256;&#26412;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#39034;&#24207;&#20026; $O(\sqrt{TA\ln{K}})$ &#21644; $O(\sqrt{T\sqrt{AK}})$ &#30340;&#25509;&#36817;&#26368;&#20248;&#30028;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#20174;&#30561;&#30496;&#19987;&#23478;&#33719;&#24471;&#24314;&#35758;&#30340;&#33218;&#20915;&#31574;&#38382;&#39064;&#35774;&#32622;&#65292;&#21516;&#26102;&#25512;&#24191;&#20102;EXP4&#12290;&#36825;&#20026;&#29616;&#26377;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#21644;&#36319;&#36394;&#36951;&#25022;&#30028;&#30340;&#26032;&#35777;&#26126;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01315v1 Announce Type: new  Abstract: We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for 
&lt;/p&gt;</description></item><item><title>VNLP&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#21253;&#21547;&#22810;&#31181;NLP&#24037;&#20855;&#65292;&#20854;&#20013;&#30340;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#12290;</title><link>https://arxiv.org/abs/2403.01309</link><description>&lt;p&gt;
VNLP&#65306;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
VNLP: Turkish NLP Package
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01309
&lt;/p&gt;
&lt;p&gt;
VNLP&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#21253;&#21547;&#22810;&#31181;NLP&#24037;&#20855;&#65292;&#20854;&#20013;&#30340;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VNLP&#65306;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#23436;&#25972;&#12289;&#24320;&#28304;&#12289;&#25991;&#26723;&#23436;&#22791;&#12289;&#36731;&#37327;&#32423;&#12289;&#21487;&#25237;&#20837;&#29983;&#20135;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#21508;&#31181;&#24037;&#20855;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#22914;&#21477;&#23376;&#20998;&#21106;&#21644;&#25991;&#26412;&#35268;&#33539;&#21270;&#65292;&#21040;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#21644;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#12290;&#20854;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#26082;&#26159;&#32534;&#30721;&#22120;&#21448;&#26159;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26032;&#39062;&#26550;&#26500;&#12290;VNLP&#27169;&#22411;&#35299;&#20915;&#30340;NLP&#20219;&#21153;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24418;&#24577;&#20998;&#26512;&#21644;&#28040;&#27495;&#20197;&#21450;&#35789;&#24615;&#26631;&#27880;&#12290;&#27492;&#22806;&#65292;&#23427;&#37197;&#22791;&#20102;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#21644;&#30456;&#24212;&#30340;SentencePiece Unigram&#26631;&#35760;&#22120;&#12290;VNLP&#20855;&#26377;&#24320;&#28304;&#30340;GitHub&#23384;&#20648;&#24211;&#12289;ReadtheDocs&#25991;&#26723;&#12289;&#26041;&#20415;&#23433;&#35013;&#30340;PyPi&#21253;&#12289;Python&#21644;&#36887;&#21495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01309v1 Announce Type: cross  Abstract: In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language. It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization, to the more advanced ones, such as text and token classification models. Its token classification models are based on "Context Model", a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological Analysis \&amp; Disambiguation and Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and comma
&lt;/p&gt;</description></item><item><title>VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01308</link><description>&lt;p&gt;
VBART: &#22303;&#32819;&#20854;LLM
&lt;/p&gt;
&lt;p&gt;
VBART: The Turkish LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01308
&lt;/p&gt;
&lt;p&gt;
VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VBART&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#22823;&#35821;&#26009;&#24211;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;VBART&#26159;&#22522;&#20110;BART&#21644;mBART&#27169;&#22411;&#30340;&#22909;&#24605;&#36335;&#26500;&#24314;&#30340;&#32039;&#20945;&#22411;LLMs&#65292;&#20998;&#20026;Large&#21644;XLarge&#20004;&#20010;&#23610;&#23544;&#12290;&#24494;&#35843;&#21518;&#30340;VBART&#27169;&#22411;&#22312;&#25552;&#21462;&#24615;&#25991;&#26412;&#25688;&#35201;&#12289;&#26631;&#39064;&#29983;&#25104;&#12289;&#25991;&#26412;&#25913;&#20889;&#12289;&#38382;&#31572;&#21644;&#38382;&#39064;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#23427;&#20204;&#20801;&#35768;&#20026;&#26410;&#26469;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25317;&#26377;&#20026;&#22303;&#32819;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLM&#27604;&#22810;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#20026;&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#35821;&#20998;&#35789;&#22120;&#27604;OpenAI&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#26356;&#39640;&#25928;7&#20493;&#12290;&#26368;&#21518;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#29616;&#26377;&#39044;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.01306</link><description>&lt;p&gt;
ICC&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#31579;&#36873;&#30340;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#38024;&#23545;&#37197;&#23545;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#30340;Web&#35268;&#27169;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#25361;&#25112;&#22312;&#37326;&#22806;&#25968;&#25454;&#38598;&#30340;&#39640;&#22122;&#22768;&#29305;&#24615;&#12290;&#26631;&#20934;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#25104;&#21151;&#21435;&#38500;&#20102;&#19981;&#21305;&#37197;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#65292;&#20294;&#20801;&#35768;&#35821;&#20041;&#30456;&#20851;&#20294;&#38750;&#24120;&#25277;&#35937;&#25110;&#20027;&#35266;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#33021;&#21147;&#26469;&#38548;&#31163;&#25552;&#20379;&#22312;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#35780;&#20272;&#27809;&#26377;&#22270;&#20687;&#21442;&#32771;&#30340;&#26631;&#39064;&#25991;&#26412;&#20197;&#34913;&#37327;&#20854;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#20379;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#34913;&#37327;&#35270;&#35273;-&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#30340;&#24378;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19982;&#20154;&#31867;&#23545;&#21333;&#35789;&#21644;&#21477;&#23376;&#32423;&#25991;&#26412;&#20855;&#20307;&#24615;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 Announce Type: new  Abstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#36135;&#36816;&#22312;&#32447;&#37319;&#36141;&#20013;&#36741;&#21161;&#36827;&#34892;&#20379;&#24212;&#21830;&#21457;&#29616;&#65292;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#23458;&#25143;&#30340;&#38656;&#27714;&#21644;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.01301</link><description>&lt;p&gt;
&#22312;&#32447;&#37319;&#36141;&#20013;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Supplier Recommendation in Online Procurement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#36135;&#36816;&#22312;&#32447;&#37319;&#36141;&#20013;&#36741;&#21161;&#36827;&#34892;&#20379;&#24212;&#21830;&#21457;&#29616;&#65292;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#23458;&#25143;&#30340;&#38656;&#27714;&#21644;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20379;&#24212;&#38142;&#20248;&#21270;&#23545;&#20110;&#20581;&#24247;&#21644;&#30408;&#21033;&#30340;&#20225;&#19994;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#20844;&#21496;&#20351;&#29992;&#22312;&#32447;&#37319;&#36141;&#31995;&#32479;&#19982;&#20379;&#24212;&#21830;&#31614;&#35746;&#21512;&#21516;&#12290;&#36992;&#35831;&#26368;&#20855;&#31454;&#20105;&#21147;&#30340;&#20379;&#24212;&#21830;&#31454;&#26631;&#36825;&#20123;&#21512;&#21516;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#21327;&#21161;&#22312;&#36947;&#36335;&#36135;&#36816;&#22312;&#32447;&#37319;&#36141;&#20013;&#36827;&#34892;&#20379;&#24212;&#21830;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#23458;&#25143;&#30340;&#38656;&#27714;&#21644;&#20559;&#22909;&#12290;&#36825;&#26159;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#31181;&#26032;&#39062;&#24212;&#29992;&#65292;&#38656;&#35201;&#35774;&#35745;&#36873;&#25321;&#20197;&#31526;&#21512;&#22312;&#32447;&#37319;&#36141;&#30340;&#29420;&#29305;&#35201;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01301v1 Announce Type: cross  Abstract: Supply chain optimization is key to a healthy and profitable business. Many companies use online procurement systems to agree contracts with suppliers. It is vital that the most competitive suppliers are invited to bid for such contracts. In this work, we propose a recommender system to assist with supplier discovery in road freight online procurement. Our system is able to provide personalized supplier recommendations, taking into account customer needs and preferences. This is a novel application of recommender systems, calling for design choices that fit the unique requirements of online procurement. Our preliminary results, using real-world data, are promising.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20809;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#23545;&#22810;&#20540;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#30340;&#38887;&#24615;&#65292;&#21457;&#29616;&#38656;&#35201;&#22823;&#37327;CRPs&#25165;&#33021;&#25104;&#21151;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20809;&#23376;PUF&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01299</link><description>&lt;p&gt;
&#19968;&#20010;&#20809;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#23545;&#22810;&#20540;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Photonic Physically Unclonable Function's Resilience to Multiple-Valued Machine Learning Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20809;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#23545;&#22810;&#20540;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#30340;&#38887;&#24615;&#65292;&#21457;&#29616;&#38656;&#35201;&#22823;&#37327;CRPs&#25165;&#33021;&#25104;&#21151;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20809;&#23376;PUF&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#65288;PUFs&#65289;&#20351;&#29992;&#38750;&#32447;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;-&#21709;&#24212;&#23545;&#65288;CRPs&#65289;&#26469;&#35782;&#21035;&#38598;&#25104;&#30005;&#36335;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25361;&#25112;&#21644;&#30456;&#24212;&#21709;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#65292;&#21363;&#20351;&#24050;&#30693;&#26576;&#20123;CRPs&#30340;&#23376;&#38598;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#20809;&#23376;PUF&#65292;&#30456;&#27604;&#38750;&#20809;&#23398;&#23545;&#24212;&#29289;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23433;&#20840;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#31181;PUF&#23545;&#22522;&#20110;&#22810;&#20540;&#36923;&#36753;&#30340;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#32422;&#38656;&#35201;1,000&#20010;CRPs&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25165;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#22320;&#39044;&#27979;&#21709;&#24212;&#20301;&#12290;&#32771;&#34385;&#21040;&#20174;&#20809;&#23376;PUF&#33719;&#21462;&#22823;&#37327;CRPs&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20809;&#23376;PUF&#23545;&#27492;&#31867;&#25915;&#20987;&#20855;&#26377;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01299v1 Announce Type: cross  Abstract: Physically unclonable functions (PUFs) identify integrated circuits using nonlinearly-related challenge-response pairs (CRPs). Ideally, the relationship between challenges and corresponding responses is unpredictable, even if a subset of CRPs is known. Previous work developed a photonic PUF offering improved security compared to non-optical counterparts. Here, we investigate this PUF's susceptibility to Multiple-Valued-Logic-based machine learning attacks. We find that approximately 1,000 CRPs are necessary to train models that predict response bits better than random chance. Given the significant challenge of acquiring a vast number of CRPs from a photonic PUF, our results demonstrate photonic PUF resilience against such attacks.
&lt;/p&gt;</description></item><item><title>NoMAD-Attention&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;CPU&#19978;&#20351;&#29992;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#21462;&#20195;MAD&#25805;&#20316;&#65292;&#20197;&#23454;&#29616;LLM&#25512;&#26029;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.01273</link><description>&lt;p&gt;
NoMAD-Attention: &#36890;&#36807;&#26080;MAD&#25805;&#20316;&#23454;&#29616;CPU&#19978;&#39640;&#25928;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01273
&lt;/p&gt;
&lt;p&gt;
NoMAD-Attention&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;CPU&#19978;&#20351;&#29992;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#21462;&#20195;MAD&#25805;&#20316;&#65292;&#20197;&#23454;&#29616;LLM&#25512;&#26029;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#19978;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#23384;&#22312;&#22823;&#37327;&#26114;&#36149;&#30340;MAD&#30697;&#38453;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#29616;&#20195;CPU&#20013;&#30340;&#21333;&#25351;&#20196;&#22810;&#25968;&#25454;&#65288;SIMD&#65289;&#23492;&#23384;&#22120;&#26159;&#19968;&#31181;&#29645;&#36149;&#30340;&#23453;&#30707;&#65292;&#23427;&#20801;&#35768;&#22312;&#25209;&#22788;&#29702;&#20013;&#36827;&#34892;&#36229;&#20302;&#24310;&#36831;&#26597;&#25214;&#12290;&#25105;&#20204;&#21033;&#29992;CPU&#30340;&#36825;&#19968;&#29420;&#29305;&#33021;&#21147;&#25552;&#20986;&#20102;NoMAD-Attention&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;MAD&#25805;&#20316;&#26367;&#25442;&#20026;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#12290;&#36890;&#36807;&#30828;&#20214;&#24863;&#30693;&#30340;&#31639;&#27861;&#35774;&#35745;&#65292;NoMAD-Attention&#23454;&#29616;&#20102;&#36890;&#36807;&#37325;&#22797;&#24555;&#36895;&#35775;&#38382;SIMD&#23492;&#23384;&#22120;&#26469;&#35745;&#31639;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#22823;&#23567;&#38750;&#24120;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;NoMAD-Attention&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;LLM&#65292;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;NoMAD-Attention&#24456;&#22909;&#22320;&#20445;&#25345;&#20102;&#21407;&#22987;LLM&#30340;&#36136;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;4&#20301;&#37327;&#21270;&#30340;LLaMA-7B-bas&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01273v1 Announce Type: cross  Abstract: Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations. In this paper, we argue that there is a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for ultra-low-latency lookups in batch. We leverage this unique capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups. Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based LLMs without model finetuning. Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well, and speeds up the 4-bit quantized LLaMA-7B-bas
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#23558;&#21518;&#39564;&#35843;&#25972;&#26367;&#25442;&#20026;&#22686;&#21152;&#20449;&#24515;&#30340;&#20808;&#39564;&#20998;&#24067;&#30340;&#21487;&#34892;&#24615;&#65292;&#24341;&#20837;&#20102;&#23454;&#29992;&#30340;&#8220;DirClip&#8221;&#20808;&#39564;&#21644;&#8220;&#20449;&#24515;&#20808;&#39564;&#8221;&#65292;&#25552;&#20379;&#20102;&#23545;&#20449;&#24515;&#20808;&#39564;&#30340;&#19968;&#33324;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.01272</link><description>&lt;p&gt;
&#21487;&#20197;&#29992;&#33258;&#20449;&#20808;&#39564;&#26367;&#20195;&#20919;&#21364;&#21518;&#39564;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a Confident Prior Replace a Cold Posterior?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01272
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#23558;&#21518;&#39564;&#35843;&#25972;&#26367;&#25442;&#20026;&#22686;&#21152;&#20449;&#24515;&#30340;&#20808;&#39564;&#20998;&#24067;&#30340;&#21487;&#34892;&#24615;&#65292;&#24341;&#20837;&#20102;&#23454;&#29992;&#30340;&#8220;DirClip&#8221;&#20808;&#39564;&#21644;&#8220;&#20449;&#24515;&#20808;&#39564;&#8221;&#65292;&#25552;&#20379;&#20102;&#23545;&#20449;&#24515;&#20808;&#39564;&#30340;&#19968;&#33324;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24448;&#24448;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#26631;&#31614;&#22122;&#22768;&#27700;&#24179;&#12290;&#24403;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#32463;&#24120;&#27424;&#25311;&#21512;&#65292;&#38169;&#35823;&#22320;&#34920;&#31034;&#25968;&#25454;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#35843;&#25972;&#21518;&#39564;&#27010;&#29575;&#65292;&#36825;&#26679;&#21487;&#20197;&#25913;&#21892;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#25311;&#21512;&#65292;&#20294;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#35299;&#37322;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21518;&#39564;&#35843;&#25972;&#26159;&#21542;&#21487;&#20197;&#34987;&#19968;&#31181;&#25552;&#39640;&#20449;&#24515;&#30340;&#20808;&#39564;&#20998;&#24067;&#26367;&#20195;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#37319;&#26679;&#8220;DirClip&#8221;&#20808;&#39564;&#65292;&#24182;&#19988;&#20960;&#20046;&#19982;&#20919;&#21364;&#21518;&#39564;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#20449;&#24515;&#20808;&#39564;&#8221;&#65292;&#23427;&#22312;&#28201;&#24230;&#36235;&#20110;&#38646;&#26102;&#30452;&#25509;&#36817;&#20284;&#20110;&#20919;&#24067;&#23616;&#65292;&#20294;&#19981;&#33021;&#36731;&#26494;&#25277;&#26679;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#25552;&#39640;&#20449;&#24515;&#30340;&#20808;&#39564;&#30340;&#19968;&#33324;&#35265;&#35299;&#65292;&#20363;&#22914;&#20309;&#26102;&#21487;&#33021;&#20986;&#29616;&#20998;&#27495;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#24494;&#35843;&#26469;&#32531;&#35299;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01272v1 Announce Type: new  Abstract: Benchmark datasets used for image classification tend to have very low levels of label noise. When Bayesian neural networks are trained on these datasets, they often underfit, misrepresenting the aleatoric uncertainty of the data. A common solution is to cool the posterior, which improves fit to the training data but is challenging to interpret from a Bayesian perspective. We explore whether posterior tempering can be replaced by a confidence-inducing prior distribution. First, we introduce a "DirClip" prior that is practical to sample and nearly matches the performance of a cold posterior. Second, we introduce a "confidence prior" that directly approximates a cold likelihood in the limit of decreasing temperature but cannot be easily sampled. Lastly, we provide several general insights into confidence-inducing priors, such as when they might diverge and how fine-tuning can mitigate numerical instability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#35777;&#32852;&#37030;&#23398;&#20064;&#22312;&#38754;&#20020;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01268</link><description>&lt;p&gt;
&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65306;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#35777;&#32852;&#37030;&#23398;&#20064;&#22312;&#38754;&#20020;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20132;&#25442;&#21442;&#25968;&#32780;&#38750;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#35757;&#32451;&#19968;&#20010;&#40657;&#21283;&#23376;&#21644;&#39640;&#32500;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30001;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#25110;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65288;DRA&#65289;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;DRA&#20174;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29616;&#26377;&#25216;&#26415;&#65288;&#22914;&#24046;&#20998;&#38544;&#31169;&#65289;&#26080;&#27861;&#26377;&#25928;&#22320;&#38459;&#27490;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01268v1 Announce Type: new  Abstract: Federated Learning (FL) trains a black-box and high-dimensional model among different clients by exchanging parameters instead of direct data sharing, which mitigates the privacy leak incurred by machine learning. However, FL still suffers from membership inference attacks (MIA) or data reconstruction attacks (DRA). In particular, an attacker can extract the information from local datasets by constructing DRA, which cannot be effectively throttled by existing techniques, e.g., Differential Privacy (DP).   In this paper, we aim to ensure a strong privacy guarantee for FL under DRA. We prove that reconstruction errors under DRA are constrained by the information acquired by an attacker, which means that constraining the transmitted information can effectively throttle DRA. To quantify the information leakage incurred by FL, we establish a channel model, which depends on the upper bound of joint mutual information between the local dataset 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01267</link><description>&lt;p&gt;
&#35299;&#21078;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dissecting Language Models: Machine Unlearning via Selective Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01267
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35774;&#35745;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#36873;&#25321;&#24615;&#20462;&#21098;&#26041;&#27861;&#65292;&#26681;&#25454;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;&#33021;&#21147;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#31227;&#38500;&#31070;&#32463;&#20803;&#65292;&#32780;&#38750;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21024;&#38500;&#33021;&#22815;&#23454;&#29616;&#29305;&#23450;&#34892;&#20026;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#20013;&#30340;&#21069;&#39304;&#31070;&#32463;&#20803;&#21644;&#27880;&#24847;&#21147;&#31070;&#32463;&#20803;&#26159;&#19987;&#38376;&#21270;&#30340;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#26576;&#20123;&#31070;&#32463;&#20803;&#27604;&#20854;&#20182;&#31070;&#32463;&#20803;&#26356;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01267v1 Announce Type: cross  Abstract: Understanding and shaping the behaviour of Large Language Models (LLMs) is increasingly important as applications become more powerful and more frequently adopted. This paper introduces a machine unlearning method specifically designed for LLMs. We introduce a selective pruning method for LLMs that removes neurons based on their relative importance on a targeted capability compared to overall network performance. This approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. Our findings reveal that both feed-forward and attention neurons in LLMs are specialized; that is, for specific tasks, certain neurons are more crucial than others.
&lt;/p&gt;</description></item><item><title>SceneCraft&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#21487;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#20195;&#30721;&#65292;&#23454;&#29616;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#36890;&#36807;&#20808;&#24314;&#27169;&#31354;&#38388;&#20851;&#31995;&#20877;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#24182;&#20511;&#21161;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22330;&#26223;&#20248;&#21270;&#21644;&#24211;&#23398;&#20064;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01248</link><description>&lt;p&gt;
SceneCraft&#65306;&#19968;&#20010;&#29992;&#20110;&#23558;&#25991;&#26412;&#25551;&#36848;&#21512;&#25104;&#20026;Blender&#20195;&#30721;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01248
&lt;/p&gt;
&lt;p&gt;
SceneCraft&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#21487;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#20195;&#30721;&#65292;&#23454;&#29616;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#36890;&#36807;&#20808;&#24314;&#27169;&#31354;&#38388;&#20851;&#31995;&#20877;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#24182;&#20511;&#21161;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22330;&#26223;&#20248;&#21270;&#21644;&#24211;&#23398;&#20064;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SceneCraft&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#21487;&#25191;&#34892;&#30340;Python&#33050;&#26412;&#65292;&#29992;&#20110;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#35813;&#36807;&#31243;&#38656;&#35201;&#22797;&#26434;&#30340;&#31354;&#38388;&#35268;&#21010;&#21644;&#24067;&#23616;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#32423;&#25277;&#35937;&#12289;&#25112;&#30053;&#35268;&#21010;&#21644;&#24211;&#23398;&#20064;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;SceneCraft&#39318;&#20808;&#23558;&#22330;&#26223;&#22270;&#24314;&#27169;&#20026;&#34013;&#22270;&#65292;&#35814;&#32454;&#25551;&#36848;&#22330;&#26223;&#20013;&#21508;&#36164;&#20135;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;SceneCraft&#26681;&#25454;&#36825;&#20010;&#22270;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#23558;&#20851;&#31995;&#36716;&#21270;&#20026;&#36164;&#20135;&#24067;&#23616;&#30340;&#25968;&#20540;&#32422;&#26463;&#12290;&#25509;&#19979;&#26469;&#65292;SceneCraft&#21033;&#29992;&#20687;GPT-V&#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#24863;&#30693;&#20248;&#21183;&#26469;&#20998;&#26512;&#28210;&#26579;&#22270;&#20687;&#24182;&#36845;&#20195;&#22320;&#20248;&#21270;&#22330;&#26223;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20043;&#19978;&#65292;SceneCraft&#20855;&#22791;&#19968;&#20010;&#24211;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#24120;&#35265;&#30340;&#33050;&#26412;&#20989;&#25968;&#32534;&#35793;&#20026;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#24211;&#65292;&#20419;&#36827;&#25345;&#32493;&#30340;&#33258;&#25105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01248v1 Announce Type: cross  Abstract: This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self
&lt;/p&gt;</description></item><item><title>AcME-AD&#26159;&#19968;&#31181;&#26681;&#26893;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#21644;&#20551;&#35774;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#27169;&#22411;&#29305;&#23450;&#25110;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.01245</link><description>&lt;p&gt;
AcME-AD: &#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#21152;&#36895;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
AcME-AD: Accelerated Model Explanations for Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01245
&lt;/p&gt;
&lt;p&gt;
AcME-AD&#26159;&#19968;&#31181;&#26681;&#26893;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#21644;&#20551;&#35774;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#27169;&#22411;&#29305;&#23450;&#25110;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#36861;&#27714;&#24555;&#36895;&#21644;&#31283;&#20581;&#30340;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20256;&#32479;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#22312;&#24322;&#24120;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24448;&#24448;&#26159;&#40657;&#30418;&#23376;&#65292;&#25552;&#20379;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#27934;&#23519;&#26377;&#38480;&#12290;&#32570;&#20047;&#36879;&#26126;&#24230;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#29702;&#35299;&#24322;&#24120;&#26816;&#27979;&#21407;&#22240;&#33267;&#20851;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#30340;&#37319;&#29992;&#12290;&#21516;&#26102;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35201;&#36805;&#36895;&#33719;&#24471;&#35299;&#37322;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;AcME-AD&#65292;&#19968;&#31181;&#26681;&#26893;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#28548;&#28165;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290; AcME-AD&#36890;&#36807;&#25552;&#20379;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#21644;&#20551;&#35774;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#27169;&#22411;&#29305;&#23450;&#25110;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38754;&#21521;&#27169;&#22411;&#30340;&#12289;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20419;&#36827;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01245v1 Announce Type: new  Abstract: Pursuing fast and robust interpretability in Anomaly Detection is crucial, especially due to its significance in practical applications. Traditional Anomaly Detection methods excel in outlier identification but are often black-boxes, providing scant insights into their decision-making process. This lack of transparency compromises their reliability and hampers their adoption in scenarios where comprehending the reasons behind anomaly detection is vital. At the same time, getting explanations quickly is paramount in practical scenarios. To bridge this gap, we present AcME-AD, a novel approach rooted in Explainable Artificial Intelligence principles, designed to clarify Anomaly Detection models for tabular data. AcME-AD transcends the constraints of model-specific or resource-heavy explainability techniques by delivering a model-agnostic, efficient solution for interoperability. It offers local feature importance scores and a what-if analy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01242</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#33258;&#21160;&#21270;&#65306;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Augmenting Automation: Intent-Based User Instruction Classification with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#33258;&#21160;&#21270;&#31995;&#32479;&#22312;&#25511;&#21046;&#30005;&#36335;&#21644;&#35774;&#22791;&#26102;&#25552;&#20379;&#20102;&#26041;&#20415;&#21644;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#21629;&#20196;&#36827;&#34892;&#25511;&#21046;&#65292;&#38480;&#21046;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#22686;&#24378;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#29992;&#25143;&#25351;&#20196;&#34920;&#31034;&#20026;&#24847;&#22270;&#65292;&#20801;&#35768;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#21629;&#20196;&#30340;&#24773;&#20917;&#19979;&#21160;&#24577;&#25511;&#21046;&#30005;&#36335;&#12290;&#36890;&#36807;&#35757;&#32451;&#22312;&#26631;&#35760;&#30340;&#29992;&#25143;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#23545;&#24847;&#22270;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#30452;&#35266;&#21644;&#21487;&#36866;&#24212;&#30340;&#25511;&#21046;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#24847;&#22270;&#30340;&#30005;&#21160;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#29992;&#20110;&#24847;&#22270;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01242v1 Announce Type: cross  Abstract: Electric automation systems offer convenience and efficiency in controlling electrical circuits and devices. Traditionally, these systems rely on predefined commands for control, limiting flexibility and adaptability. In this paper, we propose a novel approach to augment automation by introducing intent-based user instruction classification using machine learning techniques. Our system represents user instructions as intents, allowing for dynamic control of electrical circuits without relying on predefined commands. Through a machine learning model trained on a labeled dataset of user instructions, our system classifies intents from user input, enabling a more intuitive and adaptable control scheme. We present the design and implementation of our intent-based electric automation system, detailing the development of the machine learning model for intent classification. Experimental results demonstrate the effectiveness of our approach i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#30340;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20256;&#32479;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#23545;&#27604;&#20998;&#26512;&#65292;&#21019;&#36896;&#20102;&#20248;&#20808;&#32771;&#34385;&#20998;&#23376;&#21151;&#33021;&#24615;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#37325;&#26032;&#35745;&#31639;&#23884;&#20837;&#21521;&#37327;&#23454;&#29616;&#20102;&#26356;&#22909;&#32452;&#32455;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.01234</link><description>&lt;p&gt;
&#27963;&#36291;&#28145;&#24230;&#26680;&#23398;&#20064;&#20998;&#23376;&#21151;&#33021;&#24615;&#65306;&#23454;&#29616;&#21160;&#24577;&#32467;&#26500;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#30340;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20256;&#32479;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#23545;&#27604;&#20998;&#26512;&#65292;&#21019;&#36896;&#20102;&#20248;&#20808;&#32771;&#34385;&#20998;&#23376;&#21151;&#33021;&#24615;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#37325;&#26032;&#35745;&#31639;&#23884;&#20837;&#21521;&#37327;&#23454;&#29616;&#20102;&#26356;&#22909;&#32452;&#32455;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20998;&#23376;&#31354;&#38388;&#23545;&#20110;&#25512;&#36827;&#25105;&#20204;&#23545;&#21270;&#23398;&#24615;&#36136;&#21644;&#21453;&#24212;&#30340;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#22312;&#26448;&#26009;&#31185;&#23398;&#12289;&#21307;&#23398;&#21644;&#33021;&#28304;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#24615;&#21019;&#26032;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#36827;&#34892;&#20998;&#23376;&#21457;&#29616;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#38480;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#20351;&#29992;QM9&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23558;DKL&#19982;&#20256;&#32479;VAEs&#36827;&#34892;&#23545;&#27604;&#65292;&#21518;&#32773;&#22522;&#20110;&#30456;&#20284;&#24615;&#20998;&#26512;&#20998;&#23376;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#31232;&#30095;&#35268;&#24459;&#24615;&#32780;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;DKL&#36890;&#36807;&#23558;&#32467;&#26500;&#19982;&#24615;&#36136;&#30456;&#20851;&#32852;&#65292;&#21019;&#36896;&#20102;&#20248;&#20808;&#32771;&#34385;&#20998;&#23376;&#21151;&#33021;&#24615;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;&#36825;&#26159;&#36890;&#36807;&#36845;&#20195;&#37325;&#26032;&#35745;&#31639;&#23884;&#20837;&#21521;&#37327;&#26469;&#23454;&#29616;&#30340;&#65292;&#19982;&#30446;&#26631;&#24615;&#36136;&#30340;&#23454;&#39564;&#21487;&#29992;&#24615;&#20445;&#25345;&#19968;&#33268;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#28508;&#22312;&#31354;&#38388;&#19981;&#20165;&#32452;&#32455;&#26356;&#22909;&#65292;&#32780;&#19988;&#20855;&#26377;&#29420;&#29305;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01234v1 Announce Type: new  Abstract: Exploring molecular spaces is crucial for advancing our understanding of chemical properties and reactions, leading to groundbreaking innovations in materials science, medicine, and energy. This paper explores an approach for active learning in molecular discovery using Deep Kernel Learning (DKL), a novel approach surpassing the limits of classical Variational Autoencoders (VAEs). Employing the QM9 dataset, we contrast DKL with traditional VAEs, which analyze molecular structures based on similarity, revealing limitations due to sparse regularities in latent spaces. DKL, however, offers a more holistic perspective by correlating structure with properties, creating latent spaces that prioritize molecular functionality. This is achieved by recalculating embedding vectors iteratively, aligning with the experimental availability of target properties. The resulting latent spaces are not only better organized but also exhibit unique characteri
&lt;/p&gt;</description></item><item><title>Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01232</link><description>&lt;p&gt;
Polynormer: &#22810;&#39033;&#24335;&#34920;&#36798;&#30340;&#32447;&#24615;&#26102;&#38388;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01232
&lt;/p&gt;
&lt;p&gt;
Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#65292;&#29702;&#35770;&#19978;&#23427;&#27604;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;GT&#27169;&#22411;&#33267;&#23569;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#32447;&#24615;GTs&#65292;&#20294;&#23427;&#20204;&#22312;&#20960;&#20010;&#28909;&#38376;&#22270;&#25968;&#25454;&#38598;&#19978;&#20173;&#33853;&#21518;&#20110;GNN&#23545;&#24212;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#21147;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#24179;&#34913;GTs&#30340;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polynormer&#65292;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#12290;Polynormer&#26500;&#24314;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#36755;&#20837;&#29305;&#24449;&#19978;&#23398;&#20064;&#39640;&#27425;&#22810;&#39033;&#24335;&#12290;&#20026;&#20102;&#20351;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#20998;&#24320;&#38598;&#25104;&#65292;&#20174;&#32780;&#20135;&#29983;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#20851;&#27880;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;Polynormer&#37319;&#29992;&#20102;&#32447;&#24615;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#20851;&#27880;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35270;&#39057;&#21644;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#38544;&#31169;&#20445;&#25252;&#22320;&#35782;&#21035;&#35828;&#35805;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;&#22312;&#37326;&#22806;&#33719;&#21462;&#20010;&#20154;&#24405;&#38899;&#22256;&#38590;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.01229</link><description>&lt;p&gt;
REWIND&#25968;&#25454;&#38598;&#65306;&#22312;&#37326;&#22806;&#22810;&#27169;&#24577;&#36523;&#20307;&#36816;&#21160;&#20449;&#21495;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#38899;&#29366;&#24577;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01229
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35270;&#39057;&#21644;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#38544;&#31169;&#20445;&#25252;&#22320;&#35782;&#21035;&#35828;&#35805;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;&#22312;&#37326;&#22806;&#33719;&#21462;&#20010;&#20154;&#24405;&#38899;&#22256;&#38590;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20154;&#31867;&#30340;&#35828;&#35805;&#26159;&#29702;&#35299;&#31038;&#20250;&#20114;&#21160;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20250;&#20174;&#20010;&#20154;&#24405;&#38899;&#20013;&#26816;&#27979;&#35828;&#35805;&#65292;&#23601;&#20687;&#20043;&#21069;&#20026;&#20250;&#35758;&#22330;&#26223;&#25152;&#20570;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#22312;&#25317;&#25380;&#30340;&#32858;&#20250;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#12289;&#21518;&#21220;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24456;&#38590;&#33719;&#21462;&#20010;&#20154;&#24405;&#38899;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#35270;&#39057;&#21644;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36890;&#36807;&#26816;&#27979;&#20854;&#30456;&#20851;&#25163;&#21183;&#26469;&#35782;&#21035;&#35821;&#38899;&#65292;&#36825;&#31181;&#26041;&#24335;&#26082;&#19981;&#24341;&#20154;&#27880;&#30446;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26412;&#36523;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#20351;&#29992;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#33719;&#21462;&#30340;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32858;&#20250;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#35760;&#24405;&#12290;&#30456;&#21453;&#65292;&#23545;&#35828;&#35805;&#29366;&#24577;&#30340;&#27880;&#37322;&#36890;&#24120;&#26159;&#36890;&#36807;&#20154;&#31867;&#26631;&#27880;&#32773;&#20174;&#35270;&#39057;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#32780;&#27809;&#26377;&#23545;&#36825;&#31181;&#26041;&#27861;&#38024;&#23545;&#22522;&#20110;&#38899;&#39057;&#30340;&#22320;&#38754;&#30495;&#23454;&#24615;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#38750;&#38899;&#39057;&#35828;&#35805;&#29366;&#24577;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01229v1 Announce Type: cross  Abstract: Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking statu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.01221</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25104;&#26412;&#25928;&#29575;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#32467;&#26524;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25512;&#33616;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25805;&#20316;&#30340;&#36755;&#20837;&#26356;&#25913;&#65292;&#23558;&#19981;&#33391;&#31995;&#32479;&#36755;&#20986;&#36716;&#21464;&#20026;&#26399;&#26395;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#26041;&#27861;&#35299;&#37322;&#21333;&#20010;&#23454;&#20363;&#65292;&#20294;&#19968;&#20123;&#30495;&#23454;&#30340;&#29992;&#20363;&#65288;&#22914;&#23458;&#25143;&#28385;&#24847;&#24230;&#65289;&#38656;&#35201;&#35782;&#21035;&#33021;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;&#23454;&#20363;&#65288;&#20363;&#22914;&#23458;&#25143;&#65289;&#30340;&#21333;&#19968;&#21453;&#20107;&#23454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25214;&#21040;&#36825;&#26679;&#30340;&#23454;&#20363;&#32452;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01221v1 Announce Type: cross  Abstract: Counterfactual explanations constitute among the most popular methods for analyzing the predictions of black-box systems since they can recommend cost-efficient and actionable changes to the input to turn an undesired system's output into a desired output. While most of the existing counterfactual methods explain a single instance, several real-world use cases, such as customer satisfaction, require the identification of a single counterfactual that can satisfy multiple instances (e.g. customers) simultaneously. In this work, we propose a flexible two-stage algorithm for finding groups of instances along with cost-efficient multi-instance counterfactual explanations. This is motivated by the fact that in most previous works the aspect of finding such groups is not addressed.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#38024;&#23545;&#21453;&#23398;&#20064;&#29615;&#22659;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23545;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.01218</link><description>&lt;p&gt;
&#31895;&#31961;&#21453;&#23398;&#20064;&#38656;&#35201;&#26356;&#21152;&#35880;&#24910;&#30340;&#35780;&#20272;&#20197;&#36991;&#20813;&#34394;&#20551;&#38544;&#31169;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01218
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#38024;&#23545;&#21453;&#23398;&#20064;&#29615;&#22659;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23545;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#20351;&#24471;&#24320;&#21457;&#21453;&#23398;&#20064;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#21024;&#38500;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#19968;&#26086;&#27169;&#22411;&#23436;&#25104;&#21453;&#23398;&#20064;&#65292;&#19982;&#35813;&#27169;&#22411;&#20132;&#20114;&#30340;&#23545;&#25163;&#23601;&#19981;&#24212;&#20877;&#33021;&#22815;&#21028;&#26029;&#21453;&#23398;&#20064;&#30340;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#22312;&#38544;&#31169;&#39046;&#22495;&#65292;&#36825;&#34987;&#31216;&#20026;&#25104;&#21592;&#25512;&#26029;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#23545;&#21453;&#23398;&#20064;&#35774;&#32622;&#30340;&#35843;&#25972;&#65288;&#23548;&#33268;&#23427;&#20204;&#30340;&#8220;U-MIA&#8221;&#23545;&#24212;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23558;&#20854;&#20998;&#20026;&#8220;&#20154;&#21475;U-MIA&#8221;&#65292;&#20854;&#20013;&#21516;&#19968;&#25915;&#20987;&#32773;&#36866;&#29992;&#20110;&#25152;&#26377;&#31034;&#20363;&#65292;&#21644;&#8220;&#27599;&#20010;&#31034;&#20363;U-MIA&#8221;&#65292;&#20854;&#20013;&#20026;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21518;&#19968;&#31867;&#21035;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#20026;&#27599;&#20010;&#23454;&#20363;&#23450;&#21046;&#20854;&#25104;&#21592;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01218v1 Announce Type: new  Abstract: The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01216</link><description>&lt;p&gt;
API&#23601;&#22815;&#20102;&#65306;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#27861;&#35775;&#38382;&#23545;&#25968;&#26102;&#22914;&#20309;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36825;&#19968;&#26222;&#36941;&#25361;&#25112;&#12290;&#25972;&#20307;&#39044;&#27979;&#65288;CP&#65289;&#20197;&#20854;&#19982;&#27169;&#22411;&#26080;&#20851;&#21644;&#26080;&#38656;&#20998;&#24067;&#30340;&#29305;&#28857;&#32780;&#38395;&#21517;&#65292;&#26159;&#21508;&#31181;LLMs&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#24819;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#23545;&#25968;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#20165;&#25903;&#25345;API&#30340;LLMs&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#23545;&#25968;&#21487;&#33021;&#23384;&#22312;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;CP&#26041;&#27861;&#65292;&#65288;1&#65289;&#19987;&#20026;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;API-only LLMs&#37327;&#36523;&#23450;&#21046;; (2) &#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;; &#20197;&#21450;(3)&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#31895;&#31890;&#24230;&#65288;&#20363;&#22914;&#65292;&#26679;&#26412;&#39057;&#29575;&#65289;&#21644;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#26469;&#21046;&#23450;&#19981;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;Massart&#22122;&#22768;&#30340;&#32447;&#24615;&#21644;ReLU&#22238;&#24402;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20855;&#26377;&#26032;&#39062;&#30340;&#36817;&#20046;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#39318;&#27425;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#20026;&#40065;&#26834;ReLU&#22238;&#24402;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01204</link><description>&lt;p&gt;
&#20855;&#26377;Massart&#22122;&#22768;&#30340;&#27969;&#24335;&#32447;&#24615;&#21644;&#20462;&#27491;&#32447;&#24615;&#31995;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01204
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;Massart&#22122;&#22768;&#30340;&#32447;&#24615;&#21644;ReLU&#22238;&#24402;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20855;&#26377;&#26032;&#39062;&#30340;&#36817;&#20046;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#39318;&#27425;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#20026;&#40065;&#26834;ReLU&#22238;&#24402;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SGD-exp&#65292;&#19968;&#31181;&#29992;&#20110;&#32447;&#24615;&#21644;ReLU&#22238;&#24402;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#22312;Massart&#22122;&#22768;&#65288;&#23545;&#25239;&#24615;&#21322;&#38543;&#26426;&#30772;&#22351;&#27169;&#22411;&#65289;&#19979;&#65292;&#23436;&#20840;&#27969;&#24335;&#35774;&#32622;&#19979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SGD-exp&#23545;&#30495;&#23454;&#21442;&#25968;&#30340;&#36817;&#20046;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#26368;&#39640;&#21487;&#36798;50%&#30340;Massart&#30772;&#22351;&#29575;&#65292;&#22312;&#23545;&#31216;&#26080;&#24551;&#30772;&#22351;&#24773;&#20917;&#19979;&#65292;&#20219;&#24847;&#30772;&#22351;&#29575;&#20063;&#26377;&#20445;&#35777;&#12290;&#36825;&#26159;&#27969;&#24335;&#35774;&#32622;&#20013;&#40065;&#26834;ReLU&#22238;&#24402;&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#32467;&#26524;&#65292;&#23427;&#26174;&#31034;&#20102;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#40065;&#26834;&#26041;&#27861;&#23545;&#20110;L1&#32447;&#24615;&#22238;&#24402;&#20855;&#26377;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#26159;&#30001;&#20110;&#36873;&#25321;&#20102;&#25351;&#25968;&#34928;&#20943;&#27493;&#38271;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#31163;&#25955;&#38543;&#26426;&#36807;&#31243;&#30340;&#28418;&#31227;&#20998;&#26512;&#65292;&#36825;&#26412;&#36523;&#20063;&#21487;&#33021;&#26159;&#26377;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01204v1 Announce Type: new  Abstract: We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of SGD-exp to the true parameter with up to $50\%$ Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for $L_1$ linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete stochastic process, which could also be interesting on its own.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#26657;&#20934;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#36807;&#28388;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#65292;&#22686;&#24378;&#27169;&#24577;&#19981;&#21464;&#24615;&#20849;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01203</link><description>&lt;p&gt;
&#20266;&#26631;&#31614;&#26657;&#20934;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#26657;&#20934;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#36807;&#28388;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#65292;&#22686;&#24378;&#27169;&#24577;&#19981;&#21464;&#24615;&#20849;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;(MMEA)&#26088;&#22312;&#35782;&#21035;&#20004;&#20010;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#31561;&#20215;&#23454;&#20307;&#65292;&#20197;&#36827;&#34892;&#25972;&#21512;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25913;&#36827;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21364;&#24573;&#35270;&#20102;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20266;&#26631;&#31614;&#26657;&#20934;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;(PCMEA)&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#29983;&#25104;&#20840;&#38754;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#21508;&#31181;&#23884;&#20837;&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#21462;&#35270;&#35273;&#12289;&#32467;&#26500;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#29305;&#24449;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#36807;&#28388;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#24182;&#22686;&#24378;&#27169;&#24577;&#19981;&#21464;&#24615;&#20849;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20266;&#26631;&#31614;&#26657;&#20934;&#19982;&#22522;&#20110;&#21160;&#37327;&#30340;&#23545;&#27604;&#34701;&#21512;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01203v1 Announce Type: cross  Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities between two multi-modal knowledge graphs for integration. Unfortunately, prior arts have attempted to improve the interaction and fusion of multi-modal information, which have overlooked the influence of modal-specific noise and the usage of labeled and unlabeled data in semi-supervised settings. In this work, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment (PCMEA) in a semi-supervised way. Specifically, in order to generate holistic entity representations, we first devise various embedding modules and attention mechanisms to extract visual, structural, relational, and attribute features. Different from the prior direct fusion methods, we next propose to exploit mutual information maximization to filter the modal-specific noise and to augment modal-invariant commonality. Then, we combine pseudo-label calibration with momentum-based contrastive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#21512;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#24046;&#20998;&#20998;&#32452;&#21644;&#19968;&#33324;&#20998;&#32452;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#35299;&#20934;&#30830;&#20998;&#35299;&#21508;&#31181;&#38382;&#39064;&#31867;&#22411;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01192</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#20840;&#23616;&#20248;&#21270;&#30340;&#22797;&#21512;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Composite Decomposition Method for Large-Scale Global Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#21512;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#24046;&#20998;&#20998;&#32452;&#21644;&#19968;&#33324;&#20998;&#32452;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#35299;&#20934;&#30830;&#20998;&#35299;&#21508;&#31181;&#38382;&#39064;&#31867;&#22411;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#65288;CC&#65289;&#31639;&#27861;&#22522;&#20110;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#20840;&#23616;&#20248;&#21270;&#65288;LSGO&#65289;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20998;&#32452;&#38454;&#27573;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26174;&#33879;&#24433;&#21709;&#20248;&#21270;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;&#19968;&#33324;&#21487;&#20998;&#31163;&#20998;&#32452;&#65288;GSG&#65289;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#24448;&#24046;&#20998;&#20998;&#32452;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#24471;&#38750;&#21487;&#21152;&#20998;&#31163;&#20989;&#25968;&#30340;&#20998;&#35299;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#20854;&#23384;&#22312;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#21512;&#21487;&#20998;&#31163;&#20998;&#32452;&#65288;CSG&#65289;&#26041;&#27861;&#65292;&#23558;DG&#21644;GSG&#26080;&#32541;&#25972;&#21512;&#21040;&#19968;&#20010;&#38382;&#39064;&#20998;&#35299;&#26694;&#26550;&#20013;&#65292;&#20197;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;CSG&#24341;&#20837;&#20102;&#19968;&#20010;&#36880;&#27493;&#20998;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#20934;&#30830;&#20998;&#35299;&#21508;&#31181;&#38382;&#39064;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01192v1 Announce Type: cross  Abstract: Cooperative co-evolution (CC) algorithms, based on the divide-and-conquer strategy, have emerged as the predominant approach to solving large-scale global optimization (LSGO) problems. The efficiency and accuracy of the grouping stage significantly impact the performance of the optimization process. While the general separability grouping (GSG) method has overcome the limitation of previous differential grouping (DG) methods by enabling the decomposition of non-additively separable functions, it suffers from high computational complexity. To address this challenge, this article proposes a composite separability grouping (CSG) method, seamlessly integrating DG and GSG into a problem decomposition framework to utilize the strengths of both approaches. CSG introduces a step-by-step decomposition framework that accurately decomposes various problem types using fewer computational resources. By sequentially identifying additively, multiplic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#30340;&#37325;&#35201;&#24615;&#37325;&#36171;&#26435;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#25193;&#25955;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#27604;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#37325;&#36171;&#26435;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#29983;&#25104;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2403.01189</link><description>&lt;p&gt;
&#20174;&#20559;&#20506;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#26080;&#20559;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Unbiased Diffusion Models From Biased Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#30340;&#37325;&#35201;&#24615;&#37325;&#36171;&#26435;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#25193;&#25955;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#27604;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#37325;&#36171;&#26435;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#29983;&#25104;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#35299;&#20915;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#28508;&#22312;&#39118;&#38505;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#36755;&#20986;&#30452;&#25509;&#21463;&#21040;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#20943;&#36731;&#28508;&#22312;&#20559;&#24046;&#25104;&#20026;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#21644;&#27604;&#20363;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#30340;&#37325;&#35201;&#24615;&#37325;&#36171;&#26435;&#26041;&#27861;&#26469;&#20943;&#36731;&#25193;&#25955;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#27604;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#31934;&#30830;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#20102;&#29983;&#25104;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;&#34429;&#28982;&#30452;&#25509;&#23558;&#20854;&#24212;&#29992;&#20110;&#24471;&#20998;&#21305;&#37197;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#27604;&#26082;&#36827;&#34892;&#37325;&#26032;&#36171;&#26435;&#21448;&#36827;&#34892;&#24471;&#20998;&#26657;&#27491;&#21487;&#20197;&#23548;&#33268;&#19968;&#31181;&#21487;&#35299;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#65292;&#26469;&#37325;&#26032;&#29983;&#25104;&#26080;&#20559;&#25968;&#25454;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#19982;&#20256;&#32479;&#24471;&#20998;&#21305;&#37197;&#30340;&#32852;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#25910;&#25947;&#21040;&#19968;&#20010;&#26080;&#20559;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01189v1 Announce Type: new  Abstract: With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01183</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#22330;&#26223;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01183
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
21&#19990;&#32426;&#30340;&#29359;&#32618;&#20998;&#20026;&#34394;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#24050;&#32463;&#25104;&#20026;&#23545;&#21518;&#32773;&#20154;&#20204;&#31119;&#31049;&#21644;&#23433;&#20840;&#26500;&#25104;&#20840;&#29699;&#23041;&#32961;&#12290;&#23427;&#25552;&#20986;&#30340;&#25361;&#25112;&#24517;&#39035;&#36890;&#36807;&#32479;&#19968;&#30340;&#20840;&#29699;&#21512;&#20316;&#26469;&#38754;&#23545;&#65292;&#25105;&#20204;&#24517;&#39035;&#27604;&#20197;&#24448;&#26356;&#21152;&#20381;&#36182;&#33258;&#21160;&#21270;&#20294;&#20540;&#24471;&#20449;&#36182;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;&#32593;&#32476;&#29359;&#32618;&#26085;&#30410;&#22686;&#38271;&#30340;&#26412;&#36136;&#12290;&#27599;&#24180;&#26377;&#36229;&#36807;1000&#19975;&#36215;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#25552;&#20132;&#32473;&#32654;&#22269;&#22269;&#23478;&#22833;&#36394;&#21644;&#34987;&#21093;&#21066;&#20799;&#31461;&#20013;&#24515;&#65292;&#36229;&#36807;80%&#26469;&#33258;&#32593;&#32476;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#35843;&#26597;&#20013;&#24515;&#21644;&#28165;&#38500;&#20013;&#24515;&#26080;&#27861;&#25163;&#21160;&#22788;&#29702;&#21644;&#27491;&#30830;&#35843;&#26597;&#25152;&#26377;&#22270;&#20687;&#12290;&#22522;&#20110;&#27492;&#65292;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#38752;&#33258;&#21160;&#21270;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22330;&#26223;&#35782;&#21035;&#20219;&#21153;&#23547;&#25214;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#33021;&#22815;&#32452;&#32455;&#21644;&#20998;&#31867;&#20799;&#31461;&#24615;&#34384;&#24453;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01183v1 Announce Type: cross  Abstract: Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing &amp; Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22996;&#21592;&#20250;&#26426;&#21046;&#30340; BCM &#28508;&#21183;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#21547;&#27687;&#26377;&#26426;&#21270;&#21512;&#29289;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#32467;&#26500;&#21644;&#36866;&#24212;&#24615;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.01158</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22996;&#21592;&#20250;&#26426;&#21046;&#30340;&#21547;&#27687;&#26377;&#26426;&#21270;&#21512;&#29289;&#28508;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22996;&#21592;&#20250;&#26426;&#21046;&#30340; BCM &#28508;&#21183;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#21547;&#27687;&#26377;&#26426;&#21270;&#21512;&#29289;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#21487;&#25193;&#23637;&#32467;&#26500;&#21644;&#36866;&#24212;&#24615;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21547;&#27687;&#26377;&#26426;&#21270;&#21512;&#29289;&#22312;&#20026;&#29983;&#29289;&#20307;&#25552;&#20379;&#33021;&#37327;&#26469;&#28304;&#21644;&#20419;&#36827;&#34507;&#30333;&#36136;&#24418;&#25104;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#23545;&#29983;&#29289;&#21270;&#23398;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#29702;&#35299;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPI&#65289;&#21644;&#24320;&#21457;&#34507;&#30333;&#36136;&#21644;&#26377;&#26426;&#21270;&#21512;&#29289;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#37327;&#21270;&#23427;&#20204;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#20843;&#32452;CHO&#20869;&#21547;&#27687;&#26377;&#26426;&#21270;&#21512;&#29289;&#30340;&#20027;&#21160;&#36125;&#21494;&#26031;&#22996;&#21592;&#20250;&#26426;&#28508;&#21183;&#65288;BCM&#65289;&#12290;BCM&#28508;&#21183;&#37319;&#29992;&#22522;&#20110;&#22996;&#21592;&#20250;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19982;&#26680;&#22238;&#24402;&#22120;&#30456;&#20851;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#12290;&#20854;&#21487;&#36866;&#24212;&#24615;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#25193;&#23637;&#65292;&#20445;&#25345;&#36716;&#31227;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#30340;BCM&#28508;&#21183;&#23450;&#20301;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01158v1 Announce Type: cross  Abstract: Understanding the pivotal role of oxygen-containing organic compounds in serving as an energy source for living organisms and contributing to protein formation is crucial in the field of biochemistry. This study addresses the challenge of comprehending protein-protein interactions (PPI) and developing predicitive models for proteins and organic compounds, with a specific focus on quantifying their binding affinity. Here, we introduce the active Bayesian Committee Machine (BCM) potential, specifically designed to predict oxygen-containing organic compounds within eight groups of CHO. The BCM potential adopts a committee-based approach to tackle scalability issues associated with kernel regressors, particularly when dealing with large datasets. Its adaptable structure allows for efficient and cost-effective expansion, maintaing both transferability and scalability. Through systematic benchmarking, we position the sparse BCM potential as 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#25552;&#39640;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25968;&#25454;&#38598;&#21644;&#23454;&#29616;&#24179;&#34913;&#27604;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;</title><link>https://arxiv.org/abs/2403.01147</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#30340;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#25552;&#39640;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25968;&#25454;&#38598;&#21644;&#23454;&#29616;&#24179;&#34913;&#27604;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22686;&#24378;&#20132;&#36890;&#23433;&#20840;&#24182;&#20419;&#36827;&#21450;&#26102;&#24212;&#24613;&#21709;&#24212;&#22806;&#65292;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#36890;&#36807;&#25552;&#20379;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#20449;&#24687;&#65292;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#36215;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38500;&#20102;&#37319;&#29992;&#20808;&#36827;&#30340;&#31639;&#27861;&#27169;&#22411;&#22806;&#65292;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#36824;&#21463;&#21040;&#33719;&#21462;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;Transformer&#22312;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;GANs&#25193;&#23637;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;1:4&#12289;2:3&#21644;1:1&#30340;&#24179;&#34913;&#27604;&#12290;&#35813;&#27169;&#22411;&#38024;&#23545;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01147v1 Announce Type: cross  Abstract: In addition to enhancing traffic safety and facilitating prompt emergency response, traffic incident detection plays an indispensable role in intelligent transportation systems by providing real-time traffic status information. This enables the realization of intelligent traffic control and management. Previous research has identified that apart from employing advanced algorithmic models, the effectiveness of detection is also significantly influenced by challenges related to acquiring large datasets and addressing dataset imbalances. A hybrid model combining transformer and generative adversarial networks (GANs) is proposed to address these challenges. Experiments are conducted on four real datasets to validate the superiority of the transformer in traffic incident detection. Additionally, GANs are utilized to expand the dataset and achieve a balanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against the baseline mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;LLM-PQ&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#27169;&#22411;&#37327;&#21270;&#21644;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#65292;&#22312;&#24322;&#26500;GPU&#38598;&#32676;&#19978;&#25552;&#39640;&#20102;LLM&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01136</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#21644;&#33258;&#36866;&#24212;&#37327;&#21270;&#30340;&#24322;&#26500;&#38598;&#32676;&#19978;&#25552;&#20379;LLM-PQ
&lt;/p&gt;
&lt;p&gt;
LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01136
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;LLM-PQ&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#27169;&#22411;&#37327;&#21270;&#21644;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#65292;&#22312;&#24322;&#26500;GPU&#38598;&#32676;&#19978;&#25552;&#39640;&#20102;LLM&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;LLMs&#30340;&#24040;&#22823;&#35268;&#27169;&#23548;&#33268;&#20102;&#38750;&#24120;&#39640;&#30340;&#36164;&#28304;&#38656;&#27714;&#21644;&#25104;&#26412;&#12290;&#23613;&#31649;&#30446;&#21069;&#20027;&#35201;&#20351;&#29992;&#32479;&#19968;&#39640;&#24615;&#33021;GPU&#26469;&#26381;&#21153;&#36825;&#20123;&#27169;&#22411;&#65292;&#20294;&#21033;&#29992;&#19968;&#31181;&#28151;&#21512;&#21487;&#29992;&#39640;&#20302;&#23481;&#37327;GPU&#30340;&#24322;&#26500;&#38598;&#32676;&#21487;&#33021;&#20250;&#22823;&#24133;&#38477;&#20302;&#26381;&#21153;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#25903;&#25345;&#20351;&#29992;&#24322;&#26500;&#38598;&#32676;&#39640;&#25928;&#25552;&#20379;LLM&#26381;&#21153;&#30340;&#35774;&#35745;&#65292;&#32780;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#20998;&#21306;&#21644;&#22343;&#21248;&#21387;&#32553;&#22312;&#21516;&#36136;&#35774;&#22791;&#20043;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLM-PQ&#65292;&#36825;&#26159;&#19968;&#20010;&#20513;&#23548;&#33258;&#36866;&#24212;&#27169;&#22411;&#37327;&#21270;&#21644;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#20197;&#25552;&#39640;&#24322;&#26500;GPU&#38598;&#32676;&#19978;LLM&#26381;&#21153;&#25928;&#29575;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;&#20998;&#24067;&#24335;LLM&#26381;&#21153;&#20013;&#20180;&#32454;&#36873;&#25321;&#20102;&#28151;&#21512;&#31934;&#24230;&#27169;&#22411;&#37327;&#21270;&#12289;&#30456;&#20301;&#24863;&#30693;&#27169;&#22411;&#20998;&#21306;&#21644;&#24494;&#25209;&#37327;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01136v1 Announce Type: cross  Abstract: Recent breakthroughs in Large-scale language models (LLMs) have demonstrated impressive performance on various tasks. The immense sizes of LLMs have led to very high resource demand and cost for running the models. Though the models are largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous cluster with a mix of available high- and low-capacity GPUs can potentially substantially reduce the serving cost. There is a lack of designs to support efficient LLM serving using a heterogeneous cluster, while the current solutions focus on model partition and uniform compression among homogeneous devices. This paper proposes LLM-PQ, a system that advocates adaptive model quantization and phase-aware partition to improve LLM serving efficiency on heterogeneous GPU clusters. We carefully decide on mixed-precision model quantization together with phase-aware model partition and micro-batch sizing in distributed LLM servin
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#34394;&#25311;&#26631;&#27880;&#22120;&#65292;&#30452;&#25509;&#20351;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#65292;&#21487;&#33021;&#35299;&#20915;&#20256;&#32479;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01133</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#29289;&#29702;&#24863;&#24212;&#25968;&#25454;&#30340;&#34394;&#25311;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01133
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#34394;&#25311;&#26631;&#27880;&#22120;&#65292;&#30452;&#25509;&#20351;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#65292;&#21487;&#33021;&#35299;&#20915;&#20256;&#32479;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65288;&#22914;&#24815;&#24615;&#25968;&#25454;&#65289;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#26469;&#33258;&#29615;&#22659;&#30340;&#35270;&#39057;&#25110;&#38899;&#39057;&#31561;&#20854;&#20182;&#27169;&#24577;&#12290;&#36825;&#20123;&#22791;&#29992;&#26469;&#28304;&#20026;&#20154;&#31867;&#26631;&#27880;&#32773;&#25552;&#20379;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#22240;&#20026;&#21407;&#22987;&#25968;&#23383;&#25968;&#25454;&#23545;&#20110;&#19987;&#23478;&#26469;&#35828;&#36890;&#24120;&#36807;&#20110;&#38590;&#20197;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20256;&#32479;&#26041;&#27861;&#23384;&#22312;&#35768;&#22810;&#20851;&#20110;&#24635;&#20307;&#25104;&#26412;&#12289;&#25928;&#29575;&#12289;&#39069;&#22806;&#27169;&#24577;&#30340;&#23384;&#20648;&#12289;&#26102;&#38388;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#30340;&#38382;&#39064;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#26159;&#36890;&#36807;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#23383;&#27597;&#25968;&#23383;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20351;&#23427;&#20204;&#33021;&#22815;&#29702;&#35299;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20197;&#22806;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#33258;&#28982;&#22320;&#65292;&#36825;&#20026;&#25506;&#32034;LLMs&#20316;&#20026;&#34394;&#25311;&#26631;&#27880;&#22120;&#24320;&#36767;&#20102;&#28508;&#22312;&#36884;&#24452;&#65292;&#20854;&#20013;LLMs&#23558;&#30452;&#25509;&#25552;&#20379;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#22791;&#29992;&#27169;&#24577;&#12290;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#20915;&#25104;&#26412;&#12289;&#25928;&#29575;&#12289;&#23384;&#20648;&#12289;&#26102;&#38388;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01133v1 Announce Type: new  Abstract: Traditional human-in-the-loop-based annotation for time-series data like inertial data often requires access to alternate modalities like video or audio from the environment. These alternate sources provide the necessary information to the human annotator, as the raw numeric data is often too obfuscated even for an expert. However, this traditional approach has many concerns surrounding overall cost, efficiency, storage of additional modalities, time, scalability, and privacy. Interestingly, recent large language models (LLMs) are also trained with vast amounts of publicly available alphanumeric data, which allows them to comprehend and perform well on tasks beyond natural language processing. Naturally, this opens up a potential avenue to explore LLMs as virtual annotators where the LLMs will be directly provided the raw sensor data for annotation instead of relying on any alternate modality. Naturally, this could mitigate the problems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#20449;&#24687;PointNet&#65288;MPIPN&#65289;&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;&#21270;&#22768;&#23398;-&#32467;&#26500;&#31995;&#32479;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#28857;&#20113;&#26550;&#26500;&#21644;&#25552;&#21462;&#23616;&#37096;&#20840;&#23616;&#29305;&#24449;&#26469;&#35299;&#20915;&#28041;&#21450;&#26174;&#24335;&#21644;&#38544;&#24335;&#29289;&#29702;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01132</link><description>&lt;p&gt;
MPIPN: &#29992;&#20110;&#27714;&#35299;&#21442;&#25968;&#22768;&#23398;-&#32467;&#26500;&#31995;&#32479;&#30340;&#22810;&#29289;&#29702;&#20449;&#24687;PointNet
&lt;/p&gt;
&lt;p&gt;
MPIPN: A Multi Physics-Informed PointNet for solving parametric acoustic-structure systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#20449;&#24687;PointNet&#65288;MPIPN&#65289;&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;&#21270;&#22768;&#23398;-&#32467;&#26500;&#31995;&#32479;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#28857;&#20113;&#26550;&#26500;&#21644;&#25552;&#21462;&#23616;&#37096;&#20840;&#23616;&#29305;&#24449;&#26469;&#35299;&#20915;&#28041;&#21450;&#26174;&#24335;&#21644;&#38544;&#24335;&#29289;&#29702;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#29992;&#20110;&#27714;&#35299;&#30001;&#19968;&#33324;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#22768;&#23398;-&#32467;&#26500;&#32806;&#21512;&#20043;&#31867;&#30340;&#22797;&#26434;&#22810;&#29289;&#29702;&#31995;&#32479;&#36890;&#24120;&#30001;&#19968;&#31995;&#21015;&#21253;&#21547;&#21464;&#37327;&#29289;&#29702;&#37327;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#65292;&#36825;&#34987;&#31216;&#20026;&#21442;&#25968;&#21270;&#31995;&#32479;&#12290;&#30446;&#21069;&#32570;&#20047;&#35299;&#20915;&#28041;&#21450;&#26174;&#24335;&#21644;&#38544;&#24335;&#37327;&#30340;&#21442;&#25968;&#21270;&#31995;&#32479;&#30340;PDE&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#20449;&#24687;PointNet&#65288;MPIPN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;&#21270;&#22768;&#23398;-&#32467;&#26500;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;MPIPN&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#28857;&#20113;&#26550;&#26500;&#65292;&#21253;&#25324;&#35745;&#31639;&#22495;&#30340;&#26174;&#24335;&#29289;&#29702;&#37327;&#21644;&#20960;&#20309;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;MPIPN&#25552;&#21462;&#37325;&#26500;&#28857;&#20113;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20316;&#20026;&#35299;&#20915;&#21442;&#25968;&#21270;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#38544;&#24335;&#29289;&#29702;&#37327;&#34987;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01132v1 Announce Type: new  Abstract: Machine learning is employed for solving physical systems governed by general nonlinear partial differential equations (PDEs). However, complex multi-physics systems such as acoustic-structure coupling are often described by a series of PDEs that incorporate variable physical quantities, which are referred to as parametric systems. There are lack of strategies for solving parametric systems governed by PDEs that involve explicit and implicit quantities. In this paper, a deep learning-based Multi Physics-Informed PointNet (MPIPN) is proposed for solving parametric acoustic-structure systems. First, the MPIPN induces an enhanced point-cloud architecture that encompasses explicit physical quantities and geometric features of computational domains. Then, the MPIPN extracts local and global features of the reconstructed point-cloud as parts of solving criteria of parametric systems, respectively. Besides, implicit physical quantities are embe
&lt;/p&gt;</description></item><item><title>LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01131</link><description>&lt;p&gt;
LLaMoCo&#65306;&#29992;&#20110;&#20248;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01131
&lt;/p&gt;
&lt;p&gt;
LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#26041;&#27861;&#21253;&#25324;&#20174;LLMs&#36845;&#20195;&#22320;&#23547;&#25214;&#19979;&#19968;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#30452;&#25509;&#25552;&#31034;LLMs&#20197;&#33719;&#21462;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#20302;&#12289;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#24230;&#39640;&#20197;&#21450;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMoCo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#28165;&#26224;&#25551;&#36848;&#30340;&#38382;&#39064;&#25552;&#31034;&#21644;&#26377;&#25928;&#20248;&#21270;&#20195;&#30721;&#30340;&#20840;&#38754;&#25351;&#20196;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#20043;&#21069;&#65292;&#35813;&#31574;&#30053;&#25972;&#21512;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28909;&#36523;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#24494;&#35843;&#26399;&#38388;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;LLaMoCo&#31934;&#35843;&#30340;CodeGen&#65288;350M&#65289;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#19968;&#12289;&#20108;&#21644;&#19977;&#38454;&#23548;&#25968;&#36827;&#34892;&#25439;&#22833;&#26223;&#35266;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19982;Spearman&#31209;&#30456;&#20851;&#31995;&#25968;&#31867;&#20284;&#21487;&#35270;&#21270;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#21644;&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#24102;&#26469;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.01128</link><description>&lt;p&gt;
&#25439;&#22833;&#26223;&#35266;&#30340;&#28789;&#25935;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sensitivity Analysis On Loss Landscape
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01128
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19968;&#12289;&#20108;&#21644;&#19977;&#38454;&#23548;&#25968;&#36827;&#34892;&#25439;&#22833;&#26223;&#35266;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19982;Spearman&#31209;&#30456;&#20851;&#31995;&#25968;&#31867;&#20284;&#21487;&#35270;&#21270;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#21644;&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#24102;&#26469;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21487;&#29992;&#20110;&#28789;&#25935;&#24230;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#25439;&#22833;&#26223;&#35266;&#30340;&#20248;&#21183;&#65292;&#20102;&#35299;&#21738;&#20123;&#33258;&#21464;&#37327;&#24433;&#21709;&#22240;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#21033;&#29992;&#19968;&#38454;&#12289;&#20108;&#38454;&#21644;&#19977;&#38454;&#23548;&#25968;&#26469;&#29702;&#35299;&#25439;&#22833;&#26223;&#35266;&#12290;&#25105;&#20204;&#30693;&#36947;Spearman&#31209;&#30456;&#20851;&#31995;&#25968;&#21487;&#20197;&#26816;&#27979;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#21333;&#35843;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#21457;&#29616;&#22312;&#29305;&#23450;&#37197;&#32622;&#21644;&#21442;&#25968;&#19979;&#65292;&#20108;&#38454;&#26799;&#24230;&#25552;&#20379;&#30340;&#20449;&#24687;&#21487;&#20197;&#31867;&#20284;&#20110;Spearman&#30340;&#32467;&#26524;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#25439;&#22833;&#20989;&#25968;&#19982;&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#65292;&#23548;&#33268;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#23545;&#25439;&#22833;&#26223;&#35266;&#30340;&#27599;&#27425;&#25506;&#32034;&#37117;&#25552;&#20379;&#26032;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#19968;&#38454;&#21644;&#19977;&#38454;&#23548;&#25968;&#20063;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26174;&#31034;&#20102;&#33258;&#21464;&#37327;&#23545;&#22240;&#21464;&#37327;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01128v1 Announce Type: new  Abstract: Gradients can be employed for sensitivity analysis. Here, we leverage the advantages of the Loss Landscape to comprehend which independent variables impact the dependent variable. We seek to grasp the loss landscape by utilizing first, second, and third derivatives through automatic differentiation. we know that Spearman's rank correlation coefficient can detect the monotonic relationship between two variables. However, I have found that second-order gradients, with certain configurations and parameters, provide information that can be visualized similarly to Spearman's results.In our approach, we incorporate a loss function with an activation function, resulting in a non-linear pattern. Each exploration of the loss landscape through retraining yields new valuable information. Furthermore, the first and third derivatives are also beneficial, as they indicate the extent to which independent variables influence the dependent variable.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;&#65288;EMU&#65289;&#65292;&#21033;&#29992;&#35821;&#20041;&#19968;&#33268;&#20869;&#23384;&#21152;&#36895;&#23398;&#20064;&#65292;&#26377;&#36873;&#25321;&#22320;&#20419;&#36827;&#29702;&#24819;&#30340;&#36716;&#25442;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.01112</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01112
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;&#65288;EMU&#65289;&#65292;&#21033;&#29992;&#35821;&#20041;&#19968;&#33268;&#20869;&#23384;&#21152;&#36895;&#23398;&#20064;&#65292;&#26377;&#36873;&#25321;&#22320;&#20419;&#36827;&#29702;&#24819;&#30340;&#36716;&#25442;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#65292;&#26234;&#33021;&#20307;&#26088;&#22312;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#27604;&#22914;&#20987;&#36133;&#25932;&#20154;&#25110;&#36827;&#29699;&#12290;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#23398;&#20064;&#26102;&#38388;&#65292;&#36890;&#24120;&#20250;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#38543;&#21518;&#26410;&#33021;&#21457;&#29616;&#36798;&#25104;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;MARL&#30340;&#39640;&#25928;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;&#65288;EMU&#65289;&#65292;&#20854;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#65306;&#65288;a&#65289;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#24773;&#33410;&#32531;&#20914;&#21306;&#30340;&#35821;&#20041;&#19968;&#33268;&#20869;&#23384;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#65288;b&#65289;&#26377;&#36873;&#25321;&#22320;&#20419;&#36827;&#29702;&#24819;&#30340;&#36716;&#25442;&#20197;&#38450;&#27490;&#23616;&#37096;&#25910;&#25947;&#12290;&#20026;&#23454;&#29616;&#65288;a&#65289;&#65292;EMU&#22312;MARL&#26049;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21019;&#24314;&#20102;&#26377;&#21161;&#20110;&#25506;&#32034;&#24615;&#20869;&#23384;&#22238;&#24518;&#30340;&#36830;&#36143;&#35760;&#24518;&#23884;&#20837;&#12290;&#20026;&#23454;&#29616;&#65288;b&#65289;&#65292;EMU&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#24895;&#26395;&#24615;&#30340;&#26032;&#39062;&#22870;&#21169;&#32467;&#26500;&#65292;&#31216;&#20026;&#24773;&#33410;&#28608;&#21169;&#12290;&#36825;&#31181;&#22870;&#21169;&#25913;&#21892;&#20102;TD
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01112v1 Announce Type: new  Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01101</link><description>&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#65306;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#32972;&#26223;&#19979;&#36890;&#36807;&#20195;&#29702;&#24605;&#32771;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26377;&#26395;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#22312;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#19978;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#65292;&#29978;&#33267;&#21487;&#33021;&#36229;&#36807;&#35745;&#31639;&#25104;&#26412;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01101v1 Announce Type: cross  Abstract: Fine-tuning the pre-trained model with active learning holds promise for reducing annotation costs. However, this combination introduces significant computational costs, particularly with the growing scale of pre-trained models. Recent research has proposed proxy-based active learning, which pre-computes features to reduce computational costs. Yet, this approach often incurs a significant loss in active learning performance, which may even outweigh the computational cost savings. In this paper, we argue the performance drop stems not only from pre-computed features' inability to distinguish between categories of labeled samples, resulting in the selection of redundant samples but also from the tendency to compromise valuable pre-trained information when fine-tuning with samples selected through the proxy model. To address this issue, we propose a novel method called aligned selection via proxy to update pre-computed features while sele
&lt;/p&gt;</description></item><item><title>Pair-Align&#26041;&#27861;&#36890;&#36807;&#19968;&#23545;&#19968;&#23545;&#40784;&#26469;&#23545;&#25239;&#22270;&#32467;&#26500;&#36716;&#31227;&#65292;&#20351;&#29992;&#36793;&#26435;&#37325;&#20943;&#36731;&#26465;&#20214;&#32467;&#26500;&#36716;&#31227;&#65288;CSS&#65289;&#24182;&#35843;&#25972;&#20998;&#31867;&#25439;&#22833;&#20197;&#22788;&#29702;&#26631;&#31614;&#36716;&#31227;&#65288;LS&#65289;&#65292;&#22312;&#22788;&#29702;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01092</link><description>&lt;p&gt;
&#19968;&#23545;&#19968;&#23545;&#40784;&#25913;&#36827;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Pairwise Alignment Improves Graph Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01092
&lt;/p&gt;
&lt;p&gt;
Pair-Align&#26041;&#27861;&#36890;&#36807;&#19968;&#23545;&#19968;&#23545;&#40784;&#26469;&#23545;&#25239;&#22270;&#32467;&#26500;&#36716;&#31227;&#65292;&#20351;&#29992;&#36793;&#26435;&#37325;&#20943;&#36731;&#26465;&#20214;&#32467;&#26500;&#36716;&#31227;&#65288;CSS&#65289;&#24182;&#35843;&#25972;&#20998;&#31867;&#25439;&#22833;&#20197;&#22788;&#29702;&#26631;&#31614;&#36716;&#31227;&#65288;LS&#65289;&#65292;&#22312;&#22788;&#29702;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#23545;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#26631;&#31614;&#25512;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#22914;&#26524;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#22270;&#19982;&#29992;&#20110;&#27979;&#35797;&#30340;&#22270;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#24448;&#24448;&#20250;&#36935;&#21040;&#27867;&#21270;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#28145;&#20837;&#25506;&#35752;&#20102;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#25968;&#25454;&#28857;&#32463;&#21382;&#29305;&#24449;&#12289;&#26631;&#31614;&#21644;&#23588;&#20854;&#26159;&#36830;&#25509;&#27169;&#24335;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#22312;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#19968;&#23545;&#19968;&#23545;&#40784;&#65288;Pair-Align&#65289;&#65292;&#36890;&#36807;&#20943;&#36731;&#26465;&#20214;&#32467;&#26500;&#36716;&#31227;&#65288;CSS&#65289;&#21644;&#26631;&#31614;&#36716;&#31227;&#65288;LS&#65289;&#26469;&#23545;&#25239;&#22270;&#32467;&#26500;&#36716;&#31227;&#12290;Pair-Align&#20351;&#29992;&#36793;&#26435;&#37325;&#37325;&#26032;&#26657;&#20934;&#37051;&#36817;&#33410;&#28857;&#20043;&#38388;&#30340;&#24433;&#21709;&#20197;&#22788;&#29702;CSS&#65292;&#24182;&#36890;&#36807;&#26631;&#31614;&#26435;&#37325;&#35843;&#25972;&#20998;&#31867;&#25439;&#22833;&#20197;&#22788;&#29702;LS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#21306;&#22495;&#33410;&#28857;&#20998;&#31867;&#22312;&#20869;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01092v1 Announce Type: new  Abstract: Graph-based methods, pivotal for label inference over interconnected objects in many real-world applications, often encounter generalization challenges, if the graph used for model training differs significantly from the graph used for testing. This work delves into Graph Domain Adaptation (GDA) to address the unique complexities of distribution shifts over graph data, where interconnected data points experience shifts in features, labels, and in particular, connecting patterns. We propose a novel, theoretically principled method, Pairwise Alignment (Pair-Align) to counter graph structure shift by mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align uses edge weights to recalibrate the influence among neighboring nodes to handle CSS and adjusts the classification loss with label weights to handle LS. Our method demonstrates superior performance in real-world applications, including node classification with region
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COOL&#30340;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20849;&#21516;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.01091</link><description>&lt;p&gt;
COOL&#65306;&#19968;&#31181;&#34701;&#21512;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#30340;&#20849;&#21516;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COOL&#30340;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20849;&#21516;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20132;&#36890;&#39044;&#27979;&#65292;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#24773;&#20917;&#39044;&#27979;&#20132;&#36890;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#37492;&#20110;&#20854;&#23545;&#22810;&#20010;&#22330;&#26223;&#30340;&#25345;&#32493;&#20851;&#27880;&#65292;&#24182;&#20419;&#36827;&#20102;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#65292;&#20363;&#22914;&#22478;&#24066;&#35268;&#21010;&#21644;&#20132;&#36890;&#31649;&#29702;&#65292;&#35813;&#38382;&#39064;&#24050;&#32463;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#29420;&#31435;&#22320;&#24314;&#27169;&#26102;&#31354;&#20851;&#31995;&#65292;&#22240;&#27492;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#20004;&#32773;&#30340;&#22797;&#26434;&#39640;&#38454;&#20114;&#21160;&#65292;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#36807;&#28193;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#65292;&#38656;&#35201;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#36825;&#31181;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#32553;&#20889;&#20026;COOL&#65289;&#65292;&#23427;&#20174;&#20808;&#21069;&#21644;&#21518;&#32493;&#20449;&#24687;&#20013;&#24314;&#27169;&#24322;&#26500;&#22270;&#65292;&#20197;&#20849;&#21516;&#25429;&#25417;&#39640;&#38454;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01091v1 Announce Type: cross  Abstract: This paper investigates traffic forecasting, which attempts to forecast the future state of traffic based on historical situations. This problem has received ever-increasing attention in various scenarios and facilitated the development of numerous downstream applications such as urban planning and transportation management. However, the efficacy of existing methods remains sub-optimal due to their tendency to model temporal and spatial relationships independently, thereby inadequately accounting for complex high-order interactions of both worlds. Moreover, the diversity of transitional patterns in traffic forecasting makes them challenging to capture for existing approaches, warranting a deeper exploration of their diversity. Toward this end, this paper proposes Conjoint Spatio-Temporal graph neural network (abbreviated as COOL), which models heterogeneous graphs from prior and posterior information to conjointly capture high-order sp
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01081</link><description>&lt;p&gt;
LAB&#65306;&#38024;&#23545;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
LAB: Large-Scale Alignment for ChatBots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01081
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;LAB&#65288;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;LAB&#26174;&#33879;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LAB&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#19982;&#20351;&#29992;&#20256;&#32479;&#20154;&#31867;&#27880;&#37322;&#25110;GPT-4&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#21644;&#25351;&#20196;&#36981;&#24490;&#34892;&#20026;&#65292;&#26631;&#24535;&#30528;&#22312;&#39640;&#25928;&#35757;&#32451;&#21508;&#31181;&#24212;&#29992;&#30340;LLM&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;&#20301;&#32622;&#20449;&#24687;&#65292;&#24341;&#20837;&#31070;&#32463;&#28909;&#26680;&#22788;&#29702;&#22270;&#25968;&#25454;&#65292;&#36890;&#36807;&#38544;&#34255;&#23618;&#36755;&#20986;&#21305;&#37197;&#25552;&#39640;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01079</link><description>&lt;p&gt;
&#25945;&#25480;&#22810;&#23618;&#24863;&#30693;&#26426;&#26356;&#22810;&#22270;&#20449;&#24687;&#65306;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01079
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;&#20301;&#32622;&#20449;&#24687;&#65292;&#24341;&#20837;&#31070;&#32463;&#28909;&#26680;&#22788;&#29702;&#22270;&#25968;&#25454;&#65292;&#36890;&#36807;&#38544;&#34255;&#23618;&#36755;&#20986;&#21305;&#37197;&#25552;&#39640;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65306;&#24040;&#22823;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#23581;&#35797;&#36890;&#36807;&#20943;&#23569;&#23545;&#22270;&#32467;&#26500;&#30340;&#20381;&#36182;&#26469;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#12290;&#23613;&#31649;&#23558;&#22270;&#30693;&#35782;&#33976;&#39311;&#21040;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#26159;&#19968;&#20010;&#19981;&#38169;&#30340;&#24819;&#27861;&#65292;&#20294;&#23427;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#20301;&#32622;&#20449;&#24687;&#20002;&#22833;&#21644;&#27867;&#21270;&#33021;&#21147;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#33976;&#39311;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#31070;&#32463;&#28909;&#26680;&#26469;&#36127;&#36131;&#22270;&#25968;&#25454;&#22788;&#29702;&#65292;&#22312;GNN&#20013;&#21033;&#29992;&#38544;&#34255;&#23618;&#36755;&#20986;&#21305;&#37197;&#26469;&#25552;&#39640;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#22270;&#19978;&#24341;&#20837;&#38544;&#34255;&#23618;&#33976;&#39311;&#29992;&#20110;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#65292;&#24182;&#32467;&#21512;&#22270;&#20301;&#32622;&#32534;&#30721;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#31181;&#35774;&#32622;&#27979;&#35797;&#20102;&#20854;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01079v1 Announce Type: cross  Abstract: We study the challenging problem for inference tasks on large-scale graph datasets of Graph Neural Networks: huge time and memory consumption, and try to overcome it by reducing reliance on graph structure. Even though distilling graph knowledge to student MLP is an excellent idea, it faces two major problems of positional information loss and low generalization. To solve the problems, we propose a new three-stage multitask distillation framework. In detail, we use Positional Encoding to capture positional information. Also, we introduce Neural Heat Kernels responsible for graph data processing in GNN and utilize hidden layer outputs matching for better performance of student MLP's hidden layers. To the best of our knowledge, it is the first work to include hidden layer distillation for student MLP on graphs and to combine graph Positional Encoding with MLP. We test its performance and robustness with several settings and draw the conc
&lt;/p&gt;</description></item><item><title>$\Gamma$-VAE&#36890;&#36807;&#27491;&#21017;&#21270;&#26354;&#29575;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#26032;&#20852;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;</title><link>https://arxiv.org/abs/2403.01078</link><description>&lt;p&gt;
$\Gamma$-VAE: &#26354;&#29575;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#26032;&#20852;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
$\Gamma$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01078
&lt;/p&gt;
&lt;p&gt;
$\Gamma$-VAE&#36890;&#36807;&#27491;&#21017;&#21270;&#26354;&#29575;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#26032;&#20852;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26032;&#20852;&#34892;&#20026;&#30340;&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#27839;&#30528;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#23376;&#38598;&#36827;&#34892;&#32452;&#32455;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#26377;&#25968;&#19975;&#20010;&#22522;&#22240;&#65292;&#20294;&#22522;&#22240;&#32452;&#23398;&#30340;&#21407;&#21017;&#30740;&#31350;&#23500;&#26377;&#25104;&#26524;&#65292;&#22240;&#20026;&#29983;&#29289;&#36807;&#31243;&#20381;&#36182;&#20110;&#21327;&#35843;&#32452;&#32455;&#65292;&#20174;&#32780;&#20135;&#29983;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#22411;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#31181;&#32452;&#32455;&#65292;&#35768;&#22810;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#24050;&#25104;&#21151;&#22320;&#23558;&#39640;&#32500;&#25968;&#25454;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#26041;&#27861;&#26159;&#20445;&#25345;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#38750;&#32447;&#24615;&#24615;&#20801;&#35768;&#36807;&#22810;&#30340;&#26354;&#29575;&#26469;&#20445;&#25345;&#36328;&#22810;&#20010;&#38750;&#30456;&#37051;&#25968;&#25454;&#38598;&#32676;&#30340;&#19968;&#33324;&#36235;&#21183;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#35268;&#33539;&#21270;&#30001;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#27969;&#24418;&#30340;&#26354;&#29575;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#36825;&#19968;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;$\Gamma$-VAE&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01078v1 Announce Type: cross  Abstract: Natural systems with emergent behaviors often organize along low-dimensional subsets of high-dimensional spaces. For example, despite the tens of thousands of genes in the human genome, the principled study of genomics is fruitful because biological processes rely on coordinated organization that results in lower dimensional phenotypes. To uncover this organization, many nonlinear dimensionality reduction techniques have successfully embedded high-dimensional data into low-dimensional spaces by preserving local similarities between data points. However, the nonlinearities in these methods allow for too much curvature to preserve general trends across multiple non-neighboring data clusters, thereby limiting their interpretability and generalizability to out-of-distribution data. Here, we address both of these limitations by regularizing the curvature of manifolds generated by variational autoencoders, a process we coin ``$\Gamma$-VAE''.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Uncertainty Quantification&#25216;&#26415;&#20174;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#20013;&#25552;&#21462;&#21487;&#29992;&#39044;&#27979;&#65292;&#26377;&#25928;&#24573;&#30053;&#19981;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#33410;&#30465;&#20102;&#39640;&#36798;80%&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.01076</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20174;&#37327;&#21270;&#32593;&#32476;&#20013;&#25552;&#21462;&#21487;&#29992;&#39044;&#27979;&#20197;&#36827;&#34892;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01076
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Uncertainty Quantification&#25216;&#26415;&#20174;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#20013;&#25552;&#21462;&#21487;&#29992;&#39044;&#27979;&#65292;&#26377;&#25928;&#24573;&#30053;&#19981;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#33410;&#30465;&#20102;&#39640;&#36798;80%&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OOD&#26816;&#27979;&#38543;&#30528;&#32593;&#32476;&#35774;&#35745;&#30340;&#36827;&#27493;&#21644;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#30830;&#23450;&#32473;&#23450;&#32593;&#32476;&#35823;&#20998;&#31867;&#25968;&#25454;&#30340;&#37096;&#20998;&#21464;&#24471;&#19982;&#32593;&#32476;&#25972;&#20307;&#24615;&#33021;&#19968;&#26679;&#26377;&#20215;&#20540;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#26469;&#21387;&#32553;&#27169;&#22411;&#65292;&#20294;&#20250;&#26377;&#36731;&#24494;&#24615;&#33021;&#25439;&#22833;&#12290;&#24615;&#33021;&#25439;&#22833;&#36827;&#19968;&#27493;&#38656;&#35201;&#20174;&#32593;&#32476;&#39044;&#27979;&#20013;&#25512;&#23548;&#20986;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#37327;&#21270;&#26469;&#33258;&#39044;&#20808;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#24573;&#30053;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#23558;&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#30340;80%&#20445;&#23384;&#19979;&#26469;&#12290;&#21516;&#26679;&#30340;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01076v1 Announce Type: cross  Abstract: OOD detection has become more pertinent with advances in network design and increased task complexity. Identifying which parts of the data a given network is misclassifying has become as valuable as the network's overall performance. We can compress the model with quantization, but it suffers minor performance loss. The loss of performance further necessitates the need to derive the confidence estimate of the network's predictions. In line with this thinking, we introduce an Uncertainty Quantification(UQ) technique to quantify the uncertainty in the predictions from a pre-trained vision model. We subsequently leverage this information to extract valuable predictions while ignoring the non-confident predictions. We observe that our technique saves up to 80% of ignored samples from being misclassified. The code for the same is available here.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01071</link><description>&lt;p&gt;
GraphRCG: &#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#36890;&#24120;&#26088;&#22312;&#21019;&#24314;&#19982;&#29305;&#23450;&#22270;&#20998;&#24067;&#23494;&#20999;&#23545;&#40784;&#30340;&#26032;&#22270;&#12290;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#36890;&#36807;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#38544;&#24335;&#25429;&#33719;&#36825;&#31181;&#20998;&#24067;&#65292;&#21487;&#33021;&#24573;&#35270;&#20998;&#24067;&#26412;&#36523;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#23545;&#22270;&#29983;&#25104;&#30340;&#35265;&#35299;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#26126;&#30830;&#24314;&#27169;&#22270;&#20998;&#24067;&#24182;&#21033;&#29992;&#36825;&#20123;&#20998;&#24067;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#33258;&#26465;&#20214;&#24314;&#27169;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22270;&#26679;&#26412;&#36716;&#25442;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#19968;&#20010;&#34920;&#31034;&#29983;&#25104;&#22120;&#26469;&#25429;&#33719;&#22270;&#20998;&#24067;&#24182;&#29983;&#25104;&#21453;&#26144;&#23398;&#20064;&#20998;&#24067;&#30340;&#26032;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#24341;&#23548;&#34920;&#31034;&#20316;&#20026;&#33258;&#26465;&#20214;&#25351;&#23548;&#26469;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01071v1 Announce Type: cross  Abstract: Graph generation generally aims to create new graphs that closely align with a specific graph distribution. Existing works often implicitly capture this distribution through the optimization of generators, potentially overlooking the intricacies of the distribution itself. Furthermore, these approaches generally neglect the insights offered by the learned distribution for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance f
&lt;/p&gt;</description></item><item><title>CMZ-DRIL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#38646;&#22343;&#20540;&#22870;&#21169;&#20989;&#25968;&#21644;&#26234;&#33021;&#20307;&#38598;&#21512;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#65292;&#25552;&#39640;&#20102;&#21482;&#26377;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#30340;&#26234;&#33021;&#20307;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01059</link><description>&lt;p&gt;
&#36830;&#32493;&#38646;&#22343;&#20540;&#20105;&#35758;&#27491;&#21017;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;CMZ-DRIL&#65289;
&lt;/p&gt;
&lt;p&gt;
Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01059
&lt;/p&gt;
&lt;p&gt;
CMZ-DRIL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#38646;&#22343;&#20540;&#22870;&#21169;&#20989;&#25968;&#21644;&#26234;&#33021;&#20307;&#38598;&#21512;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#65292;&#25552;&#39640;&#20102;&#21482;&#26377;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#30340;&#26234;&#33021;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#22914;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#22797;&#26434;&#29615;&#22659;&#20013;&#29983;&#25104;&#39640;&#24615;&#33021;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#21644;/&#25110;&#24050;&#30693;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#38646;&#22343;&#20540;&#20105;&#35758;&#27491;&#21017;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;CMZ-DRIL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#32467;&#26500;&#26469;&#25552;&#39640;&#21482;&#26377;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;CMZ-DRIL&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#26368;&#23567;&#21270;&#35757;&#32451;&#20026;&#27169;&#20223;&#19987;&#23478;&#28436;&#31034;&#30340;&#26234;&#33021;&#20307;&#38598;&#21512;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#22870;&#21169;&#65292;&#32780;&#26159;&#36890;&#36807;&#26234;&#33021;&#20307;&#38598;&#21512;&#30340;&#21160;&#20316;&#19981;&#19968;&#33268;&#24615;&#21019;&#24314;&#36830;&#32493;&#30340;&#38646;&#22343;&#20540;&#22870;&#21169;&#20989;&#25968;&#12290;&#22914;&#22312;&#36335;&#24452;&#23548;&#33322;&#29615;&#22659;&#21644;&#20004;&#20010;MuJoCo&#29615;&#22659;&#20013;&#23637;&#31034;&#30340;&#65292;CMZ-DRIL&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01059v1 Announce Type: new  Abstract: Machine-learning paradigms such as imitation learning and reinforcement learning can generate highly performant agents in a variety of complex environments. However, commonly used methods require large quantities of data and/or a known reward function. This paper presents a method called Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) that employs a novel reward structure to improve the performance of imitation-learning agents that have access to only a handful of expert demonstrations. CMZ-DRIL uses reinforcement learning to minimize uncertainty among an ensemble of agents trained to model the expert demonstrations. This method does not use any environment-specific rewards, but creates a continuous and mean-zero reward function from the action disagreement of the agent ensemble. As demonstrated in a waypoint-navigation environment and in two MuJoCo environments, CMZ-DRIL can generate performant agents that be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22330;&#20998;&#31867;&#22120;&#65288;NFC&#65289;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#31070;&#32463;&#22330;&#26041;&#27861;&#35268;&#23450;&#20026;&#20998;&#31867;&#20219;&#21153;&#32780;&#19981;&#26159;&#22238;&#24402;&#20219;&#21153;&#65292;&#20174;&#32780;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#26041;&#27861;&#20013;&#22238;&#24402;&#27169;&#22411;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#20248;&#21155;&#12290;</title><link>https://arxiv.org/abs/2403.01058</link><description>&lt;p&gt;
&#36890;&#36807;&#30446;&#26631;&#32534;&#30721;&#21644;&#20998;&#31867;&#25439;&#22833;&#30340;&#31070;&#32463;&#22330;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Field Classifiers via Target Encoding and Classification Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22330;&#20998;&#31867;&#22120;&#65288;NFC&#65289;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#31070;&#32463;&#22330;&#26041;&#27861;&#35268;&#23450;&#20026;&#20998;&#31867;&#20219;&#21153;&#32780;&#19981;&#26159;&#22238;&#24402;&#20219;&#21153;&#65292;&#20174;&#32780;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#26041;&#27861;&#20013;&#22238;&#24402;&#27169;&#22411;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#20248;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22330;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#21508;&#31181;&#38271;&#26399;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#21253;&#25324;&#26032;&#35270;&#22270;&#21512;&#25104;&#21644;&#20960;&#20309;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31070;&#32463;&#22330;&#26041;&#27861;&#23581;&#35797;&#39044;&#27979;&#19968;&#20123;&#22522;&#20110;&#22352;&#26631;&#30340;&#36830;&#32493;&#30446;&#26631;&#20540;&#65292;&#22914;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;RGB&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#22238;&#24402;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35282;&#35752;&#35770;&#36825;&#20010;&#38750;&#24120;&#22522;&#26412;&#20294;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65306;&#23545;&#20110;&#31070;&#32463;&#22330;&#26041;&#27861;&#26469;&#35828;&#65292;&#22238;&#24402;&#27169;&#22411;&#30495;&#30340;&#27604;&#20998;&#31867;&#27169;&#22411;&#26356;&#22909;&#21527;&#65311;&#25105;&#20204;&#25104;&#21151;&#22320;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22330;&#20998;&#31867;&#22120;&#65288;NFC&#65289;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#31070;&#32463;&#22330;&#26041;&#27861;&#35268;&#23450;&#20026;&#20998;&#31867;&#20219;&#21153;&#32780;&#19981;&#26159;&#22238;&#24402;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;NFC&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#30446;&#26631;&#32534;&#30721;&#26041;&#27861;&#23558;&#20219;&#24847;&#31070;&#32463;&#22330;&#22238;&#24402;&#22120;&#65288;NFR&#65289;&#36731;&#26494;&#36716;&#25442;&#20026;&#20854;&#20998;&#31867;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01058v1 Announce Type: cross  Abstract: Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target En
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01053</link><description>&lt;p&gt;
&#36879;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#21457;&#29616;&#26032;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#26426;&#22120;&#23398;&#20064;&#20197;&#20854;&#25968;&#25454;&#39537;&#21160;&#30340;&#29305;&#24615;&#65292;&#23545;&#31185;&#23398;&#21457;&#29616;&#30340;&#22522;&#26412;&#23454;&#36341;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#25913;&#21464;&#12290;&#38543;&#30528;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#33258;&#21160;&#25506;&#32034;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#35265;&#35299;&#65292;&#21457;&#29616;&#26032;&#30340;&#34920;&#22411;&#31867;&#21035;&#21644;&#27010;&#24565;&#23558;&#20250;&#21464;&#24471;&#26356;&#21152;&#21560;&#24341;&#20154;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#32047;&#31215;&#25968;&#25454;&#20013;&#23384;&#22312;&#33509;&#24178;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#26032;&#31867;&#21457;&#29616;&#30340;&#36827;&#23637;&#12290;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#20276;&#38543;&#30528;&#19981;&#21516;&#31867;&#21035;&#32452;&#20043;&#38388;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#65292;&#26412;&#36136;&#19978;&#23548;&#33268;&#27169;&#31946;&#21644;&#20559;&#20506;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#23454;&#20363;&#23884;&#20837;&#30340;&#36817;&#20284;&#21518;&#39564;&#21442;&#25968;&#21270;&#20026;&#36793;&#38469; von Mises-Fisher &#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#23884;&#20837;&#26041;&#26696;&#30340;&#27169;&#31946;&#24615;&#19982;&#20559;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 Announce Type: cross  Abstract: Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the int
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.01046</link><description>&lt;p&gt;
&#19968;&#20010;&#38236;&#23376;&#30340;&#24211;&#65306;&#20302;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#21453;&#23556;&#29305;&#24449;&#30340;&#20984;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01046
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#22266;&#23450;&#12289;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#12290;&#20855;&#20307;&#30340;&#23383;&#20856;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#65292;&#28145;&#31364;&#30340;ReLU&#32593;&#32476;&#26368;&#22810;&#26377;4&#23618;&#65292;&#20197;&#21450;&#20855;&#26377;&#31526;&#21495;&#28608;&#27963;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#30697;&#24418;&#21644;&#26641;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#31532;&#22235;&#23618;&#21019;&#24314;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#20851;&#20110;&#33258;&#36523;&#30340;&#21453;&#23556;&#30340;&#29305;&#24449;&#12290;Lasso&#34920;&#31034;&#27861;&#25581;&#31034;&#20102;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#32852;&#21512;&#28304;&#36890;&#36947;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35774;&#22791;&#20449;&#36947;&#20449;&#24687;&#19981;&#21487;&#30693;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#23383;&#36890;&#20449;&#20013;&#33021;&#22815;&#36827;&#34892;&#31354;&#20013;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.01023</link><description>&lt;p&gt;
&#36890;&#36807;&#26684;&#28857;&#32852;&#21512;&#28304;&#36890;&#36947;&#32534;&#30721;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning via Lattice Joint Source-Channel Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01023
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#32852;&#21512;&#28304;&#36890;&#36947;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35774;&#22791;&#20449;&#36947;&#20449;&#24687;&#19981;&#21487;&#30693;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#23383;&#36890;&#20449;&#20013;&#33021;&#22815;&#36827;&#34892;&#31354;&#20013;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26032;&#30340;&#32852;&#21512;&#28304;&#36890;&#36947;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#25968;&#23383;&#36890;&#20449;&#23454;&#29616;&#31354;&#20013;&#35745;&#31639;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26041;&#26696;&#19981;&#20381;&#36182;&#35774;&#22791;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#32780;&#26159;&#21033;&#29992;&#26684;&#28857;&#32534;&#30721;&#37327;&#21270;&#27169;&#22411;&#21442;&#25968;&#24182;&#21033;&#29992;&#26469;&#33258;&#35774;&#22791;&#30340;&#24178;&#25200;&#12290;&#26381;&#21153;&#22120;&#22788;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#23618;&#25509;&#25910;&#26426;&#32467;&#26500;&#65292;&#29992;&#20110;&#21487;&#38752;&#35299;&#30721;&#23450;&#37327;&#21270;&#27169;&#22411;&#21442;&#25968;&#30340;&#25972;&#25968;&#32452;&#21512;&#20316;&#20026;&#32858;&#21512;&#30340;&#26684;&#28857;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;&#21363;&#20351;&#38754;&#20020;&#20449;&#36947;&#26465;&#20214;&#21644;&#35774;&#22791;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01023v1 Announce Type: cross  Abstract: This paper introduces a universal federated learning framework that enables over-the-air computation via digital communications, using a new joint source-channel coding scheme. Without relying on channel state information at devices, this scheme employs lattice codes to both quantize model parameters and exploit interference from the devices. A novel two-layer receiver structure at the server is designed to reliably decode an integer combination of the quantized model parameters as a lattice point for the purpose of aggregation. Numerical experiments validate the effectiveness of the proposed scheme. Even with the challenges posed by channel conditions and device heterogeneity, the proposed scheme markedly surpasses other over-the-air FL strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#20102;&#33258;&#20027;&#26080;&#20154;&#26426;&#20219;&#21153;&#25104;&#21151;&#23454;&#26045;&#30340;&#25361;&#25112;&#21644;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#20811;&#26381;&#25361;&#25112;&#30340;&#25216;&#26415;&#26041;&#26696;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#26080;&#20154;&#26426;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.01022</link><description>&lt;p&gt;
&#33258;&#20027;&#25171;&#20987;&#26080;&#20154;&#26426;&#29992;&#20110;&#21453;&#24656;&#20219;&#21153;&#65306;&#25361;&#25112;&#19982;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Autonomous Strike UAVs for Counterterrorism Missions: Challenges and Preliminary Solutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#20102;&#33258;&#20027;&#26080;&#20154;&#26426;&#20219;&#21153;&#25104;&#21151;&#23454;&#26045;&#30340;&#25361;&#25112;&#21644;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#20811;&#26381;&#25361;&#25112;&#30340;&#25216;&#26415;&#26041;&#26696;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#26080;&#20154;&#26426;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#27491;&#22312;&#25104;&#20026;&#29616;&#20195;&#25112;&#20105;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#39118;&#38505;&#38477;&#20302;&#20197;&#21450;&#33021;&#22815;&#25191;&#34892;&#26356;&#24191;&#27867;&#27963;&#21160;&#30340;&#29305;&#28857;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#20351;&#29992;&#33258;&#20027;&#26080;&#20154;&#26426;&#38024;&#23545;&#39640;&#20215;&#20540;&#30446;&#26631;&#25191;&#34892;&#25171;&#20987;&#20219;&#21153;&#12290;&#30001;&#20110;&#36134;&#26412;&#25216;&#26415;&#12289;&#26234;&#33021;&#21512;&#32422;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20197;&#21069;&#30001;&#19987;&#19994;&#20154;&#21592;&#25110;&#36828;&#31243;&#39134;&#34892;&#30340;&#26080;&#20154;&#26426;&#25191;&#34892;&#30340;&#36825;&#20123;&#27963;&#21160;&#29616;&#22312;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#20102;&#33258;&#20027;&#26080;&#20154;&#26426;&#20219;&#21153;&#25104;&#21151;&#23454;&#26045;&#30340;&#25361;&#25112;&#21644;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24517;&#39035;&#20811;&#26381;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#33258;&#20027;&#26080;&#20154;&#26426;&#20219;&#21153;&#25104;&#21151;&#27010;&#29575;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#26080;&#20154;&#26426;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01022v1 Announce Type: cross  Abstract: Unmanned Aircraft Vehicles (UAVs) are becoming a crucial tool in modern warfare, primarily due to their cost-effectiveness, risk reduction, and ability to perform a wider range of activities. The use of autonomous UAVs to conduct strike missions against highly valuable targets is the focus of this research. Due to developments in ledger technology, smart contracts, and machine learning, such activities formerly carried out by professionals or remotely flown UAVs are now feasible. Our study provides the first in-depth analysis of challenges and preliminary solutions for successful implementation of an autonomous UAV mission. Specifically, we identify challenges that have to be overcome and propose possible technical solutions for the challenges identified. We also derive analytical expressions for the success probability of an autonomous UAV mission, and describe a machine learning model to train the UAV.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39564;&#35777;&#24754;&#35266;&#23398;&#20064;&#65288;VPL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#30340;&#39564;&#35777;&#32531;&#20914;&#21306;&#35843;&#25972;&#24754;&#35266;&#27700;&#24179;&#65292;&#20197;&#26368;&#23567;&#21270;&#35780;&#35770;&#23478;&#30446;&#26631;&#30340;&#36924;&#36817;&#35823;&#24046;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01014</link><description>&lt;p&gt;
&#35770;&#35777;&#24754;&#35266;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#20013;&#39564;&#35777;&#32531;&#20914;&#21306;&#30340;&#24517;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Case for Validation Buffer in Pessimistic Actor-Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39564;&#35777;&#24754;&#35266;&#23398;&#20064;&#65288;VPL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#30340;&#39564;&#35777;&#32531;&#20914;&#21306;&#35843;&#25972;&#24754;&#35266;&#27700;&#24179;&#65292;&#20197;&#26368;&#23567;&#21270;&#35780;&#35770;&#23478;&#30446;&#26631;&#30340;&#36924;&#36817;&#35823;&#24046;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#24754;&#35266;&#26102;&#24207;&#24046;&#24322;&#30446;&#26631;&#26356;&#26032;&#30340;&#35780;&#35770;&#23478;&#32593;&#32476;&#20013;&#38169;&#35823;&#32047;&#31215;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35780;&#35770;&#23478;&#36924;&#36817;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;&#31867;&#20284;&#36125;&#23572;&#26364;&#20540;&#30340;&#36882;&#24402;&#19981;&#21160;&#28857;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#36882;&#24402;&#23450;&#20041;&#26469;&#25214;&#21040;&#24754;&#35266;&#35780;&#35770;&#23478;&#26080;&#20559;&#30340;&#26465;&#20214;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39564;&#35777;&#24754;&#35266;&#23398;&#20064;&#65288;VPL&#65289;&#31639;&#27861;&#12290;VPL&#20351;&#29992;&#19968;&#20010;&#23567;&#30340;&#39564;&#35777;&#32531;&#20914;&#21306;&#26469;&#35843;&#25972;&#25972;&#20010;&#20195;&#29702;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24754;&#35266;&#27700;&#24179;&#65292;&#20854;&#20013;&#24754;&#35266;&#35774;&#32622;&#20026;&#20351;&#35780;&#35770;&#23478;&#30446;&#26631;&#30340;&#36924;&#36817;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#36816;&#21160;&#21644;&#25805;&#32437;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#25253;&#21578;&#20102;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01014v1 Announce Type: new  Abstract: In this paper, we investigate the issue of error accumulation in critic networks updated via pessimistic temporal difference objectives. We show that the critic approximation error can be approximated via a recursive fixed-point model similar to that of the Bellman value. We use such recursive definition to retrieve the conditions under which the pessimistic critic is unbiased. Building on these insights, we propose Validation Pessimism Learning (VPL) algorithm. VPL uses a small validation buffer to adjust the levels of pessimism throughout the agent training, with the pessimism set such that the approximation error of the critic targets is minimized. We investigate the proposed approach on a variety of locomotion and manipulation tasks and report improvements in sample efficiency and performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#20219;&#21153;&#20998;&#35299;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#25552;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#30340;&#27599;&#31867;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#25968;&#25454;&#32534;&#30721;&#65292;&#24182;&#32467;&#21512;&#35299;&#30721;&#22120;&#65292;&#23558;&#25968;&#25454;&#38598;&#25552;&#28860;&#25104;&#26356;&#33410;&#30465;&#20869;&#23384;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#24418;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.00999</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#20219;&#21153;&#20998;&#35299;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Distributional Dataset Distillation with Subtask Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00999
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#20219;&#21153;&#20998;&#35299;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#25552;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#30340;&#27599;&#31867;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#25968;&#25454;&#32534;&#30721;&#65292;&#24182;&#32467;&#21512;&#35299;&#30721;&#22120;&#65292;&#23558;&#25968;&#25454;&#38598;&#25552;&#28860;&#25104;&#26356;&#33410;&#30465;&#20869;&#23384;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#24418;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;&#32508;&#21512;&#36825;&#31181;&#30693;&#35782;&#26159;&#25968;&#25454;&#38598;&#25552;&#28860;&#32972;&#21518;&#30340;&#20013;&#24515;&#24605;&#24819;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#19968;&#23567;&#32452;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#38598;&#20851;&#38190;&#26041;&#38754;&#30340;&#36755;&#20837;&#26631;&#31614;&#23545; ($\textit{prototypes}$)&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#29616;&#26377;&#30340;&#25552;&#21462;&#26126;&#30830;&#21407;&#22411;&#30340;&#26041;&#27861;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#65292;&#23548;&#33268;&#36890;&#36807;&#25552;&#28860;&#26631;&#31614;&#32780;&#20135;&#29983;&#24847;&#22806;&#30340;&#23384;&#20648;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#25552;&#28860;}$ (D3)&#65292;&#23427;&#20351;&#29992;&#26368;&#23567;&#30340;&#27599;&#31867;&#32479;&#35745;&#20449;&#24687;&#23545;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#19982;&#35299;&#30721;&#22120;&#37197;&#23545;&#65292;&#23558;&#25968;&#25454;&#38598;&#25552;&#28860;&#25104;&#19968;&#31181;&#26356;&#33410;&#30465;&#20869;&#23384;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#24418;&#24335;&#65292;&#19982;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#26356;&#39640;&#25928;&#12290;&#20026;&#20102;&#25193;&#22823;&#23398;&#20064;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{&#32852;&#21512;&#25552;&#28860;}$&#65292;&#23427;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00999v1 Announce Type: new  Abstract: What does a neural network learn when training from a task-specific dataset? Synthesizing this knowledge is the central idea behind Dataset Distillation, which recent work has shown can be used to compress large datasets into a small set of input-label pairs ($\textit{prototypes}$) that capture essential aspects of the original dataset. In this paper, we make the key observation that existing methods distilling into explicit prototypes are very often suboptimal, incurring in unexpected storage cost from distilled labels. In response, we propose $\textit{Distributional Dataset Distillation}$ (D3), which encodes the data using minimal sufficient per-class statistics and paired with a decoder, we distill dataset into a compact distributional representation that is more memory-efficient compared to prototype-based methods. To scale up the process of learning these representations, we propose $\textit{Federated distillation}$, which decompose
&lt;/p&gt;</description></item><item><title>&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.00993</link><description>&lt;p&gt;
&#35770;&#37096;&#20998;&#21487;&#35266;&#23519;&#24207;&#21015;&#22242;&#38431;&#21644;&#28216;&#25103;&#20013;&#20449;&#24687;&#32467;&#26500;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00993
&lt;/p&gt;
&lt;p&gt;
&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#20449;&#24687;&#32467;&#26500;&#25551;&#36848;&#20102;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#21051;&#20107;&#20214;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#12290;&#26412;&#25991;&#20027;&#24352;&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20855;&#26377;&#26126;&#30830;&#20449;&#24687;&#32467;&#26500;&#34920;&#31034;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00993v1 Announce Type: cross  Abstract: In a sequential decision-making problem, the information structure is the description of how events in the system occurring at different points in time affect each other. Classical models of reinforcement learning (e.g., MDPs, POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure.   In this paper, we argue for the perspective that explicit representation of information structures is an important component of analyzing and solving reinforcement learning problems. We propose novel reinforcement learning models with an explicit representation of information structure, capturing 
&lt;/p&gt;</description></item><item><title>SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00991</link><description>&lt;p&gt;
SELFI: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#20197;&#36827;&#34892;&#31038;&#20132;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00991
&lt;/p&gt;
&lt;p&gt;
SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#30340;&#26426;&#22120;&#20154;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#21644;&#32463;&#39564;&#31215;&#32047;&#26469;&#23454;&#29616;&#23558;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25237;&#20837;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;SELFI&#65292;&#21033;&#29992;&#22312;&#32447;&#26426;&#22120;&#20154;&#32463;&#39564;&#26469;&#24555;&#36895;&#39640;&#25928;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;SELFI&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#20043;&#19978;&#65292;&#20197;&#21457;&#25381;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#30340;&#20248;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SELFI&#36890;&#36807;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#23398;&#20064;&#30446;&#26631;&#19982;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#30456;&#32467;&#21512;&#65292;&#31283;&#23450;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#29616;&#23454;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SELFI&#65292;&#24182;&#25253;&#21578;&#20102;&#22312;&#36991;&#25758;&#26041;&#38754;&#30340;&#25913;&#21892;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#30740;&#31350;&#27979;&#37327;&#30340;&#26356;&#20855;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#12290;SELFI&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26377;&#29992;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#20943;&#23569;&#20102;&#39044;&#20808;&#24178;&#39044;&#30340;&#20154;&#21592;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00991v1 Announce Type: cross  Abstract: Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-e
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#26469;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#21151;&#29575;&#36755;&#20986;&#65292;&#23454;&#29616;&#20934;&#30830;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#26816;&#27979;&#24615;&#33021;&#24694;&#21270;&#65292;&#20197;&#25512;&#21160;&#31215;&#26497;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#20581;&#24247;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.00975</link><description>&lt;p&gt;
&#39118;&#21147;&#21457;&#30005;&#26426;&#24615;&#33021;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#35774;&#22791;&#20581;&#24247;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00975
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#26469;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#21151;&#29575;&#36755;&#20986;&#65292;&#23454;&#29616;&#20934;&#30830;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#26816;&#27979;&#24615;&#33021;&#24694;&#21270;&#65292;&#20197;&#25512;&#21160;&#31215;&#26497;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#20581;&#24247;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;SCADA&#25968;&#25454;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#39044;&#27979;&#21151;&#29575;&#36755;&#20986;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;FNN&#21644;LSTM&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#21033;&#29992;&#23427;&#20204;&#30340;&#38598;&#20307;&#23398;&#20064;&#12290;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#32988;&#36807;&#21333;&#20010;&#27169;&#22411;&#65292;&#30830;&#20445;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#21151;&#29575;&#36755;&#20986;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#24615;&#33021;&#24694;&#21270;&#65292;&#23454;&#29616;&#31215;&#26497;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#20581;&#24247;&#35780;&#20272;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27599;&#21488;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#29420;&#29305;&#24615;&#65292;&#38656;&#35201;&#20026;&#26368;&#20339;&#39044;&#27979;&#23450;&#21046;&#27169;&#22411;&#12290;&#36825;&#20123;&#35265;&#35299;&#24378;&#35843;&#25552;&#20379;&#19981;&#21516;&#21457;&#30005;&#26426;&#33258;&#21160;&#21270;&#23450;&#21046;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20445;&#25345;&#20154;&#21147;&#24314;&#27169;&#24037;&#20316;&#37327;&#20302;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#20998;&#26512;&#20013;&#24320;&#21457;&#30340;&#26041;&#27861;&#19981;&#23616;&#38480;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00975v1 Announce Type: cross  Abstract: In this study, we leverage SCADA data from diverse wind turbines to predict power output, employing advanced time series methods, specifically Functional Neural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key innovation lies in the ensemble of FNN and LSTM models, capitalizing on their collective learning. This ensemble approach outperforms individual models, ensuring stable and accurate power output predictions. Additionally, machine learning techniques are applied to detect wind turbine performance deterioration, enabling proactive maintenance strategies and health assessment. Crucially, our analysis reveals the uniqueness of each wind turbine, necessitating tailored models for optimal predictions. These insight underscores the importance of providing automatized customization for different turbines to keep human modeling effort low. Importantly, the methodologies developed in this analysis are not limited to wind tu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32593;&#32476;&#27169;&#24335;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#25509;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#23613;&#31649;&#38543;&#26426;&#21021;&#22987;&#21270;&#21442;&#25968;&#65292;&#24378;&#21046;&#31232;&#30095;&#20250;&#23548;&#33268;&#36825;&#20123;&#32593;&#32476;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#32467;&#26500;</title><link>https://arxiv.org/abs/2403.00974</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#24335;&#20998;&#24067;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Motif distribution and function of sparse deep neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00974
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#27169;&#24335;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#25509;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#23613;&#31649;&#38543;&#26426;&#21021;&#22987;&#21270;&#21442;&#25968;&#65292;&#24378;&#21046;&#31232;&#30095;&#20250;&#23548;&#33268;&#36825;&#20123;&#32593;&#32476;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#32593;&#32476;&#27169;&#24335;&#29702;&#35770;&#34920;&#24449;&#21069;&#39304;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#36830;&#25509;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#29305;&#23450;&#27169;&#24335;&#20998;&#24067;&#26159;&#21542;&#26159;&#35757;&#32451;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#36824;&#26159;DNN&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35757;&#32451;&#20197;&#27169;&#25311;&#29983;&#29289;&#21147;&#23398;&#39134;&#34892;&#25511;&#21046;&#31995;&#32479;&#30340;350&#20010;DNN&#30340;&#36830;&#25509;&#32467;&#26500;&#65292;&#36825;&#20123;DNN&#25317;&#26377;&#19981;&#21516;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#21442;&#25968;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#23454;&#26045;&#20102;&#29992;&#20110;&#35745;&#31639;&#20108;&#38454;&#21644;&#19977;&#38454;&#27169;&#24335;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;Z&#20998;&#25968;&#35745;&#31639;&#20854;&#26174;&#30528;&#24615;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;DNN&#34987;&#35757;&#32451;&#26469;&#35299;&#20915; Bustamante &#31561;&#20154;&#65288;2022&#24180;&#65289;&#20013;&#30340;&#39134;&#34892;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#21453;&#38382;&#39064;&#65288;&#21363;&#20174;&#21021;&#22987;&#21644;&#26368;&#32456;&#29366;&#24577;&#31354;&#38388;&#36755;&#20837;&#39044;&#27979;&#25511;&#21046;&#25152;&#38656;&#65289;&#12290;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#20462;&#21098;&#21644;&#37325;&#26032;&#35757;&#32451;&#31639;&#27861; Zahn &#31561;&#20154;&#65288;2022&#24180;&#65289;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#31232;&#30095;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#32593;&#32476;&#21442;&#25968;&#38543;&#26426;&#21021;&#22987;&#21270;&#65292;&#20294;&#24378;&#21046;&#31232;&#30095;&#20250;&#23548;&#33268;DNN&#25910;&#25947;&#21040;&#31867;&#20284;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00974v1 Announce Type: new  Abstract: We characterize the connectivity structure of feed-forward, deep neural networks (DNNs) using network motif theory. To address whether a particular motif distribution is characteristic of the training task, or function of the DNN, we compare the connectivity structure of 350 DNNs trained to simulate a bio-mechanical flight control system with different randomly initialized parameters. We develop and implement algorithms for counting second- and third-order motifs and calculate their significance using their Z-score. The DNNs are trained to solve the inverse problem of the flight dynamics model in Bustamante, et al. (2022) (i.e., predict the controls necessary for controlled flight from the initial and final state-space inputs) and are sparsified through an iterative pruning and retraining algorithm Zahn, et al. (2022). We show that, despite random initialization of network parameters, enforced sparsity causes DNNs to converge to similar 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415; Binary Gaussian Copula Synthesis (BGCS)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#22312;&#26089;&#26399;&#39044;&#27979;&#24930;&#24615;&#32958;&#30149;&#24739;&#32773;&#36879;&#26512;&#38656;&#27714;&#20013;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.00965</link><description>&lt;p&gt;
&#20108;&#20540;&#39640;&#26031;Copula&#21512;&#25104;&#65306;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#36827;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#26089;&#26399;&#39044;&#27979;&#24930;&#24615;&#32958;&#30149;&#24739;&#32773;&#30340;&#36879;&#26512;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415; Binary Gaussian Copula Synthesis (BGCS)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#22312;&#26089;&#26399;&#39044;&#27979;&#24930;&#24615;&#32958;&#30149;&#24739;&#32773;&#36879;&#26512;&#38656;&#27714;&#20013;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;&#23398;&#26415;&#65306;2403.00965v1  &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028;  &#25688;&#35201;&#65306;&#32654;&#22269;&#30142;&#30149;&#25511;&#21046;&#20013;&#24515;&#20272;&#35745;&#65292;&#36229;&#36807;3700&#19975;&#25104;&#24180;&#32654;&#22269;&#20154;&#24739;&#26377;&#24930;&#24615;&#32958;&#30149;&#65288;CKD&#65289;&#65292;&#28982;&#32780;&#20854;&#20013;&#30340;9&#25104;&#24739;&#32773;&#30001;&#20110;&#26089;&#26399;&#27809;&#26377;&#30151;&#29366;&#32780;&#19981;&#30693;&#36947;&#33258;&#24049;&#30340;&#29366;&#20917;&#12290;&#26089;&#26399;&#39044;&#27979;&#36879;&#26512;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#65292;&#24182;&#24110;&#21161;&#21307;&#30103;&#25552;&#20379;&#32773;&#21450;&#26102;&#20570;&#20986;&#30693;&#24773;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#26377;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26089;&#26399;&#36879;&#26512;&#39044;&#27979;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#38754;&#20020;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20108;&#20540;&#39640;&#26031;Copula&#21512;&#25104;&#65288;BGCS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00965v1 Announce Type: cross  Abstract: The Center for Disease Control estimates that over 37 million US adults suffer from chronic kidney disease (CKD), yet 9 out of 10 of these individuals are unaware of their condition due to the absence of symptoms in the early stages. It has a significant impact on patients' quality of life, particularly when it progresses to the need for dialysis. Early prediction of dialysis is crucial as it can significantly improve patient outcomes and assist healthcare providers in making timely and informed decisions. However, developing an effective machine learning (ML)-based Clinical Decision Support System (CDSS) for early dialysis prediction poses a key challenge due to the imbalanced nature of data. To address this challenge, this study evaluates various data augmentation techniques to understand their effectiveness on real-world datasets. We propose a new approach named Binary Gaussian Copula Synthesis (BGCS). BGCS is tailored for binary me
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#25237;&#31080;&#38598;&#25104;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26816;&#27979;LLM&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.00964</link><description>&lt;p&gt;
&#22312;SemEval-2024&#20219;&#21153;6&#20013;&#30340;MALTO&#65306;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26816;&#27979;LLM&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00964
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#25237;&#31080;&#38598;&#25104;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26816;&#27979;LLM&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20013;&#65292;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#27969;&#30021;&#20294;&#19981;&#20934;&#30830;&#30340;&#36755;&#20986;&#20197;&#21450;&#20381;&#36182;&#20110;&#27969;&#30021;&#24615;&#20026;&#20013;&#24515;&#30340;&#24230;&#37327;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#20986;&#29616;&#8220;&#24187;&#35273;&#8221;&#12290;SHROOM&#25361;&#25112;&#26088;&#22312;&#33258;&#21160;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#36825;&#20123;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#21253;&#25324;LLM&#36741;&#21161;&#30340;&#20266;&#26631;&#35760;&#21644;&#21477;&#23376;&#25913;&#20889;&#65292;&#20197;&#21450;&#19968;&#20010;&#25237;&#31080;&#38598;&#25104;&#26469;&#33258;&#19977;&#20010;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00964v1 Announce Type: new  Abstract: In Natural Language Generation (NLG), contemporary Large Language Models (LLMs) face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting "hallucinations". The SHROOM challenge focuses on automatically identifying these hallucinations in the generated text. To tackle these issues, we introduce two key components, a data augmentation pipeline incorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a voting ensemble from three models pre-trained on Natural Language Inference (NLI) tasks and fine-tuned on diverse datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26641;&#27491;&#21017;&#21270;&#34920;&#24449;&#26469;&#27491;&#21017;&#21270;&#34920;&#26684;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#21464;&#37327;&#36716;&#25442;&#20026;&#21333;&#20010;&#21521;&#37327;&#25110;&#19968;&#31995;&#21015;&#26631;&#35760;&#65292;&#26377;&#25928;&#32553;&#23567;&#20102;&#19982;&#26641;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.00963</link><description>&lt;p&gt;
&#26641;&#27491;&#21017;&#21270;&#30340;&#34920;&#26684;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Tree-Regularized Tabular Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26641;&#27491;&#21017;&#21270;&#34920;&#24449;&#26469;&#27491;&#21017;&#21270;&#34920;&#26684;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#21464;&#37327;&#36716;&#25442;&#20026;&#21333;&#20010;&#21521;&#37327;&#25110;&#19968;&#31995;&#21015;&#26631;&#35760;&#65292;&#26377;&#25928;&#32553;&#23567;&#20102;&#19982;&#26641;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24341;&#36215;&#20102;&#26174;&#30528;&#30340;&#20851;&#27880;&#65292;&#20854;&#26368;&#26032;&#36827;&#23637;&#36880;&#28176;&#32553;&#23567;&#20102;&#19982;&#35768;&#22810;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#26641;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#34429;&#28982;&#20027;&#27969;&#20851;&#27880;&#20110;&#26657;&#20934;NN&#20197;&#36866;&#24212;&#34920;&#26684;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#24378;&#35843;&#21516;&#36136;&#23884;&#20837;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20132;&#26367;&#38598;&#20013;&#22312;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#27491;&#21017;&#21270;&#34920;&#26684;&#36755;&#20837;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#19968;&#20010;&#24037;&#20316;&#65288;DeepTLF&#65289;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#26641;&#38598;&#21512;&#30340;&#32467;&#26500;&#23558;&#21407;&#22987;&#21464;&#37327;&#36716;&#25442;&#20026;&#21333;&#20010;&#21521;&#37327;&#65288;T2V&#65289;&#25110;&#19968;&#31995;&#21015;&#26631;&#35760;&#65288;T2T&#65289;&#12290;&#36825;&#20123;&#20108;&#20803;&#21270;&#23884;&#20837;&#21487;&#20197;&#36890;&#36807;&#24102;&#26377;&#20840;&#36830;&#25509;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26500;&#24314;&#22359;&#30340;&#32463;&#20856;&#34920;&#26684;NN&#28040;&#32791;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#31354;&#38388;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;88&#20010;OpenML&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#37327;&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26641;&#27491;&#21017;&#21270;&#34920;&#24449;&#19981;&#20165;&#32553;&#23567;&#20102;&#19982;&#26641;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00963v1 Announce Type: new  Abstract: Tabular neural network (NN) has attracted remarkable attentions and its recent advances have gradually narrowed the performance gap with respect to tree-based models on many public datasets. While the mainstreams focus on calibrating NN to fit tabular data, we emphasize the importance of homogeneous embeddings and alternately concentrate on regularizing tabular inputs through supervised pretraining. Specifically, we extend a recent work (DeepTLF) and utilize the structure of pretrained tree ensembles to transform raw variables into a single vector (T2V), or an array of tokens (T2T). Without loss of space efficiency, these binarized embeddings can be consumed by canonical tabular NN with fully-connected or attention-based building blocks. Through quantitative experiments on 88 OpenML datasets with binary classification task, we validated that the proposed tree-regularized representation not only tapers the difference with respect to tree-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#23454;&#36341;&#31038;&#21306;&#65292;&#20998;&#20139;&#26368;&#20339;&#23454;&#36341;&#21644;&#32463;&#39564;&#25945;&#35757;&#65292;&#20026;&#23558;&#25968;&#25454;&#31185;&#23398;&#32435;&#20837;&#22823;&#23398;&#29289;&#29702;&#35838;&#31243;&#30340;&#20851;&#38190;&#31574;&#30053;&#21644;&#25361;&#25112;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.00961</link><description>&lt;p&gt;
&#22823;&#23398;&#29289;&#29702;&#23398;&#20013;&#30340;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#65306;&#26469;&#33258;&#23454;&#36341;&#31038;&#21306;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00961
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#23454;&#36341;&#31038;&#21306;&#65292;&#20998;&#20139;&#26368;&#20339;&#23454;&#36341;&#21644;&#32463;&#39564;&#25945;&#35757;&#65292;&#20026;&#23558;&#25968;&#25454;&#31185;&#23398;&#32435;&#20837;&#22823;&#23398;&#29289;&#29702;&#35838;&#31243;&#30340;&#20851;&#38190;&#31574;&#30053;&#21644;&#25361;&#25112;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#26085;&#30410;&#20016;&#23500;&#65292;&#20174;&#23567;&#35268;&#27169;&#23454;&#39564;&#25968;&#25454;&#28857;&#21040;&#22823;&#22411;&#22797;&#26434;&#25968;&#25454;&#23384;&#20648;&#24211;&#21644;&#24378;&#22823;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#29289;&#29702;&#25945;&#32946;&#24037;&#20316;&#32773;&#36234;&#26469;&#36234;&#37325;&#35270;&#35013;&#22791;&#20182;&#20204;&#30340;&#23398;&#29983;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25945;&#32946;&#24037;&#20316;&#32773;&#21487;&#33021;&#32570;&#20047;&#25945;&#25480;&#36825;&#20123;&#25216;&#33021;&#25152;&#38656;&#30340;&#25968;&#25454;&#31185;&#23398;&#22521;&#35757;&#21644;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#23454;&#36341;&#31038;&#21306;&#65288;DSECOP&#65289;&#65292;&#27719;&#38598;&#20102;&#19981;&#21516;&#38498;&#26657;&#21644;&#32972;&#26223;&#30340;&#30740;&#31350;&#29983;&#21644;&#29289;&#29702;&#25945;&#32946;&#24037;&#20316;&#32773;&#65292;&#20998;&#20139;&#25972;&#21512;&#25968;&#25454;&#31185;&#23398;&#21040;&#22823;&#23398;&#29289;&#29702;&#25945;&#32946;&#20013;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20010;&#23454;&#36341;&#31038;&#21306;&#30340;&#35265;&#35299;&#21644;&#32463;&#39564;&#65292;&#31361;&#20986;&#20102;&#22312;&#23558;&#25968;&#25454;&#31185;&#23398;&#32435;&#20837;&#21021;&#32423;&#29289;&#29702;&#35838;&#31243;&#20013;&#30340;&#20851;&#38190;&#31574;&#30053;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#25351;&#23548;&#21644;ins
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00961v1 Announce Type: cross  Abstract: With the increasing availability of diverse datasets, ranging from small-scale experimental data points to large and complex data repositories and powerful data analysis tools, it is increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned in integrating data science into undergraduate physics education. In this article, we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and ins
&lt;/p&gt;</description></item><item><title>MediSwift&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24341;&#20837;&#20102;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;75%&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#35757;&#32451;FLOPs&#20943;&#23569;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00952</link><description>&lt;p&gt;
MediSwift&#65306;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MediSwift: Efficient Sparse Pre-trained Biomedical Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00952
&lt;/p&gt;
&lt;p&gt;
MediSwift&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24341;&#20837;&#20102;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;75%&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#35757;&#32451;FLOPs&#20943;&#23569;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#36890;&#29992;&#28304;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20294;&#26368;&#36817;&#39046;&#22495;&#29305;&#23450;&#30340;LLMs&#28608;&#22686;&#34920;&#26126;&#23427;&#20204;&#22312;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#65288;&#20363;&#22914;&#29983;&#29289;&#21307;&#23398;&#65289;&#20013;&#30340;&#28508;&#21147;&#36229;&#36807;&#20102;&#36890;&#29992;&#22411;&#27169;&#22411;&#12290;&#34429;&#28982;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#25928;&#29575;&#24182;&#23548;&#33268;&#27169;&#22411;&#26356;&#23567;&#65292;&#20294;&#36825;&#20123;LLMs&#30340;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#65292;&#26500;&#25104;&#20102;&#39044;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MediSwift&#65292;&#19968;&#22871;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;LM&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#39640;&#36798;75&#65285;&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#65292;MediSwift&#22312;&#35757;&#32451;FLOPs&#26041;&#38754;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#20943;&#23569;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#22343;&#22312;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23454;&#29616;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#30340;&#21152;&#36895;&#22909;&#22788;&#30340;Cerebras CS-2&#31995;&#32479;&#19978;&#36827;&#34892;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#20102;MediSwift&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00952v1 Announce Type: new  Abstract: Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00946</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;Dropout&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning with Very Large Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#19981;&#21487;&#33021;&#20551;&#35013;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#19982;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#30456;&#21516;&#20998;&#24067;&#30340;&#35266;&#24565;&#26159;&#20860;&#23481;&#30340;&#12290;&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;&#20002;&#24323;&#29575;&#26469;&#33719;&#24471;&#36825;&#31181;&#20016;&#23500;&#34920;&#31034;&#65292;&#23613;&#31649;&#20351;&#29992;&#36825;&#26679;&#30340;&#20002;&#24323;&#29575;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#19981;&#20165;&#26159;&#21487;&#33021;&#30340;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#36229;&#36234;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00946v1 Announce Type: new  Abstract: It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.   This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00942</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of Entropy Model in Distributed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#29109;&#32534;&#30721;&#34987;&#24341;&#20837;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#20998;&#24067;&#24335;DNN&#19982;&#29109;&#27169;&#22411;&#32852;&#21512;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#38388;&#29992;&#20316;&#36793;&#20449;&#24687;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#28508;&#22312;&#34920;&#31034;&#32534;&#30721;&#20026;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#30340;&#27604;&#29305;&#27969;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#24182;&#35843;&#26597;&#20102;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#65289;&#21644;&#26080;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#22825;&#27668;&#21464;&#21270;&#21644;&#36816;&#21160;&#27169;&#31946;&#65289;&#30340;&#38887;&#24615;&#12290;&#36890;&#36807;&#23545;3&#31181;&#19981;&#21516;DNN&#26550;&#26500;&#12289;2&#20010;&#29109;&#27169;&#22411;&#21644;4&#20010;&#36895;&#29575;&#22833;&#30495;&#26435;&#34913;&#22240;&#23376;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00942v1 Announce Type: cross  Abstract: Distributed deep neural networks (DNNs) have emerged as a key technique to reduce communication overhead without sacrificing performance in edge computing systems. Recently, entropy coding has been introduced to further reduce the communication overhead. The key idea is to train the distributed DNN jointly with an entropy model, which is used as side information during inference time to adaptively encode latent representations into bit streams with variable length. To the best of our knowledge, the resilience of entropy models is yet to be investigated. As such, in this paper we formulate and investigate the resilience of entropy models to intentional interference (e.g., adversarial attacks) and unintentional interference (e.g., weather changes and motion blur). Through an extensive experimental campaign with 3 different DNN architectures, 2 entropy models and 4 rate-distortion trade-off factors, we demonstrate that the entropy attacks
&lt;/p&gt;</description></item><item><title>&#36801;&#31227;&#23398;&#20064;&#22312;&#23433;&#20840;&#39046;&#22495;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#36328;&#22495;&#20256;&#36882;&#30693;&#35782;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#65292;&#20943;&#23569;&#25968;&#25454;&#26631;&#27880;&#24037;&#20316;&#65292;&#24182;&#26412;&#25991;&#26088;&#22312;&#22238;&#39038;&#21644;&#25506;&#35752;&#20854;&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.00935</link><description>&lt;p&gt;
&#23433;&#20840;&#39046;&#22495;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#25361;&#25112;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Security: Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00935
&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#22312;&#23433;&#20840;&#39046;&#22495;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#36328;&#22495;&#20256;&#36882;&#30693;&#35782;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#65292;&#20943;&#23569;&#25968;&#25454;&#26631;&#27880;&#24037;&#20316;&#65292;&#24182;&#26412;&#25991;&#26088;&#22312;&#22238;&#39038;&#21644;&#25506;&#35752;&#20854;&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#38656;&#35201;&#23545;&#19968;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#25105;&#20204;&#21482;&#26377;&#26469;&#33258;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#20805;&#36275;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#12290;&#21518;&#32773;&#30340;&#25968;&#25454;&#21487;&#33021;&#36981;&#24490;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#22320;&#22312;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#23545;&#24191;&#27867;&#25968;&#25454;&#26631;&#27880;&#24037;&#20316;&#30340;&#38656;&#27714;&#12290;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#22240;&#27492;&#25104;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#39038;&#21033;&#29992;TL&#25216;&#26415;&#26469;&#24212;&#29992;&#20110;&#23433;&#20840;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22312;&#23433;&#20840;&#39046;&#22495;&#24212;&#29992;TL&#20013;&#23384;&#22312;&#30340;&#30740;&#31350;&#24046;&#36317;&#65292;&#24182;&#25506;&#32034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00935v1 Announce Type: cross  Abstract: Many machine learning and data mining algorithms rely on the assumption that the training and testing data share the same feature space and distribution. However, this assumption may not always hold. For instance, there are situations where we need to classify data in one domain, but we only have sufficient training data available from a different domain. The latter data may follow a distinct distribution. In such cases, successfully transferring knowledge across domains can significantly improve learning performance and reduce the need for extensive data labeling efforts. Transfer learning (TL) has thus emerged as a promising framework to tackle this challenge, particularly in security-related tasks. This paper aims to review the current advancements in utilizing TL techniques for security. The paper includes a discussion of the existing research gaps in applying TL in the security domain, as well as exploring potential future researc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.00932</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#25991;&#26412;&#29983;&#25104;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Knowledge Distillation via Synthetic Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38544;&#31169;&#30340;&#22686;&#21152;&#32039;&#36843;&#24615;&#35201;&#27714;LLMs&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;(DP)&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#38656;&#35201;&#21387;&#32553;LLMs&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#25110;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#30495;&#23454;&#37096;&#32626;&#12290;&#24046;&#20998;&#38544;&#31169;&#21644;&#27169;&#22411;&#21387;&#32553;&#36890;&#24120;&#24517;&#39035;&#22312;&#23454;&#29616;&#20854;&#30446;&#26631;&#30340;&#36807;&#31243;&#20013;&#26435;&#34913;&#25928;&#29992;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#32773;&#21487;&#33021;&#23548;&#33268;&#26356;&#22810;&#30340;&#25928;&#29992;&#25439;&#22833;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#30001;&#24046;&#20998;&#31169;&#23494;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20197;&#20004;&#31181;&#26041;&#24335;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#19978;&#65306;&#19968;&#31181;&#26159;&#26469;&#33258;&#21512;&#25104;&#25968;&#25454;&#26412;&#36523;&#30340;&#30828;&#26631;&#31614;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00932v1 Announce Type: cross  Abstract: Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires LLMs to train with Differential Privacy (DP) on private data. Concurrently it is also necessary to compress LLMs for real-life deployments on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private LLM. The knowledge of a teacher model is transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher model evaluated on the synthetic data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#39318;&#20010;&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;SCB&#65292;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;MDP&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2403.00930</link><description>&lt;p&gt;
&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scale-free Adversarial Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#39318;&#20010;&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;SCB&#65292;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;MDP&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#26080;&#23610;&#24230;&#23398;&#20064;&#65292;&#20854;&#22870;&#21169;/&#25439;&#22833;&#30340;&#23610;&#24230;&#20026;&#23398;&#20064;&#32773;&#25152;&#19981;&#30693;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;\underline{S}cale \underline{C}lipping \underline{B}ound&#65288;\texttt{SCB}&#65289;&#65292;&#24182;&#23558;&#36825;&#19968;&#26694;&#26550;&#23454;&#20363;&#21270;&#21040;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#35774;&#32622;&#21644;&#23545;&#25239;&#24615;MDP&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#22312;&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;MABs&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#26368;&#23567;&#20540;&#26368;&#20248;&#26399;&#26395;&#36951;&#25022;&#30028;&#21644;&#31532;&#19968;&#20010;&#39640;&#27010;&#29575;&#36951;&#25022;&#30028;&#65292;&#35299;&#20915;&#20102;\cite{hadiji2023adaptation}&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#23545;&#25239;&#24615;MDPs&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#35806;&#29983;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;$\tilde{\mathcal{O}}(\sqrt{T})$&#39640;&#27010;&#29575;&#36951;&#25022;&#20445;&#35777;&#30340;&#26080;&#23610;&#24230;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00930v1 Announce Type: cross  Abstract: This paper initiates the study of scale-free learning in Markov Decision Processes (MDPs), where the scale of rewards/losses is unknown to the learner. We design a generic algorithmic framework, \underline{S}cale \underline{C}lipping \underline{B}ound (\texttt{SCB}), and instantiate this framework in both the adversarial Multi-armed Bandit (MAB) setting and the adversarial MDP setting. Through this framework, we achieve the first minimax optimal expected regret bound and the first high-probability regret bound in scale-free adversarial MABs, resolving an open problem raised in \cite{hadiji2023adaptation}. On adversarial MDPs, our framework also give birth to the first scale-free RL algorithm with a $\tilde{\mathcal{O}}(\sqrt{T})$ high-probability regret guarantee.
&lt;/p&gt;</description></item><item><title>PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00929</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21407;&#35821;&#25645;&#24314;&#20219;&#21153;&#30340;&#26694;&#26550;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00929
&lt;/p&gt;
&lt;p&gt;
PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#23398;&#20250;&#22797;&#26434;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#22797;&#21512;&#35823;&#24046;&#20250;&#22312;&#20219;&#21153;&#26102;&#27573;&#20869;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRIME&#65288;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#25968;&#25454;&#25928;&#29575;&#27169;&#20223;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;PRIME&#36890;&#36807;&#23558;&#20219;&#21153;&#28436;&#31034;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#26469;&#25645;&#24314;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#26469;&#23545;&#21407;&#35821;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PRIME&#22312;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#39640;&#20986;10-34&#65285;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#39640;&#20986;20-48&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21010;&#20998;&#20102;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#36335;&#24452;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#31639;&#27861;&#37197;&#32622;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00898</link><description>&lt;p&gt;
&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Algorithm Configuration Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21010;&#20998;&#20102;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#36335;&#24452;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#31639;&#27861;&#37197;&#32622;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20248;&#21270;&#39046;&#22495;&#38543;&#30528;&#33258;&#21160;&#37197;&#32622;&#31639;&#27861;&#21442;&#25968;&#26041;&#27861;&#30340;&#21457;&#23637;&#32780;&#26174;&#33879;&#36827;&#27493;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#26088;&#22312;&#20248;&#21270;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#20915;&#31574;/&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#30340;&#21442;&#25968;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#19981;&#20165;&#24418;&#24335;&#21270;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#36824;&#27010;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#35813;&#25991;&#31456;&#23558;&#29616;&#26377;&#26041;&#27861;&#35770;&#21010;&#20998;&#20026;&#22522;&#20110;&#23454;&#20363;&#21644;&#22522;&#20110;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21306;&#20998;&#31163;&#32447;&#21644;&#22312;&#32447;&#31574;&#30053;&#29992;&#20110;&#27169;&#22411;&#26500;&#24314;&#21644;&#37096;&#32626;&#12290;&#36890;&#36807;&#32508;&#21512;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#29702;&#35299;&#21644;&#35299;&#20915;&#31639;&#27861;&#37197;&#32622;&#20013;&#22266;&#26377;&#22797;&#26434;&#24615;&#25552;&#20379;&#28165;&#26224;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00898v1 Announce Type: new  Abstract: The field of algorithmic optimization has significantly advanced with the development of methods for the automatic configuration of algorithmic parameters. This article delves into the Algorithm Configuration Problem, focused on optimizing parametrized algorithms for solving specific instances of decision/optimization problems. We present a comprehensive framework that not only formalizes the Algorithm Configuration Problem, but also outlines different approaches for its resolution, leveraging machine learning models and heuristic strategies. The article categorizes existing methodologies into per-instance and per-problem approaches, distinguishing between offline and online strategies for model construction and deployment. By synthesizing these approaches, we aim to provide a clear pathway for both understanding and addressing the complexities inherent in algorithm configuration.
&lt;/p&gt;</description></item><item><title>VisRec&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#23556;&#30005;&#24178;&#25200;&#25968;&#25454;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#38477;&#20302;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#23478;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;</title><link>https://arxiv.org/abs/2403.00897</link><description>&lt;p&gt;
VisRec:&#19968;&#31181;&#29992;&#20110;&#23556;&#30005;&#24178;&#28041;&#25968;&#25454;&#37325;&#24314;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00897
&lt;/p&gt;
&lt;p&gt;
VisRec&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#23556;&#30005;&#24178;&#25200;&#25968;&#25454;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#38477;&#20302;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#23478;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#30005;&#26395;&#36828;&#38236;&#20135;&#29983;&#20851;&#20110;&#22825;&#20307;&#23545;&#35937;&#30340;&#21487;&#35265;&#24615;&#25968;&#25454;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#31232;&#30095;&#19988;&#22024;&#26434;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#21407;&#22987;&#21487;&#35265;&#24615;&#25968;&#25454;&#21019;&#24314;&#30340;&#22270;&#20687;&#36136;&#37327;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#24314;&#21487;&#35265;&#24615;&#25968;&#25454;&#65292;&#20197;&#33719;&#24471;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#23556;&#30005;&#22825;&#25991;&#23398;&#23478;&#22823;&#37327;&#30340;&#26631;&#27880;&#24037;&#20316;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VisRec&#65292;&#19968;&#31181;&#38754;&#21521;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#21487;&#35265;&#24615;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VisRec&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#12290;&#22312;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#32452;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;VisRec&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#20250;&#22686;&#21152;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#38750;&#22686;&#24378;&#21487;&#35265;&#24615;&#25968;&#25454;&#30340;&#37325;&#24314;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00897v1 Announce Type: cross  Abstract: Radio telescopes produce visibility data about celestial objects, but these data are sparse and noisy. As a result, images created on raw visibility data are of low quality. Recent studies have used deep learning models to reconstruct visibility data to get cleaner images. However, these methods rely on a substantial amount of labeled training data, which requires significant labeling effort from radio astronomers. Addressing this challenge, we propose VisRec, a model-agnostic semi-supervised learning approach to the reconstruction of visibility data. Specifically, VisRec consists of both a supervised learning module and an unsupervised learning module. In the supervised learning module, we introduce a set of data augmentation functions to produce diverse training examples. In comparison, the unsupervised learning module in VisRec augments unlabeled data and uses reconstructions from non-augmented visibility data as pseudo-labels for t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>PowerFlowMultiNet&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#23884;&#20837;&#26426;&#21046;&#26469;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.00892</link><description>&lt;p&gt;
PowerFlowMultiNet&#65306;&#29992;&#20110;&#19981;&#24179;&#34913;&#19977;&#30456;&#37197;&#30005;&#31995;&#32479;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00892
&lt;/p&gt;
&lt;p&gt;
PowerFlowMultiNet&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#23884;&#20837;&#26426;&#21046;&#26469;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#35299;&#20915;&#37197;&#30005;&#32593;&#20013;&#19981;&#24179;&#34913;&#30340;&#19977;&#30456;&#21151;&#29575;&#27969;&#38382;&#39064;&#23545;&#20110;&#32593;&#26684;&#20998;&#26512;&#21644;&#20223;&#30495;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#24613;&#38656;&#21487;&#22788;&#29702;&#22823;&#35268;&#27169;&#19981;&#24179;&#34913;&#21151;&#29575;&#32593;&#26684;&#24182;&#33021;&#25552;&#20379;&#20934;&#30830;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#24179;&#34913;&#32593;&#32476;&#19978;&#65292;&#32570;&#20047;&#25903;&#25345;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#32476;&#30340;&#20851;&#38190;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PowerFlowMultiNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#22270;&#34920;&#31034;&#20013;&#20998;&#21035;&#23545;&#27599;&#20010;&#30456;&#36827;&#34892;&#24314;&#27169;&#65292;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#22266;&#26377;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#24341;&#20837;&#20102;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#23884;&#20837;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00892v1 Announce Type: cross  Abstract: Efficiently solving unbalanced three-phase power flow in distribution grids is pivotal for grid analysis and simulation. There is a pressing need for scalable algorithms capable of handling large-scale unbalanced power grids that can provide accurate and fast solutions. To address this, deep learning techniques, especially Graph Neural Networks (GNNs), have emerged. However, existing literature primarily focuses on balanced networks, leaving a critical gap in supporting unbalanced three-phase power grids. This letter introduces PowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for unbalanced three-phase power grids. The proposed approach models each phase separately in a multigraph representation, effectively capturing the inherent asymmetry in unbalanced grids. A graph embedding mechanism utilizing message passing is introduced to capture spatial dependencies within the power system network. PowerFlowMultiNet out
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#23454;&#29616;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#29992;&#30693;&#35782;&#30340;&#36801;&#31227;</title><link>https://arxiv.org/abs/2403.00891</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#23454;&#29616;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#29992;&#30693;&#35782;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#22797;&#26434;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#24050;&#20026;&#21508;&#31181;IE&#20219;&#21153;&#26500;&#24314;&#20102;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#25968;&#25454;&#26631;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#26041;&#27861;&#20391;&#37325;&#20110;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#23545;&#19981;&#21516;IE&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30701;&#35821;&#21487;&#33021;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#65292;&#36825;&#23545;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#36801;&#31227;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25152;&#26377;&#33879;&#21517;IE&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#19968;&#20010;&#25351;&#23548;&#27744;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#20010;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#65292;&#26681;&#25454;&#30456;&#24212;&#30340;&#25351;&#23548;&#23558;&#21508;&#31181;&#22797;&#26434;&#32467;&#26500;&#22343;&#21248;&#22320;&#35299;&#30721;&#20026;&#22270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#20849;&#20139;&#30340;&#36890;&#29992;&#30693;&#35782;&#21487;&#20197;&#26356;&#22909;&#22320;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00891v1 Announce Type: cross  Abstract: Information extraction (IE) aims to extract complex structured information from the text. Numerous datasets have been constructed for various IE tasks, leading to time-consuming and labor-intensive data annotations. Nevertheless, most prevailing methods focus on training task-specific models, while the common knowledge among different IE tasks is not explicitly modeled. Moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for knowledge transfer using a unified model. In this study, we propose a regularization-based transfer learning method for IE (TIE) via an instructed graph decoder. Specifically, we first construct an instruction pool for datasets from all well-known IE tasks, and then present an instructed graph decoder, which decodes various complex structures into a graph uniformly based on corresponding instructions. In this way, the common knowledge shared with existing datasets 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Proteus&#30340;&#21019;&#26032;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26102;&#38480;&#19978;&#19979;&#25991;&#29983;&#29289;&#36523;&#20221;&#30340;&#29983;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26497;&#31616;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#23454;&#26102;&#35748;&#35777;&#25361;&#25112;&#65292;&#20419;&#36827;&#35774;&#22791;&#21327;&#20316;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.00889</link><description>&lt;p&gt;
&#38754;&#21521;&#26497;&#31616;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26102;&#38480;&#19978;&#19979;&#25991;&#29983;&#29289;&#36523;&#20221;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Time-bound Contextual Bio-ID Generation for Minimalist Wearables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Proteus&#30340;&#21019;&#26032;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26102;&#38480;&#19978;&#19979;&#25991;&#29983;&#29289;&#36523;&#20221;&#30340;&#29983;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26497;&#31616;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#23454;&#26102;&#35748;&#35777;&#25361;&#25112;&#65292;&#20419;&#36827;&#35774;&#22791;&#21327;&#20316;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#31359;&#25140;&#35774;&#22791;&#36234;&#26469;&#36234;&#23567;&#22411;&#21270;&#21644;&#24378;&#22823;&#65292;&#21363;&#26102;&#21160;&#24577;&#30340;&#35774;&#22791;&#23545;&#35774;&#22791;&#21327;&#20316;&#21644;&#20154;&#23545;&#35774;&#22791;&#20132;&#20114;&#30340;&#26032;&#26426;&#36935;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#36825;&#20123;&#26497;&#31616;&#21487;&#31359;&#25140;&#35774;&#22791;&#32570;&#20047;&#23454;&#26102;&#36523;&#20221;&#39564;&#35777;&#26426;&#21046;&#65292;&#32473;&#25968;&#25454;&#38544;&#31169;&#21644;&#25972;&#20307;&#23433;&#20840;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Proteus&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26102;&#38480;&#19978;&#19979;&#25991;&#29983;&#29289;&#36523;&#20221;&#27010;&#24565;&#65292;&#36825;&#20123;&#29983;&#29289;&#36523;&#20221;&#26159;&#20174;&#35774;&#22791;&#20256;&#24863;&#22120;&#25968;&#25454;&#29983;&#25104;&#24182;&#23884;&#20837;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#36825;&#20123;&#29983;&#29289;&#36523;&#20221;&#20805;&#24403;&#30528;&#26102;&#38480;&#30340;&#29420;&#29305;&#29992;&#25143;&#26631;&#35782;&#31526;&#65292;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#29992;&#20110;&#35782;&#21035;&#20329;&#25140;&#32773;&#12290;Proteus&#23454;&#29616;&#20102;&#21160;&#24577;&#30340;&#19978;&#19979;&#25991;&#35774;&#22791;&#21327;&#20316;&#20197;&#21450;&#24378;&#22823;&#30340;&#20154;&#23545;&#35774;&#22791;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#22312;&#26497;&#31616;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#32972;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00889v1 Announce Type: cross  Abstract: As wearable devices become increasingly miniaturized and powerful, a new opportunity arises for instant and dynamic device-to-device collaboration and human-to-device interaction. However, this progress presents a unique challenge: these minimalist wearables lack inherent mechanisms for real-time authentication, posing significant risks to data privacy and overall security. To address this, we introduce Proteus that realizes an innovative concept of time-bound contextual bio-IDs, which are generated from on-device sensor data and embedded into a common latent space. These bio-IDs act as a time-bound unique user identifier that can be used to identify the wearer in a certain context. Proteus enables dynamic and contextual device collaboration as well as robust human-to-device interaction. Our evaluations demonstrate the effectiveness of our method, particularly in the context of minimalist wearables.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#24314;&#31435;&#65292;&#35299;&#20915;&#20102;&#22312;MDTC&#31639;&#27861;&#35774;&#35745;&#20013;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00888</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#24314;&#31435;&#65292;&#35299;&#20915;&#20102;&#22312;MDTC&#31639;&#27861;&#35774;&#35745;&#20013;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;(MDTC)&#33268;&#21147;&#20110;&#21033;&#29992;&#30456;&#20851;&#39046;&#22495;&#30340;&#21487;&#29992;&#36164;&#28304;&#65292;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#20849;&#20139;-&#31169;&#26377;&#33539;&#24335;&#30340;MDTC&#26041;&#27861;&#34920;&#29616;&#20986;&#23574;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#19968;&#20010;&#19981;&#21487;&#24573;&#35270;&#30340;&#25361;&#25112;&#65306;&#22312;MDTC&#31639;&#27861;&#35774;&#35745;&#20013;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#29702;&#35770;&#22522;&#30784;&#30340;&#32570;&#20047;&#32473;MDTC&#31639;&#27861;&#30340;&#21457;&#23637;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;MDTC&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#39046;&#22495;&#33258;&#36866;&#24212;&#20219;&#21153;&#26469;&#25552;&#20379;MDTC&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#36793;&#38469;&#24046;&#24322;&#20316;&#20026;&#22495;&#24046;&#24322;&#30340;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;Rademacher&#22797;&#26434;&#24615;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;MDAT&#65289;&#26041;&#27861;&#29992;&#20110;MDTC&#65292;&#31526;&#21512;&#25105;&#20204;&#30340;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00888v1 Announce Type: new  Abstract: Multi-domain text classification (MDTC) endeavors to harness available resources from correlated domains to enhance the classification accuracy of the target domain. Presently, most MDTC approaches that embrace adversarial training and the shared-private paradigm exhibit cutting-edge performance. Unfortunately, these methods face a non-negligible challenge: the absence of theoretical guarantees in the design of MDTC algorithms. The dearth of theoretical underpinning poses a substantial impediment to the advancement of MDTC algorithms. To tackle this problem, we first provide a theoretical analysis of MDTC by decomposing the MDTC task into multiple domain adaptation tasks. We incorporate the margin discrepancy as the measure of domain divergence and establish a new generalization bound based on Rademacher complexity. Subsequently, we propose a margin discrepancy-based adversarial training (MDAT) approach for MDTC, in accordance with our t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20174;&#35821;&#38899;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25506;&#32034;&#20102;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.00887</link><description>&lt;p&gt;
SEGAA: &#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20174;&#35821;&#38899;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25506;&#32034;&#20102;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22768;&#38899;&#30340;&#35299;&#37322;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#24456;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#35821;&#38899;&#32447;&#32034;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#39046;&#22495;&#12290;&#22768;&#38899;&#20998;&#26512;&#25216;&#26415;&#30340;&#36827;&#23637;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#25913;&#21892;&#23458;&#25143;&#20114;&#21160;&#21040;&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#21644;&#38646;&#21806;&#20307;&#39564;&#12290;&#36776;&#35782;&#24773;&#32490;&#26377;&#21161;&#20110;&#24515;&#29702;&#20581;&#24247;&#65292;&#32780;&#24180;&#40836;&#21644;&#24615;&#21035;&#30340;&#26816;&#27979;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25506;&#32034;&#36825;&#20123;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28041;&#21450;&#27604;&#36739;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26412;&#25991;&#20013;&#24471;&#21040;&#20102;&#37325;&#28857;&#23637;&#31034;&#12290;&#23547;&#25214;&#21512;&#36866;&#30340;&#25968;&#25454;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;CREMA-D&#21644;EMO-DB&#25968;&#25454;&#38598;&#30340;&#34701;&#21512;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#20010;&#21035;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#20102;&#36825;&#19977;&#20010;&#21464;&#37327;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#20010;&#21035;&#27169;&#22411;&#26041;&#27861;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#20513;&#23548;&#25105;&#20204;&#30340;&#26032;&#39062;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;Speech-based Emotion Gender&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00887v1 Announce Type: cross  Abstract: The interpretation of human voices holds importance across various applications. This study ventures into predicting age, gender, and emotion from vocal cues, a field with vast applications. Voice analysis tech advancements span domains, from improving customer interactions to enhancing healthcare and retail experiences. Discerning emotions aids mental health, while age and gender detection are vital in various contexts. Exploring deep learning models for these predictions involves comparing single, multi-output, and sequential models highlighted in this paper. Sourcing suitable data posed challenges, resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work showed promise in individual predictions, but limited research considered all three variables simultaneously. This paper identifies flaws in an individual model approach and advocates for our novel multi-output learning architecture Speech-based Emotion Gender an
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#37096;&#32626;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26102;&#65292;&#26412;&#30740;&#31350;&#23558;&#20854;&#35270;&#20026;&#22240;&#26524;&#22495;&#36716;&#31227;&#65292;&#24182;&#25552;&#20986;&#26032;&#39062;&#30340;&#36328;&#22495;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00886</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#22495;&#36716;&#31227;&#35780;&#20272;&#21644;&#32416;&#27491;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#34920;&#29616;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Correcting Performative Effects of Decision Support Systems via Causal Domain Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00886
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#37096;&#32626;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26102;&#65292;&#26412;&#30740;&#31350;&#23558;&#20854;&#35270;&#20026;&#22240;&#26524;&#22495;&#36716;&#31227;&#65292;&#24182;&#25552;&#20986;&#26032;&#39062;&#30340;&#36328;&#22495;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20174;&#29305;&#24449;X&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;Y&#26102;&#65292;&#39044;&#27979;Y&#21487;&#33021;&#21464;&#24471;&#34920;&#29616;&#24615;&#24378;&#65306;&#19968;&#20010;&#20195;&#29702;&#21487;&#33021;&#26681;&#25454;&#36825;&#20010;&#39044;&#27979;&#37319;&#21462;&#34892;&#21160;&#65292;&#24433;&#21709;&#25105;&#20204;&#26368;&#32456;&#35266;&#23519;&#21040;&#30340;Y&#30340;&#20540;&#12290; &#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#65288;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#27861;&#24459;&#12289;&#39044;&#27979;&#24615;&#25191;&#27861;&#25110;&#20799;&#31461;&#31119;&#21033;&#31579;&#36873;&#65289;&#37096;&#32626;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#26102;&#65292;&#24517;&#39035;&#20180;&#32454;&#35780;&#20272;DSS&#30340;&#34920;&#29616;&#25928;&#26524;&#12290; &#25105;&#20204;&#25552;&#20986;&#23558;DSS&#30340;&#37096;&#32626;&#24314;&#27169;&#20026;&#22240;&#26524;&#22495;&#36716;&#31227;&#65292;&#24182;&#25552;&#20379;&#26032;&#39062;&#30340;&#36328;&#22495;&#35782;&#21035;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00886v1 Announce Type: new  Abstract: When predicting a target variable $Y$ from features $X$, the prediction $\hat{Y}$ can be performative: an agent might act on this prediction, affecting the value of $Y$ that we eventually observe. Performative predictions are deliberately prevalent in algorithmic decision support, where a Decision Support System (DSS) provides a prediction for an agent to affect the value of the target variable. When deploying a DSS in high-stakes settings (e.g. healthcare, law, predictive policing, or child welfare screening) it is imperative to carefully assess the performative effects of the DSS. In the case that the DSS serves as an alarm for a predicted negative outcome, naive retraining of the prediction model is bound to result in a model that underestimates the risk, due to effective workings of the previous model. In this work, we propose to model the deployment of a DSS as causal domain shift and provide novel cross-domain identification result
&lt;/p&gt;</description></item><item><title>FedRDMA&#26159;&#19968;&#20010;&#22522;&#20110;&#20998;&#22359;RDMA&#20256;&#36755;&#30340;&#39640;&#25928;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#25216;&#26415;&#25552;&#39640;&#20102;&#36890;&#20449;&#25928;&#29575;&#65292;&#30456;&#27604;&#20256;&#32479;&#31995;&#32479;&#21487;&#23454;&#29616;&#26368;&#22810;3.8&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00881</link><description>&lt;p&gt;
FedRDMA&#65306;&#22522;&#20110;&#20998;&#22359;RDMA&#20256;&#36755;&#30340;&#23569;&#20132;&#27969;&#36328;&#36793;&#32536;&#32852;&#37030;LLM&#36890;&#20449;&#39640;&#25928;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00881
&lt;/p&gt;
&lt;p&gt;
FedRDMA&#26159;&#19968;&#20010;&#22522;&#20110;&#20998;&#22359;RDMA&#20256;&#36755;&#30340;&#39640;&#25928;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#25216;&#26415;&#25552;&#39640;&#20102;&#36890;&#20449;&#25928;&#29575;&#65292;&#30456;&#27604;&#20256;&#32479;&#31995;&#32479;&#21487;&#23454;&#29616;&#26368;&#22810;3.8&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#24320;&#38144;&#26159;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#65292;&#38543;&#30528;AI&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36825;&#19968;&#29942;&#39048;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedRDMA&#65292;&#19968;&#20010;&#36890;&#20449;&#39640;&#25928;&#30340;&#36328;&#36793;&#32536;FL&#31995;&#32479;&#65292;&#23558;RDMA&#38598;&#25104;&#21040;FL&#36890;&#20449;&#21327;&#35758;&#20013;&#12290;&#20026;&#20102;&#20811;&#26381;RDMA&#22312;&#24191;&#22495;&#32593;&#65288;WANs&#65289;&#20013;&#30340;&#38480;&#21046;&#65292;FedRDMA&#23558;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#20998;&#25104;&#22359;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#20248;&#21270;&#25216;&#26415;&#26469;&#25913;&#21892;&#22522;&#20110;RDMA&#30340;&#36890;&#20449;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#24037;&#19994;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19978;&#23454;&#29616;&#20102;FedRDMA&#65292;&#24182;&#22312;&#23454;&#38469;&#36328;&#36793;&#32536;FL&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#22522;&#20110;TCP/IP&#30340;FL&#31995;&#32479;&#30456;&#27604;&#65292;\sys&#30340;&#36890;&#20449;&#25928;&#29575;&#26368;&#22810;&#21487;&#20197;&#25552;&#39640;3.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00881v1 Announce Type: new  Abstract: Communication overhead is a significant bottleneck in federated learning (FL), which has been exaggerated with the increasing size of AI models. In this paper, we propose FedRDMA, a communication-efficient cross-silo FL system that integrates RDMA into the FL communication protocol. To overcome the limitations of RDMA in wide-area networks (WANs), FedRDMA divides the updated model into chunks and designs a series of optimization techniques to improve the efficiency and robustness of RDMA-based communication. We implement FedRDMA atop the industrial federated learning framework and evaluate it on a real-world cross-silo FL scenario. The experimental results show that \sys can achieve up to 3.8$\times$ speedup in communication efficiency compared to traditional TCP/IP-based FL systems.
&lt;/p&gt;</description></item><item><title>Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00877</link><description>&lt;p&gt;
Disaggregated Multi-Tower: &#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#25512;&#33616;&#24314;&#27169;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00877
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#30340;&#25153;&#24179;&#26550;&#26500;&#12289;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#24335;&#21644;&#20998;&#23618;&#25968;&#25454;&#20013;&#24515;&#25299;&#25169;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#30456;&#20851;&#30340;&#20302;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disaggregated Multi-Tower&#65288;DMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24314;&#27169;&#25216;&#26415;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35821;&#20041;&#20445;&#30041;&#30340;Tower Transform&#65288;SPTT&#65289;&#65292;&#19968;&#20010;&#23558;&#21333;&#29255;&#20840;&#23616;&#23884;&#20837;&#26597;&#25214;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#30456;&#20132;&#22612;&#20197;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#20301;&#32622;&#20851;&#31995;&#30340;&#26032;&#22411;&#35757;&#32451;&#27169;&#24335;&#65307;&#65288;2&#65289;Tower Module&#65288;TM&#65289;&#65292;&#19968;&#20010;&#38468;&#21152;&#21040;&#27599;&#20010;&#22612;&#30340;&#21327;&#21516;&#31264;&#23494;&#32452;&#20214;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#20132;&#20114;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#36890;&#20449;&#37327;&#65307;&#21644;&#65288;3&#65289;Tower Partitioner&#65288;TP&#65289;&#65292;&#19968;&#20010;&#29305;&#24449;&#20998;&#21306;&#22120;&#65292;&#31995;&#32479;&#22320;&#21019;&#24314;&#20855;&#26377;&#26377;&#24847;&#20041;&#29305;&#24449;&#20132;&#20114;&#21644;&#36127;&#36733;&#24179;&#34913;&#20998;&#37197;&#30340;&#22612;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23884;&#20837;&#26469;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DMT&#30456;&#27604;&#20110;&#26368;&#26032;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;1.9&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#29255;&#21644;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25193;&#23637;&#21040;&#34507;&#30333;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34507;&#30333;&#35821;&#20041;&#32423;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#38598;&#25104;&#21040;&#19968;&#20010;&#22686;&#24378;&#27744;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#21160;&#34507;&#30333;&#22686;&#24378;&#65288;APA&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00875</link><description>&lt;p&gt;
&#36890;&#36807;&#34507;&#30333;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#34507;&#30333;&#39044;&#27979;&#27169;&#22411;&#65306;&#22522;&#20934;&#21644;&#26032;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#29255;&#21644;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25193;&#23637;&#21040;&#34507;&#30333;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34507;&#30333;&#35821;&#20041;&#32423;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#38598;&#25104;&#21040;&#19968;&#20010;&#22686;&#24378;&#27744;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#21160;&#34507;&#30333;&#22686;&#24378;&#65288;APA&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#34507;&#30333;&#25968;&#25454;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20391;&#37325;&#20110;&#35774;&#35745;&#26032;&#30340;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23545;&#20110;&#34507;&#30333;&#30340;&#25968;&#25454;&#22686;&#24378;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#23558;&#20808;&#21069;&#29992;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25193;&#23637;&#21040;&#34507;&#30333;&#65292;&#28982;&#21518;&#22312;&#21508;&#31181;&#19982;&#34507;&#30333;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#23545;&#34507;&#30333;&#22686;&#24378;&#30340;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35821;&#20041;&#32423;&#34507;&#30333;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#38598;&#25104;&#26799;&#24230;&#26367;&#25442;&#21644;&#22238;&#35793;&#26367;&#25442;&#65292;&#36890;&#36807;&#26174;&#33879;&#24615;&#26816;&#27979;&#21644;&#29983;&#29289;&#30693;&#35782;&#23454;&#29616;&#34507;&#30333;&#35821;&#20041;&#24863;&#30693;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25193;&#23637;&#21644;&#25552;&#20986;&#30340;&#22686;&#24378;&#38598;&#25104;&#21040;&#19968;&#20010;&#22686;&#24378;&#27744;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#33258;&#21160;&#34507;&#30333;&#22686;&#24378;&#65288;APA&#65289;&#65292;&#21487;&#23545;&#34507;&#30333;&#36827;&#34892;&#33258;&#21160;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00875v1 Announce Type: cross  Abstract: Augmentation is an effective alternative to utilize the small amount of labeled protein data. However, most of the existing work focuses on design-ing new architectures or pre-training tasks, and relatively little work has studied data augmentation for proteins. This paper extends data augmentation techniques previously used for images and texts to proteins and then benchmarks these techniques on a variety of protein-related tasks, providing the first comprehensive evaluation of protein augmentation. Furthermore, we propose two novel semantic-level protein augmentation methods, namely Integrated Gradients Substitution and Back Translation Substitution, which enable protein semantic-aware augmentation through saliency detection and biological knowledge. Finally, we integrate extended and proposed augmentations into an augmentation pool and propose a simple but effective framework, namely Automated Protein Augmentation (APA), which can a
&lt;/p&gt;</description></item><item><title>&#21306;&#22359;&#38142;&#25216;&#26415;&#34987;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00873</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#36171;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00873
&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#34987;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#32858;&#21512;&#26469;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#23613;&#31649;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;FL&#31995;&#32479;&#38754;&#20020;&#21333;&#28857;&#25925;&#38556;&#12289;&#32570;&#20047;&#28608;&#21169;&#21644;&#19981;&#36275;&#30340;&#23433;&#20840;&#24615;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#23558;&#21306;&#22359;&#38142;&#25216;&#26415;&#25972;&#21512;&#21040;FL&#31995;&#32479;&#20013;&#65292;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#21306;&#22359;&#38142;&#36171;&#33021;&#30340;FL(BC-FL)&#31995;&#32479;&#23545;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#38656;&#27714;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;&#26368;&#36817;&#20851;&#20110;BC-FL&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#19982;&#21306;&#22359;&#38142;&#25972;&#21512;&#30456;&#20851;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21306;&#22359;&#38142;&#20026;&#20309;&#36866;&#29992;&#20110;FL&#65292;&#22914;&#20309;&#23454;&#26045;&#20197;&#21450;&#25972;&#21512;&#30340;&#25361;&#25112;&#21644;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00873v1 Announce Type: cross  Abstract: Federated learning (FL) is a distributed machine learning approach that protects user data privacy by training models locally on clients and aggregating them on a parameter server. While effective at preserving privacy, FL systems face limitations such as single points of failure, lack of incentives, and inadequate security. To address these challenges, blockchain technology is integrated into FL systems to provide stronger security, fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems introduce additional demands on network, computing, and storage resources. This survey provides a comprehensive review of recent research on BC-FL systems, analyzing the benefits and challenges associated with blockchain integration. We explore why blockchain is applicable to FL, how it can be implemented, and the challenges and existing solutions for its integration. Additionally, we offer insights on future research directions fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#30340;&#26032;&#22411;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#20351;&#23545;&#25163;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#20449;&#24687;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#33267;50%&#12290;</title><link>https://arxiv.org/abs/2403.00871</link><description>&lt;p&gt;
&#25945;&#20250;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38035;&#40060;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#31363;&#21462;&#31169;&#20154;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Teach LLMs to Phish: Stealing Private Information from Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#30340;&#26032;&#22411;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#20351;&#23545;&#25163;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#20449;&#24687;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#33267;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31169;&#20154;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23558;&#25935;&#24863;&#20449;&#24687;&#35760;&#24518;&#24182;&#37325;&#22797;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#31216;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#12290;&#36825;&#31181;&#25915;&#20987;&#20351;&#23545;&#25163;&#33021;&#22815;&#20174;&#19968;&#20010;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#29978;&#33267;50%&#22320;&#25552;&#21462;&#25935;&#24863;&#25110;&#21487;&#35782;&#21035;&#20010;&#20154;&#36523;&#20221;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#20449;&#29992;&#21345;&#21495;&#12290;&#25915;&#20987;&#20165;&#20551;&#35774;&#23545;&#25163;&#21487;&#20197;&#23558;&#23569;&#37327;&#30475;&#20284;&#33391;&#24615;&#30340;&#21477;&#23376;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20165;&#20351;&#29992;&#23545;&#29992;&#25143;&#25968;&#25454;&#32467;&#26500;&#30340;&#27169;&#31946;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00871v1 Announce Type: cross  Abstract: When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call "neural phishing". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;CDAM&#21644;TAM&#27169;&#22411;&#20197;&#25913;&#36827;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20887;&#20313;&#20449;&#24687;&#24182;&#22686;&#24378;&#20114;&#20449;&#24687;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00869</link><description>&lt;p&gt;
&#21033;&#29992;&#20114;&#20449;&#24687;&#39537;&#21160;&#30340;&#36328;&#21464;&#37327;&#21644;&#26102;&#38388;&#24314;&#27169;&#26469;&#22686;&#24378;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00869
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;CDAM&#21644;TAM&#27169;&#22411;&#20197;&#25913;&#36827;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20887;&#20313;&#20449;&#24687;&#24182;&#22686;&#24378;&#20114;&#20449;&#24687;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#24378;&#35843;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;MTSF&#65289;&#30340;&#24433;&#21709;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#25216;&#26415;&#34987;&#20998;&#20026;&#20004;&#31867;&#65306;&#36890;&#36947;&#29420;&#31435;&#21644;&#36890;&#36947;&#28151;&#21512;&#26041;&#27861;&#12290;&#34429;&#28982;&#36890;&#36947;&#29420;&#31435;&#26041;&#27861;&#36890;&#24120;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36890;&#36947;&#28151;&#21512;&#29702;&#35770;&#19978;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21464;&#37327;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#20379;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#36890;&#36947;&#28151;&#21512;&#26041;&#27861;&#20013;&#25972;&#21512;&#19981;&#30456;&#20851;&#20449;&#24687;&#21487;&#33021;&#20250;&#21066;&#24369;MTSF&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#22312;&#22686;&#24378;&#12290;&#20026;&#20102;&#35777;&#23454;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#36890;&#36947;&#28151;&#21512;&#26041;&#27861;&#30340;&#36328;&#21464;&#37327;&#21435;&#30456;&#20851;&#24863;&#30693;&#29305;&#24449;&#24314;&#27169;&#65288;CDAM&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#36890;&#36947;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#21516;&#26102;&#22686;&#24378;&#30456;&#20851;&#30340;&#20114;&#20449;&#24687;&#26469;&#25913;&#36827;&#36890;&#36947;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#24207;&#30456;&#20851;&#24863;&#30693;&#24314;&#27169;&#65288;TAM&#65289;&#26469;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00869v1 Announce Type: new  Abstract: Recent advancements have underscored the impact of deep learning techniques on multivariate time series forecasting (MTSF). Generally, these techniques are bifurcated into two categories: Channel-independence and Channel-mixing approaches. Although Channel-independence methods typically yield better results, Channel-mixing could theoretically offer improvements by leveraging inter-variable correlations. Nonetheless, we argue that the integration of uncorrelated information in channel-mixing methods could curtail the potential enhancement in MTSF model performance. To substantiate this claim, we introduce the Cross-variable Decorrelation Aware feature Modeling (CDAM) for Channel-mixing approaches, aiming to refine Channel-mixing by minimizing redundant information between channels while enhancing relevant mutual information. Furthermore, we introduce the Temporal correlation Aware Modeling (TAM) to exploit temporal correlations, a step be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00867</link><description>&lt;p&gt;
&#26799;&#24230;&#34987;&#32602;&#65306;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#65292;LLM&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#20260;&#23475;&#21644;&#28389;&#29992;&#65292;&#20154;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35757;&#32451;&#25216;&#26415;&#22914;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23558;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;LLMs&#23545;&#20110;&#35797;&#22270;&#39072;&#35206;&#23884;&#20837;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#30340;&#23545;&#25239;&#24615;&#36234;&#29425;&#23581;&#35797;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23450;&#20041;&#24182;&#35843;&#26597;&#20102;LLMs&#30340;&#25298;&#32477;&#25439;&#22833;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36234;&#29425;&#23581;&#35797;&#12290;Gradient Cuff&#21033;&#29992;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21253;&#25324;&#21151;&#33021;&#20540;&#21450;&#20854;&#20809;&#28369;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00865</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#30340;&#24555;&#36895;&#39640;&#25928;&#23616;&#37096;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00865
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#30340;&#35805;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20803;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#23398;&#20064;&#33021;&#26174;&#33879;&#25913;&#21892;&#32463;&#36807;&#20854;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;&#36951;&#20256;&#32534;&#31243;&#25214;&#21040;&#19968;&#32452;&#31526;&#21495;&#25439;&#22833;&#20989;&#25968;&#12290;&#20854;&#27425;&#65292;&#23398;&#20064;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#38543;&#21518;&#36890;&#36807;&#23637;&#24320;&#30340;&#24494;&#20998;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#32463;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#34920;&#26684;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#19978;&#24102;&#26469;&#20102;&#25913;&#21892;&#30340;&#25910;&#25947;&#24615;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#24615;&#33021;&#65292;&#20351;&#29992;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00865v1 Announce Type: cross  Abstract: In this paper, we develop upon the topic of loss function learning, an emergent meta-learning paradigm that aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for task and model-agnostic loss function learning via a hybrid search approach. The framework first uses genetic programming to find a set of symbolic loss functions. Second, the set of learned loss functions is subsequently parameterized and optimized via unrolled differentiation. The versatility and performance of the proposed framework are empirically validated on a diverse set of supervised learning tasks. Results show that the learned loss functions bring improved convergence, sample efficiency, and inference performance on tabulated, computer vision, and natural language processing problems, using a variety of task-specific neural network architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#37325;&#26500;&#29616;&#20195;&#38646;&#21806;&#20379;&#24212;&#38142;&#65292;&#36890;&#36807;&#25552;&#20379;DGMs&#30340;&#20998;&#31867;&#27861;&#12289;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#24212;&#29992;&#26696;&#20363;&#22238;&#39038;&#20197;&#21450;&#28508;&#22312;&#21033;&#29992;DGMs&#35299;&#20915;&#38646;&#21806;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.00861</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#29983;&#25104;&#25216;&#26415;&#37325;&#26500;&#38646;&#21806;&#20379;&#24212;&#38142;&#65306;&#20998;&#31867;&#27861;&#12289;&#35843;&#30740;&#21644;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy, Survey and Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#37325;&#26500;&#29616;&#20195;&#38646;&#21806;&#20379;&#24212;&#38142;&#65292;&#36890;&#36807;&#25552;&#20379;DGMs&#30340;&#20998;&#31867;&#27861;&#12289;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#24212;&#29992;&#26696;&#20363;&#22238;&#39038;&#20197;&#21450;&#28508;&#22312;&#21033;&#29992;DGMs&#35299;&#20915;&#38646;&#21806;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;ChatGPT&#25110;DALL-E&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#25110;&#22270;&#20687;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#20123;AI&#24212;&#29992;&#30340;&#31185;&#23398;&#21033;&#30410;&#30456;&#20851;&#32773;&#26159;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;DGMs&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#24182;&#29983;&#25104;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#22312;&#32479;&#35745;&#19978;&#30456;&#20284;&#30340;&#26032;&#25968;&#25454;&#28857;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;DGMs&#24212;&#29992;&#20110;&#29616;&#20195;&#38646;&#21806;&#20379;&#24212;&#38142;&#39046;&#22495;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#23457;&#26597;DGMs&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#29616;&#26377;&#21644;&#28508;&#22312;&#29992;&#20363;&#65292;&#26041;&#27861;&#26159;(1)&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;DGMs&#21450;&#20854;&#21464;&#20307;&#30340;&#20998;&#31867;&#27861;&#21644;&#27010;&#36848;&#65292;(2)&#20174;&#31471;&#21040;&#31471;&#30340;&#35270;&#35282;&#22238;&#39038;&#29616;&#26377;DGM&#22312;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;(3)&#35752;&#35770;&#20851;&#20110;&#22914;&#20309;&#36827;&#19968;&#27493;&#21033;&#29992;DGM&#35299;&#20915;&#38646;&#21806;&#38382;&#39064;&#30340;&#35265;&#35299;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00861v1 Announce Type: new  Abstract: Generative AI applications, such as ChatGPT or DALL-E, have shown the world their impressive capabilities in generating human-like text or image. Diving deeper, the science stakeholder for those AI applications are Deep Generative Models, a.k.a DGMs, which are designed to learn the underlying distribution of the data and generate new data points that are statistically similar to the original dataset. One critical question is raised: how can we leverage DGMs into morden retail supply chain realm? To address this question, this paper expects to provide a comprehensive review of DGMs and discuss their existing and potential usecases in retail supply chain, by (1) providing a taxonomy and overview of state-of-the-art DGMs and their variants, (2) reviewing existing DGM applications in retail supply chain from a end-to-end view of point, and (3) discussing insights and potential directions on how DGMs can be further utilized on solving retail 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28145;&#24230;&#65288;&#21644;&#27973;&#23618;&#65289;&#31070;&#32463;&#32593;&#32476;&#20013;&#31934;&#30830;&#26522;&#20030;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#21644;&#24182;&#34892;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#22312;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#21306;&#22495;&#25968;&#37327;&#23545;&#36816;&#34892;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.00860</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#21306;&#22495;&#30340;&#31934;&#30830;&#26522;&#20030;&#24182;&#34892;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parallel Algorithms for Exact Enumeration of Deep Neural Network Activation Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28145;&#24230;&#65288;&#21644;&#27973;&#23618;&#65289;&#31070;&#32463;&#32593;&#32476;&#20013;&#31934;&#30830;&#26522;&#20030;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#21644;&#24182;&#34892;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#22312;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#21306;&#22495;&#25968;&#37327;&#23545;&#36816;&#34892;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#20854;&#36755;&#20837;&#31354;&#38388;&#21010;&#20998;&#20026;&#19968;&#32452;&#20984;&#21306;&#22495;&#26469;&#26500;&#24314;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#26144;&#23556;&#65292;&#21306;&#22495;&#20869;&#30340;&#28857;&#20849;&#20139;&#21333;&#19968;&#20223;&#23556;&#21464;&#25442;&#12290;&#20026;&#20102;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#22833;&#36133;&#21407;&#22240;&#20197;&#21450;&#19982;&#29983;&#29289;&#26234;&#33021;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#21306;&#22495;&#30340;&#32452;&#32455;&#21644;&#24418;&#25104;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31934;&#30830;&#26522;&#20030;&#28145;&#24230;&#65288;&#21644;&#27973;&#23618;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00860v1 Announce Type: cross  Abstract: A feedforward neural network using rectified linear units constructs a mapping from inputs to outputs by partitioning its input space into a set of convex regions where points within a region share a single affine transformation. In order to understand how neural networks work, when and why they fail, and how they compare to biological intelligence, we need to understand the organization and formation of these regions. Step one is to design and implement algorithms for exact region enumeration in networks beyond toy examples.   In this work, we present parallel algorithms for exact enumeration in deep (and shallow) neural networks. Our work has three main contributions: (1) we present a novel algorithm framework and parallel algorithms for region enumeration; (2) we implement one of our algorithms on a variety of network architectures and experimentally show how the number of regions dictates runtime; and (3) we show, using our algorit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.00854</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Speaker-Independent Dysarthria Severity Classification using Self-Supervised Transformers and Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#38556;&#30861;&#26159;&#30001;&#20110;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#23548;&#33268;&#35328;&#35821;&#32908;&#32905;&#25511;&#21046;&#33021;&#21147;&#21463;&#25439;&#32780;&#20135;&#29983;&#30340;&#19968;&#31181;&#29366;&#20917;&#65292;&#20005;&#37325;&#24433;&#21709;&#24739;&#32773;&#30340;&#27807;&#36890;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#35821;&#38899;&#25968;&#25454;&#20013;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#23458;&#35266;&#12289;&#21487;&#37325;&#22797;&#12289;&#21487;&#35775;&#38382;&#12289;&#26631;&#20934;&#21270;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00854v1 Announce Type: cross  Abstract: Dysarthria, a condition resulting from impaired control of the speech muscles due to neurological disorders, significantly impacts the communication and quality of life of patients. The condition's complexity, human scoring and varied presentations make its assessment and management challenging. This study presents a transformer-based framework for automatically assessing dysarthria severity from raw speech data. It can offer an objective, repeatable, accessible, standardised and cost-effective and compared to traditional methods requiring human expert assessors. We develop a transformer framework, called Speaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task learning objective and contrastive learning for speaker-independent multi-class dysarthria severity classification. The multi-task framework is designed to reduce reliance on speaker-specific characteristics and address the intrinsic intra-class variability of d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#24314;&#31435;&#20102;&#20851;&#20110;&#19968;&#33324;&#38750;&#20984;&#21644;$\mu$-PL&#38750;&#20984;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#38480;&#65292;&#35206;&#30422;&#20102;&#19968;&#33324;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#26799;&#24230;&#20272;&#35745;&#26377;&#20559;&#26102;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#21644;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.00853</link><description>&lt;p&gt;
&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Momentum Methods Under Biased Gradient Estimations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#24314;&#31435;&#20102;&#20851;&#20110;&#19968;&#33324;&#38750;&#20984;&#21644;$\mu$-PL&#38750;&#20984;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#38480;&#65292;&#35206;&#30422;&#20102;&#19968;&#33324;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#26799;&#24230;&#20272;&#35745;&#26377;&#20559;&#26102;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#21644;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#22312;&#35299;&#20915;&#28041;&#21450;&#20998;&#24067;&#22312;&#22810;&#20010;&#33410;&#28857;&#19978;&#30340;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#26085;&#30410;&#21463;&#21040;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26080;&#20559;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#29702;&#35770;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26799;&#24230;&#20272;&#35745;&#24456;&#23481;&#26131;&#21464;&#24471;&#26377;&#20559;&#65292;&#20363;&#22914;&#65292;&#22312;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#65292;&#25968;&#25454;&#34987;&#27927;&#29260;&#26102;&#65292;&#20197;&#21450;&#22312;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00853v1 Announce Type: new  Abstract: Distributed stochastic gradient methods are gaining prominence in solving large-scale machine learning problems that involve data distributed across multiple nodes. However, obtaining unbiased stochastic gradients, which have been the focus of most theoretical research, is challenging in many distributed machine learning applications. The gradient estimations easily become biased, for example, when gradients are compressed or clipped, when data is shuffled, and in meta-learning and reinforcement learning. In this work, we establish non-asymptotic convergence bounds on distributed momentum methods under biased gradient estimation on both general non-convex and $\mu$-PL non-convex problems. Our analysis covers general distributed optimization problems, and we work out the implications for special cases where gradient estimates are biased, i.e., in meta-learning and when the gradients are compressed or clipped. Our numerical experiments on 
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.00849</link><description>&lt;p&gt;
NeuraLUT: &#22312;Boolean&#21512;&#25104;&#20989;&#25968;&#20013;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00849
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#21152;&#36895;&#22120;&#24050;&#32463;&#35777;&#26126;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#36164;&#28304;&#20851;&#38190;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#31070;&#32463;&#32593;&#32476;&#20013;&#35745;&#31639;&#23494;&#38598;&#24230;&#26368;&#39640;&#30340;&#25805;&#20316;&#20043;&#19968;&#26159;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#20043;&#38388;&#30340;&#28857;&#31215;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;FPGA&#21152;&#36895;&#24037;&#20316;&#25552;&#20986;&#23558;&#20855;&#26377;&#37327;&#21270;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31070;&#32463;&#20803;&#30452;&#25509;&#26144;&#23556;&#21040;&#26597;&#25214;&#34920;&#65288;LUTs&#65289;&#20197;&#36827;&#34892;&#30828;&#20214;&#23454;&#29616;&#12290;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#65292;&#31070;&#32463;&#20803;&#30340;&#36793;&#30028;&#19982;LUTs&#30340;&#36793;&#30028;&#37325;&#21512;&#12290;&#25105;&#20204;&#24314;&#35758;&#25918;&#23485;&#36825;&#20123;&#36793;&#30028;&#65292;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#12290;&#30001;&#20110;&#23376;&#32593;&#32476;&#34987;&#21560;&#25910;&#21040;LUT&#20013;&#65292;&#20998;&#21306;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#20998;&#21306;&#20869;&#20351;&#29992;&#20855;&#26377;&#28014;&#28857;&#31934;&#24230;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#36825;&#20123;&#23618;&#21463;&#30410;&#20110;&#25104;&#20026;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00849v1 Announce Type: cross  Abstract: Field-Programmable Gate Array (FPGA) accelerators have proven successful in handling latency- and resource-critical deep neural network (DNN) inference tasks. Among the most computationally intensive operations in a neural network (NN) is the dot product between the feature and weight vectors. Thus, some previous FPGA acceleration works have proposed mapping neurons with quantized inputs and outputs directly to lookup tables (LUTs) for hardware implementation. In these works, the boundaries of the neurons coincide with the boundaries of the LUTs. We propose relaxing these boundaries and mapping entire sub-networks to a single LUT. As the sub-networks are absorbed within the LUT, the NN topology and precision within a partition do not affect the size of the lookup tables generated. Therefore, we utilize fully connected layers with floating-point precision inside each partition, which benefit from being universal function approximators, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#24191;&#21578;&#20027;&#20004;&#31181;&#19981;&#21516;&#25112;&#30053;&#34892;&#20026;&#27169;&#22411;&#30340;&#22312;&#32447;&#26426;&#21046;&#65292;&#20854;&#20013;&#19968;&#31181;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#32039;&#33268;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21478;&#19968;&#31181;&#21017;&#35299;&#20915;&#20102;&#26356;&#22797;&#26434;&#30340;&#38271;&#26399;&#25928;&#29992;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00845</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved Online Learning Algorithms for CTR Prediction in Ad Auctions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#24191;&#21578;&#20027;&#20004;&#31181;&#19981;&#21516;&#25112;&#30053;&#34892;&#20026;&#27169;&#22411;&#30340;&#22312;&#32447;&#26426;&#21046;&#65292;&#20854;&#20013;&#19968;&#31181;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#32039;&#33268;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21478;&#19968;&#31181;&#21017;&#35299;&#20915;&#20102;&#26356;&#22797;&#26434;&#30340;&#38271;&#26399;&#25928;&#29992;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#21578;&#25293;&#21334;&#20013;&#25910;&#20837;&#26368;&#22823;&#21270;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21334;&#26041;&#38656;&#35201;&#23398;&#20064;&#27599;&#20010;&#24191;&#21578;&#20505;&#36873;&#30340;&#28857;&#20987;&#29575;(CTR)&#65292;&#24182;&#36890;&#36807;&#25353;&#28857;&#20987;&#20184;&#36153;&#30340;&#26041;&#24335;&#25910;&#21462;&#33719;&#32988;&#32773;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#20851;&#27880;&#24191;&#21578;&#20027;&#30340;&#20004;&#31181;&#25112;&#30053;&#34892;&#20026;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20551;&#35774;&#24191;&#21578;&#20027;&#23436;&#20840;&#36817;&#35270;&#65307;&#21363;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20182;&#20204;&#21482;&#38024;&#23545;&#24403;&#21069;&#36718;&#27425;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;&#25928;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22522;&#20110;&#32622;&#20449;&#19978;&#30028;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#26426;&#21046;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;$O(\sqrt{T})$&#36951;&#25022;&#65292;&#24403;&#20540;&#22312;&#25152;&#26377;&#25293;&#21334;&#20013;&#38745;&#24577;&#24182;&#19988;&#26368;&#39640;&#26399;&#26395;&#20540;(&#21363;&#20540;&#20056;&#20197;&#20182;&#20204;&#30340;CTR)&#19982;&#27425;&#39640;&#26399;&#26395;&#20540;&#24191;&#21578;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#26102;&#65292;&#23384;&#22312;&#36127;&#36951;&#25022;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20551;&#35774;&#24191;&#21578;&#20027;&#26159;&#38750;&#36817;&#35270;&#30340;&#65292;&#24182;&#20851;&#24515;&#20182;&#20204;&#30340;&#38271;&#26399;&#25928;&#29992;&#12290;&#36825;&#31181;&#35774;&#32622;&#35201;&#22797;&#26434;&#24471;&#22810;&#65292;&#22240;&#20026;&#24191;&#21578;&#20027;&#26377;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00845v1 Announce Type: cross  Abstract: In this work, we investigate the online learning problem of revenue maximization in ad auctions, where the seller needs to learn the click-through rates (CTRs) of each ad candidate and charge the price of the winner through a pay-per-click manner. We focus on two models of the advertisers' strategic behaviors. First, we assume that the advertiser is completely myopic; i.e.~in each round, they aim to maximize their utility only for the current round. In this setting, we develop an online mechanism based on upper-confidence bounds that achieves a tight $O(\sqrt{T})$ regret in the worst-case and negative regret when the values are static across all the auctions and there is a gap between the highest expected value (i.e.~value multiplied by their CTR) and second highest expected value ad. Next, we assume that the advertiser is non-myopic and cares about their long term utility. This setting is much more complex since an advertiser is incen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25351;&#26631;Lower-Left Partial AUC&#65288;LLPAUC&#65289;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#31867;&#20284;&#20110;AUC&#65292;&#20294;&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#24378;&#30456;&#20851;&#65292;&#33021;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00844</link><description>&lt;p&gt;
&#19979;-&#24038;&#37096;&#20998;AUC&#65306;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#20248;&#21270;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00844
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25351;&#26631;Lower-Left Partial AUC&#65288;LLPAUC&#65289;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#31867;&#20284;&#20110;AUC&#65292;&#20294;&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#24378;&#30456;&#20851;&#65292;&#33021;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#25351;&#26631;&#23545;&#20110;&#26500;&#24314;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#23454;&#29992;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#25351;&#26631;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#23613;&#31649;Top-K&#25490;&#21517;&#25351;&#26631;&#26159;&#20248;&#21270;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#30528;&#26174;&#30528;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26356;&#39640;&#25928;&#30340;&#20934;&#30830;&#24615;&#21644;AUC&#25351;&#26631;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#25512;&#33616;&#20219;&#21153;&#30340;&#30495;&#27491;&#30446;&#26631;&#65292;&#23548;&#33268;&#24615;&#33021;&#20122;&#20248;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;Lower-Left Partial AUC&#65288;LLPAUC&#65289;&#65292;&#23427;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#31867;&#20284;&#20110;AUC&#65292;&#20294;&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#24378;&#30456;&#20851;&#12290;&#19982;AUC&#30456;&#27604;&#65292;LLPAUC&#20165;&#32771;&#34385;ROC&#26354;&#32447;&#19979;&#26041;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#20197;&#23558;&#20248;&#21270;&#28966;&#28857;&#25918;&#22312;Top-K&#19978;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;LLPAUC&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22024;&#26434;&#29992;&#25143;&#21453;&#39304;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00844v1 Announce Type: cross  Abstract: Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;</title><link>https://arxiv.org/abs/2403.00843</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#21487;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00843
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20542;&#21521;&#20110;&#36807;&#20998;&#36814;&#21512;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#32780;&#24573;&#35270;&#20182;&#20204;&#30340;&#38271;&#26399;&#21442;&#19982;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#20915;&#31574;&#36807;&#31243;&#20013;&#21512;&#24182;&#35268;&#21010;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24320;&#21457;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#21363;&#26102;&#20852;&#36259;&#21644;&#38271;&#26399;&#21442;&#19982;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31232;&#30095;&#25968;&#25454;&#30340;&#26174;&#33879;&#35268;&#21010;&#33021;&#21147;&#29992;&#20110;&#38271;&#26399;&#25512;&#33616;&#12290;&#20851;&#38190;&#22312;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#22330;&#26223;&#20013;&#26377;&#25928;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#26410;&#33258;&#28982;&#21253;&#21547;&#36825;&#20123;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.00841</link><description>&lt;p&gt;
&#31454;&#20105;&#28216;&#25103;&#30340;&#31163;&#32447;&#34394;&#26500;&#33258;&#25105;&#23545;&#24328;
&lt;/p&gt;
&lt;p&gt;
Offline Fictitious Self-Play for Competitive Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22240;&#20854;&#22312;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#25913;&#36827;&#31574;&#30053;&#32780;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#23613;&#31649;&#22312;&#21333;&#19968;&#26234;&#33021;&#20307;&#35774;&#32622;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;RL&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#31454;&#20105;&#28216;&#25103;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00841v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ChatGPT&#29983;&#25104;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;AI-Catcher&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.00828</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Detection Method for Large Language Models-Generated Scientific Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00828
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ChatGPT&#29983;&#25104;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;AI-Catcher&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), &#22914;GPT-3&#21644;BERT&#65292;&#25913;&#21464;&#20102;&#25991;&#26412;&#20869;&#23481;&#30340;&#20889;&#20316;&#21644;&#20256;&#25773;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#28508;&#21147;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#26080;&#27861;&#21306;&#20998;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;LLMs&#20250;&#32473;&#31185;&#23398;&#30028;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#31185;&#23398;&#30028;&#20381;&#36182;&#20110;&#20986;&#29256;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ChatGPT&#29983;&#25104;&#30340;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;AI-Catcher&#12290;AI-Catcher&#38598;&#25104;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;MLP&#23398;&#20064;&#35821;&#35328;&#21644;&#32479;&#35745;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;CNN&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#39034;&#24207;&#27169;&#24335;&#30340;&#39640;&#32423;&#34920;&#31034;&#12290;AI-Catcher&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;MLP&#21644;CNN&#23548;&#20986;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;ChatGPT&#29983;&#25104;&#30340;&#31185;&#23398;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00828v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual content is written and communicated. These models have the potential to generate scientific content that is indistinguishable from that written by humans. Hence, LLMs carry severe consequences for the scientific community, which relies on the integrity and reliability of publications. This research paper presents a novel ChatGPT-generated scientific text detection method, AI-Catcher. AI-Catcher integrates two deep learning models, multilayer perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the feature representations of the linguistic and statistical features. The CNN extracts high-level representations of the sequential patterns from the textual content. AI-Catcher is a multimodal model that fuses hidden patterns derived from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset is collected to enhance AI-generated tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Proxy Metric-based Self-Refinement (ProMiSe)&#26041;&#27861;&#65292;&#36890;&#36807;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#33258;&#25105;&#23436;&#21892;&#65292;&#20174;&#32780;&#25913;&#36827;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00827</link><description>&lt;p&gt;
&#26469;&#33258;&#22806;&#37096;&#20195;&#29702;&#25351;&#26631;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#23436;&#21892;
&lt;/p&gt;
&lt;p&gt;
Self-Refinement of Language Models from External Proxy Metrics Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Proxy Metric-based Self-Refinement (ProMiSe)&#26041;&#27861;&#65292;&#36890;&#36807;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#33258;&#25105;&#23436;&#21892;&#65292;&#20174;&#32780;&#25913;&#36827;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#21709;&#24212;&#29983;&#25104;&#20013;&#65292;&#26399;&#26395;&#20195;&#29702;&#21709;&#24212;&#19981;&#20165;&#19982;&#29992;&#25143;&#30340;&#26597;&#35810;&#30456;&#20851;&#65292;&#36824;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#30456;&#20851;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#29702;&#25351;&#26631;&#30340;&#33258;&#25105;&#23436;&#21892;&#65288;ProMiSe&#65289;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27839;&#30528;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#24341;&#23548;&#30340;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#20248;&#21270;&#20854;&#21021;&#22987;&#21709;&#24212;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#26368;&#32456;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00827v1 Announce Type: cross  Abstract: It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning 
&lt;/p&gt;</description></item><item><title>LLMGuard&#26159;&#19968;&#20010;&#30417;&#35270;&#29992;&#25143;&#19982;LLM&#24212;&#29992;&#31243;&#24207;&#20114;&#21160;&#30340;&#24037;&#20855;&#65292;&#21487;&#26631;&#35760;&#36829;&#32972;&#29305;&#23450;&#34892;&#20026;&#25110;&#23545;&#35805;&#20027;&#39064;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.00826</link><description>&lt;p&gt;
LLMGuard&#65306;&#38450;&#33539;&#19981;&#23433;&#20840;&#30340;LLM&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LLMGuard: Guarding Against Unsafe LLM Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00826
&lt;/p&gt;
&lt;p&gt;
LLMGuard&#26159;&#19968;&#20010;&#30417;&#35270;&#29992;&#25143;&#19982;LLM&#24212;&#29992;&#31243;&#24207;&#20114;&#21160;&#30340;&#24037;&#20855;&#65292;&#21487;&#26631;&#35760;&#36829;&#32972;&#29305;&#23450;&#34892;&#20026;&#25110;&#23545;&#35805;&#20027;&#39064;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#30340;&#20852;&#36215;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#33021;&#21147;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19981;&#24403;&#12289;&#20559;&#20506;&#25110;&#35823;&#23548;&#24615;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#35813;&#20869;&#23481;&#36829;&#21453;&#35268;&#23450;&#24182;&#21487;&#33021;&#28041;&#21450;&#27861;&#24459;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;LLMGuard&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#21487;&#30417;&#35270;&#29992;&#25143;&#19982;LLM&#24212;&#29992;&#31243;&#24207;&#30340;&#20114;&#21160;&#65292;&#24182;&#26631;&#35760;&#36829;&#32972;&#29305;&#23450;&#34892;&#20026;&#25110;&#23545;&#35805;&#20027;&#39064;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;LLMGuard&#37319;&#29992;&#20102;&#19968;&#32452;&#25506;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00826v1 Announce Type: new  Abstract: Although the rise of Large Language Models (LLMs) in enterprise settings brings new opportunities and capabilities, it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns. To alleviate this, we present "LLMGuard", a tool that monitors user interactions with an LLM application and flags content against specific behaviours or conversation topics. To do this robustly, LLMGuard employs an ensemble of detectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#25512;&#29305;&#25968;&#25454;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#20083;&#33146;&#30284;&#24739;&#32773;/&#24184;&#23384;&#32773;&#65292;&#24182;&#35774;&#35745;&#20102;&#22810;&#23618;&#35268;&#21017;&#27169;&#22411;&#20197;&#30740;&#31350;&#20083;&#33146;&#30284;&#30103;&#27861;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00821</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20316;&#20026;&#20256;&#24863;&#22120;&#65306;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#25512;&#29305;&#25968;&#25454;&#20197;&#30740;&#31350;&#20083;&#33146;&#30284;&#33647;&#29289;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer Medication Effects Using Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#25512;&#29305;&#25968;&#25454;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#20083;&#33146;&#30284;&#24739;&#32773;/&#24184;&#23384;&#32773;&#65292;&#24182;&#35774;&#35745;&#20102;&#22810;&#23618;&#35268;&#21017;&#27169;&#22411;&#20197;&#30740;&#31350;&#20083;&#33146;&#30284;&#30103;&#27861;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#20063;&#26159;&#22919;&#22899;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23613;&#31649;&#20083;&#33146;&#30284;&#27835;&#30103;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33647;&#29289;&#19981;&#20381;&#20174;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#36890;&#24120;&#19981;&#25429;&#25417;&#21487;&#33021;&#25581;&#31034;&#20851;&#20110;&#33647;&#29289;&#30456;&#20851;&#32463;&#21382;&#30340;&#24739;&#32773;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#31038;&#20132;&#23186;&#20307;&#20026;&#22686;&#36827;&#25105;&#20204;&#23545;&#24739;&#32773;&#27835;&#30103;&#32463;&#21382;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#36164;&#28304;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#33258;&#21160;&#31574;&#21010;&#30340;&#20083;&#33146;&#30284;&#38431;&#21015;&#21457;&#24067;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#35782;&#21035;&#33258;&#25105;&#25253;&#21578;&#20449;&#24687;&#30340;&#20083;&#33146;&#30284;&#24739;&#32773;/&#24184;&#23384;&#32773;&#65292;&#25105;&#20204;&#20174;&#20854;&#20010;&#20154;&#36164;&#26009;&#20013;&#25910;&#38598;&#20102;&#32437;&#21521;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23618;&#35268;&#21017;&#27169;&#22411;&#26469;&#24320;&#21457;&#19982;&#20083;&#33146;&#30284;&#30103;&#27861;&#30456;&#20851;&#30340;sid
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00821v1 Announce Type: new  Abstract: Breast cancer is a significant public health concern and is the leading cause of cancer-related deaths among women. Despite advances in breast cancer treatments, medication non-adherence remains a major problem. As electronic health records do not typically capture patient-reported outcomes that may reveal information about medication-related experiences, social media presents an attractive resource for enhancing our understanding of the patients' treatment experiences. In this paper, we developed natural language processing (NLP) based methodologies to study information posted by an automatically curated breast cancer cohort from social media. We employed a transformer-based classifier to identify breast cancer patients/survivors on X (Twitter) based on their self-reported information, and we collected longitudinal data from their profiles. We then designed a multi-layer rule-based model to develop a breast cancer therapy-associated sid
&lt;/p&gt;</description></item><item><title>DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00818</link><description>&lt;p&gt;
DenseMamba: &#20855;&#26377;&#23494;&#38598;&#38544;&#34255;&#36830;&#25509;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00818
&lt;/p&gt;
&lt;p&gt;
DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30528;&#30001;&#26222;&#36941;&#20351;&#29992;&#30340;Transformer&#26550;&#26500;&#36807;&#39640;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#32780;&#24102;&#26469;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#32780;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#30784;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20294;&#20854;&#24615;&#33021;&#23578;&#26410;&#23436;&#20840;&#33021;&#19982;Transformer&#30456;&#23218;&#32654;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;DenseSSM&#65292;&#19968;&#31181;&#22686;&#24378;SSMs&#20013;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#27969;&#21160;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#23558;&#27973;&#23618;&#38544;&#34255;&#29366;&#24577;&#38598;&#25104;&#21040;&#26356;&#28145;&#23618;&#65292;DenseSSM&#20445;&#30041;&#20102;&#23545;&#26368;&#32456;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#30340;DenseSSM&#20173;&#20445;&#25345;&#20102;&#35757;&#32451;&#30340;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;RetNet&#21644;Mamba&#31561;&#21508;&#31181;SSM&#31867;&#22411;&#12290;&#22312;&#30456;&#20284;&#30340;&#27169;&#22411;&#22823;&#23567;&#19979;&#65292;DenseSSM&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20363;&#22914;DenseRetNet&#27604;&#21407;&#22987;RetNet&#25552;&#39640;&#20102;&#39640;&#36798;5%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26657;&#20934;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.00817</link><description>&lt;p&gt;
&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#29992;&#20110;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26657;&#20934;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#21463;&#21040;&#36873;&#25321;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#29992;&#25143;&#20542;&#21521;&#20110;&#35780;&#20215;&#20182;&#20204;&#21916;&#27426;&#30340;&#29289;&#21697;&#12290;&#22312;&#36825;&#31181;&#26465;&#20214;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#34920;&#29616;&#20986;&#19981;&#38543;&#26426;&#32570;&#22833;&#30340;&#26465;&#30446;&#65292;&#22240;&#27492;&#19981;&#26159;&#20195;&#34920;&#30446;&#26631;&#20154;&#32676;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#21450;&#20854;&#22686;&#24378;&#22411;&#21464;&#20307;&#65292;&#22240;&#20026;&#23427;&#20204;&#30830;&#20445;&#22312;&#25552;&#20379;&#20934;&#30830;&#30340;&#25554;&#34917;&#35823;&#24046;&#25110;&#39044;&#27979;&#27010;&#29575;&#26102;&#26080;&#20559;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#20381;&#36182;&#20110;&#38169;&#35823;&#26657;&#20934;&#30340;&#25554;&#34917;&#35823;&#24046;&#21644;&#27010;&#29575;&#20998;&#25968;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20272;&#35745;&#30340;&#22522;&#26412;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#27934;&#23519;&#65292;&#35828;&#26126;&#38169;&#35823;&#26657;&#20934;&#30340;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#21487;&#33021;&#38480;&#21046;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#39564;&#35777;&#25105;&#20204;&#30340;&#23450;&#29702;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#23545;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#30340;&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00817v1 Announce Type: cross  Abstract: Recommender systems often suffer from selection bias as users tend to rate their preferred items. The datasets collected under such conditions exhibit entries missing not at random and thus are not randomized-controlled trials representing the target population. To address this challenge, a doubly robust estimator and its enhanced variants have been proposed as they ensure unbiasedness when accurate imputed errors or predicted propensities are provided. However, we argue that existing estimators rely on miscalibrated imputed errors and propensity scores as they depend on rudimentary models for estimation. We provide theoretical insights into how miscalibrated imputation and propensity models may limit the effectiveness of doubly robust estimators and validate our theorems using real-world datasets. On this basis, we propose a Doubly Calibrated Estimator that involves the calibration of both the imputation and propensity models. To achi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00804</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#25581;&#31034;&#23458;&#25143;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Uncovering Customer Issues through Topological Natural Language Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#23458;&#25143;&#26381;&#21153;&#35831;&#27714;&#12290;&#23613;&#31649;&#36890;&#24120;&#20351;&#29992;&#31616;&#21333;&#30340;&#27880;&#37322;&#31995;&#32479;&#26469;&#24635;&#32467;&#23458;&#25143;&#32852;&#31995;&#30340;&#20027;&#39064;&#65292;&#20294;&#28145;&#20837;&#25506;&#35752;&#27599;&#20010;&#20855;&#20307;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26469;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26631;&#35760;&#27599;&#20010;&#23458;&#25143;&#23545;&#35805;&#35760;&#24405;&#30340;&#20027;&#35201;&#38382;&#39064;&#21477;&#65292;&#24182;&#29983;&#25104;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#23884;&#20837;&#21521;&#37327;&#36827;&#34892;&#30333;&#21270;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#26080;&#21521;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#27599;&#20010;&#23545;&#35805;&#35760;&#24405;&#30340;&#25299;&#25169;&#29305;&#24615;&#26469;&#23450;&#20041;&#28909;&#38376;&#21644;&#26032;&#20852;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00804v1 Announce Type: cross  Abstract: E-commerce companies deal with a high volume of customer service requests daily. While a simple annotation system is often used to summarize the topics of customer contacts, thoroughly exploring each specific issue can be challenging. This presents a critical concern, especially during an emerging outbreak where companies must quickly identify and address specific issues. To tackle this challenge, we propose a novel machine learning algorithm that leverages natural language techniques and topological data analysis to monitor emerging and trending customer issues. Our approach involves an end-to-end deep learning framework that simultaneously tags the primary question sentence of each customer's transcript and generates sentence embedding vectors. We then whiten the embedding vectors and use them to construct an undirected graph. From there, we define trending and emerging issues based on the topological properties of each transcript. W
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21521;&#19981;&#21516;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.00803</link><description>&lt;p&gt;
LiMAML: &#36890;&#36807;&#20803;&#23398;&#20064;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiMAML: Personalization of Deep Recommender Models via Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21521;&#19981;&#21516;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36941;&#37319;&#29992;&#24050;&#32463;&#25104;&#20026;&#24314;&#27169;&#21508;&#31181;&#19994;&#21153;&#30446;&#26631;&#30340;&#20027;&#23548;&#33539;&#24335;&#12290;&#38543;&#30528;&#29992;&#25143;&#22522;&#25968;&#30340;&#25345;&#32493;&#22686;&#38271;&#65292;&#20010;&#24615;&#21270;&#21644;&#39057;&#32321;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#24517;&#35201;&#24615;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#21521;&#21508;&#31181;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38024;&#23545;&#20010;&#20154;&#25104;&#21592;&#21644;&#20854;&#20182;&#23454;&#20307;&#30340;&#27169;&#22411;&#20010;&#24615;&#21270;&#65292;&#32467;&#21512;&#20102;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#36827;&#34892;&#39057;&#32321;&#26356;&#26032;&#30340;&#21151;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#36817;&#30340;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#26469;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#12290;&#32771;&#34385;&#21040;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29983;&#20135;&#21407;&#22987;MAML&#27169;&#22411;&#20960;&#20046;&#19981;&#21487;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23558;&#20803;&#23398;&#20064;&#30340;&#23376;&#32593;&#32476;&#25512;&#24191;&#24212;&#29992;&#21040;&#29983;&#20135;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00803v1 Announce Type: cross  Abstract: In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in prod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;Brain&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#26126;&#30830;&#25552;&#21462;&#20986;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.00800</link><description>&lt;p&gt;
&#20511;&#37492;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#30340;&#33041;&#21551;&#21457;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00800
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;Brain&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#26126;&#30830;&#25552;&#21462;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25913;&#36827;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#22312;&#24320;&#28304;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#30417;&#30563;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21069;&#39069;&#21494;&#27169;&#22411;&#29983;&#25104;&#35745;&#21010;&#65292;&#28982;&#21518;&#20351;&#29992;&#39030;&#21494;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#24182;&#25191;&#34892;&#20197;&#33719;&#24471;&#31572;&#26696;&#65292;&#26469;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#27492;&#26041;&#27861;&#19982;&#22522;&#20110;Code LLaMA 7B&#30340;&#27169;&#22411;&#30456;&#27604;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#26126;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://github.com/cyzhh/Brain&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00800v1 Announce Type: cross  Abstract: Although large language models demonstrate emergent abilities in solving math word problems, there is a challenging task in complex multi-step mathematical reasoning tasks. To improve model performance on mathematical reasoning tasks, previous work has conducted supervised fine-tuning on open-source models by improving the quality and quantity of data. In this paper, we propose a novel approach, named Brain, to imitate human thought processes to enhance mathematical reasoning abilities, using the Frontal Lobe Model to generate plans, and then employing the Parietal Lobe Model to generate code and execute to obtain answers. First, we achieve SOTA performance in comparison with Code LLaMA 7B based models through this method. Secondly, we find that plans can be explicitly extracted from natural language, code, or formal language. Our code and data are publicly available at https://github.com/cyzhh/Brain.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30830;&#23450;&#26368;&#20248;&#36335;&#24452;&#38598;&#65292;&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#26469;&#32047;&#31215;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00799</link><description>&lt;p&gt;
LLM&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#25968;&#25454;&#33021;&#21147;&#36793;&#30028;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00799
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30830;&#23450;&#26368;&#20248;&#36335;&#24452;&#38598;&#65292;&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#26469;&#32047;&#31215;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#23637;&#31034;&#23545;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22686;&#24378;&#24320;&#28304;LLMs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#19968;&#20010;&#36890;&#29992;&#30340;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#20197;&#24110;&#21161;&#20248;&#21270;&#21644;&#25299;&#23637;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#25512;&#29702;&#36335;&#24452;&#30340;&#26368;&#20248;&#36335;&#24452;&#38598;&#30830;&#23450;&#25512;&#29702;&#36335;&#24452;&#22686;&#24378;&#30340;&#33021;&#21147;&#36793;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#39564;&#35777;&#27169;&#22411;&#19981;&#21516;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#28151;&#21512;&#26469;&#32047;&#31215;&#22686;&#24378;&#65292;&#32780;&#25105;&#20204;&#30340;&#27169;&#22411;MMOS&#22312;&#26356;&#20302;&#30340;&#26500;&#24314;&#25104;&#26412;&#19979;&#23454;&#29616;&#20102;&#31995;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;SOTA&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;GSM-HARD&#24182;&#19981;&#30495;&#27491;&#22256;&#38590;&#65292;&#24403;&#20170;&#30340;LLMs&#19981;&#20877;&#32570;&#20047;&#25968;&#20540;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#29992;&#20110;&#31283;&#20581;&#24615;&#27979;&#35797;&#21644;&#25945;&#32946;&#24212;&#29992;&#30340;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00799v1 Announce Type: cross  Abstract: Large language models (LLMs) are displaying emergent abilities for math reasoning tasks,and there is a growing attention on enhancing the ability of open-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to explore a general data strategy for supervised data to help optimize and expand math reasoning ability.Firstly, we determine the ability boundary of reasoning paths augmentation by identifying these paths' minimal optimal set.Secondly, we validate that different abilities of the model can be cumulatively enhanced by Mix of Minimal Optimal Sets of corresponding types of data, while our models MMOS achieve SOTA performance on series base models under much lower construction costs.Besides, we point out GSM-HARD is not really hard and today's LLMs no longer lack numerical robustness.Also, we provide an Auto Problem Generator for robustness testing and educational applications.Our code and data are publicly available
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#25506;&#35752;CTR&#39044;&#27979;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;&#39057;&#29575;&#19982;&#26368;&#22823;Hessian&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#24378;&#27491;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#39057;&#32321;&#20986;&#29616;&#30340;&#29305;&#24449;&#20250;&#36235;&#21521;&#20110;&#25910;&#25947;&#21040;&#23574;&#38160;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00798</link><description>&lt;p&gt;
Helen: &#20351;&#29992;&#39057;&#29575;&#21152;&#26435;Hessian&#29305;&#24449;&#20540;&#27491;&#21017;&#21270;&#20248;&#21270;CTR&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#25506;&#35752;CTR&#39044;&#27979;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;&#39057;&#29575;&#19982;&#26368;&#22823;Hessian&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#24378;&#27491;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#39057;&#32321;&#20986;&#29616;&#30340;&#29305;&#24449;&#20250;&#36235;&#21521;&#20110;&#25910;&#25947;&#21040;&#23574;&#38160;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20987;&#29575;(CTR)&#39044;&#27979;&#22312;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#22330;&#26223;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#23613;&#31649;&#26368;&#36817;CTR&#39044;&#27979;&#27169;&#22411;&#30340;&#24191;&#27867;&#22686;&#21152;&#65292;&#20294;&#24615;&#33021;&#25913;&#36827;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#24320;&#28304;&#22522;&#20934;&#35780;&#20272;&#20013;&#24471;&#21040;&#35777;&#23454;&#12290;&#24403;&#21069;&#30740;&#31350;&#20154;&#21592;&#20542;&#21521;&#20110;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#24320;&#21457;&#26032;&#27169;&#22411;&#65292;&#24120;&#24120;&#24573;&#35270;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#20160;&#20040;&#30495;&#27491;&#20351;CTR&#39044;&#27979;&#22914;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00798v1 Announce Type: cross  Abstract: Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding?   In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22312;&#37329;&#34701;&#39044;&#27979;&#20013;&#25506;&#32034;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#24182;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2403.00796</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#36807;&#31243;&#22686;&#24378;&#22343;&#20540;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#37329;&#34701;&#39044;&#27979;&#20013;&#30340;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22312;&#37329;&#34701;&#39044;&#27979;&#20013;&#25506;&#32034;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#24182;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#39044;&#27979;&#20855;&#26377;&#28508;&#22312;&#32467;&#26500;&#30340;&#22343;&#20540;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#65292;&#20351;&#29992;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#12290;&#34429;&#28982;&#35768;&#22810;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#19987;&#27880;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30701;&#26399;&#21160;&#24577;&#65292;&#20294;GPs&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#19981;&#20165;&#21487;&#20197;&#39044;&#27979;&#24179;&#22343;&#39044;&#27979;&#20540;&#65292;&#36824;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#19978;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#22312;&#37329;&#34701;&#29615;&#22659;&#20013;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#22914;&#26524;&#19981;&#27491;&#30830;&#30340;&#27874;&#21160;&#29575;&#35780;&#20272;&#23548;&#33268;&#36164;&#26412;&#25439;&#22833;&#65292;&#20165;&#20934;&#30830;&#30340;&#39044;&#27979;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#27492;&#22806;&#65292;&#22312;&#20132;&#26131;&#36873;&#25321;&#20013;&#65292;GPs&#20801;&#35768;&#39044;&#27979;&#22810;&#20010;&#22799;&#26222;&#27604;&#29575;&#65292;&#32771;&#34385;&#20132;&#26131;&#25104;&#26412;&#21518;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#21161;&#20110;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#20960;&#24180;&#30340;&#20449;&#24687;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#26399;&#30340;&#39044;&#27979;&#65292;&#21363;&#20351;&#39044;&#27979;&#33073;&#31163;&#20102;&#24403;&#21069;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00796v1 Announce Type: cross  Abstract: In this paper, we explore the application of Gaussian Processes (GPs) for predicting mean-reverting time series with an underlying structure, using relatively unexplored functional and augmented data structures. While many conventional forecasting methods concentrate on the short-term dynamics of time series data, GPs offer the potential to forecast not just the average prediction but the entire probability distribution over a future trajectory. This is particularly beneficial in financial contexts, where accurate predictions alone may not suffice if incorrect volatility assessments lead to capital losses. Moreover, in trade selection, GPs allow for the forecasting of multiple Sharpe ratios adjusted for transaction costs, aiding in decision-making. The functional data representation utilized in this study enables longer-term predictions by leveraging information from previous years, even as the forecast moves away from the current year
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00794</link><description>&lt;p&gt;
&#35748;&#30495;&#23545;&#24453;&#24189;&#40664;&#65306;&#21033;&#29992;&#19981;&#39118;&#36259;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#24189;&#40664;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00794
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#20114;&#21160;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#36827;&#23637;&#65292;&#24189;&#40664;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#22240;&#20026;&#24189;&#40664;&#25991;&#26412;&#19982;&#31867;&#20284;&#38750;&#24189;&#40664;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#31232;&#32570;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36890;&#36807;&#32534;&#36753;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;&#24189;&#40664;&#26816;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#24403;&#21069;LLMs&#22312;&#8220;&#21462;&#28040;&#39118;&#36259;&#8221;&#31505;&#35805;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#30001;&#20154;&#31867;&#21028;&#26029;&#21644;&#24189;&#40664;&#26816;&#27979;&#30340;&#19979;&#28216;&#20219;&#21153;&#34913;&#37327;&#32780;&#24471;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#28151;&#21512;&#32534;&#30721;&#30340;&#33521;&#35821;-&#21360;&#22320;&#35821;&#24189;&#40664;&#25968;&#25454;&#38598;&#65292;&#22312;&#37027;&#37324;&#25105;&#20204;&#21457;&#29616;GPT-4&#30340;&#21512;&#25104;&#25968;&#25454;&#34987;&#21452;&#35821;&#27880;&#37322;&#21592;&#39640;&#24230;&#35780;&#20215;&#65292;&#24182;&#20026;&#24189;&#40664;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00794v1 Announce Type: cross  Abstract: Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;</title><link>https://arxiv.org/abs/2403.00793</link><description>&lt;p&gt;
&#22312;&#19968;&#20010;&#28151;&#20081;&#32780;&#32416;&#32544;&#30340;&#19990;&#30028;&#20013;&#30340;&#24191;&#21578;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ad Recommendation in a Collapsed and Entangled World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00793
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23637;&#31034;&#22914;&#20309;&#22312;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#23884;&#20837;&#34920;&#31034;&#26102;&#20445;&#30041;&#20808;&#39564;&#24320;&#22987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24207;&#21015;&#29305;&#24449;&#12289;&#25968;&#20540;&#29305;&#24449;&#12289;&#39044;&#35757;&#32451;&#23884;&#20837;&#29305;&#24449;&#20197;&#21450;&#31232;&#30095;ID&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;&#29305;&#24449;&#34920;&#31034;&#30456;&#20851;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#22810;&#20010;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#31181;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#20248;&#21270;&#65292;&#20943;&#23569;&#20559;&#24046;&#24182;&#22686;&#24378;&#25506;&#32034;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#20998;&#26512;&#24037;&#20855;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20840;&#38754;&#30740;&#31350;&#29305;&#24449;&#30456;&#20851;&#24615;&#12289;&#32500;&#24230;&#22349;&#32553;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00793v1 Announce Type: cross  Abstract: In this paper, we present an industry ad recommendation system, paying attention to the challenges and practices of learning appropriate representations. Our study begins by showcasing our approaches to preserving priors when encoding features of diverse types into embedding representations. Specifically, we address sequence features, numeric features, pre-trained embedding features, as well as sparse ID features. Moreover, we delve into two pivotal challenges associated with feature representation: the dimensional collapse of embeddings and the interest entanglement across various tasks or scenarios. Subsequently, we propose several practical approaches to effectively tackle these two challenges. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Furthermore, we introduce three analysis tools that enable us to comprehensively study feature correlation, dimensional collap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#25216;&#26415;&#25552;&#20379;&#26356;&#26131;&#35835;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.00788</link><description>&lt;p&gt;
PRECISE&#26694;&#26550;&#65306;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#20197;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#23454;&#29616;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
PRECISE Framework: GPT-based Text For Improved Readability, Reliability, and Understandability of Radiology Reports For Patient-Centered Care
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#25216;&#26415;&#25552;&#20379;&#26356;&#26131;&#35835;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;OpenAI&#30340;GPT-4&#26469;&#22686;&#24378;&#24739;&#32773;&#21442;&#19982;&#24230;&#65292;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26356;&#26131;&#35835;&#30340;&#20845;&#24180;&#32423;&#38405;&#35835;&#27700;&#24179;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#12290;&#35813;&#26694;&#26550;&#22312;500&#20221;&#25253;&#21578;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#22312;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#32479;&#35745;&#20998;&#26512;&#35777;&#23454;&#20102;PRECISE&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20915;&#31574;&#20013;&#24515;&#30340;&#25252;&#29702;&#20132;&#20184;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00788v1 Announce Type: cross  Abstract: This study introduces and evaluates the PRECISE framework, utilizing OpenAI's GPT-4 to enhance patient engagement by providing clearer and more accessible chest X-ray reports at a sixth-grade reading level. The framework was tested on 500 reports, demonstrating significant improvements in readability, reliability, and understandability. Statistical analyses confirmed the effectiveness of the PRECISE approach, highlighting its potential to foster patient-centric care delivery in healthcare decision-making.
&lt;/p&gt;</description></item><item><title>&#24773;&#24863;&#20998;&#26512;&#22312;&#22806;&#27719;&#20132;&#26131;&#20013;&#30340;&#36816;&#29992;&#23545;&#39044;&#27979;&#24066;&#22330;&#36208;&#21183;&#21644;&#21046;&#23450;&#20132;&#26131;&#20449;&#21495;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20854;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#24066;&#22330;&#26465;&#20214;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.00785</link><description>&lt;p&gt;
&#24212;&#29992;&#26032;&#38395;&#21644;&#23186;&#20307;&#24773;&#24863;&#20998;&#26512;&#29983;&#25104;&#22806;&#27719;&#20132;&#26131;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Applying News and Media Sentiment Analysis for Generating Forex Trading Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00785
&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#22312;&#22806;&#27719;&#20132;&#26131;&#20013;&#30340;&#36816;&#29992;&#23545;&#39044;&#27979;&#24066;&#22330;&#36208;&#21183;&#21644;&#21046;&#23450;&#20132;&#26131;&#20449;&#21495;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20854;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#24066;&#22330;&#26465;&#20214;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#32771;&#23519;&#24773;&#24863;&#20998;&#26512;&#22914;&#20309;&#24212;&#29992;&#20110;&#29983;&#25104;&#22806;&#27719;&#24066;&#22330;&#20132;&#26131;&#20449;&#21495;&#12290;&#20316;&#32773;&#36816;&#29992;&#22522;&#20110;&#35789;&#24211;&#30340;&#20998;&#26512;&#21644;&#26420;&#32032;&#36125;&#21494;&#26031;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35780;&#20272;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#26032;&#38395;&#25991;&#31456;&#20013;&#28041;&#21450;&#32654;&#20803;&#65288;USD&#65289;&#30340;&#24773;&#24863;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#39044;&#27979;&#24066;&#22330;&#36208;&#21183;&#21644;&#21046;&#23450;&#20132;&#26131;&#20449;&#21495;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20854;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#30340;&#24066;&#22330;&#26465;&#20214;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;&#20316;&#32773;&#24471;&#20986;&#32467;&#35770;&#65292;&#36890;&#36807;&#20998;&#26512;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#65292;&#20132;&#26131;&#21592;&#21487;&#20197;&#33719;&#21462;&#20851;&#20110;&#32654;&#20803;&#21644;&#20854;&#20182;&#30456;&#20851;&#22269;&#23478;&#24403;&#21069;&#24066;&#22330;&#24773;&#32490;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#20915;&#31574;&#20132;&#26131;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#24773;&#24863;&#20998;&#26512;&#32435;&#20837;&#20132;&#26131;&#31574;&#30053;&#20316;&#20026;&#39044;&#27979;&#24066;&#22330;&#21160;&#24577;&#30340;&#37325;&#35201;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00785v1 Announce Type: cross  Abstract: The objective of this research is to examine how sentiment analysis can be employed to generate trading signals for the Foreign Exchange (Forex) market. The author assessed sentiment in social media posts and news articles pertaining to the United States Dollar (USD) using a combination of methods: lexicon-based analysis and the Naive Bayes machine learning algorithm. The findings indicate that sentiment analysis proves valuable in forecasting market movements and devising trading signals. Notably, its effectiveness is consistent across different market conditions. The author concludes that by analyzing sentiment expressed in news and social media, traders can glean insights into prevailing market sentiments towards the USD and other pertinent countries, thereby aiding trading decision-making. This study underscores the importance of weaving sentiment analysis into trading strategies as a pivotal tool for predicting market dynamics.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#29359;&#32618;&#39044;&#27979;&#25216;&#26415;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#20998;&#29359;&#32618;&#39044;&#27979;&#31639;&#27861;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#27861;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2403.00780</link><description>&lt;p&gt;
&#23545;&#29359;&#32618;&#39044;&#27979;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#27934;&#35265;&#65306;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Empirical and Experimental Insights into Data Mining Techniques for Crime Prediction: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00780
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#29359;&#32618;&#39044;&#27979;&#25216;&#26415;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#20998;&#29359;&#32618;&#39044;&#27979;&#31639;&#27861;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#27861;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#29359;&#32618;&#39044;&#27979;&#26041;&#27861;&#35770;&#65292;&#25506;&#35752;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#21508;&#31181;&#25216;&#26415;&#21644;&#25216;&#26415;&#12290;&#35813;&#35770;&#25991;&#28085;&#30422;&#20102;&#29992;&#20110;&#20998;&#26512;&#29359;&#32618;&#25968;&#25454;&#30340;&#32479;&#35745;&#26041;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#23457;&#35270;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29359;&#32618;&#39044;&#27979;&#31639;&#27861;&#20998;&#31867;&#20026;&#29305;&#23450;&#25216;&#26415;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#27861;&#12290;&#35813;&#20998;&#31867;&#27861;&#20998;&#20026;&#22235;&#20010;&#23618;&#27425;&#65292;&#21253;&#25324;&#26041;&#27861;&#35770;&#31867;&#21035;&#12289;&#26041;&#27861;&#35770;&#23376;&#31867;&#21035;&#12289;&#26041;&#27861;&#35770;&#25216;&#26415;&#21644;&#26041;&#27861;&#35770;&#25216;&#26415;&#23376;&#31867;&#21035;&#12290;&#25552;&#20379;&#20102;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#20197;&#23545;&#19981;&#21516;&#25216;&#26415;&#36827;&#34892;&#25490;&#21517;&#12290;&#32463;&#39564;&#35780;&#20272;&#26681;&#25454;&#22235;&#20010;&#26631;&#20934;&#35780;&#20272;&#20102;&#29359;&#32618;&#39044;&#27979;&#25216;&#26415;&#65292;&#32780;&#23454;&#39564;&#35780;&#20272;&#21017;&#23545;&#37319;&#29992;&#30456;&#21516;&#23376;&#25216;&#26415;&#30340;&#31639;&#27861;&#12289;&#37319;&#29992;&#30456;&#21516;&#25216;&#26415;&#30340;&#19981;&#21516;&#23376;&#25216;&#26415;&#12289;&#20197;&#21450;&#30456;&#21516;&#25216;&#26415;&#30340;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00780v1 Announce Type: cross  Abstract: This survey paper presents a comprehensive analysis of crime prediction methodologies, exploring the various techniques and technologies utilized in this area. The paper covers the statistical methods, machine learning algorithms, and deep learning techniques employed to analyze crime data, while also examining their effectiveness and limitations. We propose a methodological taxonomy that classifies crime prediction algorithms into specific techniques. This taxonomy is structured into four tiers, including methodology category, methodology sub-category, methodology techniques, and methodology sub-techniques. Empirical and experimental evaluations are provided to rank the different techniques. The empirical evaluation assesses the crime prediction techniques based on four criteria, while the experimental evaluation ranks the algorithms that employ the same sub-technique, the different sub-techniques that employ the same technique, the d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#23558;&#32858;&#31867;&#26041;&#27861;&#19982;&#22235;&#31181;&#38477;&#32500;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#21453;&#27927;&#38065;&#25968;&#25454;&#20013;&#30340;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#32858;&#31867;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00777</link><description>&lt;p&gt;
&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;&#25239;&#37329;&#34701;&#29359;&#32618;&#65306;&#32858;&#31867;&#21644;&#38477;&#32500;&#22312;&#21453;&#27927;&#38065;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Combating Financial Crimes with Unsupervised Learning Techniques: Clustering and Dimensionality Reduction for Anti-Money Laundering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#23558;&#32858;&#31867;&#26041;&#27861;&#19982;&#22235;&#31181;&#38477;&#32500;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#21453;&#27927;&#38065;&#25968;&#25454;&#20013;&#30340;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#32858;&#31867;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#27927;&#38065;&#65288;AML&#65289;&#26159;&#30830;&#20445;&#37329;&#34701;&#31995;&#32479;&#23436;&#25972;&#24615;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;AML&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22522;&#20110;&#34892;&#20026;&#35782;&#21035;&#39640;&#39118;&#38505;&#32676;&#20307;&#12290;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#32858;&#31867;&#65292;&#26159;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25968;&#30334;&#20010;&#29305;&#24449;&#25551;&#36848;&#34892;&#20026;&#20250;&#23548;&#33268;&#39640;&#32500;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#23545;&#32858;&#31867;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#32858;&#31867;&#26041;&#27861;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#19982;&#22235;&#31181;&#38477;&#32500;&#25216;&#26415;&#8212;&#8212;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#12289;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;KPCA&#65289;&#12289;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#12289;&#23616;&#37096;&#20445;&#25345;&#25237;&#24433;&#65288;LPP&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;AML&#25968;&#25454;&#20013;&#30340;&#39640;&#32500;&#38382;&#39064;&#24182;&#25552;&#39640;&#32858;&#31867;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#20943;&#23569;AML&#25968;&#25454;&#32500;&#24230;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00777v1 Announce Type: cross  Abstract: Anti-Money Laundering (AML) is a crucial task in ensuring the integrity of financial systems. One keychallenge in AML is identifying high-risk groups based on their behavior. Unsupervised learning, particularly clustering, is a promising solution for this task. However, the use of hundreds of features todescribe behavior results in a highdimensional dataset that negatively impacts clustering performance.In this paper, we investigate the effectiveness of combining clustering method agglomerative hierarchicalclustering with four dimensionality reduction techniques -Independent Component Analysis (ICA), andKernel Principal Component Analysis (KPCA), Singular Value Decomposition (SVD), Locality Preserving Projections (LPP)- to overcome the issue of high-dimensionality in AML data and improve clusteringresults. This study aims to provide insights into the most effective way of reducing the dimensionality ofAML data and enhance the accuracy 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;&#27969;&#31243;&#25366;&#25496;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19994;&#21153;&#27969;&#31243;&#20013;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#30340;&#26032;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#25153;&#24179;&#21270;&#20107;&#20214;&#26085;&#24535;&#24102;&#26469;&#30340;&#20154;&#20026;&#24322;&#24120;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00775</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#21521;&#23545;&#35937;&#30340;&#19994;&#21153;&#27969;&#31243;&#20013;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Detecting Anomalous Events in Object-centric Business Processes via Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00775
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;&#27969;&#31243;&#25366;&#25496;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19994;&#21153;&#27969;&#31243;&#20013;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#30340;&#26032;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#25153;&#24179;&#21270;&#20107;&#20214;&#26085;&#24535;&#24102;&#26469;&#30340;&#20154;&#20026;&#24322;&#24120;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#24322;&#24120;&#23545;&#20110;&#35782;&#21035;&#19994;&#21153;&#27969;&#31243;&#20013;&#30340;&#20302;&#25928;&#29575;&#12289;&#38169;&#35823;&#25110;&#27450;&#35784;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#27969;&#31243;&#25366;&#25496;&#26041;&#27861;&#38598;&#20013;&#22312;&#20998;&#26512;&#22522;&#20110;&#21333;&#19968;&#26696;&#20363;&#27010;&#24565;&#30340;'&#25153;&#24179;&#21270;'&#12289;&#39034;&#24207;&#20107;&#20214;&#26085;&#24535;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#27969;&#31243;&#25191;&#34892;&#23637;&#29616;&#20986;&#31867;&#20284;&#22270;&#24418;&#30340;&#32467;&#26500;&#65292;&#20854;&#20013;&#20107;&#20214;&#21487;&#20197;&#19982;&#22810;&#20010;&#26696;&#20363;&#30456;&#20851;&#32852;&#12290;&#23558;&#20107;&#20214;&#26085;&#24535;&#25153;&#24179;&#21270;&#38656;&#35201;&#36873;&#25321;&#21333;&#20010;&#26696;&#20363;&#26631;&#35782;&#31526;&#65292;&#36825;&#23548;&#33268;&#19982;&#30495;&#23454;&#20107;&#20214;&#25968;&#25454;&#23384;&#22312;&#24046;&#36317;&#24182;&#22312;&#20107;&#20214;&#26085;&#24535;&#20013;&#20154;&#20026;&#24341;&#20837;&#24322;&#24120;&#12290;&#38754;&#21521;&#23545;&#35937;&#30340;&#27969;&#31243;&#25366;&#25496;&#36890;&#36807;&#20801;&#35768;&#20107;&#20214;&#19982;&#19981;&#21516;&#26696;&#20363;&#30456;&#20851;&#32852;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;&#27969;&#31243;&#25366;&#25496;&#25552;&#20379;&#30340;&#22686;&#24378;&#20449;&#24687;&#36827;&#34892;&#19994;&#21153;&#27969;&#31243;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38754;&#21521;&#23545;&#35937;&#30340;&#20107;&#20214;&#26085;&#24535;&#30340;&#36807;&#31243;&#20381;&#36182;&#20851;&#31995;&#37325;&#24314;&#21644;&#34920;&#31034;&#20026;&#24102;&#23646;&#24615;&#30340;&#22270;&#65292;&#28982;&#21518;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00775v1 Announce Type: cross  Abstract: Detecting anomalies is important for identifying inefficiencies, errors, or fraud in business processes. Traditional process mining approaches focus on analyzing 'flattened', sequential, event logs based on a single case notion. However, many real-world process executions exhibit a graph-like structure, where events can be associated with multiple cases. Flattening event logs requires selecting a single case identifier which creates a gap with the real event data and artificially introduces anomalies in the event logs. Object-centric process mining avoids these limitations by allowing events to be related to different cases. This study proposes a novel framework for anomaly detection in business processes that exploits graph neural networks and the enhanced information offered by object-centric process mining. We first reconstruct and represent the process dependencies of the object-centric event logs as attributed graphs and then empl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#21518;&#36873;&#25321;&#20013;&#30340;&#19981;&#31471;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.00773</link><description>&lt;p&gt;
&#21518;&#36873;&#25321;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19981;&#31471;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Misconduct in Post-Selections and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00773
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#21518;&#36873;&#25321;&#20013;&#30340;&#19981;&#31471;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#20851;&#20110;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#29305;&#21035;&#26159;&#19968;&#33324;&#21518;&#36873;&#25321;&#20013;&#19981;&#31471;&#34892;&#20026;&#30340;&#29702;&#35770;&#35770;&#25991;&#12290;&#20316;&#32773;&#25152;&#30693;&#65292;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#19981;&#31471;&#34892;&#20026;&#30340;&#31532;&#19968;&#31687;&#21516;&#34892;&#35780;&#23457;&#35770;&#25991;&#26159;[32]&#65292;[37]&#21644;[36]&#12290;&#26080;&#35770;&#23398;&#20064;&#27169;&#24335;&#26159;&#30417;&#30563;&#65292;&#24378;&#21270;&#65292;&#23545;&#25239;&#36824;&#26159;&#36827;&#21270;&#65292;&#20960;&#20046;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#38500;&#20102;&#23569;&#25968;&#35757;&#32451;&#21333;&#19968;&#31995;&#32479;&#30340;&#26041;&#27861;&#65289;&#37117;&#26681;&#28304;&#20110;&#21516;&#26679;&#30340;&#19981;&#31471;&#34892;&#20026;-&#20316;&#24330;&#21644;&#38544;&#34255;-&#65288;1&#65289;&#22312;&#27809;&#26377;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#20316;&#24330;&#20197;&#21450;&#65288;2&#65289;&#38544;&#34255;&#22806;&#35266;&#19981;&#20339;&#30340;&#25968;&#25454;&#12290;&#22312;[32]&#65292;[37]&#65292;[36]&#20013;&#25512;&#29702;&#65292;&#20316;&#32773;&#24517;&#39035;&#33267;&#23569;&#25253;&#21578;&#25152;&#26377;&#24050;&#35757;&#32451;&#32593;&#32476;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#24179;&#22343;&#35823;&#24046;&#65288;&#26412;&#25991;&#20013;&#31216;&#20026;&#36890;&#29992;&#20132;&#21449;&#39564;&#35777;&#65289;&#12290;&#26368;&#22909;&#36824;&#25253;&#21578;&#25490;&#21517;&#35823;&#24046;&#30340;&#20116;&#20010;&#30334;&#20998;&#27604;&#20301;&#32622;&#12290;&#20174;&#36825;&#37324;&#30340;&#26032;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#30475;&#21040;&#38544;&#34255;&#30340;&#32618;&#39745;&#31096;&#39318;&#26159;&#21518;&#36873;&#25321;&#12290;&#23545;&#25163;&#21160;&#35843;&#25972;&#25110;&#25628;&#32034;&#36229;&#21442;&#25968;&#30340;&#21518;&#36873;&#25321;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00773v1 Announce Type: new  Abstract: This is a theoretical paper on "Deep Learning" misconduct in particular and Post-Selection in general. As far as the author knows, the first peer-reviewed papers on Deep Learning misconduct are [32], [37], [36]. Regardless of learning modes, e.g., supervised, reinforcement, adversarial, and evolutional, almost all machine learning methods (except for a few methods that train a sole system) are rooted in the same misconduct -- cheating and hiding -- (1) cheating in the absence of a test and (2) hiding bad-looking data. It was reasoned in [32], [37], [36] that authors must report at least the average error of all trained networks, good and bad, on the validation set (called general cross-validation in this paper). Better, report also five percentage positions of ranked errors. From the new analysis here, we can see that the hidden culprit is Post-Selection. This is also true for Post-Selection on hand-tuned or searched hyperparameters, bec
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;BERT&#24773;&#24863;&#20998;&#31867;&#21644;LSTM&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22312;&#24494;&#21338;&#24179;&#21488;&#25480;&#26435;&#21644;&#26410;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#32929;&#24066;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.00772</link><description>&lt;p&gt;
&#24494;&#21338;&#24179;&#21488;&#19987;&#23478;&#22312;&#39044;&#27979;&#32929;&#24066;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Weibo platform experts perform better at predicting stock market?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00772
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;BERT&#24773;&#24863;&#20998;&#31867;&#21644;LSTM&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22312;&#24494;&#21338;&#24179;&#21488;&#25480;&#26435;&#21644;&#26410;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#32929;&#24066;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#21487;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#23578;&#26410;&#30740;&#31350;&#29992;&#25143;&#30340;&#37329;&#34701;&#32972;&#26223;&#23545;&#22522;&#20110;&#24773;&#24863;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#21512;&#26469;&#35780;&#20272;&#22522;&#20110;&#20154;&#32676;&#37329;&#34701;&#32972;&#26223;&#30340;&#24773;&#24863;&#32929;&#24066;&#39044;&#27979;&#12290;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#26469;&#20998;&#31867;&#24773;&#24863;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#32929;&#24066;&#39044;&#27979;&#12290;&#35780;&#20272;&#26102;&#65292;&#20351;&#29992;&#24494;&#21338;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#20316;&#20026;&#24773;&#24863;&#25968;&#25454;&#25910;&#38598;&#26469;&#28304;&#12290;&#26681;&#25454;&#20854;&#32972;&#26223;&#65292;&#23558;&#24494;&#21338;&#29992;&#25143;&#65288;&#21450;&#20854;&#35780;&#35770;&#65289;&#20998;&#20026;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#65288;AFA&#65289;&#21644;&#26410;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#65288;UFA&#65289;&#20004;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00772v1 Announce Type: cross  Abstract: Sentiment analysis can be used for stock market prediction. However, existing research has not studied the impact of a user's financial background on sentiment-based forecasting of the stock market using artificial neural networks. In this work, a novel combination of neural networks is used for the assessment of sentiment-based stock market prediction, based on the financial background of the population that generated the sentiment. The state-of-the-art language processing model Bidirectional Encoder Representations from Transformers (BERT) is used to classify the sentiment and a Long-Short Term Memory (LSTM) model is used for time-series based stock market prediction. For evaluation, the Weibo social networking platform is used as a sentiment data collection source. Weibo users (and their comments respectively) are divided into Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor (UFA) groups according to their backg
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#31181;&#27169;&#22411;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#23558;&#27491;&#20132;X&#23556;&#32447;&#22270;&#20687;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;CT&#20307;&#31215;&#65292;&#37319;&#29992;&#20102;UNet&#26550;&#26500;&#12289;&#33258;&#23450;&#20041;&#36830;&#25509;&#12289;&#28608;&#27963;&#20989;&#25968;&#12289;&#25439;&#22833;&#20989;&#25968;&#12289;&#20248;&#21270;&#22120;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#25237;&#24433;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00771</link><description>&lt;p&gt;
&#20174;&#37197;&#23545;X&#23556;&#32447;&#29983;&#25104;CT&#20307;&#31215;&#30340;XProspeCT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
XProspeCT: CT Volume Generation from Paired X-Rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00771
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#31181;&#27169;&#22411;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#23558;&#27491;&#20132;X&#23556;&#32447;&#22270;&#20687;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;CT&#20307;&#31215;&#65292;&#37319;&#29992;&#20102;UNet&#26550;&#26500;&#12289;&#33258;&#23450;&#20041;&#36830;&#25509;&#12289;&#28608;&#27963;&#20989;&#25968;&#12289;&#25439;&#22833;&#20989;&#25968;&#12289;&#20248;&#21270;&#22120;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#25237;&#24433;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#26377;&#30410;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;CT&#25195;&#25551;&#25552;&#20379;&#20102;&#20851;&#20110;&#24739;&#32773;&#20869;&#37096;&#35299;&#21078;&#32467;&#26500;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20294;&#19982;X&#23556;&#32447;&#25104;&#20687;&#30456;&#27604;&#65292;CT&#25195;&#25551;&#20855;&#26377;&#26356;&#39640;&#30340;&#36752;&#23556;&#21058;&#37327;&#21644;&#36153;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#25506;&#32034;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#27169;&#22411;&#32467;&#26500;&#65292;&#23558;&#27491;&#20132;X&#23556;&#32447;&#22270;&#20687;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;CT&#20307;&#31215;&#12290;&#26174;&#33879;&#30340;&#27169;&#22411;&#21464;&#21270;&#21253;&#25324;UNet&#26550;&#26500;&#12289;&#33258;&#23450;&#20041;&#36830;&#25509;&#12289;&#28608;&#27963;&#20989;&#25968;&#12289;&#25439;&#22833;&#20989;&#25968;&#12289;&#20248;&#21270;&#22120;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#25237;&#24433;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00771v1 Announce Type: cross  Abstract: Computed tomography (CT) is a beneficial imaging tool for diagnostic purposes. CT scans provide detailed information concerning the internal anatomic structures of a patient, but present higher radiation dose and costs compared to X-ray imaging. In this paper, we build on previous research to convert orthogonal X-ray images into simulated CT volumes by exploring larger datasets and various model structures. Significant model variations include UNet architectures, custom connections, activation functions, loss functions, optimizers, and a novel back projection approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#24403;&#21069;&#25945;&#32946;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#26088;&#22312;&#22238;&#31572;&#25945;&#32946;&#29615;&#22659;&#20013;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#12289;&#26368;&#24120;&#29992;&#30340;&#25945;&#32946;&#36164;&#28304;&#20197;&#21450;&#20027;&#35201;&#30340;&#24212;&#29992;&#25110;&#25945;&#32946;&#30446;&#26631;&#65292;&#21516;&#26102;&#27010;&#36848;&#20102;&#32467;&#35770;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.00769</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Text mining in education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#24403;&#21069;&#25945;&#32946;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#26088;&#22312;&#22238;&#31572;&#25945;&#32946;&#29615;&#22659;&#20013;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#12289;&#26368;&#24120;&#29992;&#30340;&#25945;&#32946;&#36164;&#28304;&#20197;&#21450;&#20027;&#35201;&#30340;&#24212;&#29992;&#25110;&#25945;&#32946;&#30446;&#26631;&#65292;&#21516;&#26102;&#27010;&#36848;&#20102;&#32467;&#35770;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#29615;&#22659;&#30340;&#36805;&#29467;&#22686;&#38271;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#35770;&#22363;&#12289;&#32842;&#22825;&#12289;&#31038;&#20132;&#32593;&#32476;&#12289;&#35780;&#20272;&#12289;&#35770;&#25991;&#31561;&#25991;&#26412;&#26684;&#24335;&#30340;&#25968;&#25454;&#12290;&#22914;&#20309;&#25366;&#25496;&#25991;&#26412;&#25968;&#25454;&#20197;&#25214;&#21040;&#23545;&#25945;&#32946;&#30456;&#20851;&#20154;&#21592;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#26159;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#21457;&#34920;&#20102;&#36234;&#26469;&#36234;&#22810;&#24212;&#29992;&#25991;&#26412;&#25366;&#25496;&#20110;&#25945;&#32946;&#39046;&#22495;&#30340;&#25991;&#31456;&#65292;&#20294;&#25105;&#20204;&#23578;&#26410;&#25214;&#21040;&#20219;&#20309;&#32508;&#36848;&#36825;&#20123;&#24037;&#20316;&#30340;&#35770;&#25991;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#24403;&#21069;&#25945;&#32946;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#31995;&#32479;&#27010;&#36848;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#22238;&#31572;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#65306;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#26159;&#20160;&#20040;&#65311;&#26368;&#24120;&#29992;&#30340;&#25945;&#32946;&#36164;&#28304;&#26159;&#20160;&#20040;&#65311;&#20027;&#35201;&#24212;&#29992;&#25110;&#25945;&#32946;&#30446;&#26631;&#26159;&#20160;&#20040;&#65311;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#32467;&#35770;&#21644;&#26356;&#26377;&#36259;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00769v1 Announce Type: cross  Abstract: The explosive growth of online education environments is generating a massive volume of data, specially in text format from forums, chats, social networks, assessments, essays, among others. It produces exciting challenges on how to mine text data in order to find useful knowledge for educational stakeholders. Despite the increasing number of educational applications of text mining published recently, we have not found any paper surveying them. In this line, this work presents a systematic overview of the current status of the Educational Text Mining field. Our final goal is to answer three main research questions: Which are the text mining techniques most used in educational environments? Which are the most used educational resources? And which are the main applications or educational goals? Finally, we outline the conclusions and the more interesting future trends.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;DNN&#22810;&#31199;&#25143;&#22810;&#21152;&#36895;&#22120;&#31995;&#32479;&#20013;&#20844;&#24179;&#21644;&#31283;&#23450;&#30340;&#23454;&#26102;&#35843;&#24230;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#31199;&#25143;&#30340;&#27169;&#22411;&#29305;&#23450;QoS&#31649;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00766</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;DNN&#22810;&#31199;&#25143;&#22810;&#21152;&#36895;&#22120;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#21644;&#31283;&#23450;&#23454;&#26102;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Fair and Firm Real-Time Scheduling in DNN Multi-Tenant Multi-Accelerator Systems via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00766
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;DNN&#22810;&#31199;&#25143;&#22810;&#21152;&#36895;&#22120;&#31995;&#32479;&#20013;&#20844;&#24179;&#21644;&#31283;&#23450;&#30340;&#23454;&#26102;&#35843;&#24230;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#31199;&#25143;&#30340;&#27169;&#22411;&#29305;&#23450;QoS&#31649;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20113;&#26381;&#21153;&#20013;&#31649;&#29702;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20010;&#21035;&#31199;&#25143;&#26399;&#26395;&#21644;&#19981;&#21516;&#30340;&#26381;&#21153;&#27700;&#24179;&#25351;&#26631;&#65288;SLI&#65289;&#30340;&#32454;&#24494;&#20043;&#22788;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#31649;&#29702;&#22810;&#31199;&#25143;&#12289;&#22810;&#21152;&#36895;&#22120;&#20113;&#29615;&#22659;&#20013;&#21508;&#33258;&#29305;&#23450;&#30340;QoS&#12290;&#25152;&#36873;&#25321;&#30340;SLI&#65292;&#25130;&#27490;&#26102;&#38388;&#21629;&#20013;&#29575;&#65292;&#20801;&#35768;&#23458;&#25143;&#20026;&#27599;&#20010;&#26381;&#21153;&#35831;&#27714;&#23450;&#21046;QoS&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#21152;&#36895;&#22120;&#31995;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#35843;&#24230;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;&#22312;&#32771;&#34385;&#23454;&#26102;&#32422;&#26463;&#30340;&#21516;&#26102;&#20445;&#35777;&#31199;&#25143;&#29305;&#23450;&#65292;&#27169;&#22411;&#29305;&#23450;&#30340;QoS&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00766v1 Announce Type: cross  Abstract: This paper addresses the critical challenge of managing Quality of Service (QoS) in cloud services, focusing on the nuances of individual tenant expectations and varying Service Level Indicators (SLIs). It introduces a novel approach utilizing Deep Reinforcement Learning for tenant-specific QoS management in multi-tenant, multi-accelerator cloud environments. The chosen SLI, deadline hit rate, allows clients to tailor QoS for each service request. A novel online scheduling algorithm for Deep Neural Networks in multi-accelerator systems is proposed, with a focus on guaranteeing tenant-wise, model-specific QoS levels while considering real-time constraints.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Webots&#30340;&#26080;&#20154;&#30417;&#25511;&#23481;&#22120;&#21270;(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154; Robotino &#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#27169;&#22411;&#24320;&#21457;&#29615;&#22659;&#30340;&#20998;&#31163;&#36825;&#19968;&#19981;&#22826;&#34987;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00765</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;Webots&#30340;&#26080;&#20154;&#30417;&#25511;&#23481;&#22120;&#21270;(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
An Architecture for Unattended Containerized (Deep) Reinforcement Learning with Webots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Webots&#30340;&#26080;&#20154;&#30417;&#25511;&#23481;&#22120;&#21270;(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154; Robotino &#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#27169;&#22411;&#24320;&#21457;&#29615;&#22659;&#30340;&#20998;&#31163;&#36825;&#19968;&#19981;&#22826;&#34987;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#22312;&#21508;&#34892;&#21508;&#19994;&#20013;&#24471;&#21040;&#37319;&#29992;&#65292;&#24037;&#20855;&#26223;&#35266;&#19981;&#26029;&#25104;&#29087;&#65292;&#20197;&#20419;&#36827;&#36825;&#31867;&#24212;&#29992;&#30340;&#29983;&#21629;&#21608;&#26399;&#24182;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24212;&#23545;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#20351;&#29992;&#20195;&#29702;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#21487;&#33021;&#38754;&#20020;&#25361;&#25112;&#65306;&#20351;&#29992;&#27169;&#25311;&#36719;&#20214;&#25152;&#38656;&#30340;&#30693;&#35782;&#20197;&#21450;&#22312;&#26080;&#20154;&#30417;&#25511;&#30340;&#35757;&#32451;&#31649;&#36947;&#20013;&#21033;&#29992;&#29420;&#31435;&#30340;&#27169;&#25311;&#36719;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29992;&#20110;&#22312;3D&#19990;&#30028;&#20013;&#22521;&#35757;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154;Robotino&#36827;&#34892;&#35770;&#36848;&#65292;&#24182;&#35748;&#20026;&#20026;&#34394;&#25311;&#19990;&#30028;&#30340;&#21019;&#24314;&#32773;&#20998;&#31163;&#27169;&#25311;&#29615;&#22659;&#19982;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#27169;&#22411;&#24320;&#21457;&#29615;&#22659;&#24182;&#19981;&#26159;&#19968;&#20010;&#34987;&#24456;&#22909;&#28085;&#30422;&#30340;&#20027;&#39064;&#12290;&#36890;&#24120;&#20108;&#32773;&#30456;&#21516;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#20102;&#35299;&#27169;&#25311;&#36719;&#20214;&#65292;&#30452;&#25509;&#19982;&#20854;API&#19968;&#36215;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#26377;&#26102;&#34394;&#25311;&#19990;&#30028;&#30340;&#21019;&#24314;&#32773;&#20250;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00765v1 Announce Type: cross  Abstract: As data science applications gain adoption across industries, the tooling landscape matures to facilitate the life cycle of such applications and provide solutions to the challenges involved to boost the productivity of the people involved. Reinforcement learning with agents in a 3D world could still face challenges: the knowledge required to use a simulation software as well as the utilization of a standalone simulation software in unattended training pipelines.   In this paper we review tools and approaches to train reinforcement learning agents for robots in 3D worlds with respect to the robot Robotino and argue that the separation of the simulation environment for creators of virtual worlds and the model development environment for data scientists is not a well covered topic. Often both are the same and data scientists require knowledge of the simulation software to work directly with their APIs. Moreover, sometimes creators of vir
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00033</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#27880;&#24847;&#21147;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#22823;&#40635;&#20351;&#29992;&#32773;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00033
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#30340;&#25345;&#32493;&#20351;&#29992;&#26126;&#26174;&#24433;&#21709;&#20154;&#20204;&#30340;&#29983;&#27963;&#21644;&#20581;&#24247;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;HOGAB&#65288;High-Order Attention Graph Attention&#31070;&#32463;&#32593;&#32476;&#65289;&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#23616;&#37096;&#24322;&#24120;&#33041;&#27963;&#21160;&#12290;HOGAB&#23558;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#19982;LSTM&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25429;&#25417;&#22823;&#40635;&#29992;&#25143;fMRI&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#23545;&#37051;&#22495;&#33410;&#28857;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#22686;&#24378;&#38271;&#26399;&#22823;&#40635;&#29992;&#25143;&#30340;&#31038;&#21306;&#32858;&#31867;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22810;&#22270;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;85.1%&#30340;AUC&#21644;80.7%&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#32447;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;HODAB&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;GateLoop&#12289;Mamba&#21644;GLA&#31561;&#20855;&#26377;&#20056;&#27861;&#20132;&#20114;&#30340;&#32447;&#24615;&#36882;&#24402;&#39537;&#21160;&#19979;&#30340;&#28145;&#24230;SSM&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.19047</link><description>&lt;p&gt;
&#28145;&#24230;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical Foundations of Deep Selective State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19047
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GateLoop&#12289;Mamba&#21644;GLA&#31561;&#20855;&#26377;&#20056;&#27861;&#20132;&#20114;&#30340;&#32447;&#24615;&#36882;&#24402;&#39537;&#21160;&#19979;&#30340;&#28145;&#24230;SSM&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22914;S4&#65292;&#28304;&#33258;Gu&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#20316;&#20026;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#28145;&#24230;SSM&#22312;&#21508;&#31181;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;transformers&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#39537;&#21160;SSM&#30340;&#32447;&#24615;&#36882;&#24402;&#20801;&#35768;&#36755;&#20837;&#21644;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#20056;&#27861;&#20132;&#20114;&#65288;&#22914;GateLoop&#65292;Mamba&#65292;GLA&#65289;&#65292;&#37027;&#20040;&#25152;&#24471;&#21040;&#30340;&#26550;&#26500;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21442;&#25968;&#35268;&#27169;&#36798;&#21040;&#21313;&#20159;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Rough Path Theory&#30340;&#24037;&#20855;&#65292;&#20026;&#36825;&#19968;&#26368;&#36817;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65306;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#38543;&#26426;&#32447;&#24615;&#36882;&#24402;&#37197;&#22791;&#31616;&#21333;&#30340;&#36755;&#20837;&#25511;&#21046;&#36716;&#25442;&#65288;&#36873;&#25321;&#24615;&#26426;&#21046;&#65289;&#26102;&#65292;&#38544;&#34255;&#29366;&#24577;&#21487;&#34987;&#35777;&#26126;&#26159;&#20302;&#32500;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19047v1 Announce Type: new  Abstract: Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional proj
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18919</link><description>&lt;p&gt;
Decompose-and-Compose: &#19968;&#31181;&#32452;&#21512;&#26041;&#27861;&#26469;&#20943;&#36731;&#20266;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26631;&#20934;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20869;&#20998;&#24067;&#25968;&#25454;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22806;&#20998;&#24067;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20998;&#24067;&#36716;&#31227;&#26469;&#28304;&#26159;&#22270;&#20687;&#30340;&#32452;&#25104;&#24615;&#36136;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#30830;&#23450;&#26631;&#31614;&#30340;&#20027;&#35201;&#23545;&#35937;&#25110;&#32452;&#20214;&#22806;&#65292;&#36890;&#24120;&#36824;&#23384;&#22312;&#19968;&#20123;&#20854;&#20182;&#22270;&#20687;&#32452;&#20214;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#36755;&#20837;&#20998;&#24067;&#36716;&#31227;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#33021;&#19982;&#26631;&#31614;&#20855;&#26377;&#20266;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Decompose-and-Compose&#65288;DaC&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#32452;&#21512;&#22270;&#20687;&#20803;&#32032;&#30340;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#20351;&#29992;ERM&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#39640;&#24230;&#20851;&#27880;&#35201;&#20040;&#26159;&#22240;&#26524;&#32452;&#20214;&#65292;&#35201;&#20040;&#26159;&#19982;&#26631;&#31614;&#20855;&#26377;&#39640;&#20266;&#30456;&#20851;&#24615;&#30340;&#32452;&#20214;&#65288;&#23588;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18919v1 Announce Type: cross  Abstract: While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18839</link><description>&lt;p&gt;
&#25193;&#23637;&#27969;&#21305;&#37197;&#65306;&#20855;&#26377;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#33879;&#21517;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20197;&#22522;&#20110;&#24341;&#23548;&#30340;&#26080;&#20998;&#31867;&#22120;&#26041;&#27861;&#20026;&#39318;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24341;&#23548;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#19981;&#20165;&#35201;&#27714;&#29992;&#25143;&#24494;&#35843;&#8220;&#24341;&#23548;&#24378;&#24230;&#8221;&#65292;&#32780;&#19988;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#19981;&#19968;&#23450;&#23545;&#24212;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#26412;&#25991;&#22522;&#20110;&#27969;&#21305;&#37197;&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#27969;&#21305;&#37197;&#26159;&#25193;&#25955;&#26041;&#27861;&#30340;&#24403;&#21069;&#24378;&#22823;&#31454;&#20105;&#32773;&#20043;&#19968;&#12290;&#21463;&#23558;&#27010;&#29575;&#36335;&#24452;&#35299;&#37322;&#20026;&#36335;&#24452;&#31354;&#38388;&#19978;&#30340;&#20998;&#24067;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#19981;&#26159;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#12290;&#36825;&#19968;&#29702;&#35770;&#33258;&#28982;&#22320;&#25512;&#23548;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18839v1 Announce Type: new  Abstract: The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the "guidance strength," but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.18781</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#32423;&#20449;&#24565;&#30340;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#22312;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18781
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#20986;&#29616;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#22914;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;IT&#22522;&#30784;&#35774;&#26045;&#65292;&#20449;&#24687;&#19981;&#23545;&#31216;&#20026;&#20915;&#31574;&#23454;&#20307;&#65288;&#29609;&#23478;&#65289;&#30340;&#20915;&#31574;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#65288;AISG&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#20027;&#35201;&#26159;&#31163;&#32447;&#30340;&#65292;&#38024;&#23545;&#29305;&#27530;&#31867;&#21035;&#30340;AISG&#65292;&#20197;&#36991;&#20813;&#20449;&#24565;&#23618;&#27425;&#65292;&#24182;&#19988;&#32570;&#20047;&#36866;&#24212;&#22343;&#34913;&#20559;&#24046;&#30340;&#22312;&#32447;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#19987;&#38376;&#38024;&#23545;&#36890;&#29992;AISG&#12290;COL&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#12290;&#38024;&#23545;&#20551;&#35774;&#30340;&#23545;&#25163;&#65292;COL&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;COL&#20013;&#30340;&#20551;&#35774;&#19982;t&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18781v1 Announce Type: cross  Abstract: Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.18607</link><description>&lt;p&gt;
&#22312;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#20013;&#25506;&#35752;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65306;&#19968;&#31181;&#23545;&#25239;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36817;&#24180;&#26469;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#20998;&#24067;&#35206;&#30422;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#32452;&#32455;&#20998;&#20139;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20998;&#20139;&#31169;&#20154;&#25968;&#25454;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#19982;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#26041;&#65288;&#20998;&#20139;&#32773;&#65289;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24182;&#21521;&#21478;&#19968;&#26041;&#65288;&#25509;&#25910;&#32773;&#65289;&#25552;&#20379;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#20139;&#32773;&#21487;&#20197;&#23454;&#34892;&#30340;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18607v1 Announce Type: cross  Abstract: Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execut
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#35780;&#20272;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#35268;&#21010;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18064</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36801;&#31227;&#23398;&#20064;&#33258;&#21160;&#27979;&#35797;&#31354;&#38388;&#30456;&#20851;&#29615;&#22659;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18064
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#35780;&#20272;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#35268;&#21010;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#37319;&#26679;&#23545;&#25143;&#22806;&#20449;&#24687;&#25910;&#38598;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#39640;&#26114;&#30340;&#37319;&#26679;&#25104;&#26412;&#65292;&#22914;&#26102;&#38388;&#12289;&#33021;&#37327;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#29615;&#22659;&#30772;&#22351;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#20808;&#39564;&#25968;&#25454;&#21487;&#20197;&#26159;&#25552;&#39640;&#25928;&#29575;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#19982;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#36890;&#24120;&#20107;&#20808;&#26410;&#30693;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21033;&#29992;&#27492;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#35268;&#21010;&#25928;&#29575;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#32452;&#21512;&#65292;&#23427;&#21487;&#20197;&#30740;&#31350;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#31354;&#38388;&#65292;&#24182;&#21363;&#26102;&#35780;&#20272;&#36825;&#20123;&#20551;&#35774;&#65292;&#20351;&#27492;&#26032;&#30693;&#35782;&#33021;&#22815;&#31435;&#21363;&#20026;&#26410;&#26469;&#35745;&#21010;&#25152;&#21033;&#29992;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18064v1 Announce Type: cross  Abstract: The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate 
&lt;/p&gt;</description></item><item><title>Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.18007</link><description>&lt;p&gt;
Mixer&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixer is more than just a model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18007
&lt;/p&gt;
&lt;p&gt;
Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;MLP&#32467;&#26500;&#37325;&#26032;&#21463;&#21040;&#20851;&#27880;&#65292;&#20854;&#20013;MLP-Mixer&#20197;&#20854;&#31361;&#20986;&#30340;&#34920;&#29616;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;MLP-Mixer&#20197;&#20174;&#36890;&#36947;&#21644;&#20196;&#29260;&#20004;&#20010;&#35282;&#24230;&#25552;&#21462;&#25968;&#25454;&#20449;&#24687;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#26377;&#25928;&#22320;&#20316;&#20026;&#36890;&#36947;&#20449;&#24687;&#21644;&#20196;&#29260;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#20107;&#23454;&#19978;&#65292;Mixer&#20195;&#34920;&#20102;&#19968;&#31181;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;Mixer&#30340;&#31934;&#39635;&#22312;&#20110;&#23427;&#33021;&#22815;&#20174;&#22810;&#20803;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#65292;&#20856;&#22411;&#22320;&#20307;&#29616;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#39046;&#22495;&#30340;&#8220;&#28151;&#21512;&#8221;&#30495;&#27491;&#27010;&#24565;&#12290;&#38500;&#20102;&#32771;&#34385;&#36890;&#36947;&#21644;&#20196;&#29260;&#20197;&#22806;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#35282;&#24230;&#21019;&#36896;&#26356;&#36148;&#21512;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#30340;&#28151;&#21512;&#22120;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#38899;&#39057;&#35782;&#21035;&#39046;&#22495;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#24102;Roll-Time&#21644;Hermit FFT&#30340;&#38899;&#39057;&#39057;&#35889;&#28151;&#21512;&#22120;(ASM-RH)&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#26102;&#38388;&#21644;&#39057;&#29575;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18007v1 Announce Type: cross  Abstract: Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of "mixing" in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and freq
&lt;/p&gt;</description></item><item><title>&#21487;&#20197;&#30452;&#25509;&#20174;&#24102;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#24517;&#35201;&#24615;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#27169;&#22411;&#65292;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;</title><link>https://arxiv.org/abs/2402.17926</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#30830;&#23450;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Certain and Approximately Certain Models for Statistical Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17926
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20197;&#30452;&#25509;&#20174;&#24102;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#24517;&#35201;&#24615;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#27169;&#22411;&#65292;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;&#21253;&#21547;&#32570;&#22833;&#20540;&#12290;&#20026;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#29992;&#25143;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#22635;&#20805;&#21644;&#25214;&#21040;&#32570;&#22833;&#25968;&#25454;&#39033;&#30340;&#27491;&#30830;&#20540;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#35757;&#32451;&#25968;&#25454;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#20415;&#22312;&#21508;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#26816;&#26597;&#27492;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#32780;&#27809;&#26377;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17926v1 Announce Type: cross  Abstract: Real-world data is often incomplete and contains missing values. To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items. In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models. We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms. We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary. Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead.
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.16991</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#25581;&#31034;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16991
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#30495;&#23454;&#25968;&#25454;&#30340;&#32467;&#26500;&#22312;&#25512;&#21160;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#65292;&#34987;&#35748;&#20026;&#26159;&#30001;&#20197;&#23618;&#27425;&#21644;&#32452;&#21512;&#26041;&#24335;&#32452;&#32455;&#30340;&#29305;&#24449;&#32452;&#25104;&#30340;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#36825;&#20123;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#25429;&#25417;&#21040;&#36825;&#31181;&#28508;&#22312;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;$t$&#21518;&#20316;&#29992;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21463;&#21040;&#26576;&#20010;&#38408;&#20540;&#26102;&#38388;&#22788;&#30340;&#30456;&#21464;&#25511;&#21046;&#65292;&#27492;&#26102;&#37325;&#24314;&#39640;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#31867;&#21035;&#65289;&#30340;&#27010;&#29575;&#31361;&#28982;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#20302;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#20855;&#20307;&#32454;&#33410;&#65289;&#30340;&#37325;&#24314;&#22312;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#20013;&#24179;&#31283;&#28436;&#21464;&#12290;&#36825;&#19968;&#32467;&#26524;&#26263;&#31034;&#65292;&#22312;&#36229;&#20986;&#36716;&#21464;&#26102;&#38388;&#30340;&#26102;&#21051;&#65292;&#31867;&#21035;&#24050;&#21464;&#21270;&#65292;&#20294;&#26159;&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.16388</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#21449;&#19968;&#33268;$p$-&#20540;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16388
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#35201;&#27714;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25511;&#21046;&#31867;&#22411;I&#38169;&#35823;&#29575;($\alpha$)&#32780;&#21448;&#19981;&#25439;&#23475;&#31995;&#32479;&#30340;&#32479;&#35745;&#21151;&#29575;($1-\beta$)&#21487;&#20197;&#24314;&#31435;&#20449;&#20219;&#65292;&#24182;&#20943;&#23569;&#19982;&#20551;&#21457;&#29616;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#24403;&#21518;&#32493;&#31243;&#24207;&#26114;&#36149;&#26102;&#12290;&#21033;&#29992;&#31526;&#21512;&#39044;&#27979;&#21407;&#21017;&#30340;&#26041;&#27861;&#26377;&#26395;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20026;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#30456;&#24212;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#24314;&#31435;&#22312;&#20026;&#39044;&#27979;&#20219;&#21153;&#35774;&#35745;&#30340;&#33879;&#21517;&#20132;&#21449;&#19968;&#33268;&#26041;&#27861;&#20043;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20182;&#22635;&#34917;&#20102;&#22312;&#24402;&#32435;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#29615;&#22659;&#20013;&#25193;&#23637;&#20808;&#21069;&#30740;&#31350;&#30340;&#33258;&#28982;&#30740;&#31350;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16388v1 Announce Type: cross  Abstract: Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\alpha$) without compromising the statistical power ($1-\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty. This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, rel
&lt;/p&gt;</description></item><item><title>FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.15883</link><description>&lt;p&gt;
&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fusion Encoder Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15883
&lt;/p&gt;
&lt;p&gt;
FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;&#65288;FENs&#65289;&#30340;&#31639;&#27861;&#31867;&#65306;&#29992;&#20110;&#21019;&#24314;&#23558;&#22266;&#23450;&#38271;&#24230;&#24207;&#21015;&#26144;&#23556;&#21040;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20165;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#65288;&#20943;&#36731;&#25968;&#25454;&#22312;&#32593;&#32476;&#20013;&#20256;&#25773;&#26102;&#30340;&#36864;&#21270;&#65289;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65288;&#25110;&#32773;&#22312;&#20855;&#26377;&#32447;&#24615;&#22788;&#29702;&#22120;&#25968;&#37327;&#30340;&#23545;&#25968;&#26102;&#38388;&#20869;&#65289;&#12290;FENs&#30340;&#20851;&#38190;&#23646;&#24615;&#26159;&#23427;&#20204;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#24120;&#28145;&#24230;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#25928;&#26524;&#33391;&#22909;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#30446;&#21069;FENs&#30340;&#24615;&#33021;&#20165;&#20165;&#26159;&#25512;&#27979;&#65292;&#22240;&#20026;&#25105;&#20204;&#23578;&#26410;&#23454;&#29616;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15883v1 Announce Type: new  Abstract: In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;</title><link>https://arxiv.org/abs/2402.14264</link><description>&lt;p&gt;
&#21452;&#31283;&#20581;&#23398;&#20064;&#22312;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#32467;&#26500;&#19981;&#21487;&#30693;&#24615;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14264
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#26368;&#26680;&#24515;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#24191;&#27867;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20272;&#35745;&#31574;&#30053;&#65292;&#26368;&#36817;&#36824;&#32435;&#20837;&#20102;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#37319;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#32479;&#35745;&#19979;&#30028;&#32467;&#26500;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#24178;&#25200;&#20989;&#25968;&#27809;&#26377;&#32467;&#26500;&#24615;&#36136;&#20551;&#35774;&#65292;&#38500;&#20102;&#35775;&#38382;&#40657;&#30418;&#20272;&#35745;&#22120;&#20197;&#36798;&#21040;&#23567;&#35823;&#24046;&#65307;&#24403;&#21482;&#24895;&#24847;&#32771;&#34385;&#20351;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#20998;&#31867;&#31070;&#35861;&#20316;&#20026;&#40657;&#30418;&#23376;&#36807;&#31243;&#30340;&#20272;&#35745;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#23545;&#20110;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
&lt;/p&gt;</description></item><item><title>NeuroFlux&#26159;&#19968;&#20010;&#20026;&#20869;&#23384;&#21463;&#38480;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#35757;&#32451;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#36741;&#21161;&#32593;&#32476;&#21644;&#22359;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#21019;&#26032;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2402.14139</link><description>&lt;p&gt;
NeuroFlux: &#20351;&#29992;&#33258;&#36866;&#24212;&#23616;&#37096;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14139
&lt;/p&gt;
&lt;p&gt;
NeuroFlux&#26159;&#19968;&#20010;&#20026;&#20869;&#23384;&#21463;&#38480;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#35757;&#32451;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#36741;&#21161;&#32593;&#32476;&#21644;&#22359;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#21019;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#21644;&#36793;&#32536;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#35774;&#22791;&#20869;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroFlux&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#20869;&#23384;&#21463;&#38480;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#35757;&#32451;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#26426;&#36935;&#65306;&#31532;&#19968;&#26159;&#37319;&#29992;&#21487;&#21464;&#25968;&#37327;&#28388;&#27874;&#22120;&#30340;&#33258;&#36866;&#24212;&#36741;&#21161;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;GPU&#20869;&#23384;&#30340;&#20351;&#29992;&#65307;&#31532;&#20108;&#26159;&#38024;&#23545;&#22359;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#26082;&#28385;&#36275;GPU&#20869;&#23384;&#38480;&#21046;&#65292;&#21448;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14139v1 Announce Type: new  Abstract: Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the CNNs into
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13929</link><description>&lt;p&gt;
SDXL-Lightning: &#28176;&#36827;&#24335;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SDXL-Lightning: Progressive Adversarial Diffusion Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;SDXL&#30340;&#19968;&#27493;/&#20960;&#27493;1024&#20687;&#32032;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#36136;&#37327;&#21644;&#27169;&#24335;&#35206;&#30422;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29702;&#35770;&#20998;&#26512;&#12289;&#21028;&#21035;&#22120;&#35774;&#35745;&#12289;&#27169;&#22411;&#20844;&#24335;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#25105;&#20204;&#20197;LoRA&#21644;&#23436;&#25972;UNet&#26435;&#37325;&#30340;&#24418;&#24335;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;SDXL-Lightning&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12694</link><description>&lt;p&gt;
&#22797;&#20852;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21487;&#23398;&#20064;&#20998;&#35299;&#19982;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12694
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#35201;&#27714;&#31934;&#30830;&#24314;&#27169;&#38169;&#32508;&#22797;&#26434;&#27169;&#24335;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#24207;&#21015;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21160;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#36235;&#21183;&#29305;&#24449;&#24102;&#26469;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22522;&#26412;&#30340;&#31227;&#21160;&#24179;&#22343;&#26680;&#21487;&#33021;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#21644;&#22797;&#26434;&#36235;&#21183;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20998;&#35299;&#31574;&#30053;&#65292;&#26356;&#21512;&#29702;&#22320;&#25429;&#25417;&#21160;&#24577;&#36235;&#21183;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#19987;&#38376;&#29992;&#20110;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20854;&#20013;&#36890;&#36807;&#36890;&#36947;&#33258;&#27880;&#24847;&#21147;&#21644;&#33258;&#22238;&#24402;&#33258;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20843;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340; Leddam...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#20027;&#24341;&#23548;&#30340;GSR&#26694;&#26550;&#65288;SG-GSR&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#25915;&#20987;&#22270;&#20013;&#21457;&#29616;&#30340;&#24178;&#20928;&#23376;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#22686;&#24378;&#21644;&#20998;&#32452;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;GSR&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11837</link><description>&lt;p&gt;
&#33258;&#20027;&#24341;&#23548;&#30340;&#31283;&#20581;&#22270;&#32467;&#26500;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Robust Graph Structure Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#20027;&#24341;&#23548;&#30340;GSR&#26694;&#26550;&#65288;SG-GSR&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#25915;&#20987;&#22270;&#20013;&#21457;&#29616;&#30340;&#24178;&#20928;&#23376;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#22686;&#24378;&#21644;&#20998;&#32452;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;GSR&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20026;&#20102;&#25269;&#24481;&#27492;&#31867;&#25915;&#20987;&#65292;&#31283;&#20581;&#22270;&#32467;&#26500;&#20462;&#27491;&#65288;GSR&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#33410;&#28857;&#29305;&#24449;&#12289;&#22270;&#32467;&#26500;&#25110;&#22806;&#37096;&#20449;&#24687;&#26469;&#26368;&#23567;&#21270;&#23545;&#25239;&#24615;&#36793;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;GSR&#26041;&#27861;&#21463;&#21040;&#29421;&#31364;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#27604;&#22914;&#20551;&#35774;&#24178;&#20928;&#30340;&#33410;&#28857;&#29305;&#24449;&#12289;&#36866;&#24230;&#30340;&#32467;&#26500;&#25915;&#20987;&#20197;&#21450;&#21487;&#29992;&#30340;&#22806;&#37096;&#24178;&#20928;&#22270;&#65292;&#23548;&#33268;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#21463;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#20027;&#24341;&#23548;&#30340;GSR&#26694;&#26550;&#65288;SG-GSR&#65289;&#65292;&#20854;&#21033;&#29992;&#32473;&#23450;&#34987;&#25915;&#20987;&#22270;&#20013;&#21457;&#29616;&#30340;&#24178;&#20928;&#23376;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#22686;&#24378;&#21644;&#20998;&#32452;&#35757;&#32451;&#31574;&#30053;&#26469;&#22788;&#29702;&#22312;&#24178;&#20928;&#23376;&#22270;&#25552;&#21462;&#20013;&#30340;&#20004;&#20010;&#25216;&#26415;&#25361;&#25112;&#65306;1&#65289;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;2&#65289;&#33410;&#28857;&#24230;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11837v1 Announce Type: new  Abstract: Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.09963</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#22256;&#38590;?
&lt;/p&gt;
&lt;p&gt;
Why are Sensitive Functions Hard for Transformers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#23384;&#22312;&#19968;&#31995;&#21015;&#30340;&#23398;&#20064;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#22914;&#22312;&#23398;&#20064;&#35745;&#31639;&#31616;&#21333;&#24418;&#24335;&#35821;&#35328;&#65288;&#22914;PARITY&#65289;&#26102;&#30340;&#25345;&#20037;&#22256;&#38590;&#65292;&#20197;&#21450;&#23545;&#20302;&#38454;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#29702;&#35770;&#35201;&#20040;&#36807;&#24230;&#39044;&#27979;&#65292;&#35201;&#20040;&#20302;&#20272;&#20102;&#23454;&#38469;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65306;&#36755;&#20986;&#23545;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#22810;&#20010;&#37096;&#20998;&#25935;&#24863;&#30340;Transformer&#23384;&#22312;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#23396;&#31435;&#28857;&#65292;&#23548;&#33268;&#27867;&#21270;&#20013;&#30340;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#65292;&#22914;&#23427;&#20204;&#23545;&#20302;&#25935;&#24863;&#24615;&#21644;&#20302;&#38454;&#30340;&#27867;&#21270;&#20559;&#24046;&#65292;&#20197;&#21450;&#22312;&#38271;&#24230;&#27867;&#21270;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09963v1 Announce Type: new  Abstract: Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalizati
&lt;/p&gt;</description></item><item><title>MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09782</link><description>&lt;p&gt;
MC-DBN&#65306;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MC-DBN: A Deep Belief Network-Based Model for Modality Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09782
&lt;/p&gt;
&lt;p&gt;
MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#39046;&#22495;&#12290;&#21033;&#29992;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#25968;&#25454;&#21487;&#33021;&#19981;&#24635;&#26159;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#21563;&#21512;&#12290;&#25554;&#20540;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20294;&#22312;&#31232;&#30095;&#20449;&#24687;&#24773;&#20917;&#19979;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#34917;&#20840;&#30340;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#27169;&#22411;&#65288;MC-DBN&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#33258;&#36523;&#19982;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#30830;&#20445;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29305;&#24615;&#23494;&#20999;&#30456;&#31526;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26469;&#33258;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;MC-DBN&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09782v1 Announce Type: cross  Abstract: Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;(SEV)&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;&#21363;&#20351;&#27169;&#22411;&#19981;&#26159;&#31232;&#30095;&#30340;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20173;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09702</link><description>&lt;p&gt;
&#26080;&#38656;&#31232;&#30095;&#27169;&#22411;&#30340;&#31232;&#30095;&#19988;&#20934;&#30830;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Sparse and Faithful Explanations Without Sparse Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09702
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;(SEV)&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;&#21363;&#20351;&#27169;&#22411;&#19981;&#26159;&#31232;&#30095;&#30340;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20173;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#27169;&#22411;&#19981;&#28385;&#36275;&#20840;&#23616;&#30340;&#31232;&#30095;&#24615;&#65292;&#20915;&#31574;&#20173;&#28982;&#21487;&#20197;&#29992;&#23569;&#37327;&#30340;&#29305;&#24449;&#20934;&#30830;&#22320;&#25551;&#36848;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#26576;&#20154;&#32780;&#35328;&#65292;&#23613;&#31649;&#27809;&#26377;&#20449;&#29992;&#21382;&#21490;&#65292;&#20294;&#30003;&#35831;&#22823;&#31508;&#36151;&#27454;&#21487;&#33021;&#20250;&#34987;&#25298;&#32477;&#65292;&#36825;&#23601;&#24573;&#35270;&#20102;&#19982;&#20854;&#20449;&#29992;&#20215;&#20540;&#30456;&#20851;&#30340;&#20219;&#20309;&#35777;&#25454;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;&#65288;SEV&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31232;&#30095;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20197;&#19978;&#36151;&#27454;&#25298;&#32477;&#30340;&#20363;&#23376;&#20013;&#65292;SEV&#20026;1&#65292;&#22240;&#20026;&#21482;&#38656;&#35201;&#19968;&#20010;&#22240;&#32032;&#26469;&#35299;&#37322;&#20026;&#20160;&#20040;&#36151;&#27454;&#34987;&#25298;&#32477;&#12290;SEV&#26159;&#23545;&#20915;&#31574;&#31232;&#30095;&#24615;&#30340;&#34913;&#37327;&#65292;&#32780;&#19981;&#26159;&#23545;&#25972;&#20307;&#27169;&#22411;&#31232;&#30095;&#24615;&#30340;&#34913;&#37327;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21363;&#20351;&#23427;&#20204;&#19981;&#26159;&#31232;&#30095;&#30340;&#8212;&#8212;&#23454;&#38469;&#19978;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;SEV&#20351;&#29992;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#31227;&#21160;&#36827;&#34892;&#23450;&#20041;&#65292;&#20351;&#24471;SEV&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#21035;&#19978;&#19968;&#33268;&#22320;&#23450;&#20041;&#65292;&#20854;&#20013;&#31227;&#21160;&#38480;&#21046;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09702v1 Announce Type: new  Abstract: Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features. For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness. In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models. In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied. SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV. SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflectin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08640</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#39044;&#27979;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Forecasting high-impact research topics via machine learning on evolving knowledge graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08640
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26500;&#25104;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#23427;&#36843;&#20351;&#30740;&#31350;&#32773;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#26356;&#29421;&#31364;&#30340;&#23376;&#39046;&#22495;&#19978;&#65292;&#20351;&#24471;&#21457;&#29616;&#20854;&#20182;&#39046;&#22495;&#30340;&#26032;&#39062;&#19988;&#26377;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#21512;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#34429;&#28982;&#26377;&#21150;&#27861;&#39044;&#27979;&#31185;&#23398;&#35770;&#25991;&#26410;&#26469;&#30340;&#24341;&#29992;&#27425;&#25968;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#31561;&#21040;&#30740;&#31350;&#23436;&#25104;&#24182;&#19988;&#35770;&#25991;&#20889;&#25104;&#21518;&#25165;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26679;&#23601;&#38169;&#36807;&#20102;&#24819;&#27861;&#26500;&#24605;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39044;&#27979;&#20174;&#26410;&#34987;&#30740;&#31350;&#32773;&#21457;&#24067;&#30340;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#12290;&#23427;&#32467;&#21512;&#20102;&#20174;&#35770;&#25991;&#20869;&#23481;&#20013;&#21019;&#24314;&#30340;&#35821;&#20041;&#32593;&#32476;&#21644;&#20174;&#21382;&#21490;&#24341;&#29992;&#20013;&#21019;&#24314;&#30340;&#24433;&#21709;&#32593;&#32476;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#28436;&#21270;&#32593;&#32476;&#30340;&#21160;&#24577;&#24773;&#20917;&#65292;&#20174;&#32780;&#39044;&#27979;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#39044;&#26399;&#36825;&#31181;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#32773;&#21457;&#29616;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07388</link><description>&lt;p&gt;
&#26080;&#20551;&#35774;&#27979;&#35797;&#31639;&#27861;&#24615;&#33021;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Limits of Assumption-free Tests for Algorithm Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07388
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#35780;&#20215;&#21644;&#27604;&#36739;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#19968;&#20010;&#31639;&#27861;&#22312;&#32473;&#23450;&#30340;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#65292;&#21738;&#20010;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65311;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#65292;&#36890;&#24120;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;&#23558;&#24863;&#20852;&#36259;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#30041;&#20986;&#25968;&#25454;&#28857;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#31243;&#24207;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#19979;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#26412;&#38480;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#20010;&#38382;&#39064;: &#31639;&#27861;$A$&#22312;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#38598;&#19978;&#23398;&#20064;&#38382;&#39064;&#26377;&#22810;&#22909;&#65292;&#20197;&#21450;&#22312;&#29305;&#23450;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;$A$&#25152;&#20135;&#29983;&#30340;&#29305;&#23450;&#25311;&#21512;&#27169;&#22411;&#26377;&#22810;&#22909;&#65311;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#23558;&#31639;&#27861;&#35270;&#20026;&#40657;&#30418;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#26080;&#27861;&#20934;&#30830;&#22320;&#22238;&#31572;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06104</link><description>&lt;p&gt;
&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#26126;&#30830;&#23398;&#20064;&#20989;&#25968;&#23548;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22238;&#24402;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#27599;&#20010;&#20010;&#20307;&#25968;&#25454;&#26679;&#26412;&#30340;&#30495;&#23454;&#20540;&#23545;&#40784;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#20851;&#31995;&#30340;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24037;&#20316;&#24341;&#20837;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#25913;&#36827;&#22238;&#24402;&#26041;&#27861;&#65292;&#20294;&#22312;&#23436;&#20840;&#25429;&#25417;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAR&#65288;&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65289;&#20316;&#20026;&#19968;&#31181;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#39046;&#22495;&#30340;&#20843;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.05663</link><description>&lt;p&gt;
&#23454;&#26102;&#29942;&#39048;&#21644;&#28608;&#27874;&#39044;&#27979;&#30340;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#22312;&#20132;&#36890;&#25511;&#21046;&#30740;&#31350;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;CIRCLES&#32852;&#21512;&#39033;&#30446;&#38656;&#35201;&#39044;&#27979;&#25216;&#26415;&#26469;&#20943;&#36731;&#25968;&#25454;&#28304;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#22312;MegaVanderTest&#23454;&#39564;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#24403;&#21069;&#31995;&#32479;&#38480;&#21046;&#65292;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#19979;&#19968;&#36718;&#23454;&#39564;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SA-LSTM&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#22312;&#31354;&#38388;&#32500;&#24230;&#19978;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27493;&#39044;&#27979;&#65292;&#20351;&#29992;n-step SA-LSTM&#65292;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#30340;&#24179;&#34913;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#22810;&#27493;&#39044;&#27979;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.05448</link><description>&lt;p&gt;
Minecraft-ify&#65306;&#29992;&#20110;&#28216;&#25103;&#24212;&#29992;&#30340;Minecraft&#39118;&#26684;&#22270;&#20687;&#29983;&#25104;&#19982;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#38754;&#21521;Minecraft&#35270;&#39057;&#28216;&#25103;&#30340;&#35282;&#33394;&#32441;&#29702;&#29983;&#25104;&#31995;&#32479;"Minecraft-ify"&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#38024;&#23545;&#20855;&#26377;&#31435;&#26041;&#20307;&#27969;&#24418;&#30340;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#20197;&#36827;&#34892;&#32441;&#29702;&#26144;&#23556;&#12290;&#19982;&#29616;&#26377;&#39033;&#30446;&#25110;&#20316;&#21697;&#21482;&#29983;&#25104;&#32441;&#29702;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#21453;&#36716;&#29992;&#25143;&#25552;&#20379;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#25110;&#20174;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#24179;&#22343;/&#38543;&#26426;&#22806;&#35266;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;StyleGAN&#21644;StyleCLIP&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#25805;&#20316;&#12290;&#36825;&#20123;&#21151;&#33021;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#26356;&#22810;&#30340;&#33258;&#30001;&#65292;&#26159;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;AI&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#24335;&#65292;&#39318;&#27425;&#23581;&#35797;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#21407;&#22987;&#22270;&#35889;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05011</link><description>&lt;p&gt;
&#23548;&#33322;&#22797;&#26434;&#24615;&#65306;&#36890;&#36807;&#25193;&#23637;&#31383;&#21475;&#21305;&#37197;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;
&lt;/p&gt;
&lt;p&gt;
Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#24335;&#65292;&#39318;&#27425;&#23581;&#35797;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#21407;&#22987;&#22270;&#35889;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35889;&#31934;&#31616;&#26088;&#22312;&#36890;&#36807;&#21512;&#25104;&#32039;&#20945;&#30340;&#22270;&#35889;&#26469;&#20943;&#23569;&#22823;&#35268;&#27169;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20026;&#20943;&#23569;&#35757;&#32451;GNNs&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#21407;&#22987;&#22270;&#35889;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#26080;&#25439;&#31934;&#31616;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#65292;&#24182;&#25581;&#31034;&#20102;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#21305;&#37197;&#26041;&#27861;&#22312;&#20248;&#21270;&#31934;&#31616;&#22270;&#35889;&#26102;&#25552;&#20379;&#20102;&#26469;&#33258;&#21407;&#22987;&#22270;&#35889;&#30340;&#20559;&#20506;&#21644;&#21463;&#38480;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#31934;&#31616;&#22270;&#35889;&#30340;&#35268;&#27169;&#21644;&#21151;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03496</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21435;&#25481;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21527;&#65311;&#19968;&#20010;&#20108;&#38454;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03496
&lt;/p&gt;
&lt;p&gt;
&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#22914;Adam(W)&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65288;&#22914;transformers&#65289;&#30340;&#40664;&#35748;&#35757;&#32451;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#23545;&#35282;&#20808;&#39564;&#22522;&#20110;&#26799;&#24230;&#22806;&#31215;&#65292;&#36890;&#36807;&#24179;&#26041;&#26681;&#21152;&#20837;&#21040;&#21442;&#25968;&#26356;&#26032;&#20013;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#36817;&#20284;&#30340;&#20108;&#38454;&#26041;&#27861;&#65292;&#20294;&#24179;&#26041;&#26681;&#34920;&#31034;&#20102;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21435;&#25481;&#24179;&#26041;&#26681;&#21518;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#34892;&#20026;&#22914;&#20309;&#21464;&#21270;&#65292;&#21363;&#21152;&#24378;&#23427;&#20204;&#30340;&#20108;&#38454;&#21160;&#26426;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21435;&#25481;&#24179;&#26041;&#26681;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#33021;&#22815;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#32553;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22312;transformers&#19978;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20108;&#38454;&#35282;&#24230;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;&#23545;&#35282;&#20808;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#20063;&#20855;&#26377;&#23454;&#38469;&#22909;&#22788;&#12290;&#19982;&#20687;Shampoo&#36825;&#26679;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#23545;&#24212;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#19981;&#38656;&#35201;&#25968;&#20540;&#19981;&#31283;&#23450;&#30340;&#30697;&#38453;&#24179;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02564</link><description>&lt;p&gt;
&#19968;&#20010;&#30495;&#27491;&#32852;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20998;&#21106;&#21644;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Truly Joint Neural Architecture for Segmentation and Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#22810;&#35821;&#35328;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#21487;&#20197;&#35299;&#26512;&#22810;&#31181;&#35821;&#35328;&#65292;&#20294;&#23545;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#20854;&#24615;&#33021;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#30001;&#20110;&#36755;&#20837;&#26631;&#35760;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21644;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#20316;&#20026;&#26641;&#20013;&#33410;&#28857;&#30340;&#35821;&#35328;&#21333;&#20301;&#20107;&#20808;&#26159;&#26410;&#30693;&#30340;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#36981;&#24490;&#32852;&#21512;&#24418;&#24577;-&#21477;&#27861;&#20551;&#35774;&#65292;&#21363;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#24212;&#35813;&#22312;&#35299;&#26512;&#36807;&#31243;&#20013;&#19968;&#24182;&#35299;&#20915;&#65292;&#32780;&#19981;&#26159;&#20808;&#36827;&#34892;&#20998;&#21106;&#20877;&#36827;&#34892;&#35299;&#26512;&#30340;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#37319;&#29992;&#20005;&#26684;&#30340;&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#20445;&#30041;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#25552;&#20379;&#32473;&#19968;&#20010;&#22522;&#20110;&#24359;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#24076;&#20271;&#26469;&#35821;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#35821;&#35328;&#24418;&#24577;&#20016;&#23500;&#19988;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and
&lt;/p&gt;</description></item><item><title>LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.02446</link><description>&lt;p&gt;
LQER: &#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#37325;&#24314;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
LQER: Low-Rank Quantization Error Reconstruction for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02446
&lt;/p&gt;
&lt;p&gt;
LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#20943;&#23569;&#65288;LQER&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#37327;&#21270;&#21644;&#20302;&#31209;&#36924;&#36817;&#26469;&#24674;&#22797;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;LQER&#21033;&#29992;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#23558;&#37327;&#21270;&#35823;&#24046;&#30340;&#22855;&#24322;&#20540;&#20998;&#24067;&#25512;&#21521;&#26399;&#26395;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;LLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#36817;&#20046;&#26080;&#25439;&#30340;W4A8&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#12289;&#32593;&#26684;&#25628;&#32034;&#25110;&#22522;&#20110;&#26799;&#24230;&#30340;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;LQER&#30340;&#35745;&#31639;&#27169;&#24335;&#28040;&#38500;&#20102;&#20174;&#19981;&#35268;&#21017;&#20869;&#23384;&#20301;&#32622;&#25910;&#38598;&#39640;&#31934;&#24230;&#26435;&#37325;&#25152;&#38656;&#30340;&#19987;&#29992;Scatter&#21644;Gather&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;W4A8 LLMs&#22312;&#20845;&#20010;&#28909;&#38376;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#30828;&#20214;&#36164;&#28304;&#27604;&#39046;&#20808;&#30340;&#26368;&#26032;&#26041;&#27861;&#23569;1.36&#20493;&#12290;&#19968;&#26086;&#35770;&#25991;&#34987;&#25509;&#21463;&#65292;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;</title><link>https://arxiv.org/abs/2402.02042</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDP&#36827;&#34892;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#39046;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#30740;&#31350;&#20855;&#26377;&#36890;&#29992;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#24179;&#22343;&#22238;&#25253;CMDP&#30340;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#30830;&#20445;&#20302;&#36951;&#25022;&#20445;&#35777;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#19978;&#20855;&#26377; $\tilde{\mathcal{O}}({T}^{3/4})$ &#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#26680;&#24515;&#38598;&#36873;&#25321;&#26041;&#27861;&#26410;&#33021;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29305;&#24449;&#32500;&#24230;&#32467;&#26500;&#20013;&#32500;&#24230;&#20043;&#38388;&#24046;&#24322;&#23545;&#26368;&#32456;&#30456;&#20284;&#24615;&#30340;&#36129;&#29486;&#65292;&#23548;&#33268;&#22810;&#26679;&#24615;&#26679;&#26412;&#36873;&#25321;&#30340;&#32467;&#26524;&#27425;&#20248;&#12290;</title><link>https://arxiv.org/abs/2401.16193</link><description>&lt;p&gt;
&#20026;&#26680;&#24515;&#38598;&#36873;&#25321;&#36129;&#29486;&#30340;&#28145;&#24230;&#29305;&#24449;&#32500;&#24230;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Contributing Dimension Structure of Deep Feature for Coreset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16193
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26680;&#24515;&#38598;&#36873;&#25321;&#26041;&#27861;&#26410;&#33021;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29305;&#24449;&#32500;&#24230;&#32467;&#26500;&#20013;&#32500;&#24230;&#20043;&#38388;&#24046;&#24322;&#23545;&#26368;&#32456;&#30456;&#20284;&#24615;&#30340;&#36129;&#29486;&#65292;&#23548;&#33268;&#22810;&#26679;&#24615;&#26679;&#26412;&#36873;&#25321;&#30340;&#32467;&#26524;&#27425;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#24515;&#38598;&#36873;&#25321;&#26088;&#22312;&#20026;&#39640;&#25928;&#23398;&#20064;&#36873;&#25321;&#19968;&#32452;&#20851;&#38190;&#35757;&#32451;&#26679;&#26412;&#12290;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#28608;&#22686;&#65292;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#26679;&#26412;&#36873;&#25321;&#21462;&#20915;&#20110;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65306;&#26679;&#26412;&#22312;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#34920;&#31034;&#20197;&#21450;&#26679;&#26412;&#22810;&#26679;&#24615;&#22312;&#36991;&#20813;&#36807;&#25311;&#21512;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#35832;&#22914;L2&#33539;&#25968;&#20043;&#31867;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#34913;&#37327;&#25968;&#25454;&#30340;&#34920;&#31034;&#21644;&#22810;&#26679;&#24615;&#12290;&#23427;&#20204;&#36890;&#36807;&#29305;&#24449;&#12289;&#26799;&#24230;&#25110;&#20854;&#20182;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#24341;&#23548;&#20998;&#24067;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#34920;&#31034;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22810;&#26679;&#24615;&#26679;&#26412;&#36873;&#25321;&#30340;&#32467;&#26524;&#21364;&#21463;&#22256;&#20110;&#27425;&#20248;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#30456;&#20284;&#24615;&#24230;&#37327;&#36890;&#24120;&#20165;&#20165;&#32858;&#21512;&#32500;&#24230;&#30456;&#20284;&#24615;&#65292;&#32780;&#26410;&#24847;&#35782;&#21040;&#23545;&#20110;&#26368;&#32456;&#30456;&#20284;&#24615;&#36129;&#29486;&#26174;&#33879;&#30340;&#32500;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#33021;&#36798;&#21040;&#20805;&#20998;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16193v2 Announce Type: replace  Abstract: Coreset selection seeks to choose a subset of crucial training samples for efficient learning. It has gained traction in deep learning, particularly with the surge in training dataset sizes. Sample selection hinges on two main aspects: a sample's representation in enhancing performance and the role of sample diversity in averting overfitting. Existing methods typically measure both the representation and diversity of data based on similarity metrics, such as L2-norm. They have capably tackled representation via distribution matching guided by the similarities of features, gradients, or other information between data. However, the results of effectively diverse sample selection are mired in sub-optimality. This is because the similarity metrics usually simply aggregate dimension similarities without acknowledging disparities among the dimensions that significantly contribute to the final similarity. As a result, they fall short of ade
&lt;/p&gt;</description></item><item><title>INCPrompt&#37319;&#29992;&#33258;&#36866;&#24212;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#32467;&#21512;&#36890;&#29992;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#26377;&#25928;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#34920;&#29616;&#20248;&#36234;&#65292;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.11667</link><description>&lt;p&gt;
INCPrompt&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#22686;&#37327;&#25552;&#31034;&#65292;&#26080;&#38656;&#37325;&#22797;&#32451;&#20064;&#30340;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11667
&lt;/p&gt;
&lt;p&gt;
INCPrompt&#37319;&#29992;&#33258;&#36866;&#24212;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#32467;&#21512;&#36890;&#29992;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#26377;&#25928;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#34920;&#29616;&#20248;&#36234;&#65292;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INCPrompt&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290; INCPrompt&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290; &#36825;&#31181;&#29420;&#29305;&#32452;&#21512;&#23553;&#35013;&#20102;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#30693;&#35782;&#24182;&#32534;&#30721;&#20102;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290; &#25105;&#20204;&#22312;&#22810;&#20010;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;INCPrompt&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290; &#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#38754;&#21521;&#20219;&#21153;&#30340;&#22686;&#37327;&#25552;&#31034;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11667v2 Announce Type: replace  Abstract: This paper introduces INCPrompt, an innovative continual learning solution that effectively addresses catastrophic forgetting. INCPrompt's key innovation lies in its use of adaptive key-learner and task-aware prompts that capture task-relevant information. This unique combination encapsulates general knowledge across tasks and encodes task-specific knowledge. Our comprehensive evaluation across multiple continual learning benchmarks demonstrates INCPrompt's superiority over existing algorithms, showing its effectiveness in mitigating catastrophic forgetting while maintaining high performance. These results highlight the significant impact of task-aware incremental prompting on continual learning performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;GNNInfer&#65292;&#36825;&#26159;&#29992;&#20110;GNN&#30340;&#39318;&#20010;&#33258;&#21160;&#23646;&#24615;&#25512;&#26029;&#25216;&#26415;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#36716;&#25442;&#20195;&#34920;&#24615;&#24433;&#21709;&#32467;&#26500;&#65292;&#25429;&#25417;&#24182;&#25512;&#24191;GNN&#29305;&#23450;&#23646;&#24615;&#65292;&#26368;&#32456;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#25552;&#39640;&#25512;&#26029;&#23646;&#24615;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.03790</link><description>&lt;p&gt;
&#25512;&#26029;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Inferring Properties of Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03790
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GNNInfer&#65292;&#36825;&#26159;&#29992;&#20110;GNN&#30340;&#39318;&#20010;&#33258;&#21160;&#23646;&#24615;&#25512;&#26029;&#25216;&#26415;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#36716;&#25442;&#20195;&#34920;&#24615;&#24433;&#21709;&#32467;&#26500;&#65292;&#25429;&#25417;&#24182;&#25512;&#24191;GNN&#29305;&#23450;&#23646;&#24615;&#65292;&#26368;&#32456;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#25552;&#39640;&#25512;&#26029;&#23646;&#24615;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;GNNInfer&#65292;&#36825;&#26159;&#29992;&#20110;GNN&#30340;&#39318;&#20010;&#33258;&#21160;&#23646;&#24615;&#25512;&#26029;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;GNN&#20013;&#21487;&#21464;&#36755;&#20837;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;GNNInfer&#39318;&#20808;&#35782;&#21035;&#19968;&#32452;&#23545;GNN&#39044;&#27979;&#26377;&#26174;&#33879;&#36129;&#29486;&#30340;&#20195;&#34920;&#24615;&#24433;&#21709;&#32467;&#26500;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#65292;GNNInfer&#23558;&#27599;&#23545;&#24433;&#21709;&#32467;&#26500;&#21644;GNN&#36716;&#25442;&#20026;&#23427;&#20204;&#31561;&#25928;&#30340;FNN&#65292;&#28982;&#21518;&#21033;&#29992;&#29616;&#26377;&#30340;&#23646;&#24615;&#25512;&#26029;&#25216;&#26415;&#26377;&#25928;&#25429;&#25417;&#19982;&#36825;&#20123;&#24433;&#21709;&#32467;&#26500;&#29305;&#23450;&#30340;GNN&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;GNNInfer&#23558;&#25429;&#33719;&#30340;&#23646;&#24615;&#25512;&#24191;&#21040;&#21253;&#21547;&#36825;&#20123;&#24433;&#21709;&#32467;&#26500;&#30340;&#20219;&#20309;&#36755;&#20837;&#22270;&#20013;&#12290;&#26368;&#21518;&#65292;GNNInfer&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#27169;&#22411;&#65288;&#20915;&#31574;&#26641;&#25110;&#32447;&#24615;&#22238;&#24402;&#65289;&#26469;&#25552;&#39640;&#25512;&#26029;&#23646;&#24615;&#30340;&#27491;&#30830;&#24615;&#65292;&#35813;&#27169;&#22411;&#20272;&#35745;&#32473;&#23450;&#23436;&#25972;&#36755;&#20837;&#22270;&#26102;GNN&#36755;&#20986;&#19982;&#25512;&#26029;&#23646;&#24615;&#30340;&#20559;&#24046;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;GNNInfer&#25193;&#23637;&#25512;&#26029;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03790v2 Announce Type: replace  Abstract: We propose GNNInfer, the first automatic property inference technique for GNNs. To tackle the challenge of varying input structures in GNNs, GNNInfer first identifies a set of representative influential structures that contribute significantly towards the prediction of a GNN. Using these structures, GNNInfer converts each pair of an influential structure and the GNN to their equivalent FNN and then leverages existing property inference techniques to effectively capture properties of the GNN that are specific to the influential structures. GNNINfer then generalizes the captured properties to any input graphs that contain the influential structures. Finally, GNNInfer improves the correctness of the inferred properties by building a model (either a decision tree or linear regression) that estimates the deviation of GNN output from the inferred properties given full input graphs. The learned model helps GNNInfer extend the inferred prope
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23402;&#29983;&#32467;&#26500;&#23545;GPT-3&#23884;&#20837;&#36827;&#34892;&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25216;&#26415;&#24086;&#23376;&#30340;&#37325;&#22797;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.15068</link><description>&lt;p&gt;
&#20351;&#29992;&#23402;&#29983;&#32467;&#26500;&#23545;GPT-3&#23884;&#20837;&#36827;&#34892;&#32454;&#21270;&#65292;&#29992;&#20110;&#25216;&#26415;&#24086;&#23376;&#30340;&#37325;&#22797;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Refining GPT-3 Embeddings with a Siamese Structure for Technical Post Duplicate Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23402;&#29983;&#32467;&#26500;&#23545;GPT-3&#23884;&#20837;&#36827;&#34892;&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25216;&#26415;&#24086;&#23376;&#30340;&#37325;&#22797;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#22312;&#32447;&#31038;&#21306;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#24110;&#21161;&#24320;&#21457;&#32773;&#22312;&#19968;&#20010;&#22320;&#26041;&#25214;&#21040;&#27491;&#30830;&#31572;&#26696;&#12290;&#19968;&#20010;&#38382;&#39064;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21644;&#25514;&#36766;&#34987;&#25552;&#20986;&#65292;&#23548;&#33268;&#25216;&#26415;&#35770;&#22363;&#19978;&#23384;&#22312;&#37325;&#22797;&#24086;&#23376;&#12290;&#22914;&#20309;&#21457;&#29616;&#21644;&#38142;&#25509;&#37325;&#22797;&#24086;&#23376;&#24341;&#36215;&#24320;&#21457;&#32773;&#31038;&#21306;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;Stack Overflow&#37319;&#29992;&#22522;&#20110;&#25237;&#31080;&#30340;&#26426;&#21046;&#26469;&#26631;&#35760;&#21644;&#20851;&#38381;&#37325;&#22797;&#24086;&#23376;&#12290;&#28982;&#32780;&#65292;&#21450;&#26102;&#22788;&#29702;&#36825;&#20123;&#19981;&#26029;&#20986;&#29616;&#30340;&#37325;&#22797;&#24086;&#23376;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#26816;&#27979;&#25216;&#26415;&#35770;&#22363;&#24086;&#23376;&#20013;&#30340;&#37325;&#22797;&#24086;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#24086;&#23376;&#30340;&#35821;&#20041;&#65292;&#35201;&#20040;&#32570;&#20047;&#30417;&#30563;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15068v2 Announce Type: replace-cross  Abstract: One goal of technical online communities is to help developers find the right answer in one place. A single question can be asked in different ways with different wordings, leading to the existence of duplicate posts on technical forums. The question of how to discover and link duplicate posts has garnered the attention of both developer communities and researchers. For example, Stack Overflow adopts a voting-based mechanism to mark and close duplicate posts. However, addressing these constantly emerging duplicate posts in a timely manner continues to pose challenges. Therefore, various approaches have been proposed to detect duplicate posts on technical forum posts automatically. The existing methods suffer from limitations either due to their reliance on handcrafted similarity metrics which can not sufficiently capture the semantics of posts, or their lack of supervision to improve the performance. Additionally, the efficienc
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; Hutchinson &#36857;&#20272;&#35745;&#65288;HTE&#65289;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010; Hessian &#30697;&#38453;&#30340;&#35745;&#31639;&#36716;&#25442;&#20026; Hessian &#30690;&#37327;&#20056;&#31215;&#65288;HVP&#65289;&#65292;&#35299;&#20915;&#20102; PINNs &#22788;&#29702;&#39640;&#32500;&#21644;&#39640;&#38454; PDE &#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.14499</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#39640;&#38454;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340; Hutchinson &#36857;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14499
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; Hutchinson &#36857;&#20272;&#35745;&#65288;HTE&#65289;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010; Hessian &#30697;&#38453;&#30340;&#35745;&#31639;&#36716;&#25442;&#20026; Hessian &#30690;&#37327;&#20056;&#31215;&#65288;HVP&#65289;&#65292;&#35299;&#20915;&#20102; PINNs &#22788;&#29702;&#39640;&#32500;&#21644;&#39640;&#38454; PDE &#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14499v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#20195;&#20132;&#21449; &#25688;&#35201;&#65306;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#24403;&#19968;&#20123;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#36890;&#36807;&#26080;&#32541;&#34701;&#21512;&#25968;&#25454;&#21644;&#29289;&#29702;&#23398;&#12290;&#28982;&#32780;&#65292;&#23558;PINNs&#25193;&#23637;&#21040;&#39640;&#32500;&#29978;&#33267;&#39640;&#38454;PDE&#22312;&#33258;&#21160;&#24494;&#20998;&#22312;&#27531;&#24046;&#25439;&#22833;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Hutchinson &#36857;&#20272;&#35745;&#65288;HTE&#65289;&#26469;&#35299;&#20915;PINNs&#22788;&#29702;&#39640;&#32500;&#21644;&#39640;&#38454;PDE&#30340;&#23616;&#38480;&#24615;&#12290;&#20174;&#31185;&#23398;&#35745;&#31639;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20108;&#38454;&#39640;&#32500;PDE&#20837;&#25163;&#65292;HTE&#23558;&#25972;&#20010;Hessian&#30697;&#38453;&#30340;&#35745;&#31639;&#36716;&#25442;&#20026;Hessian&#30690;&#37327;&#20056;&#31215;&#65288;HVP&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807; Taylor &#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20943;&#36731;&#20102;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#23558;&#20869;&#23384;&#28040;&#32791;&#20174;Hessian&#30697;&#38453;&#20943;&#23569;&#21040;HVP&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;HTE&#25910;&#25947;&#21040;&#25110;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14499v2 Announce Type: replace-cross  Abstract: Physics-Informed Neural Networks (PINNs) have proven effective in solving partial differential equations (PDEs), especially when some data are available by seamlessly blending data and physics. However, extending PINNs to high-dimensional and even high-order PDEs encounters significant challenges due to the computational cost associated with automatic differentiation in the residual loss. Herein, we address the limitations of PINNs in handling high-dimensional and high-order PDEs by introducing Hutchinson Trace Estimation (HTE). Starting with the second-order high-dimensional PDEs ubiquitous in scientific computing, HTE transforms the calculation of the entire Hessian matrix into a Hessian vector product (HVP). This approach alleviates the computational bottleneck via Taylor-mode automatic differentiation and significantly reduces memory consumption from the Hessian matrix to HVP. We further showcase HTE's convergence to the or
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.11436</link><description>&lt;p&gt;
&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#22823;&#33041;&#30382;&#23618;V2&#21306;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layerwise complexity-matched learning yields an improved model of cortical area V2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11436
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35782;&#21035;&#22797;&#26434;&#35270;&#35273;&#27169;&#24335;&#30340;&#33021;&#21147;&#26159;&#36890;&#36807;&#39034;&#27425;&#21306;&#22495;&#22312;&#33145;&#20391;&#35270;&#35273;&#30382;&#23618;&#20013;&#25191;&#34892;&#30340;&#21464;&#25442;&#25152;&#24418;&#25104;&#30340;&#12290;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#23618;&#27425;&#32467;&#26500;&#30340;&#21518;&#26399;&#31070;&#32463;&#21453;&#24212;&#30340;&#26368;&#20339;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#25163;&#24037;&#35774;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#25110;&#32773;&#19982;&#20248;&#21270;&#32534;&#30721;&#25928;&#29575;&#25110;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#23545;&#21069;&#26399;&#38454;&#27573;&#25552;&#20379;&#20102;&#36739;&#24046;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#29983;&#29289;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#29420;&#31435;&#22320;&#20316;&#29992;&#20110;&#36830;&#32493;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#20102;&#23545;&#23616;&#37096;&#21464;&#24418;&#33258;&#28982;&#22270;&#20687;&#34917;&#19969;&#23545;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#37319;&#26679;&#33258;&#20854;&#20182;&#20301;&#32622;&#30340;&#34917;&#19969;&#26102;&#20351;&#29305;&#24449;&#21435;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11436v2 Announce Type: replace-cross  Abstract: Human ability to recognize complex visual patterns arises through transformations performed by successive areas in the ventral visual cortex. Deep neural networks trained end-to-end for object recognition approach human capabilities, and offer the best descriptions to date of neural responses in the late stages of the hierarchy. But these networks provide a poor account of the early stages, compared to traditional hand-engineered models, or models optimized for coding efficiency or prediction. Moreover, the gradient backpropagation used in end-to-end learning is generally considered to be biologically implausible. Here, we overcome both of these limitations by developing a bottom-up self-supervised training methodology that operates independently on successive layers. Specifically, we maximize feature similarity between pairs of locally-deformed natural image patches, while decorrelating features across patches sampled from oth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26377;&#38480;&#32500;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#21487;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#65292;&#25552;&#39640;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.07358</link><description>&lt;p&gt;
&#22522;&#20110;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Distributional Bellman Operators over Mean Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26377;&#38480;&#32500;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#21487;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#65292;&#25552;&#39640;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26377;&#38480;&#32500;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#12290; &#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#21160;&#24577;&#35268;&#21010;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#25910;&#25947;&#29702;&#35770;&#65292;&#24182;&#23545;&#36825;&#20123;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#34920;&#26684;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#34920;&#29616;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#32467;&#21512;&#65292;&#24471;&#21040;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#22312; Arcade Learning Environment &#19978;&#20248;&#20110;&#22522;&#32447;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07358v2 Announce Type: replace-cross  Abstract: We propose a novel algorithmic framework for distributional reinforcement learning, based on learning finite-dimensional mean embeddings of return distributions. We derive several new algorithms for dynamic programming and temporal-difference learning based on this framework, provide asymptotic convergence theory, and examine the empirical performance of the algorithms on a suite of tabular tasks. Further, we show that this approach can be straightforwardly combined with deep reinforcement learning, and obtain a new deep RL agent that improves over baseline distributional approaches on the Arcade Learning Environment.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23398;&#29983;t&#20998;&#24067;&#21644;&#24130;&#20998;&#27495;&#65292;&#25552;&#20986;&#20102;$t^3$VAE&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#37325;&#23614;&#25968;&#25454;&#65292;&#24182;&#25512;&#23548;&#20986;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.01133</link><description>&lt;p&gt;
$t^3$-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#23398;&#29983;t&#20998;&#24067;&#21644;&#24130;&#20998;&#27495;&#23398;&#20064;&#37325;&#23614;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01133
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23398;&#29983;t&#20998;&#24067;&#21644;&#24130;&#20998;&#27495;&#65292;&#25552;&#20986;&#20102;$t^3$VAE&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#37325;&#23614;&#25968;&#25454;&#65292;&#24182;&#25512;&#23548;&#20986;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#36890;&#24120;&#37319;&#29992;&#26631;&#20934;&#27491;&#24577;&#20808;&#39564;&#20316;&#20026;&#27010;&#29575;&#28508;&#22312;&#32534;&#30721;&#22120;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#39640;&#26031;&#23614;&#37096;&#24448;&#24448;&#34928;&#20943;&#24471;&#22826;&#24555;&#65292;&#26080;&#27861;&#26377;&#25928;&#23481;&#32435;&#32534;&#30721;&#28857;&#65292;&#26080;&#27861;&#20445;&#30041;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20851;&#38190;&#32467;&#26500;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#37325;&#23614;&#27169;&#22411;&#26469;&#25269;&#25239;&#36807;&#24230;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12290;&#20511;&#37492;&#20449;&#24687;&#20960;&#20309;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$t^3$VAE&#65292;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;VAE&#26694;&#26550;&#65292;&#23427;&#23558;&#23398;&#29983;t&#20998;&#24067;&#32467;&#21512;&#21040;&#20808;&#39564;&#12289;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24130;&#24418;&#24335;&#30340;&#32852;&#21512;&#27169;&#22411;&#20998;&#24067;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#34920;&#36798;&#35777;&#25454;&#19979;&#30028;&#20026;&#20004;&#20010;&#32479;&#35745;&#27969;&#24418;&#20043;&#38388;KL&#25955;&#24230;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#23558;&#20854;&#26367;&#25442;&#20026;$\gamma$-&#24130;&#20998;&#27495;&#65292;&#36825;&#26159;&#24130;&#26063;&#30340;&#19968;&#20010;&#33258;&#28982;&#26367;&#20195;&#26041;&#27861;&#12290;$t^3$VAE&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20302;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01133v2 Announce Type: replace-cross  Abstract: The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of KL divergence between two statistical manifolds and replacing with $\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-
&lt;/p&gt;</description></item><item><title>&#25193;&#23637;NeuralODE&#26694;&#26550;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30452;&#25509;&#23558;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#20005;&#35880;&#30340;&#21270;&#23398;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2312.00038</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#19982;CFD&#27714;&#35299;&#22120;&#32806;&#21512;&#30340;&#21518;&#39564;&#35780;&#20272;&#29992;&#20110;&#24314;&#27169;&#20005;&#35880;&#30340;&#21270;&#23398;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Posteriori Evaluation of a Physics-Constrained Neural Ordinary Differential Equations Approach Coupled with CFD Solver for Modeling Stiff Chemical Kinetics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00038
&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;NeuralODE&#26694;&#26550;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30452;&#25509;&#23558;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#20005;&#35880;&#30340;&#21270;&#23398;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#35814;&#32454;&#21270;&#23398;&#35745;&#31639;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#23545;&#20110;&#39044;&#27979;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;(CFD)&#27169;&#25311;&#28237;&#27969;&#21453;&#24212;&#27969;&#30340;&#25361;&#25112;&#24456;&#22823;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#19968;&#32452;&#32806;&#21512;&#30340;&#20005;&#35880;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;(ODE)&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#23581;&#35797;&#29992;&#20110;&#24320;&#21457;&#26356;&#24555;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#21487;&#38752;&#22320;&#19982;CFD&#27714;&#35299;&#22120;&#38598;&#25104;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#26159;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#20110;&#35757;&#32451;&#38169;&#35823;&#65292;&#21364;&#26410;&#33021;&#30830;&#20445;&#19982;ODE&#27714;&#35299;&#22120;&#20860;&#23481;&#65292;&#23548;&#33268;&#38543;&#26102;&#38388;&#32047;&#31215;&#38169;&#35823;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#21270;&#23398;&#21160;&#21147;&#23398;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30452;&#25509;&#23558;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;NeuralODE&#26694;&#26550;&#25193;&#23637;&#21040;&#20005;&#35880;&#30340;&#21270;&#23398;&#21160;&#21147;&#23398;&#12290;&#36825;&#30830;&#20445;&#20102;&#24635;&#36136;&#37327;&#30340;&#23432;&#24658;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00038v3 Announce Type: replace-cross  Abstract: The high computational cost associated with solving for detailed chemistry poses a significant challenge for predictive computational fluid dynamics (CFD) simulations of turbulent reacting flows. These models often require solving a system of coupled stiff ordinary differential equations (ODEs). While deep learning techniques have been experimented with to develop faster surrogate models, they often fail to integrate reliably with CFD solvers. This instability arises because deep learning methods optimize for training error without ensuring compatibility with ODE solvers, leading to accumulation of errors over time. Recently, NeuralODE-based techniques have offered a promising solution by effectively modeling chemical kinetics. In this study, we extend the NeuralODE framework for stiff chemical kinetics by incorporating mass conservation constraints directly into the loss function during training. This ensures that the total ma
&lt;/p&gt;</description></item><item><title>&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#39640;&#25928;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#32622;&#20449;&#24207;&#21015;&#23454;&#29616;&#20102;&#26356;&#32039;&#20945;&#30340;&#25512;&#26029;&#65292;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.18274</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#21322;&#21442;&#25968;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Semiparametric Efficient Inference in Adaptive Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18274
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#39640;&#25928;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#32622;&#20449;&#24207;&#21015;&#23454;&#29616;&#20102;&#26356;&#32039;&#20945;&#30340;&#25512;&#26029;&#65292;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#19968;&#20010;&#39034;&#24207;&#23454;&#39564;&#20013;&#65292;&#27835;&#30103;&#25928;&#24212;&#30340;&#39640;&#25928;&#25512;&#26029;&#65292;&#20854;&#20013;&#20027;&#23548;&#23558;&#21463;&#35797;&#39564;&#23545;&#35937;&#20998;&#37197;&#32473;&#27835;&#30103;&#25110;&#23545;&#29031;&#32452;&#30340;&#31574;&#30053;&#21487;&#20197;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#33258;&#36866;&#24212;&#22686;&#24378;&#21453;&#21521;&#27010;&#29575;&#21152;&#26435;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#19968;&#20010;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#35813;&#20272;&#35745;&#22120;&#22312;&#25991;&#29486;&#20013;&#30340;&#20551;&#35774;&#27604;&#20197;&#24448;&#26356;&#24369;&#12290;&#35813;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20351;&#24471;&#22312;&#22266;&#23450;&#26679;&#26412;&#37327;&#19979;&#36827;&#34892;&#39640;&#25928;&#25512;&#26029;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#38543;&#21518;&#32771;&#34385;&#20102;&#39034;&#24207;&#25512;&#26029;&#35774;&#23450;&#65292;&#25512;&#23548;&#20986;&#26082;&#21253;&#21547;&#28176;&#36817;&#24615;&#36136;&#21448;&#21253;&#21547;&#38750;&#28176;&#36817;&#24615;&#36136;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#36825;&#20123;&#24207;&#21015;&#26126;&#26174;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#32039;&#20945;&#12290;&#36825;&#20123;&#20219;&#24847;&#26377;&#25928;&#30340;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#22312;&#25968;&#25454;&#30456;&#20851;&#30340;&#20572;&#27490;&#26102;&#38388;&#65288;&#26679;&#26412;&#37327;&#65289;&#19979;&#36827;&#34892;&#25512;&#26029;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#30340;&#31163;&#32447;&#31574;&#30053;&#20272;&#35745;&#25991;&#29486;&#20013;&#30340;&#20542;&#21521;&#20998;&#25968;&#25130;&#26029;&#25216;&#26415;&#65292;&#20197;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#26041;&#24046;&#32780;&#19981;&#24433;&#21709;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18274v3 Announce Type: replace-cross  Abstract: We consider the problem of efficient inference of the Average Treatment Effect in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;GNNs&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#23376;&#22270;&#65292;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#29289;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.07548</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Interpretable Fine-Tuning for Graph Neural Network Surrogate Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;GNNs&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#23376;&#22270;&#65292;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#29289;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#24314;&#27169;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20986;&#29616;&#22312;&#26368;&#36817;&#20960;&#24180;&#20869;&#34028;&#21187;&#21457;&#23637;&#65292;GNNs&#21487;&#20197;&#30452;&#25509;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#34920;&#31034;&#19978;&#36816;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#20026;GNN&#24341;&#20837;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#26368;&#32456;&#32467;&#26524;&#26159;&#19968;&#20010;&#22686;&#24378;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#23427;&#38548;&#31163;&#20102;&#19982;&#39044;&#27979;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#65292;&#30456;&#24212;&#20110;&#23376;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22522;&#32447;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#30001;&#24494;&#35843;&#30340;GNN&#35782;&#21035;&#20986;&#30340;&#32467;&#26500;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#26159;&#33258;&#36866;&#24212;&#29983;&#25104;&#30340;&#65292;&#24182;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#38142;&#25509;&#23384;&#22312;&#20110;&#22522;&#32447;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#38382;&#39064;&#29305;&#23450;&#29289;&#29702;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#31243;&#24207;&#65292;&#24494;&#35843;&#30340;GNNs&#36824;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#29992;&#20110;&#35782;&#21035;&#23545;&#24212;&#30340;&#22270;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07548v2 Announce Type: replace  Abstract: Data-driven surrogate modeling has surged in capability in recent years with the emergence of graph neural networks (GNNs), which can operate directly on mesh-based representations of data. The goal of this work is to introduce an interpretable fine-tuning strategy for GNNs, with application to unstructured mesh-based fluid dynamics modeling. The end result is an enhanced fine-tuned model that isolates regions in physical space, corresponding to sub-graphs, that are intrinsically linked to the forecasting task while retaining the predictive capability of the baseline. These structures, identified by the fine-tuned GNNs, are adaptively produced in the forward pass and serve as explainable links between the baseline model architecture, the optimization goal, and known problem-specific physics. Additionally, through a regularization procedure, the fine-tuned GNNs can also be used to identify, during inference, graph nodes that correspon
&lt;/p&gt;</description></item><item><title>Neuro-GPT&#26159;&#19968;&#20010;&#38754;&#21521;EEG&#25968;&#25454;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21160;&#20316;&#24819;&#35937;&#20998;&#31867;&#20219;&#21153;&#65292;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#20197;&#35299;&#20915;EEG&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.03764</link><description>&lt;p&gt;
Neuro-GPT: &#38754;&#21521;EEG&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuro-GPT: Towards A Foundation Model for EEG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03764
&lt;/p&gt;
&lt;p&gt;
Neuro-GPT&#26159;&#19968;&#20010;&#38754;&#21521;EEG&#25968;&#25454;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21160;&#20316;&#24819;&#35937;&#20998;&#31867;&#20219;&#21153;&#65292;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#20197;&#35299;&#20915;EEG&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#20013;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#24322;&#36136;&#24615;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#21147;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Neuro-GPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;EEG&#32534;&#30721;&#22120;&#21644;GPT&#27169;&#22411;&#32452;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#22522;&#30784;&#27169;&#22411;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#65292;&#23398;&#20064;&#22914;&#20309;&#37325;&#26500;&#34987;&#25513;&#30721;&#30340;EEG&#29255;&#27573;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#21160;&#20316;&#24819;&#35937;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#39564;&#35777;&#20854;&#22312;&#20302;&#25968;&#25454;&#24773;&#22659;&#65288;9&#21517;&#21463;&#35797;&#32773;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#36825;&#20026;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23427;&#24212;&#23545;EEG&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;&#20195;&#30721;&#20844;&#24320;&#22312;github.com/wenhui0206/NeuroGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03764v4 Announce Type: replace  Abstract: To handle the scarcity and heterogeneity of electroencephalography (EEG) data for Brain-Computer Interface (BCI) tasks, and to harness the power of large publicly available data sets, we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model. The foundation model is pre-trained on a large-scale data set using a self-supervised task that learns how to reconstruct masked EEG segments. We then fine-tune the model on a Motor Imagery Classification task to validate its performance in a low-data regime (9 subjects). Our experiments demonstrate that applying a foundation model can significantly improve classification performance compared to a model trained from scratch, which provides evidence for the generalizability of the foundation model and its ability to address challenges of data scarcity and heterogeneity in EEG. The code is publicly available at github.com/wenhui0206/NeuroGPT.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#26399;&#26395;&#25928;&#29992;&#20551;&#35774;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#39118;&#38505;&#20013;&#24615;&#21644;&#39118;&#38505;&#24863;&#30693;RL&#30446;&#26631;&#23454;&#38469;&#19978;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#30340;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#26469;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#21452;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;DAC&#65289;&#31639;&#27861;&#65292;&#20026;&#39118;&#38505;&#24863;&#30693;&#30340;RL&#31639;&#27861;&#36129;&#29486;&#20102;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2310.19527</link><description>&lt;p&gt;
&#20851;&#20110;&#39118;&#38505;&#24863;&#30693;&#20195;&#29702;&#29702;&#35770;&#65306;&#26725;&#25509;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#32463;&#27982;&#23398;
&lt;/p&gt;
&lt;p&gt;
On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19527
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#26399;&#26395;&#25928;&#29992;&#20551;&#35774;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#39118;&#38505;&#20013;&#24615;&#21644;&#39118;&#38505;&#24863;&#30693;RL&#30446;&#26631;&#23454;&#38469;&#19978;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#30340;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#26469;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#21452;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;DAC&#65289;&#31639;&#27861;&#65292;&#20026;&#39118;&#38505;&#24863;&#30693;&#30340;RL&#31639;&#27861;&#36129;&#29486;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19527v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#39118;&#38505;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#22914;SAC&#21644;TD3&#22312;&#21508;&#31181;&#36830;&#32493;&#21160;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#35777;&#34920;&#29616;&#20248;&#20110;&#20854;&#39118;&#38505;&#20013;&#24615;&#23545;&#24212;&#29289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#37319;&#29992;&#30340;&#24754;&#35266;&#30446;&#26631;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#24314;&#31435;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#23454;&#26045;&#30340;&#20855;&#20307;&#25919;&#31574;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#26399;&#26395;&#25928;&#29992;&#20551;&#35774;&#65292;&#36825;&#26159;&#32463;&#27982;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#20197;&#38416;&#26126;&#39118;&#38505;&#20013;&#24615;&#21644;&#39118;&#38505;&#24863;&#30693;RL&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#30340;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#26469;&#35299;&#37322;&#12290; &#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#39118;&#38505;&#24863;&#30693;&#25919;&#31574;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#20102;&#20215;&#20540;&#30830;&#23450;&#24615;&#31561;&#20215;&#29289;&#65292;&#20351;&#20854;&#19982;&#20256;&#32479;&#20915;&#31574;&#29702;&#35770;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;Dual Actor-Critic&#65292;DAC&#65289;&#12290; DAC&#26159;&#19968;&#31181;&#39118;&#38505;&#24863;&#30693;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#30340;&#28436;&#21592;&#32593;&#32476;&#65306;&#19968;&#20010;&#29992;&#20110;&#26102;&#24207;&#24046;&#20998;&#30340;&#24754;&#35266;&#28436;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19527v2 Announce Type: replace  Abstract: Risk-aware Reinforcement Learning (RL) algorithms like SAC and TD3 were shown empirically to outperform their risk-neutral counterparts in a variety of continuous-action tasks. However, the theoretical basis for the pessimistic objectives these algorithms employ remains unestablished, raising questions about the specific class of policies they are implementing. In this work, we apply the expected utility hypothesis, a fundamental concept in economics, to illustrate that both risk-neutral and risk-aware RL goals can be interpreted through expected utility maximization using an exponential utility function. This approach reveals that risk-aware policies effectively maximize value certainty equivalent, aligning them with conventional decision theory principles. Furthermore, we propose Dual Actor-Critic (DAC). DAC is a risk-aware, model-free algorithm that features two distinct actor networks: a pessimistic actor for temporal-difference 
&lt;/p&gt;</description></item><item><title>DySurv&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#38745;&#24577;&#21644;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#65292;&#29992;&#20110;&#20272;&#35745;ICU&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#30340;ICU&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2310.18681</link><description>&lt;p&gt;
DySurv&#65306;ICU&#20013;&#29983;&#23384;&#39044;&#27979;&#30340;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DySurv: Dynamic Deep Learning Model for Survival Prediction in the ICU
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18681
&lt;/p&gt;
&lt;p&gt;
DySurv&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#38745;&#24577;&#21644;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#65292;&#29992;&#20110;&#20272;&#35745;ICU&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#30340;ICU&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#20391;&#37325;&#20110;&#20272;&#35745;&#26102;&#38388;&#33267;&#20107;&#20214;&#20998;&#24067;&#65292;&#21487;&#24110;&#21161;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#36827;&#34892;&#21160;&#24577;&#39118;&#38505;&#39044;&#27979;&#12290;&#25193;&#23637;&#32463;&#20856;&#30340;Cox&#27169;&#22411;&#65292;&#21457;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25670;&#33073;&#20102;&#27604;&#20363;&#39118;&#38505;&#30340;&#32422;&#26463;&#24615;&#20551;&#35774;&#12290;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#36890;&#24120;&#20165;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DySurv&#30340;&#26032;&#22411;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#38745;&#24577;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#30340;&#32452;&#21512;&#26469;&#21160;&#24577;&#20272;&#35745;&#27515;&#20129;&#39118;&#38505;&#12290;DySurv&#22312;&#22810;&#20010;&#26102;&#38388;&#33267;&#20107;&#20214;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;&#26469;&#33258;MIMIC-IV&#21644;eICU&#30340;&#29616;&#23454;&#19990;&#30028;&#37325;&#30151;&#30417;&#25252;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290; DySurv&#30340;&#39044;&#27979;&#33021;&#21147;&#25345;&#32493;&#31283;&#23450;&#65292;&#29983;&#23384;&#20272;&#35745;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18681v2 Announce Type: replace  Abstract: Survival analysis focuses on estimating time-to-event distributions which can help in dynamic risk prediction in healthcare. Extending beyond the classical Cox model, deep learning techniques have been developed which moved away from the constraining assumptions of proportional hazards. Traditional statistical models often only include static information where, in this work, we propose a novel conditional variational autoencoder-based method called DySurv, which uses a combination of static and time-series measurements from patient electronic health records to estimate the risk of death dynamically. DySurv has been tested on several time-to-event benchmarks where it outperforms existing methods, including deep learning methods, and we evaluate it on real-world intensive care unit data from MIMIC-IV and eICU. The predictive capacity of DySurv is consistent and the survival estimates remain disentangled across different datasets suppor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20844;&#24179;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;G-FairAttack&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;GNNs&#65292;&#24182;&#20445;&#25345;&#25915;&#20987;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.13822</link><description>&lt;p&gt;
&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20844;&#24179;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Fairness of Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20844;&#24179;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;G-FairAttack&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;GNNs&#65292;&#24182;&#20445;&#25345;&#25915;&#20987;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#33021;&#22815;&#20943;&#23569;&#22312;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;&#20013;&#23545;&#20219;&#20309;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#65288;&#20363;&#22914;&#22899;&#24615;&#65289;&#30340;&#39044;&#27979;&#20559;&#35265;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;GNNs&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#20294;&#20844;&#24179;&#24615;&#23481;&#26131;&#21463;&#21040;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#30772;&#22351;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;GNNs&#20844;&#24179;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;G-FairAttack&#65292;&#36825;&#26159;&#19968;&#20010;&#25915;&#20987;&#21508;&#31181;&#31867;&#22411;&#20844;&#24179;&#24615;&#24863;&#30693;GNNs&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23545;&#39044;&#27979;&#25928;&#29992;&#20960;&#20046;&#27809;&#26377;&#23519;&#35273;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#35745;&#31639;&#25216;&#26415;&#26469;&#38477;&#20302;G-FairAttack&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;G-FairAttack&#25104;&#21151;&#22320;&#30772;&#22351;&#20102;&#19981;&#21516;&#31867;&#22411;GNNs&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25915;&#20987;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#25105;&#20204;&#22312;&#20844;&#24179;&#24615;&#25915;&#20987;&#26041;&#38754;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20844;&#24179;&#24615;&#24863;&#30693;GNNs&#30340;&#28508;&#22312;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13822v2 Announce Type: replace  Abstract: Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs an
&lt;/p&gt;</description></item><item><title>Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.10207</link><description>&lt;p&gt;
Bongard-OpenWorld: &#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10207
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bongard-OpenWorld&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#30495;&#23454;&#19990;&#30028;&#23569;&#26679;&#26412;&#25512;&#29702;&#30340;&#26032;&#22522;&#20934;&#12290; &#23427;&#28304;&#33258;&#32463;&#20856;&#30340;Bongard&#38382;&#39064;&#65288;BPs&#65289;&#65306;&#32473;&#23450;&#20004;&#32452;&#22270;&#20687;&#65288;&#27491;&#21644;&#36127;&#65289;&#65292;&#27169;&#22411;&#38656;&#35201;&#36890;&#36807;&#35825;&#23548;&#35270;&#35273;&#27010;&#24565;&#26469;&#30830;&#23450;&#26597;&#35810;&#22270;&#20687;&#25152;&#23646;&#30340;&#22270;&#20687;&#38598;&#65292;&#36825;&#20123;&#27010;&#24565;&#20165;&#30001;&#27491;&#38598;&#20013;&#30340;&#22270;&#20687;&#25152;&#25551;&#36848;&#12290; &#25105;&#20204;&#30340;&#22522;&#20934;&#32487;&#25215;&#20102;&#21407;&#22987;BPs&#30340;&#23569;&#26679;&#26412;&#27010;&#24565;&#24402;&#32435;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#20004;&#23618;&#26032;&#25361;&#25112;&#65306;1&#65289;&#24320;&#25918;&#19990;&#30028;&#30340;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#65292;&#22240;&#20026;Bongard-OpenWorld&#20013;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#20174;&#24320;&#25918;&#35789;&#27719;&#34920;&#20013;&#29420;&#29305;&#32452;&#21512;&#30340;&#26415;&#35821;&#65292;&#33539;&#22260;&#20174;&#23545;&#35937;&#31867;&#21035;&#21040;&#25277;&#35937;&#35270;&#35273;&#23646;&#24615;&#21644;&#24120;&#35782;&#20107;&#23454;&#30693;&#35782;&#65307; 2&#65289;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#35768;&#22810;&#31867;&#20284;&#29289;&#20351;&#29992;&#30340;&#21512;&#25104;&#22270;&#34920;&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;Bongard-OpenWorld&#24050;&#32463;&#23545;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36828;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#22478;&#24066;&#25968;&#37327;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#35268;&#27169;&#26679;&#26412;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2310.06543</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;&#22312;&#35268;&#27169;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Traveling Salesman Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06543
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#22478;&#24066;&#25968;&#37327;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#35268;&#27169;&#26679;&#26412;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#25968;&#23398;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#26041;&#38754;&#22312;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#23398;&#20064;&#30340;TSP&#27714;&#35299;&#22120;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22266;&#23450;&#35268;&#27169;&#30340;TSP&#23454;&#20363;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#20316;&#21697;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#22478;&#24066;&#25968;&#37327;&#30340;TSP&#30340;&#25968;&#25454;&#39537;&#21160;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;TSP&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#35268;&#27169;&#26679;&#26412;&#30340;TSP&#30340;&#36793;&#32536;&#24863;&#30693;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;EdgeGAE&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06543v2 Announce Type: replace  Abstract: In recent years, there has been a notable surge in research on machine learning techniques for combinatorial optimization. It has been shown that learning-based methods outperform traditional heuristics and mathematical solvers on the Traveling Salesman Problem (TSP) in terms of both performance and computational efficiency. However, most learning-based TSP solvers are primarily designed for fixed-scale TSP instances, and also require a large number of training samples to achieve optimal performance. To fill this gap, this work proposes a data-driven graph representation learning method for solving TSPs with various numbers of cities. Specifically, we formulate the TSP as a link prediction task and propose an edge-aware graph autoencoder (EdgeGAE) model that can solve TSPs by learning from various-scale samples with an imbalanced distribution. A residual gated encoder is trained to learn latent edge embeddings, followed by an edge-ce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#26469;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20844;&#20849;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#20849;&#20139;&#30828;&#26631;&#31614;&#20195;&#26367;&#27169;&#22411;&#21442;&#25968;&#65292;&#24418;&#25104;&#20266;&#26631;&#31614;&#20197;&#32467;&#21512;&#31169;&#26377;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#24182;&#33719;&#24471;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2310.05696</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Protecting Sensitive Data through Federated Co-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#26469;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20844;&#20849;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#20849;&#20139;&#30828;&#26631;&#31614;&#20195;&#26367;&#27169;&#22411;&#21442;&#25968;&#65292;&#24418;&#25104;&#20266;&#26631;&#31614;&#20197;&#32467;&#21512;&#31169;&#26377;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#24182;&#33719;&#24471;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#25935;&#24863;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#20998;&#24067;&#30340;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#21487;&#33021;&#26080;&#27861;&#27719;&#24635;&#12290;&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#22320;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#21512;&#24182;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#25512;&#26029;&#20986;&#25935;&#24863;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#23458;&#25143;&#31471;&#20998;&#20139;&#20844;&#20849;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#30828;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#12290;&#23545;&#20849;&#20139;&#26631;&#31614;&#30340;&#19968;&#33268;&#24615;&#24418;&#25104;&#20102;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#20266;&#26631;&#31614;&#65292;&#23458;&#25143;&#31471;&#23558;&#20854;&#19982;&#31169;&#26377;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#26469;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20849;&#20139;&#30828;&#26631;&#31614;&#22823;&#22823;&#25552;&#39640;&#20102;&#19982;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#30456;&#27604;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#21516;&#26102;&#65292;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#23454;&#29616;&#20102;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#20687;(&#26799;&#24230;&#25552;&#21319;)&#20915;&#31574;&#26641;&#12289;&#35268;&#21017;&#38598;&#21512;&#31561;&#26412;&#22320;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05696v2 Announce Type: replace  Abstract: In many applications, sensitive data is inherently distributed and may not be pooled due to privacy concerns. Federated learning allows us to collaboratively train a model without pooling the data by iteratively aggregating the parameters of local models. It is possible, though, to infer upon the sensitive data from the shared model parameters. We propose to use a federated co-training approach where clients share hard labels on a public unlabeled dataset instead of model parameters. A consensus on the shared labels forms a pseudo labeling for the unlabeled dataset that clients use in combination with their private data to train local models. We show that sharing hard labels substantially improves privacy over sharing model parameters. At the same time, federated co-training achieves a model quality comparable to federated learning. Moreover, it allows us to use local models such as (gradient boosted) decision trees, rule ensembles, 
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#26102;&#31354;&#34920;&#24449;&#65292;&#21253;&#25324;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#20197;&#21450;&#20010;&#20307;&#30340;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;</title><link>https://arxiv.org/abs/2310.02207</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#31354;&#38388;&#21644;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Space and Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02207
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#26102;&#31354;&#34920;&#24449;&#65292;&#21253;&#25324;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#20197;&#21450;&#20010;&#20307;&#30340;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#21040;&#24213;&#26159;&#20165;&#20165;&#23398;&#20064;&#20102;&#24222;&#22823;&#30340;&#34920;&#38754;&#32479;&#35745;&#20449;&#24687;&#36824;&#26159;&#23398;&#21040;&#20102;&#26356;&#36830;&#36143;&#12289;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#24449;&#30340;&#20105;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;Llama-2&#31995;&#21015;&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#19977;&#20010;&#31354;&#38388;&#25968;&#25454;&#38598;&#65288;&#19990;&#30028;&#12289;&#32654;&#22269;&#12289;&#32445;&#32422;&#30340;&#22320;&#28857;&#65289;&#21644;&#19977;&#20010;&#26102;&#38388;&#25968;&#25454;&#38598;&#65288;&#21382;&#21490;&#20154;&#29289;&#12289;&#33402;&#26415;&#21697;&#12289;&#26032;&#38395;&#22836;&#26465;&#65289;&#30340;&#23398;&#20064;&#34920;&#24449;&#25214;&#21040;&#20102;&#25903;&#25345;&#21518;&#32773;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#12290;&#36825;&#20123;&#34920;&#24449;&#23545;&#25552;&#31034;&#21464;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#65288;&#20363;&#22914;&#22478;&#24066;&#21644;&#22320;&#26631;&#65289;&#20043;&#38388;&#26159;&#32479;&#19968;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21487;&#38752;&#22320;&#32534;&#30721;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#30340;&#20010;&#20307;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;&#34429;&#28982;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#29616;&#20195;LLM&#23398;&#20064;&#21040;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20016;&#23500;&#26102;&#31354;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02207v3 Announce Type: replace-cross  Abstract: The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20809;&#26463;&#26522;&#20030;&#26041;&#27861;&#65292;&#20174;&#35821;&#35328;&#20026;&#22522;&#30784;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20013;&#35814;&#23613;&#26522;&#20030;&#26368;&#21487;&#33021;&#30340;&#23376;&#24207;&#21015;&#65292;&#25552;&#21462;&#20998;&#23376;&#20122;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#25105;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#25552;&#21319;&#29983;&#25104;&#24335;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2309.13957</link><description>&lt;p&gt;
&#20809;&#26463;&#26522;&#20030;&#65306;&#33258;&#25105;&#26465;&#20214;&#21270;&#20998;&#23376;&#35774;&#35745;&#30340;&#27010;&#29575;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13957
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20809;&#26463;&#26522;&#20030;&#26041;&#27861;&#65292;&#20174;&#35821;&#35328;&#20026;&#22522;&#30784;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20013;&#35814;&#23613;&#26522;&#20030;&#26368;&#21487;&#33021;&#30340;&#23376;&#24207;&#21015;&#65292;&#25552;&#21462;&#20998;&#23376;&#20122;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#25105;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#25552;&#21319;&#29983;&#25104;&#24335;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20998;&#23376;&#35774;&#35745;&#24050;&#32463;&#20174;&#27010;&#24565;&#39564;&#35777;&#36808;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#36866;&#29992;&#24615;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#26368;&#36817;&#28044;&#29616;&#30340;&#22823;&#37327;&#30740;&#31350;&#25253;&#21578;&#23454;&#39564;&#39564;&#35777;&#30340;&#35770;&#25991;&#20013;&#30475;&#20986;&#12290;&#35299;&#37322;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#30340;&#20851;&#38190;&#25361;&#25112;&#20026;&#22686;&#24378;&#29983;&#25104;&#24335;&#35774;&#35745;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;&#26114;&#36149;&#39640;&#20445;&#30495;&#24230;&#39044;&#35328;&#24182;&#20026;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20809;&#26463;&#26522;&#20030;&#65292;&#20197;&#35814;&#23613;&#26522;&#20030;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20013;&#26368;&#21487;&#33021;&#30340;&#23376;&#24207;&#21015;&#65292;&#24182;&#23637;&#31034;&#21487;&#20197;&#25552;&#21462;&#20998;&#23376;&#20122;&#32467;&#26500;&#12290;&#24403;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#26102;&#65292;&#25552;&#21462;&#30340;&#20122;&#32467;&#26500;&#21464;&#24471;&#26377;&#24847;&#20041;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#26465;&#20214;&#29983;&#25104;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#20809;&#26463;&#26522;&#20030;&#36890;&#24120;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#65292;&#26174;&#30528;&#25913;&#36827;&#20102;&#26368;&#36817;&#25253;&#21578;&#30340;&#22686;&#24378;&#35760;&#24518;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13957v2 Announce Type: replace-cross  Abstract: Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memor
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29992;&#20110;&#20943;&#36731;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23558;&#32463;&#20856;&#21518;&#35757;&#32451;&#26041;&#27861;&#31070;&#32463;&#20803;&#20002;&#24323;&#30452;&#25509;&#24212;&#29992;&#21040;&#37327;&#23376;&#35774;&#32622;&#20013;&#20250;&#23548;&#33268;&#25104;&#21151;&#27010;&#29575;&#26174;&#33879;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102;&#32416;&#32544;&#22312;QCNN&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644;&#20854;&#23545;&#32416;&#32544;&#20002;&#22833;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.01829</link><description>&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#32531;&#35299;&#36807;&#25311;&#21512;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Post-Training Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01829
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29992;&#20110;&#20943;&#36731;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23558;&#32463;&#20856;&#21518;&#35757;&#32451;&#26041;&#27861;&#31070;&#32463;&#20803;&#20002;&#24323;&#30452;&#25509;&#24212;&#29992;&#21040;&#37327;&#23376;&#35774;&#32622;&#20013;&#20250;&#23548;&#33268;&#25104;&#21151;&#27010;&#29575;&#26174;&#33879;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102;&#32416;&#32544;&#22312;QCNN&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644;&#20854;&#23545;&#32416;&#32544;&#20002;&#22833;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNN&#65289;&#20316;&#20026;&#37327;&#23376;&#35745;&#31639;&#26426;&#22312;NISQ&#26102;&#20195;&#30340;&#26089;&#26399;&#24212;&#29992;&#65292;&#19968;&#30452;&#34987;&#35777;&#26126;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#20934;&#30830;&#24230;&#65292;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#19968;&#30452;&#34920;&#29616;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21463;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#30340;&#24433;&#21709;&#65292;QCNN&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#20943;&#36731;QCNN&#36807;&#25311;&#21512;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#23558;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#20803;&#20002;&#24323;&#30340;&#32463;&#20856;&#21518;&#35757;&#32451;&#26041;&#27861;&#36866;&#24212;&#21040;&#37327;&#23376;&#35774;&#32622;&#20013;&#65292;&#20250;&#23548;&#33268;&#19968;&#20010;&#26174;&#33879;&#19988;&#19981;&#24076;&#26395;&#30340;&#32467;&#26524;&#65306;QCNN&#30340;&#25104;&#21151;&#27010;&#29575;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#25928;&#26524;&#26292;&#38706;&#20986;&#20102;&#32416;&#32544;&#22312;QCNN&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#20197;&#21450;QCNN&#23545;&#32416;&#32544;&#20002;&#22833;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01829v2 Announce Type: replace-cross  Abstract: Quantum convolutional neural network (QCNN), an early application for quantum computers in the NISQ era, has been consistently proven successful as a machine learning (ML) algorithm for several tasks with significant accuracy. Derived from its classical counterpart, QCNN is prone to overfitting. Overfitting is a typical shortcoming of ML models that are trained too closely to the availed training dataset and perform relatively poorly on unseen datasets for a similar problem. In this work we study post-training approaches for mitigating overfitting in QCNNs. We find that a straightforward adaptation of a classical post-training method, known as neuron dropout, to the quantum setting leads to a significant and undesirable consequence: a substantial decrease in success probability of the QCNN. We argue that this effect exposes the crucial role of entanglement in QCNNs and the vulnerability of QCNNs to entanglement loss. Hence, we 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;$\mathcal{L}_\alpha$-GAN&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Jensen-$f_\alpha$-&#25955;&#24230;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;GAN&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.07233</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#32479;&#19968;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unifying Generator Loss Function for Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07233
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;$\mathcal{L}_\alpha$-GAN&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Jensen-$f_\alpha$-&#25955;&#24230;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;GAN&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;$\alpha$&#21442;&#25968;&#21270;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#21452;&#30446;&#26631;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#32463;&#20856;&#37492;&#21035;&#22120;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#21407;&#22987;GAN&#65288;VanillaGAN&#65289;&#31995;&#32479;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#23545;&#31216;&#31867;&#27010;&#29575;&#20272;&#35745;&#31867;&#22411;&#20989;&#25968;$\mathcal{L}_\alpha$&#65292;&#24471;&#21040;&#30340;GAN&#31995;&#32479;&#34987;&#31216;&#20026;$\mathcal{L}_\alpha$-GAN&#12290;&#22312;&#26368;&#20339;&#37492;&#21035;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#26126;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#38382;&#39064;&#21253;&#25324;&#26368;&#23567;&#21270;Jensen-$f_\alpha$-&#25955;&#24230;&#65292;&#36825;&#26159;Jensen-Shannon&#25955;&#24230;&#30340;&#33258;&#28982;&#27867;&#21270;&#65292;&#20854;&#20013;$f_\alpha$&#26159;&#20851;&#20110;&#25439;&#22833;&#20989;&#25968;$\mathcal{L}_\alpha$&#30340;&#20984;&#20989;&#25968;&#12290;&#36824;&#35777;&#26126;&#20102;&#36825;&#20010;$\mathcal{L}_\alpha$-GAN&#38382;&#39064;&#24674;&#22797;&#20102;&#25991;&#29486;&#20013;&#19968;&#20123;GAN&#38382;&#39064;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#21253;&#25324;VanillaGAN&#12289;&#26368;&#23567;&#20108;&#20056;GAN&#65288;LSGAN&#65289;&#12289;&#26368;&#23567;$k$&#38454;GAN&#65288;L
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07233v2 Announce Type: replace  Abstract: A unifying $\alpha$-parametrized generator loss function is introduced for a dual-objective generative adversarial network (GAN), which uses a canonical (or classical) discriminator loss function such as the one in the original GAN (VanillaGAN) system. The generator loss function is based on a symmetric class probability estimation type function, $\mathcal{L}_\alpha$, and the resulting GAN system is termed $\mathcal{L}_\alpha$-GAN. Under an optimal discriminator, it is shown that the generator's optimization problem consists of minimizing a Jensen-$f_\alpha$-divergence, a natural generalization of the Jensen-Shannon divergence, where $f_\alpha$ is a convex function expressed in terms of the loss function $\mathcal{L}_\alpha$. It is also demonstrated that this $\mathcal{L}_\alpha$-GAN problem recovers as special cases a number of GAN problems in the literature, including VanillaGAN, Least Squares GAN (LSGAN), Least $k$th order GAN (L$
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATNet&#65292;&#32467;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#19982;&#20809;&#20239;&#21457;&#30005;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#22810;&#32423;&#32852;&#21512;&#34701;&#21512;&#26041;&#27861;&#36827;&#34892;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;</title><link>https://arxiv.org/abs/2306.10356</link><description>&lt;p&gt;
MATNet: &#22810;&#32423;&#34701;&#21512;&#21464;&#21387;&#22120;&#27169;&#22411;&#29992;&#20110;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MATNet: Multi-Level Fusion Transformer-Based Model for Day-Ahead PV Generation Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10356
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATNet&#65292;&#32467;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#19982;&#20809;&#20239;&#21457;&#30005;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#22810;&#32423;&#32852;&#21512;&#34701;&#21512;&#26041;&#27861;&#36827;&#34892;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#23545;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#25972;&#21512;&#21040;&#30005;&#21147;&#31995;&#32479;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#20809;&#20239;&#21333;&#20803;&#65292;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#21487;&#20998;&#20026;&#22522;&#20110;&#29289;&#29702;&#21644;&#22522;&#20110;&#25968;&#25454;&#30340;&#31574;&#30053;&#20004;&#31867;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#29616;&#35937;&#30340;&#28508;&#22312;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MATNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22810;&#20803;&#22810;&#27493;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;&#12290;&#23427;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#20809;&#20239;&#21457;&#30005;&#30340;&#20808;&#39564;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22810;&#32423;&#32852;&#21512;&#34701;&#21512;&#26041;&#27861;&#36755;&#20837;&#21382;&#21490;&#20809;&#20239;&#25968;&#25454;&#20197;&#21450;&#21382;&#21490;&#21644;&#39044;&#27979;&#22825;&#27668;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10356v2 Announce Type: replace-cross  Abstract: Accurate forecasting of renewable generation is crucial to facilitate the integration of RES into the power system. Focusing on PV units, forecasting methods can be divided into two main categories: physics-based and data-based strategies, with AI-based models providing state-of-the-art performance. However, while these AI-based models can capture complex patterns and relationships in the data, they ignore the underlying physical prior knowledge of the phenomenon. Therefore, in this paper we propose MATNet, a novel self-attention transformer-based architecture for multivariate multi-step day-ahead PV power generation forecasting. It consists of a hybrid approach that combines the AI paradigm with the prior physical knowledge of PV power generation of physics-based methods. The model is fed with historical PV data and historical and forecast weather data through a multi-level joint fusion approach. The effectiveness of the propo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREBI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#21644;&#25193;&#25955;&#27169;&#22411;&#35268;&#21010;&#26469;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2306.00603</link><description>&lt;p&gt;
&#20855;&#26377;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#30340;&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Offline Reinforcement Learning with Real-Time Budget Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREBI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#21644;&#25193;&#25955;&#27169;&#22411;&#35268;&#21010;&#26469;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20419;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#65292;&#36817;&#24180;&#26469;&#23545;&#23433;&#20840;RL&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20173;&#19987;&#27880;&#20110;&#22312;&#32447;&#35774;&#32622;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#21457;&#29983;&#23545;&#23433;&#20840;&#39044;&#31639;&#30340;&#39118;&#38505;&#36829;&#35268;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#24471;&#31574;&#30053;&#38656;&#35201;&#23454;&#26102;&#21709;&#24212;&#21160;&#24577;&#30830;&#23450;&#30340;&#23433;&#20840;&#39044;&#31639;&#65288;&#21363;&#32422;&#26463;&#38408;&#20540;&#65289;&#12290;&#26412;&#25991;&#38024;&#23545;&#31163;&#32447;&#35774;&#32622;&#19979;&#30340;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#36712;&#36857;&#30340;&#23454;&#26102;&#39044;&#31639;&#25512;&#26029;&#65288;TREBI&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#20174;&#36712;&#36857;&#20998;&#24067;&#30340;&#35282;&#24230;&#23545;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#35268;&#21010;&#26469;&#35299;&#20915;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#23545;&#24773;&#33410;&#22870;&#21169;&#21644;&#25104;&#26412;&#30340;&#20272;&#35745;&#23384;&#22312;&#35823;&#24046;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00603v2 Announce Type: replace-cross  Abstract: Aiming at promoting the safe real-world deployment of Reinforcement Learning (RL), research on safe RL has made significant progress in recent years. However, most existing works in the literature still focus on the online setting where risky violations of the safety budget are likely to be incurred during training. Besides, in many real-world applications, the learned policy is required to respond to dynamically determined safety budgets (i.e., constraint threshold) in real time. In this paper, we target at the above real-time budget constraint problem under the offline setting, and propose Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that models this problem from the perspective of trajectory distribution and solves it through diffusion model planning. Theoretically, we prove an error bound of the estimation on the episodic reward and cost under the offline setting and thus provide a performance gua
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20102;&#24322;&#36136;&#22270;&#19978;&#29305;&#23450;&#27169;&#22411;&#35780;&#20272;&#30340;&#20551;&#35774;&#65292;&#25581;&#31034;&#20102;&#26631;&#20934;&#25968;&#25454;&#38598;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#31227;&#38500;&#37325;&#22797;&#33410;&#28857;&#23545;GNN&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;</title><link>https://arxiv.org/abs/2302.11640</link><description>&lt;p&gt;
&#23545;&#24322;&#36136;&#24615;&#26465;&#20214;&#19979;GNN&#35780;&#20272;&#30340;&#25209;&#21028;&#24615;&#30740;&#31350;&#65306;&#25105;&#20204;&#30495;&#30340;&#22312;&#21462;&#24471;&#36827;&#23637;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A critical look at the evaluation of GNNs under heterophily: Are we really making progress?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.11640
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20102;&#24322;&#36136;&#22270;&#19978;&#29305;&#23450;&#27169;&#22411;&#35780;&#20272;&#30340;&#20551;&#35774;&#65292;&#25581;&#31034;&#20102;&#26631;&#20934;&#25968;&#25454;&#38598;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#31227;&#38500;&#37325;&#22797;&#33410;&#28857;&#23545;GNN&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#20998;&#31867;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#36825;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36890;&#24120;&#35748;&#20026;&#26631;&#20934;GNNs&#21482;&#23545;&#21516;&#36136;&#22270;&#34920;&#29616;&#33391;&#22909;&#65292;&#21363;&#36793;&#20542;&#21521;&#20110;&#36830;&#25509;&#30456;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#30340;&#22270;&#12290;&#27809;&#26377;&#36825;&#31181;&#23646;&#24615;&#30340;&#22270;&#31216;&#20026;&#24322;&#36136;&#22270;&#65292;&#36890;&#24120;&#35748;&#20026;&#38656;&#35201;&#19987;&#38376;&#30340;&#26041;&#27861;&#25165;&#33021;&#22312;&#36825;&#31181;&#22270;&#19978;&#33719;&#24471;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#24322;&#36136;&#24615;&#29305;&#23450;&#27169;&#22411;&#30340;&#26631;&#20934;&#25968;&#25454;&#38598;&#20855;&#26377;&#20005;&#37325;&#32570;&#38519;&#65292;&#20351;&#24471;&#20351;&#29992;&#23427;&#20204;&#33719;&#24471;&#30340;&#32467;&#26524;&#19981;&#21487;&#38752;&#12290;&#36825;&#20123;&#32570;&#38519;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#25968;&#25454;&#38598;Squirrel&#21644;Chameleon&#20013;&#23384;&#22312;&#22823;&#37327;&#37325;&#22797;&#33410;&#28857;&#65292;&#23548;&#33268;&#35757;&#32451;-&#27979;&#35797;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#31227;&#38500;&#37325;&#22797;&#33410;&#28857;&#20250;&#20005;&#37325;&#24433;&#21709;GNN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.11640v2 Announce Type: replace  Abstract: Node classification is a classical graph machine learning task on which Graph Neural Networks (GNNs) have recently achieved strong results. However, it is often believed that standard GNNs only work well for homophilous graphs, i.e., graphs where edges tend to connect nodes of the same class. Graphs without this property are called heterophilous, and it is typically assumed that specialized methods are required to achieve strong performance on such graphs. In this work, we challenge this assumption. First, we show that the standard datasets used for evaluating heterophily-specific models have serious drawbacks, making results obtained by using them unreliable. The most significant of these drawbacks is the presence of a large number of duplicate nodes in the datasets Squirrel and Chameleon, which leads to train-test data leakage. We show that removing duplicate nodes strongly affects GNN performance on these datasets. Then, we propos
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20248;&#21270;&#30828;&#28183;&#20986;&#29289;&#20998;&#21106;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#21306;&#22495;&#30340;&#23494;&#24230;&#23545;&#27604;&#26041;&#26696;&#21644;&#21028;&#21035;&#24615;&#36793;&#32536;&#26816;&#26597;&#27169;&#22359;&#26469;&#22788;&#29702;&#30828;&#28183;&#20986;&#29289;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2302.11517</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#33258;&#21160;&#35270;&#32593;&#33180;&#28183;&#20986;&#29289;&#26816;&#27979;&#30340;&#20840;&#23616;&#21644;&#21306;&#22495;&#23545;&#27604;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.11517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20248;&#21270;&#30828;&#28183;&#20986;&#29289;&#20998;&#21106;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#21306;&#22495;&#30340;&#23494;&#24230;&#23545;&#27604;&#26041;&#26696;&#21644;&#21028;&#21035;&#24615;&#36793;&#32536;&#26816;&#26597;&#27169;&#22359;&#26469;&#22788;&#29702;&#30828;&#28183;&#20986;&#29289;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#26159;&#23548;&#33268;&#20840;&#29699;&#30450;&#30446;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26089;&#26399;&#26816;&#27979;&#30828;&#28183;&#20986;&#29289;&#23545;&#20110;&#35782;&#21035;DR&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#27835;&#30103;&#31958;&#23615;&#30149;&#21644;&#39044;&#38450;&#35270;&#21147;&#25439;&#22833;&#12290;&#38024;&#23545;&#29616;&#26377;&#20998;&#21106;&#25216;&#26415;&#38754;&#20020;&#30340;&#30828;&#28183;&#20986;&#29289;&#29420;&#29305;&#29305;&#24449;&#65292;&#20174;&#19981;&#19968;&#33268;&#30340;&#24418;&#29366;&#21040;&#27169;&#31946;&#30340;&#36793;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20248;&#21270;&#30828;&#28183;&#20986;&#29289;&#20998;&#21106;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22495;&#30340;&#23494;&#24230;&#23545;&#27604;&#26041;&#26696;&#26469;&#21306;&#20998;&#19981;&#21516;&#30149;&#21464;&#27987;&#24230;&#30340;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#20998;&#21106;&#23567;&#30149;&#21464;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22788;&#29702;&#27169;&#31946;&#30340;&#36793;&#30028;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21028;&#21035;&#24615;&#36793;&#32536;&#26816;&#26597;&#27169;&#22359;&#26469;&#21160;&#24577;&#20998;&#26512;&#36793;&#30028;&#21608;&#22260;&#30340;&#20687;&#32032;&#65292;&#24182;&#20934;&#30830;&#21246;&#21202;&#20986;&#28183;&#20986;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.11517v2 Announce Type: replace-cross  Abstract: Diabetic retinopathy (DR) is a leading global cause of blindness. Early detection of hard exudates plays a crucial role in identifying DR, which aids in treating diabetes and preventing vision loss. However, the unique characteristics of hard exudates, ranging from their inconsistent shapes to indistinct boundaries, pose significant challenges to existing segmentation techniques. To address these issues, we present a novel supervised contrastive learning framework to optimize hard exudate segmentation. Specifically, we introduce a patch-wise density contrasting scheme to distinguish between areas with varying lesion concentrations, and therefore improve the model's proficiency in segmenting small lesions. To handle the ambiguous boundaries, we develop a discriminative edge inspection module to dynamically analyze the pixels that lie around the boundaries and accurately delineate the exudates. Upon evaluation using the IDRiD dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#38376;&#25511;&#30340;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;GPs&#26500;&#24314;&#38376;&#25511;&#20989;&#25968;&#21644;&#19987;&#23478;&#65292;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#24615;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#36824;&#25552;&#20379;&#20102;&#28145;&#23618;GPs&#21644;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.04947</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#38376;&#25511;&#30340;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process-Gated Hierarchical Mixtures of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.04947
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#38376;&#25511;&#30340;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;GPs&#26500;&#24314;&#38376;&#25511;&#20989;&#25968;&#21644;&#19987;&#23478;&#65292;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#24615;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#36824;&#25552;&#20379;&#20102;&#28145;&#23618;GPs&#21644;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#38376;&#25511;&#30340;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;Gaussian Process-Gated Hierarchical Mixtures of Experts&#65292;GPHMEs&#65289;&#12290;&#19982;&#20854;&#20182;&#37319;&#29992;&#36755;&#20837;&#32447;&#24615;&#38376;&#25511;&#27169;&#22411;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26500;&#24314;&#30340;&#38376;&#25511;&#20989;&#25968;&#12290;&#36825;&#20123;&#36807;&#31243;&#22522;&#20110;&#36755;&#20837;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#38543;&#26426;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#22411;&#20013;&#30340;&#19987;&#23478;&#20063;&#26159;&#29992;GPs&#26500;&#24314;&#30340;&#12290;GPHMEs&#30340;&#20248;&#21270;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#26469;&#23454;&#29616;&#12290;&#25152;&#25552;&#20986;&#30340;GPHMEs&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#23427;&#20204;&#20248;&#20110;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#30340;&#22522;&#20110;&#26641;&#30340;HME&#22522;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20943;&#23569;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#23427;&#20204;&#20026;&#28145;&#23618;GPs&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;GPHMEs&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#25968;&#25454;&#35268;&#27169;&#30456;&#24403;&#36866;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.04947v2 Announce Type: replace  Abstract: In this paper, we propose novel Gaussian process-gated hierarchical mixtures of experts (GPHMEs). Unlike other mixtures of experts with gating models linear in the input, our model employs gating functions built with Gaussian processes (GPs). These processes are based on random features that are non-linear functions of the inputs. Furthermore, the experts in our model are also constructed with GPs. The optimization of the GPHMEs is performed by variational inference. The proposed GPHMEs have several advantages. They outperform tree-based HME benchmarks that partition the data in the input space, and they achieve good performance with reduced complexity. Another advantage is the interpretability they provide for deep GPs, and more generally, for deep Bayesian neural networks. Our GPHMEs demonstrate excellent performance for large-scale data sets, even with quite modest sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26893;&#20837;&#21518;&#38376;&#27169;&#22411;&#22312;&#35302;&#21457;&#36755;&#20837;&#21608;&#22260;&#30340;&#21464;&#21270;&#29575;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#20542;&#21521;&#20110;&#22312;&#35302;&#21457;&#36755;&#20837;&#21608;&#22260;&#27880;&#20837;&#20855;&#26377;&#20302;&#21464;&#21270;&#29575;&#29305;&#24449;&#30340;&#21518;&#38376;&#65292;&#26131;&#34987;&#26799;&#24230;&#35302;&#21457;&#22120;&#21453;&#36716;&#25429;&#33719;&#12290;</title><link>https://arxiv.org/abs/2301.12318</link><description>&lt;p&gt;
&#26799;&#24230;&#22609;&#36896;&#65306;&#22686;&#24378;&#21453;&#21521;&#24037;&#31243;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26893;&#20837;&#21518;&#38376;&#27169;&#22411;&#22312;&#35302;&#21457;&#36755;&#20837;&#21608;&#22260;&#30340;&#21464;&#21270;&#29575;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#20542;&#21521;&#20110;&#22312;&#35302;&#21457;&#36755;&#20837;&#21608;&#22260;&#27880;&#20837;&#20855;&#26377;&#20302;&#21464;&#21270;&#29575;&#29305;&#24449;&#30340;&#21518;&#38376;&#65292;&#26131;&#34987;&#26799;&#24230;&#35302;&#21457;&#22120;&#21453;&#36716;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26816;&#27979;&#26893;&#20837;&#21518;&#38376;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#37319;&#29992;&#35302;&#21457;&#22120;&#21453;&#36716;&#65288;&#20063;&#31216;&#20026;&#21453;&#21521;&#24037;&#31243;&#65289;&#21644;&#26435;&#37325;&#20998;&#26512;&#65288;&#20063;&#31216;&#20026;&#27169;&#22411;&#35786;&#26029;&#65289;&#20004;&#31181;&#26041;&#27861;&#20043;&#19968;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#20998;&#26512;&#26893;&#20837;&#21518;&#38376;&#27169;&#22411;&#22312;&#35302;&#21457;&#36755;&#20837;&#21608;&#22260;&#30340;&#21464;&#21270;&#29575;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#25915;&#20987;&#24448;&#24448;&#22312;&#35302;&#21457;&#36755;&#20837;&#21608;&#22260;&#27880;&#20837;&#20855;&#26377;&#36739;&#20302;&#21464;&#21270;&#29575;&#29305;&#24449;&#30340;&#21518;&#38376;&#65292;&#36825;&#20123;&#21518;&#38376;&#26131;&#20110;&#34987;&#22522;&#20110;&#26799;&#24230;&#30340;&#35302;&#21457;&#22120;&#21453;&#36716;&#25152;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12318v2 Announce Type: replace-cross  Abstract: Most existing methods to detect backdoored machine learning (ML) models take one of the two approaches: trigger inversion (aka. reverse engineer) and weight analysis (aka. model diagnosis). In particular, the gradient-based trigger inversion is considered to be among the most effective backdoor detection techniques, as evidenced by the TrojAI competition, Trojan Detection Challenge and backdoorBench. However, little has been done to understand why this technique works so well and, more importantly, whether it raises the bar to the backdoor attack. In this paper, we report the first attempt to answer this question by analyzing the change rate of the backdoored model around its trigger-carrying inputs. Our study shows that existing attacks tend to inject the backdoor characterized by a low change rate around trigger-carrying inputs, which are easy to capture by gradient-based trigger inversion. In the meantime, we found that the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;SCFL&#65292;&#19968;&#31181;&#20855;&#26377;&#23450;&#21046;&#38544;&#31169;&#20445;&#25252;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2212.13992</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#21046;&#38544;&#31169;&#20445;&#25252;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Social-Aware Clustered Federated Learning with Customized Privacy Preservation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.13992
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;SCFL&#65292;&#19968;&#31181;&#20855;&#26377;&#23450;&#21046;&#38544;&#31169;&#20445;&#25252;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#20445;&#25252;&#31471;&#29992;&#25143;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;FL&#20013;&#20173;&#28982;&#23384;&#22312;&#36890;&#36807;&#20132;&#25442;&#26799;&#24230;&#21487;&#33021;&#23548;&#33268;&#30340;&#28508;&#22312;&#38544;&#31169;&#27844;&#28431;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#25506;&#35752;&#24494;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35745;&#31639;&#32467;&#26524;&#28155;&#21152;&#22122;&#22768;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24320;&#38144;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#20043;&#38388;&#30340;&#26222;&#36941;&#31038;&#20132;&#36830;&#25509;&#65292;&#24179;&#34913;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;SCFL&#65292;&#20854;&#20013;&#30456;&#20114;&#20449;&#20219;&#30340;&#20010;&#20307;&#21487;&#20197;&#33258;&#30001;&#32452;&#25104;&#19968;&#20010;&#31038;&#20132;&#38598;&#32676;&#65292;&#24182;&#22312;&#27599;&#20010;&#38598;&#32676;&#20869;&#32858;&#21512;&#20182;&#20204;&#30340;&#21407;&#22987;&#27169;&#22411;&#26356;&#26032;&#65288;&#20363;&#22914;&#26799;&#24230;&#65289;&#65292;&#28982;&#21518;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#20840;&#23616;&#32858;&#21512;&#12290;&#36890;&#36807;&#22312;&#31038;&#20132;&#32676;&#20307;&#20013;&#28151;&#21512;&#27169;&#22411;&#26356;&#26032;&#65292;&#23545;&#25163;&#21482;&#33021;&#31363;&#21548;&#31038;&#20132;&#23618;&#32452;&#21512;&#30340;&#32467;&#26524;&#65292;&#32780;&#26080;&#27861;&#31363;&#21548;&#21040;&#20010;&#20307;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.13992v2 Announce Type: replace-cross  Abstract: A key feature of federated learning (FL) is to preserve the data privacy of end users. However, there still exist potential privacy leakage in exchanging gradients under FL. As a result, recent research often explores the differential privacy (DP) approaches to add noises to the computing results to address privacy concerns with low overheads, which however degrade the model performance. In this paper, we strike the balance of data privacy and efficiency by utilizing the pervasive social connections between users. Specifically, we propose SCFL, a novel Social-aware Clustered Federated Learning scheme, where mutually trusted individuals can freely form a social cluster and aggregate their raw model updates (e.g., gradients) inside each cluster before uploading to the cloud for global aggregation. By mixing model updates in a social group, adversaries can only eavesdrop the social-layer combined results, but not the privacy of in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2212.04672</link><description>&lt;p&gt;
Primal Dual Alternating Proximal Gradient&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04672
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20809;&#28369;&#38750;&#20984;&#65288;&#24378;&#65289;&#20985;&#21644;&#38750;&#20984;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#21407;&#22987;&#23545;&#20598;&#20132;&#26367;&#36817;&#31471;&#26799;&#24230;&#65288;PDAPG&#65289;&#31639;&#27861;&#21644;&#21407;&#22987;&#23545;&#20598;&#36817;&#31471;&#26799;&#24230;&#65288;PDPG-L&#65289;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#35777;&#26126;&#20026; $\mathcal{O}\left( \varepsilon ^{-2} \right)$ &#65288;&#23545;&#24212; $\mathcal{O}\left( \varepsilon ^{-4} \right)$&#65289;&#22312;&#38750;&#20984;&#24378;&#20985; &#65288;&#23545;&#24212;&#38750;&#20984;&#20985;&#65289;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450; $\mathcal{O}\left( \varepsilon ^{-3} \right)$ &#22312;&#38750;&#20984;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#36798;&#21040; $\varepsilon$-&#31283;&#24577;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#26159;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#31532;&#19968;&#25209;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04672v3 Announce Type: replace-cross  Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm and a primal-dual proximal gradient (PDPG-L) algorithm for solving nonsmooth nonconvex-(strongly) concave and nonconvex-linear minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting and $\mathcal{O}\left( \varepsilon ^{-3} \right)$ under nonconvex-linear setting to reach an $\varepsilon$-stationary point, respectively. To our knowledge, they are the first two algorithms with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22238;&#31572;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#22810;&#26679;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2212.03827</link><description>&lt;p&gt;
&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Discovering Latent Knowledge in Language Models Without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22238;&#31572;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#22810;&#26679;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#25216;&#26415;&#21487;&#33021;&#19982;&#30495;&#30456;&#19981;&#19968;&#33268;&#65306;&#22914;&#26524;&#25105;&#20204;&#29992;&#27169;&#20223;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#37325;&#29616;&#20154;&#31867;&#30340;&#38169;&#35823;&#65307;&#22914;&#26524;&#25105;&#20204;&#35757;&#32451;&#23427;&#20204;&#29983;&#25104;&#20154;&#31867;&#35780;&#20215;&#39640;&#30340;&#25991;&#26412;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#36755;&#20986;&#20154;&#31867;&#35780;&#20272;&#32773;&#26080;&#27861;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#26469;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19988;&#26159;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#21482;&#32473;&#23450;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#25214;&#21040;&#28385;&#36275;&#36923;&#36753;&#19968;&#33268;&#24615;&#23646;&#24615;&#30340;&#26041;&#21521;&#26469;&#24037;&#20316;&#65292;&#20363;&#22914;&#19968;&#20010;&#38472;&#36848;&#21450;&#20854;&#21542;&#23450;&#20855;&#26377;&#30456;&#21453;&#30340;&#30495;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#30417;&#30563;&#21644;&#27169;&#22411;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#34920;&#22810;&#26679;&#30693;&#35782;&#65306;&#22312;6&#20010;&#27169;&#22411;&#21644;10&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03827v2 Announce Type: replace-cross  Abstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21516;&#26102;&#34920;&#31034;&#19968;&#31995;&#21015;&#29366;&#24577;&#65292;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;&#19981;&#21516;&#37327;&#23376;&#24577;&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22522;&#24577;&#30340;&#20219;&#24847;&#23616;&#37096;&#24615;&#36136;&#65292;&#26080;&#38656;&#20026;&#26032;&#30340;&#21487;&#35266;&#27979;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;</title><link>https://arxiv.org/abs/2211.16943</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#39044;&#27979;&#37327;&#23376;&#31995;&#32479;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting Properties of Quantum Systems with Conditional Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.16943
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21516;&#26102;&#34920;&#31034;&#19968;&#31995;&#21015;&#29366;&#24577;&#65292;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;&#19981;&#21516;&#37327;&#23376;&#24577;&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22522;&#24577;&#30340;&#20219;&#24847;&#23616;&#37096;&#24615;&#36136;&#65292;&#26080;&#38656;&#20026;&#26032;&#30340;&#21487;&#35266;&#27979;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26368;&#36817;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#65292;&#29992;&#20110;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;&#23545;&#20110;&#35768;&#22810;&#38388;&#38553;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#65292;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20174;&#21333;&#20010;&#37327;&#23376;&#24577;&#30340;&#27979;&#37327;&#20013;&#23398;&#20064;&#65292;&#31934;&#30830;&#37325;&#26500;&#20986;&#29366;&#24577;&#65292;&#20174;&#32780;&#39044;&#27979;&#23616;&#37096;&#21487;&#35266;&#27979;&#37327;&#12290;&#21478;&#22806;&#65292;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#20294;&#30456;&#20851;&#29366;&#24577;&#30340;&#27979;&#37327;&#26469;&#39044;&#27979;&#23616;&#37096;&#21487;&#35266;&#27979;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26469;&#21516;&#26102;&#34920;&#31034;&#19968;&#31995;&#21015;&#29366;&#24577;&#65292;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;&#19981;&#21516;&#37327;&#23376;&#24577;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#22522;&#24577;&#30340;&#20219;&#24847;&#23616;&#37096;&#24615;&#36136;&#65292;&#29978;&#33267;&#23545;&#20110;&#26410;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#65292;&#20063;&#26080;&#38656;&#20026;&#26032;&#30340;&#21487;&#35266;&#27979;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#20108;&#32500;&#38543;&#26426;&#28023;&#26862;&#22561;&#27169;&#22411;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.16943v2 Announce Type: replace-cross  Abstract: Machine learning has emerged recently as a powerful tool for predicting properties of quantum many-body systems. For many ground states of gapped Hamiltonians, generative models can learn from measurements of a single quantum state to reconstruct the state accurately enough to predict local observables. Alternatively, classification and regression models can predict local observables by learning from measurements on different but related states. In this work, we combine the benefits of both approaches and propose the use of conditional generative models to simultaneously represent a family of states, learning shared structures of different quantum states from measurements. The trained model enables us to predict arbitrary local properties of ground states, even for states not included in the training data, without necessitating further training for new observables. We first numerically validate our approach on 2D random Heisenb
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#38408;&#20540;&#26469;&#32771;&#34385;&#25968;&#25454;&#38598;&#20013;&#25935;&#24863;&#23646;&#24615;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2211.10209</link><description>&lt;p&gt;
&#21033;&#29992;&#31639;&#27861;&#20844;&#24179;&#24615;&#20943;&#36731;&#40657;&#30418;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#38408;&#20540;&#26469;&#32771;&#34385;&#25968;&#25454;&#38598;&#20013;&#25935;&#24863;&#23646;&#24615;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24050;&#32463;&#34987;&#37096;&#32626;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#21009;&#20107;&#21496;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ML&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#19968;&#20123;&#32972;&#26223;&#30693;&#35782;&#35757;&#32451;ML&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#21306;&#20998;&#30340;&#27169;&#22411;&#39044;&#27979;&#26469;&#25512;&#26029;&#25935;&#24863;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#23545;&#25915;&#20987;&#32773;&#30340;&#32972;&#26223;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#25935;&#24863;&#23646;&#24615;&#30340;&#36793;&#38469;&#20998;&#24067;&#65289;&#26377;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#20250;&#24102;&#26469;&#27604;&#32479;&#35745;&#25512;&#26029;&#26356;&#22810;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#25915;&#20987;&#24182;&#26410;&#32771;&#34385;&#26469;&#33258;&#30495;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#25968;&#25454;&#38598;&#20013;&#25935;&#24863;&#23646;&#24615;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;&#20363;&#22914;&#65292;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#27492;&#19981;&#24179;&#34913;&#30340;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#20351;&#29992;&#25915;&#20987;&#27169;&#22411;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10209v2 Announce Type: replace  Abstract: Machine learning (ML) models have been deployed for high-stakes applications, e.g., healthcare and criminal justice. Prior work has shown that ML models are vulnerable to attribute inference attacks where an adversary, with some background knowledge, trains an ML attack model to infer sensitive attributes by exploiting distinguishable model predictions. However, some prior attribute inference attacks have strong assumptions about adversary's background knowledge (e.g., marginal distribution of sensitive attribute) and pose no more privacy risk than statistical inference. Moreover, none of the prior attacks account for class imbalance of sensitive attribute in datasets coming from real-world applications (e.g., Race and Sex). In this paper, we propose an practical and effective attribute inference attack that accounts for this imbalance using an adaptive threshold over the attack model's predictions. We exhaustively evaluate our propo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#21512;&#25104;&#23436;&#25972;&#22797;&#26434;&#24418;&#29366;&#38598;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#34394;&#25311;&#22855;&#32654;&#25289;&#20154;&#32676;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#21307;&#30103;&#22120;&#26800;&#30340;&#30789;&#20013;&#35797;&#39564;&#35777;</title><link>https://arxiv.org/abs/2210.01607</link><description>&lt;p&gt;
&#19968;&#20010;&#29983;&#25104;&#24418;&#29366;&#32452;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#34394;&#25311;&#22855;&#32654;&#25289;&#20154;&#32676;
&lt;/p&gt;
&lt;p&gt;
A Generative Shape Compositional Framework to Synthesise Populations of Virtual Chimaeras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01607
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#21512;&#25104;&#23436;&#25972;&#22797;&#26434;&#24418;&#29366;&#38598;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#34394;&#25311;&#22855;&#32654;&#25289;&#20154;&#32676;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#21307;&#30103;&#22120;&#26800;&#30340;&#30789;&#20013;&#35797;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25429;&#33719;&#36275;&#22815;&#21464;&#24322;&#24615;&#19988;&#20445;&#25345;&#21512;&#29702;&#30340;&#34394;&#25311;&#35299;&#21078;&#20154;&#32676;&#23545;&#20110;&#36827;&#34892;&#21307;&#30103;&#22120;&#26800;&#30340;&#30789;&#20013;&#35797;&#39564;&#35777;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#27599;&#20010;&#20010;&#20307;&#37117;&#25317;&#26377;&#25152;&#38656;&#30340;&#35299;&#21078;&#24418;&#29366;&#12290;&#22240;&#27492;&#65292;&#22312;&#20154;&#32676;&#20013;&#24120;&#24120;&#23384;&#22312;&#32570;&#22833;/&#37096;&#20998;&#37325;&#21472;&#30340;&#35299;&#21078;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#30340;&#29983;&#25104;&#24418;&#29366;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#26410;&#37197;&#23545;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#23436;&#25972;&#30340;&#22797;&#26434;&#24418;&#29366;&#38598;&#21512;&#65292;&#31216;&#20026;&#34394;&#25311;&#22855;&#32654;&#25289;&#65292;&#19982;&#33258;&#28982;&#20154;&#31867;&#22855;&#32654;&#25289;&#30456;&#23545;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;&#25972;&#20010;&#24515;&#24418;&#32452;&#20214;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314;&#34394;&#25311;&#22855;&#32654;&#25289;&#65292;&#20854;&#20013;&#27599;&#20010;&#22855;&#32654;&#25289;&#36129;&#29486;&#24515;&#33039;&#27425;&#32423;&#32467;&#26500;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#24418;&#29366;&#32452;&#21512;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214; - &#19968;&#20010;&#37096;&#20998;&#24863;&#30693;&#30340;&#29983;&#25104;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01607v2 Announce Type: replace-cross  Abstract: Generating virtual populations of anatomy that capture sufficient variability while remaining plausible is essential for conducting in-silico trials of medical devices. However, not all anatomical shapes of interest are always available for each individual in a population. Hence, missing/partially-overlapping anatomical information is often available across individuals in a population. We introduce a generative shape model for complex anatomical structures, learnable from datasets of unpaired datasets. The proposed generative model can synthesise complete whole complex shape assemblies coined virtual chimaeras, as opposed to natural human chimaeras. We applied this framework to build virtual chimaeras from databases of whole-heart shape assemblies that each contribute samples for heart substructures. Specifically, we propose a generative shape compositional framework which comprises two components - a part-aware generative shap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#24182;&#21033;&#29992;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32959;&#30244;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2204.05798</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#36229;&#22797;&#25968;&#23398;&#20064;&#29992;&#20110;&#20083;&#33146;&#30284;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-View Hypercomplex Learning for Breast Cancer Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.05798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#24182;&#21033;&#29992;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32959;&#30244;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25191;&#34892;&#21333;&#35270;&#22270;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20083;&#33146;X-ray&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#21516;&#26102;&#20998;&#26512;&#32452;&#25104;&#20083;&#25151;X&#20809;&#25668;&#24433;&#26816;&#26597;&#30340;&#25152;&#26377;&#22235;&#20010;&#35270;&#22270;&#65292;&#36825;&#20026;&#35782;&#21035;&#32959;&#30244;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25552;&#20986;&#22810;&#35270;&#22270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26679;&#30340;&#29616;&#26377;&#26550;&#26500;&#20013;&#65292;&#20083;&#25151;X&#20809;&#22270;&#20687;&#34987;&#29420;&#31435;&#30340;&#21367;&#31215;&#20998;&#25903;&#22788;&#29702;&#20026;&#29420;&#31435;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#22833;&#21435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#12290;&#30001;&#20110;&#36229;&#22797;&#25968;&#20195;&#25968;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#24314;&#27169;&#24182;&#21033;&#29992;&#32452;&#25104;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#29616;&#26377;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#27169;&#25311;&#38405;&#29255;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.05798v3 Announce Type: replace-cross  Abstract: Traditionally, deep learning methods for breast cancer classification perform a single-view analysis. However, radiologists simultaneously analyze all four views that compose a mammography exam, owing to the correlations contained in mammography views, which present crucial information for identifying tumors. In light of this, some studies have started to propose multi-view methods. Nevertheless, in such existing architectures, mammogram views are processed as independent images by separate convolutional branches, thus losing correlations among them. To overcome such limitations, in this paper, we propose a methodological approach for multi-view breast cancer classification based on parameterized hypercomplex neural networks. Thanks to hypercomplex algebra properties, our networks are able to model, and thus leverage, existing correlations between the different views that comprise a mammogram, thus mimicking the reading process
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#26041;&#27861;&#22312;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#23454;&#29616;&#20102;&#26080;&#28903;&#24405;&#25104;&#26412;&#30340;&#26497;&#23567;&#26497;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2204.05275</link><description>&lt;p&gt;
&#35299;&#20915;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling the Sample Complexity of Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.05275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#26041;&#27861;&#22312;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#23454;&#29616;&#20102;&#26080;&#28903;&#24405;&#25104;&#26412;&#30340;&#26497;&#23567;&#26497;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#23427;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26377;&#25928;&#30340;&#31163;&#32447;RL&#24212;&#33021;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#25110;&#20998;&#26512;&#35201;&#20040;&#21463;&#21040;&#27425;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22256;&#25200;&#65292;&#35201;&#20040;&#20135;&#29983;&#39640;&#26114;&#30340;&#28903;&#24405;&#25104;&#26412;&#20197;&#36798;&#21040;&#26679;&#26412;&#26368;&#20248;&#24615;&#65292;&#20174;&#32780;&#23545;&#26679;&#26412;&#21294;&#20047;&#24212;&#29992;&#20013;&#30340;&#39640;&#25928;&#31163;&#32447;RL&#26500;&#25104;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21160;&#24577;&#26356;&#26032;&#20010;&#24615;&#21270;&#36816;&#21160;&#30446;&#26631;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#29992;&#25143;&#21160;&#24577;&#34892;&#20026;&#21644;&#20581;&#24247;&#29366;&#20917;&#21464;&#21270;&#30340;&#24573;&#35270;&#12290;</title><link>https://arxiv.org/abs/2204.00961</link><description>&lt;p&gt;
&#21152;&#24378;&#25968;&#23383;&#20581;&#24247;&#26381;&#21153;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20010;&#24615;&#21270;&#36816;&#21160;&#30446;&#26631;&#35774;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Digital Health Services: A Machine Learning Approach to Personalized Exercise Goal Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.00961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21160;&#24577;&#26356;&#26032;&#20010;&#24615;&#21270;&#36816;&#21160;&#30446;&#26631;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#29992;&#25143;&#21160;&#24577;&#34892;&#20026;&#21644;&#20581;&#24247;&#29366;&#20917;&#21464;&#21270;&#30340;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25968;&#23383;&#20581;&#24247;&#30340;&#21033;&#29992;&#29575;&#26377;&#25152;&#22686;&#21152;&#65292;&#36825;&#20123;&#26381;&#21153;&#25552;&#20379;&#24191;&#27867;&#25351;&#23548;&#65292;&#36890;&#36807;&#35774;&#23450;&#27599;&#26085;&#36816;&#21160;&#30446;&#26631;&#26469;&#40723;&#21169;&#29992;&#25143;&#36827;&#34892;&#39057;&#32321;&#36816;&#21160;&#65292;&#20419;&#36827;&#20581;&#24247;&#30340;&#29983;&#27963;&#26041;&#24335;&#12290;&#36825;&#20123;&#20840;&#38754;&#30340;&#25351;&#21335;&#26159;&#20174;&#32771;&#34385;&#21508;&#31181;&#20010;&#24615;&#21270;&#34892;&#20026;&#22240;&#32032;&#21457;&#23637;&#32780;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#29992;&#25143;&#30340;&#21160;&#24577;&#34892;&#20026;&#21644;&#20854;&#20581;&#24247;&#29366;&#20917;&#30340;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22238;&#39038;&#24615;&#25968;&#25454;&#21644;&#29616;&#23454;&#34892;&#20026;&#36712;&#36857;&#21160;&#24577;&#26356;&#26032;&#33258;&#21160;&#24314;&#35758;&#30340;&#36816;&#21160;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#26041;&#27861;&#23398;&#30740;&#31350;&#65292;&#35780;&#20272;&#36816;&#21160;&#34920;&#29616;&#65292;&#32771;&#34385;&#21040;&#20307;&#33021;&#30130;&#21171;&#25928;&#24212;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24182;&#25512;&#26029;&#29992;&#25143;&#30340;&#36816;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.00961v3 Announce Type: replace  Abstract: The utilization of digital health has increased recently, and these services provide extensive guidance to encourage users to exercise frequently by setting daily exercise goals to promote a healthy lifestyle. These comprehensive guides evolved from the consideration of various personalized behavioral factors. Nevertheless, existing approaches frequently neglect the users dynamic behavior and the changing in their health conditions. This study aims to fill this gap by developing a machine learning algorithm that dynamically updates auto-suggestion exercise goals using retrospective data and realistic behavior trajectory. We conducted a methodological study by designing a deep reinforcement learning algorithm to evaluate exercise performance, considering fitness-fatigue effects. The deep reinforcement learning algorithm combines deep learning techniques to analyse time series data and infer user exercise behavior. In addition, we use 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CMGAN&#65289;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;&#35821;&#38899;&#22686;&#24378;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#20351;&#24471;&#22686;&#24378;&#20272;&#35745;&#35821;&#38899;&#30456;&#23545;&#24212;&#30340;&#35780;&#20272;&#20998;&#25968;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#22686;&#24378;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2203.15149</link><description>&lt;p&gt;
CMGAN&#65306;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;GAN&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
CMGAN: Conformer-based Metric GAN for Speech Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.15149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CMGAN&#65289;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;&#35821;&#38899;&#22686;&#24378;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#20351;&#24471;&#22686;&#24378;&#20272;&#35745;&#35821;&#38899;&#30456;&#23545;&#24212;&#30340;&#35780;&#20272;&#20998;&#25968;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#22686;&#24378;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21367;&#31215;&#22686;&#24378;&#21464;&#21387;&#22120;&#65288;Conformer&#65289;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#26102;&#22495;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#25417;&#35821;&#38899;&#20449;&#21495;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CMGAN&#65289;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;SE&#12290;&#22312;&#29983;&#25104;&#22120;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#38454;&#27573;&#30340;Conformer&#22359;&#36890;&#36807;&#23545;&#26102;&#38388;&#21644;&#39057;&#29575;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#32858;&#21512;&#25152;&#26377;&#24133;&#24230;&#21644;&#22797;&#25968;&#35889;&#20449;&#24687;&#12290;&#22312;&#35299;&#30721;&#22120;&#38454;&#27573;&#65292;&#24133;&#24230;&#21644;&#22797;&#25968;&#35889;&#30340;&#20272;&#35745;&#34987;&#35299;&#32806;&#65292;&#28982;&#21518;&#19968;&#36215;&#21512;&#24182;&#20197;&#37325;&#26500;&#22686;&#24378;&#30340;&#35821;&#38899;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#37327;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#20351;&#24471;&#22686;&#24378;&#20272;&#35745;&#35821;&#38899;&#30456;&#23545;&#24212;&#30340;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22686;&#24378;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;&#22312;Voice Bank+DEMAND&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.15149v4 Announce Type: replace-cross  Abstract: Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech enhancement (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20266;&#24494;&#20998;&#31215;&#20998;&#31639;&#23376;&#65288;PDIO&#65289;&#26469;&#20998;&#26512;&#21644;&#25512;&#24191;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#20013;&#30340;&#20613;&#31435;&#21494;&#31215;&#20998;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#30340;&#24179;&#28369;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;PDIO&#26159;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#36830;&#32493;&#20316;&#29992;&#20110;Sobolev&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2201.11967</link><description>&lt;p&gt;
&#20266;&#24494;&#20998;&#31070;&#32463;&#31639;&#23376;&#65306;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#23376;&#30340;&#24191;&#20041;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Differential Neural Operator: Generalized Fourier Neural Operator for Learning Solution Operators of Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.11967
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20266;&#24494;&#20998;&#31215;&#20998;&#31639;&#23376;&#65288;PDIO&#65289;&#26469;&#20998;&#26512;&#21644;&#25512;&#24191;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#20013;&#30340;&#20613;&#31435;&#21494;&#31215;&#20998;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#30340;&#24179;&#28369;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;PDIO&#26159;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#36830;&#32493;&#20316;&#29992;&#20110;Sobolev&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20004;&#20010;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#35299;&#31639;&#23376;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26469;&#23398;&#20064;&#35299;&#31639;&#23376;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;\textit{&#20266;&#24494;&#20998;&#31215;&#20998;&#31639;&#23376;}&#65288;PDIO&#65289;&#26469;&#20998;&#26512;&#21644;&#25512;&#24191;FNO&#20013;&#30340;&#20613;&#31435;&#21494;&#31215;&#20998;&#31639;&#23376;&#12290;PDIO&#30340;&#28789;&#24863;&#26469;&#33258;&#20266;&#24494;&#20998;&#31639;&#23376;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#26576;&#20010;&#31526;&#21495;&#29305;&#24449;&#21270;&#30340;&#24191;&#20041;&#24494;&#20998;&#31639;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#36825;&#20010;&#31526;&#21495;&#65292;&#24182;&#23637;&#31034;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#21253;&#21547;&#22312;&#24179;&#28369;&#31526;&#21495;&#31867;&#20013;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;PDIO&#26159;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#22240;&#27492;&#22312;Sobolev&#31354;&#38388;&#20013;&#26159;&#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#23558;PDIO&#19982;&#31070;&#32463;&#31639;&#23376;&#32467;&#21512;&#36215;&#26469;&#65292;&#21457;&#23637;&#19968;&#31181;\text
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.11967v3 Announce Type: replace  Abstract: Learning the mapping between two function spaces has garnered considerable research attention. However, learning the solution operator of partial differential equations (PDEs) remains a challenge in scientific computing. Fourier neural operator (FNO) was recently proposed to learn solution operators, and it achieved an excellent performance. In this study, we propose a novel \textit{pseudo-differential integral operator} (PDIO) to analyze and generalize the Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which is a generalized differential operator characterized by a certain symbol. We parameterize this symbol using a neural network and demonstrate that the neural network-based symbol is contained in a smooth symbol class. Subsequently, we verify that the PDIO is a bounded linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with the neural operator to develop a \text
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#32452;&#32455;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#65292;&#36890;&#36807;&#36141;&#20080;&#22806;&#37096;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#36741;&#21161;&#26381;&#21153;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#38024;&#23545;&#26377;&#38480;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#38543;&#26426;&#35757;&#32451;&#31639;&#27861;</title><link>https://arxiv.org/abs/2109.09307</link><description>&lt;p&gt;
&#38754;&#21521;&#25968;&#25454;&#26377;&#38480;&#19981;&#24179;&#34913;&#30340;&#32452;&#32455;&#30340;&#36741;&#21161;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Assisted Learning for Organizations with Limited Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.09307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#32452;&#32455;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#65292;&#36890;&#36807;&#36141;&#20080;&#22806;&#37096;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#36741;&#21161;&#26381;&#21153;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#38024;&#23545;&#26377;&#38480;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#38543;&#26426;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#35768;&#22810;&#22823;&#22411;&#32452;&#32455;&#27491;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#21040;&#20854;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#20197;&#20419;&#36827;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20182;&#20204;&#21487;&#33719;&#24471;&#30340;&#25968;&#25454;&#26377;&#38480;&#19988;&#19981;&#24179;&#34913;&#65292;&#20182;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#21327;&#21161;&#32452;&#32455;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#24320;&#21457;&#20102;&#19968;&#20010;&#36741;&#21161;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#20123;&#32452;&#32455;&#20855;&#26377;&#36275;&#22815;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20294;&#21463;&#21046;&#20110;&#20005;&#26684;&#30340;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#20316;&#25919;&#31574;&#12290;&#20182;&#20204;&#30340;&#26377;&#38480;&#19988;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#24448;&#24448;&#23548;&#33268;&#26377;&#20559;&#25512;&#26029;&#21644;&#27425;&#20248;&#20915;&#31574;&#12290;&#22312;&#36741;&#21161;&#23398;&#20064;&#20013;&#65292;&#32452;&#32455;&#23398;&#20064;&#32773;&#20174;&#22806;&#37096;&#26381;&#21153;&#25552;&#20379;&#21830;&#36141;&#20080;&#36741;&#21161;&#26381;&#21153;&#65292;&#26088;&#22312;&#22312;&#20165;&#38656;&#20960;&#36718;&#36741;&#21161;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20854;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#21644;&#36741;&#21161;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#38543;&#26426;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.09307v4 Announce Type: replace  Abstract: In the era of big data, many big organizations are integrating machine learning into their work pipelines to facilitate data analysis. However, the performance of their trained models is often restricted by limited and imbalanced data available to them. In this work, we develop an assisted learning framework for assisting organizations to improve their learning performance. The organizations have sufficient computation resources but are subject to stringent data-sharing and collaboration policies. Their limited imbalanced data often cause biased inference and sub-optimal decision-making. In assisted learning, an organizational learner purchases assistance service from an external service provider and aims to enhance its model performance within only a few assistance rounds. We develop effective stochastic training algorithms for both assisted deep learning and assisted reinforcement learning. Different from existing distributed algor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#36793;&#24418;&#26410;&#35843;&#25972;&#30340;&#26391;&#20043;&#19975;&#31639;&#27861;&#30340;&#26032;&#31867;&#21035;&#31639;&#27861;&#65292;&#21517;&#20026;TH$\varepsilon$O POULA&#65288;&#25110;&#31616;&#31216;&#20026;TheoPouLa&#65289;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#12289;&#38750;&#28176;&#36827;&#20998;&#26512;&#21644;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2105.13937</link><description>&lt;p&gt;
&#22810;&#36793;&#24418;&#26410;&#35843;&#25972;&#30340;&#26391;&#20043;&#19975;&#31639;&#27861;&#65306;&#20026;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#31283;&#23450;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.13937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#36793;&#24418;&#26410;&#35843;&#25972;&#30340;&#26391;&#20043;&#19975;&#31639;&#27861;&#30340;&#26032;&#31867;&#21035;&#31639;&#27861;&#65292;&#21517;&#20026;TH$\varepsilon$O POULA&#65288;&#25110;&#31616;&#31216;&#20026;TheoPouLa&#65289;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#12289;&#38750;&#28176;&#36827;&#20998;&#26512;&#21644;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26391;&#20043;&#19975;&#31639;&#27861;&#30340;&#31639;&#27861;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#29992;&#20110;&#24494;&#35843;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#30340;&#35768;&#22810;&#24050;&#30693;&#32570;&#38519;&#12290;&#20854;&#29702;&#35770;&#22522;&#30784;&#20381;&#36182;&#20110;&#36817;&#26399;&#23545;&#20110;&#20855;&#26377;&#21333;&#35843;&#31995;&#25968;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#30340;&#27431;&#25289;&#22810;&#36793;&#24418;&#36924;&#36817;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#23427;&#32487;&#25215;&#20102;&#28201;&#21644;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20854;&#20182;&#24050;&#30693;&#38382;&#39064;&#65292;&#20363;&#22914;&#26799;&#24230;&#28040;&#22833;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#26032;&#31867;&#21035;&#31639;&#27861;&#30340;&#25910;&#25947;&#29305;&#24615;&#36827;&#34892;&#20102;&#38750;&#28176;&#36827;&#20998;&#26512;&#21644;&#20840;&#38754;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#31639;&#27861;&#21629;&#21517;&#20026;TH$\varepsilon$O POULA&#65288;&#25110;&#31616;&#31216;&#20026;TheoPouLa&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#30340;&#20960;&#20010;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;TheoPouLa&#30456;&#23545;&#20110;&#35768;&#22810;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#31639;&#27861;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.13937v3 Announce Type: replace  Abstract: We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26377;&#29702;&#25968;&#20316;&#20026;&#36866;&#24212;&#24615;&#28608;&#27963;&#20989;&#25968;&#26469;&#25913;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23558;&#31616;&#21333;&#30340;DQN&#25552;&#21319;&#20026;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2102.09407</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26377;&#29702;&#28608;&#27963;&#20197;&#25552;&#21319;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Rational Activations to Boost Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.09407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26377;&#29702;&#25968;&#20316;&#20026;&#36866;&#24212;&#24615;&#28608;&#27963;&#20989;&#25968;&#26469;&#25913;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23558;&#31616;&#21333;&#30340;DQN&#25552;&#21319;&#20026;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#30340;&#26368;&#26032;&#35265;&#35299;&#26174;&#31034;&#65292;&#26234;&#33021;&#19981;&#20165;&#28304;&#33258;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#19988;&#21333;&#20010;&#31070;&#32463;&#20803;&#25215;&#25285;&#30340;&#35745;&#31639;&#36131;&#20219;&#27604;&#20197;&#24448;&#39044;&#26399;&#30340;&#26356;&#22810;&#12290;&#36825;&#31181;&#35266;&#28857;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#26041;&#27861;&#20173;&#28982;&#20027;&#35201;&#20351;&#29992;&#38745;&#24577;&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#20026;&#20160;&#20040;&#26377;&#29702;&#25968;&#36866;&#21512;&#20316;&#20026;&#21487;&#36866;&#24212;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#23558;&#20854;&#21253;&#21547;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;Residual&#32593;&#32476;&#20013;&#24490;&#29615;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#26377;&#29702;&#21333;&#20301;&#22312;&#27531;&#24046;&#36830;&#25509;&#19979;&#23553;&#38381;&#30340;&#26465;&#20214;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#33258;&#28982;&#27491;&#21017;&#21270;&#30340;&#29256;&#26412;&#65306;&#24490;&#29615;&#26377;&#29702;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20026;&#27969;&#34892;&#31639;&#27861;&#37197;&#22791;&#65288;&#24490;&#29615;&#65289;&#26377;&#29702;&#25968;&#28608;&#27963;&#20250;&#26174;&#33879;&#25552;&#39640;Atari&#28216;&#25103;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23558;&#31616;&#21333;&#30340;DQN&#36716;&#21270;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.09407v4 Announce Type: replace  Abstract: Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competiti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#30340;&#21516;&#26102;&#36820;&#22238;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#39044;&#27979;&#38598;&#12290;</title><link>https://arxiv.org/abs/2102.06202</link><description>&lt;p&gt;
&#31169;&#20154;&#39044;&#27979;&#38598;
&lt;/p&gt;
&lt;p&gt;
Private Prediction Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.06202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#30340;&#21516;&#26102;&#36820;&#22238;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#37325;&#35201;&#20915;&#31574;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#21516;&#26102;&#35270;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#39044;&#27979;&#27169;&#22411;&#65292;&#36820;&#22238;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#39044;&#27979;&#38598;&#65292;&#36825;&#20123;&#38598;&#21512;&#21487;&#20197;&#35777;&#26126;&#20197;&#29992;&#25143;&#25351;&#23450;&#30340;&#27010;&#29575;&#65288;&#22914;90%&#65289;&#35206;&#30422;&#30495;&#23454;&#21709;&#24212;&#12290;&#24403;&#19982;&#32463;&#36807;&#31169;&#20154;&#35757;&#32451;&#30340;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#20154;&#20204;&#21487;&#33021;&#24076;&#26395;&#31526;&#21512;&#24615;&#39044;&#27979;&#20250;&#20026;&#29983;&#25104;&#30340;&#39044;&#27979;&#38598;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65307;&#19981;&#24184;&#30340;&#26159;&#65292;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#36755;&#20986;&#24046;&#20998;&#31169;&#20154;&#39044;&#27979;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#20998;&#35010;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#25105;&#20204;&#20351;&#29992;&#20445;&#30041;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.06202v3 Announce Type: replace-cross  Abstract: In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately, this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31532;&#19968;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#25991;&#29486;&#20013;&#32974;&#20799;3D&#23039;&#21183;&#20272;&#35745;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#25552;&#21462;&#25972;&#20010;&#32974;&#20799;&#30340;&#39592;&#26550;&#24182;&#20998;&#37197;&#27491;&#30830;&#30340;&#36527;&#24178;/&#32930;&#20307;&#26631;&#31614;&#32473;&#19981;&#21516;&#30340;&#37096;&#20998;/&#20851;&#33410;&#12290;</title><link>https://arxiv.org/abs/1910.04935</link><description>&lt;p&gt;
FetusMap&#65306;3D&#36229;&#22768;&#32974;&#20799;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FetusMap: Fetal Pose Estimation in 3D Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.04935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31532;&#19968;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#25991;&#29486;&#20013;&#32974;&#20799;3D&#23039;&#21183;&#20272;&#35745;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#25552;&#21462;&#25972;&#20010;&#32974;&#20799;&#30340;&#39592;&#26550;&#24182;&#20998;&#37197;&#27491;&#30830;&#30340;&#36527;&#24178;/&#32930;&#20307;&#26631;&#31614;&#32473;&#19981;&#21516;&#30340;&#37096;&#20998;/&#20851;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#36229;&#22768;&#65288;US&#65289;&#25216;&#26415;&#28608;&#21457;&#20102;&#22823;&#37327;&#33258;&#21160;&#20135;&#21069;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#36229;&#22768;&#20013;&#23545;&#25972;&#20010;&#32974;&#20799;&#30340;&#32467;&#26500;&#21270;&#25551;&#36848;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#22312;US&#20307;&#31215;&#20013;&#20272;&#35745;&#32974;&#20799;&#30340;3D&#23039;&#21183;&#65292;&#20197;&#20419;&#36827;&#20854;&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#23610;&#24230;&#19978;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;3D US&#20013;&#23384;&#22312;&#24456;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#20307;&#31215;&#23610;&#23544;&#22823;&#12289;&#22270;&#20687;&#36136;&#37327;&#24046;&#12289;&#35299;&#21078;&#32467;&#26500;&#30340;&#23545;&#31216;&#24615;&#27169;&#31946;&#21644;&#32974;&#20799;&#23039;&#21183;&#30340;&#22823;&#21464;&#21270;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.04935v2 Announce Type: replace-cross  Abstract: The 3D ultrasound (US) entrance inspires a multitude of automated prenatal examinations. However, studies about the structuralized description of the whole fetus in 3D US are still rare. In this paper, we propose to estimate the 3D pose of fetus in US volumes to facilitate its quantitative analyses in global and local scales. Given the great challenges in 3D US, including the high volume dimension, poor image quality, symmetric ambiguity in anatomical structures and large variations of fetal pose, our contribution is three-fold. (i) This is the first work about 3D pose estimation of fetus in the literature. We aim to extract the skeleton of whole fetus and assign different segments/joints with correct torso/limb labels. (ii) We propose a self-supervised learning (SSL) framework to finetune the deep network to form visually plausible pose predictions. Specifically, we leverage the landmark-based registration to effectively encod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28909;&#21551;&#21160;&#21644;&#20027;&#21160;&#32456;&#27490;&#21151;&#33021;&#30340;&#20195;&#29702;&#22120;&#65292;&#29992;&#20110;&#22312;3D&#36229;&#22768;&#20013;&#33258;&#21160;&#23450;&#20301;&#32974;&#20799;&#33041;&#26631;&#20934;&#24179;&#38754;</title><link>https://arxiv.org/abs/1910.04331</link><description>&lt;p&gt;
&#20855;&#26377;&#28909;&#21551;&#21160;&#21644;&#20027;&#21160;&#32456;&#27490;&#21151;&#33021;&#30340;&#20195;&#29702;&#22120;&#29992;&#20110;&#22312;3D&#36229;&#22768;&#20013;&#23450;&#20301;&#24179;&#38754;
&lt;/p&gt;
&lt;p&gt;
Agent with Warm Start and Active Termination for Plane Localization in 3D Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.04331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28909;&#21551;&#21160;&#21644;&#20027;&#21160;&#32456;&#27490;&#21151;&#33021;&#30340;&#20195;&#29702;&#22120;&#65292;&#29992;&#20110;&#22312;3D&#36229;&#22768;&#20013;&#33258;&#21160;&#23450;&#20301;&#32974;&#20799;&#33041;&#26631;&#20934;&#24179;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#24179;&#38754;&#23450;&#20301;&#23545;&#36229;&#22768;&#65288;US&#65289;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20135;&#21069;&#36229;&#22768;&#20013;&#65292;&#20351;&#29992;2D&#25506;&#22836;&#25163;&#21160;&#33719;&#21462;&#25968;&#21313;&#20010;&#26631;&#20934;&#24179;&#38754;&#26159;&#32791;&#26102;&#19988;&#20381;&#36182;&#25805;&#20316;&#32773;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21253;&#21547;&#22810;&#20010;&#26631;&#20934;&#24179;&#38754;&#30340;3D US&#25317;&#26377;&#26356;&#23569;&#30340;&#29992;&#25143;&#20381;&#36182;&#24615;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;US&#20307;&#31215;&#20013;&#25163;&#21160;&#23450;&#20301;&#24179;&#38754;&#30001;&#20110;&#24040;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#32974;&#20799;&#23039;&#21183;&#21464;&#21270;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#23450;&#20301;3D US&#20013;&#30340;&#32974;&#20799;&#33041;&#26631;&#20934;&#24179;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.04331v2 Announce Type: replace-cross  Abstract: Standard plane localization is crucial for ultrasound (US) diagnosis. In prenatal US, dozens of standard planes are manually acquired with a 2D probe. It is time-consuming and operator-dependent. In comparison, 3D US containing multiple standard planes in one shot has the inherent advantages of less user-dependency and more efficiency. However, manual plane localization in US volume is challenging due to the huge search space and large fetal posture variation. In this study, we propose a novel reinforcement learning (RL) framework to automatically localize fetal brain standard planes in 3D US. Our contribution is two-fold. First, we equip the RL framework with a landmark-aware alignment module to provide warm start and strong spatial bounds for the agent actions, thus ensuring its effectiveness. Second, instead of passively and empirically terminating the agent inference, we propose a recurrent neural network based strategy for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;3D&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37197;&#22791;&#30528;&#20851;&#27880;&#27169;&#22359;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#30452;&#32928;&#36229;&#22768;&#22270;&#20687;&#20013;&#26356;&#22909;&#22320;&#21069;&#21015;&#33146;&#20998;&#21106;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#25972;&#21512;&#19981;&#21516;&#23618;&#32423;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#21069;&#21015;&#33146;&#20998;&#21106;&#24615;&#33021;</title><link>https://arxiv.org/abs/1907.01743</link><description>&lt;p&gt;
&#19977;&#32500;&#32463;&#30452;&#32928;&#36229;&#22768;&#28145;&#24230;&#20851;&#27880;&#29305;&#24449;&#22312;&#21069;&#21015;&#33146;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1907.01743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;3D&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37197;&#22791;&#30528;&#20851;&#27880;&#27169;&#22359;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#30452;&#32928;&#36229;&#22768;&#22270;&#20687;&#20013;&#26356;&#22909;&#22320;&#21069;&#21015;&#33146;&#20998;&#21106;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#25972;&#21512;&#19981;&#21516;&#23618;&#32423;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#21069;&#21015;&#33146;&#20998;&#21106;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:1907.01743v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;&#24230;  &#25688;&#35201;: &#22312;&#22270;&#20687;&#24341;&#23548;&#30340;&#21069;&#21015;&#33146;&#24178;&#39044;&#21644;&#27835;&#30103;&#35745;&#21010;&#20013;&#65292;&#33258;&#21160;&#21069;&#21015;&#33146;&#20998;&#21106;&#22312;&#32463;&#30452;&#32928;&#36229;&#22768;(TRUS)&#22270;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110; TRUS &#20013;&#21069;&#21015;&#33146;&#30340;&#36793;&#30028;&#32570;&#22833;/&#27169;&#31946;&#21644;&#19981;&#22343;&#21248;&#30340;&#24378;&#24230;&#20998;&#24067;&#65292;&#20197;&#21450;&#21069;&#21015;&#33146;&#24418;&#29366;&#30340;&#22823;&#37327;&#21464;&#24322;&#24615;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#33258;&#21160;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#37197;&#22791;&#20851;&#27880;&#27169;&#22359;&#30340;&#26032;&#22411;3D&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#26469;&#26356;&#22909;&#22320;&#23545; TRUS &#20013;&#30340;&#21069;&#21015;&#33146;&#36827;&#34892;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#20851;&#27880;&#27169;&#22359;&#21033;&#29992;&#20851;&#27880;&#26426;&#21046;&#65292;&#26377;&#36873;&#25321;&#22320;&#21033;&#29992;&#19981;&#21516;&#23618;&#38598;&#25104;&#30340;&#22810;&#32423;&#29305;&#24449;&#26469;&#23436;&#21892;&#27599;&#20010;&#21333;&#29420;&#23618;&#30340;&#29305;&#24449;&#65292;&#25233;&#21046; CNN &#27973;&#23618;&#20013;&#30340;&#38750;&#21069;&#21015;&#33146;&#22122;&#22768;&#65292;&#24182;&#23558;&#26356;&#22810;&#21069;&#21015;&#33146;&#32454;&#33410;&#34701;&#20837;&#29305;&#24449;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1907.01743v2 Announce Type: replace-cross  Abstract: Automatic prostate segmentation in transrectal ultrasound (TRUS) images is of essential importance for image-guided prostate interventions and treatment planning. However, developing such automatic solutions remains very challenging due to the missing/ambiguous boundary and inhomogeneous intensity distribution of the prostate in TRUS, as well as the large variability in prostate shapes. This paper develops a novel 3D deep neural network equipped with attention modules for better prostate segmentation in TRUS by fully exploiting the complementary information encoded in different layers of the convolutional neural network (CNN). Our attention module utilizes the attention mechanism to selectively leverage the multilevel features integrated from different layers to refine the features at each individual layer, suppressing the non-prostate noise at shallow layers of the CNN and increasing more prostate details into features at deep
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.16549</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Multi-Label Learning: A Comprehensive Survey. (arXiv:2401.16549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16549
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#21333;&#20010;&#36755;&#20837;&#25968;&#25454;&#28857;&#20013;&#39044;&#27979;&#22810;&#20010;&#26631;&#31614;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#28041;&#21450;&#22810;&#26631;&#31614;&#20998;&#31867;&#25110;&#25490;&#21517;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#32780;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22810;&#26631;&#31614;&#20998;&#31867;&#38754;&#20020;&#30340;&#22256;&#38590;&#21253;&#25324;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#35299;&#20915;&#26631;&#31614;&#30456;&#20851;&#24615;&#21644;&#22788;&#29702;&#37096;&#20998;&#26631;&#31614;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#30740;&#31350;&#36824;&#27604;&#36739;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label learning is a rapidly growing research area that aims to predict multiple labels from a single input data point. In the era of big data, tasks involving multi-label classification (MLC) or ranking present significant and intricate challenges, capturing considerable attention in diverse domains. Inherent difficulties in MLC include dealing with high-dimensional data, addressing label correlations, and handling partial labels, for which conventional methods prove ineffective. Recent years have witnessed a notable increase in adopting deep learning (DL) techniques to address these challenges more effectively in MLC. Notably, there is a burgeoning effort to harness the robust learning capabilities of DL for improved modelling of label dependencies and other challenges in MLC. However, it is noteworthy that comprehensive studies specifically dedicated to DL for multi-label learning are limited. Thus, this survey aims to thoroughly review recent progress in DL for multi-label lea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#26159;&#35299;&#20915;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.15422</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#26159;&#35299;&#20915;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#35821;&#35328;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#36817;&#20284;&#20154;&#31867;&#32423;&#26234;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#22823;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#26356;&#26032;&#65292;&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20648;&#22791;&#21487;&#33021;&#24456;&#24555;&#29992;&#23613;&#12290;&#36825;&#20010;&#25361;&#25112;&#20652;&#29983;&#20102;&#22823;&#37327;&#20851;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#21033;&#29992;&#22823;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#21512;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#35814;&#23613;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#30456;&#20851;&#30740;&#31350;&#20998;&#20026;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;&#22823;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#30456;&#20851;&#30340;&#21508;&#31181;&#25968;&#25454;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data au
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22810;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20855;&#26377;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#25110;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.15030</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#36890;&#29992;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the generalization capacity of neural networks during generic multimodal reasoning. (arXiv:2401.15030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22810;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20855;&#26377;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#25110;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;, &#36825;&#20123;&#27169;&#22411;&#20284;&#20046;&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31867;&#27169;&#22411;&#21644;&#20854;&#20182;&#22522;&#26412;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#30340;&#19968;&#33324;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;&#26469;&#35780;&#20272;&#19977;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#24615;&#33021;&#65306;&#20998;&#24515;&#27867;&#21270;&#65288;&#22312;&#20998;&#24515;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#65289;&#65292;&#31995;&#32479;&#30340;&#32452;&#21512;&#27867;&#21270;&#65288;&#23545;&#26032;&#30340;&#20219;&#21153;&#25490;&#21015;&#30340;&#27867;&#21270;&#65289;&#21644;&#26377;&#30410;&#30340;&#32452;&#21512;&#27867;&#21270;&#65288;&#23545;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#27867;&#21270;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#19978;&#65288;&#22914;RNN&#65292;Transformer&#65292;Perceivers&#31561;&#65289;&#65292;&#20855;&#26377;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#25110;&#32773;&#21033;&#29992;&#36755;&#20837;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#31215;&#26497;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#22810;&#27169;&#24577;&#27867;&#21270;&#65292;&#27169;&#22411;&#26550;&#26500;&#26159;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures). We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better. Our positive results demonstrate that for multi
&lt;/p&gt;</description></item><item><title>FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.14112</link><description>&lt;p&gt;
FP6-LLM: &#36890;&#36807;FP6&#20013;&#24515;&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#39640;&#25928;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14112
&lt;/p&gt;
&lt;p&gt;
FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20845;&#20301;&#37327;&#21270;&#65288;FP6&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22823;&#23567;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#19981;&#25552;&#20379;FP6&#37327;&#21270;&#30340;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#65292;&#24182;&#19988;&#22312;LLM&#25512;&#26029;&#36807;&#31243;&#20013;&#24456;&#38590;&#23454;&#29616;&#23454;&#38469;&#24615;&#33021;&#25913;&#36827;&#12290;&#30001;&#20110;&#65288;1&#65289;&#27169;&#22411;&#26435;&#37325;&#20855;&#26377;&#19981;&#35268;&#21017;&#20301;&#23485;&#30340;&#19981;&#21451;&#22909;&#20869;&#23384;&#35775;&#38382;&#21644;&#65288;2&#65289;&#26435;&#37325;&#21435;&#37327;&#21270;&#30340;&#39640;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#25903;&#25345;&#22312;GPU&#19978;&#36827;&#34892;FP6&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TC-FPx&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#32479;&#19968;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#30340;&#28014;&#28857;&#26435;&#37325;&#30340;&#23436;&#25972;GPU&#20869;&#26680;&#35774;&#35745;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#37327;&#21270;&#20301;&#23485;&#12290;&#25105;&#20204;&#23558;TC-FPx&#20869;&#26680;&#38598;&#25104;&#21040;&#29616;&#26377;&#25512;&#26029;&#31995;&#32479;&#20013;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#31471;&#21040;&#31471;&#25903;&#25345;&#65288;&#31216;&#20026;FP6-LLM&#65289;&#29992;&#20110;&#37327;&#21270;LLM&#25512;&#26029;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FP6-LLM&#20165;&#20351;&#29992;&#19968;&#37096;&#20998;&#23384;&#20648;&#31354;&#38388;&#23601;&#21487;&#20197;&#36827;&#34892;LLaMA-70b&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.12624</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#22522;&#20110;&#35821;&#35328;&#21040;&#26032;&#20852;&#36890;&#20449;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control. (arXiv:2401.12624v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12624
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#30340;&#26032;&#20852;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;LSC&#65289;&#12290;&#22312;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#23548;&#33322;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#21253;&#21547;&#20301;&#32622;&#21644;&#36890;&#36947;&#22320;&#22270;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;EC&#22312;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#20250;&#20135;&#29983;&#39640;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#22256;&#38590;&#65292;&#32780;LSC&#30001;&#20110;LLM&#23610;&#23544;&#36739;&#22823;&#65292;&#20250;&#23548;&#33268;&#39640;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#23427;&#20204;&#21508;&#33258;&#30340;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24341;&#23548;EC&#35757;&#32451;&#20351;&#29992;LSC&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#35821;&#35328;&#24341;&#23548;&#30340;EC&#65288;LEC&#65289;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;LEC&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#65292;&#36991;&#20813;&#20102;&#20449;&#36947;&#36136;&#37327;&#24046;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#19982;EC&#30456;&#27604;&#33021;&#22815;&#21152;&#36895;MADRL&#35757;&#32451;&#25910;&#25947;&#36798;&#21040;61.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.
&lt;/p&gt;</description></item><item><title>PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.11202</link><description>&lt;p&gt;
PartIR: &#20026;&#26426;&#22120;&#23398;&#20064;&#32452;&#21512;SPMD&#20998;&#21306;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11202
&lt;/p&gt;
&lt;p&gt;
PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#32467;&#21512;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20248;&#21270;&#22120;&#20998;&#29255;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#24403;&#31574;&#30053;&#21464;&#24471;&#22797;&#26434;&#26102;&#65292;&#20998;&#21306;&#24037;&#20855;&#38656;&#35201;&#20855;&#22791;&#20197;&#19979;&#29305;&#28857;&#65306;1&#65289;&#34920;&#36798;&#21147;&#24378;&#65292;&#20801;&#35768;&#32452;&#21512;&#31616;&#21333;&#31574;&#30053;&#65307;2&#65289;&#21487;&#39044;&#27979;&#24615;&#24378;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PartIR&#65292;&#19968;&#31181;&#29992;&#20110;NN&#20998;&#21306;&#30340;&#35774;&#35745;&#12290;PartIR&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#19982;&#30828;&#20214;&#21644;&#36816;&#34892;&#26102;&#26080;&#20851;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;API&#29992;&#20110;&#32452;&#21512;&#20998;&#29255;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#25972;&#20010;&#36807;&#31243;&#30001;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#26082;&#21487;&#20197;&#25163;&#21160;&#20063;&#21487;&#20197;&#33258;&#21160;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#19982;&#27169;&#22411;&#20195;&#30721;&#20998;&#24320;&#25351;&#23450;&#65292;&#26131;&#20110;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#31181;&#19981;&#21516;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#23637;&#31034;PartIR&#30340;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
&lt;/p&gt;</description></item><item><title>AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2401.10652</link><description>&lt;p&gt;
AutoChunk: &#33258;&#21160;&#28608;&#27963;&#22359;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference. (arXiv:2401.10652v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10652
&lt;/p&gt;
&lt;p&gt;
AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20869;&#23384;&#30340;&#22823;&#37327;&#38656;&#27714;&#65292;&#21253;&#25324;&#21442;&#25968;&#20869;&#23384;&#21644;&#28608;&#27963;&#20869;&#23384;&#65292;&#24050;&#32463;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#21442;&#25968;&#20869;&#23384;&#65292;&#23545;&#28608;&#27963;&#20869;&#23384;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#28608;&#27963;&#20869;&#23384;&#39044;&#35745;&#20250;&#32463;&#21382;&#26174;&#33879;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoChunk&#65292;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#30340;&#20248;&#21270;&#29983;&#25104;&#22359;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#22359;&#25628;&#32034;&#36890;&#36807;&#25506;&#32034;&#25152;&#26377;&#21487;&#33021;&#30340;&#22359;&#20505;&#36873;&#39033;&#65292;&#22359;&#36873;&#25321;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#22359;&#36827;&#34892;&#12290;&#36816;&#34892;&#26102;&#65292;AutoChunk&#37319;&#29992;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#24212;&#29992;&#22359;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;AI4PDEs&#26041;&#27861;&#23545;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#31163;&#25955;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27531;&#24046;&#20844;&#24335;&#30340;&#21387;&#32553;&#20195;&#25968;&#20307;&#31215;&#27969;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06755</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24211;&#22312;&#32467;&#26500;&#21270;&#32593;&#26684;&#19978;&#35299;&#20915;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#31163;&#25955;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving the Discretised Multiphase Flow Equations with Interface Capturing on Structured Grids Using Machine Learning Libraries. (arXiv:2401.06755v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;AI4PDEs&#26041;&#27861;&#23545;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#31163;&#25955;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27531;&#24046;&#20844;&#24335;&#30340;&#21387;&#32553;&#20195;&#25968;&#20307;&#31215;&#27969;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;AI4PDEs&#26041;&#27861;&#65288;&#21363;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#35299;&#20915;&#20102;&#20855;&#26377;&#30028;&#38754;&#25429;&#33719;&#30340;&#22810;&#30456;&#27969;&#21160;&#26041;&#31243;&#12290;AI4PDEs&#20013;&#30340;&#27714;&#35299;&#22120;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#24037;&#20855;&#26469;&#20934;&#30830;&#27714;&#35299;&#24050;&#32463;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#31163;&#25955;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#21367;&#31215;&#23618;&#21487;&#20197;&#29992;&#26469;&#23558;&#31163;&#25955;&#21270;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#26435;&#37325;&#30001;&#25968;&#20540;&#26041;&#27861;&#30830;&#23450;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#35757;&#32451;&#30830;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#20010;U-Net&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#22810;&#37325;&#32593;&#26684;&#27714;&#35299;&#22120;&#12290;&#38750;&#30456;&#28342;&#30340;&#20108;&#30456;&#27969;&#21160;&#30001;&#19977;&#32500;&#19981;&#21487;&#21387;&#32553;Navier-Stokes&#26041;&#31243;&#24314;&#27169;&#65292;&#20854;&#20013;&#21253;&#25324;&#34920;&#38754;&#24352;&#21147;&#21644;&#20307;&#31215;&#20998;&#25968;&#22330;&#30340;&#23545;&#27969;&#65292;&#35813;&#22330;&#25551;&#36848;&#20102;&#27969;&#20307;&#20043;&#38388;&#30340;&#30028;&#38754;&#12290;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27531;&#24046;&#20844;&#24335;&#30340;&#21387;&#32553;&#20195;&#25968;&#20307;&#31215;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;Petrov-Galerkin&#36827;&#34892;&#20934;&#30830;&#24615;&#65292;&#24182;&#19987;&#20026;AI4PDEs&#35774;&#35745;&#12290;&#39640;&#38454;&#26377;&#38480;&#24046;&#20998;&#26041;&#27861;&#34987;&#29992;&#20110;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper solves the multiphase flow equations with interface capturing using the AI4PDEs approach (Artificial Intelligence for Partial Differential Equations). The solver within AI4PDEs uses tools from machine learning (ML) libraries to solve (exactly) partial differential equations (PDEs) that have been discretised using numerical methods. Convolutional layers can be used to express the discretisations as a neural network, whose weights are determined by the numerical method, rather than by training. To solve the system, a multigrid solver is implemented through a neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier-Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov-Galerkin for accuracy and designed with AI4PDEs in mind. High-order fini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#26089;&#26399;C. elegans&#32986;&#32974;&#30340;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;&#32454;&#32990;&#36523;&#20221;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#32454;&#32990;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#36229;&#36807;90%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06182</link><description>&lt;p&gt;
&#20174;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#20013;&#39044;&#27979;&#32454;&#32990;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
Prediction of Cellular Identities from Trajectory and Cell Fate Information. (arXiv:2401.06182v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#26089;&#26399;C. elegans&#32986;&#32974;&#30340;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;&#32454;&#32990;&#36523;&#20221;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#32454;&#32990;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#36229;&#36807;90%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#30830;&#23450;&#32454;&#32990;&#36523;&#20221;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#26159;&#36890;&#36807;&#32454;&#32990;&#36861;&#36394;&#65292;&#36825;&#26159;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35782;&#21035;&#26089;&#26399;C. elegans&#32986;&#32974;&#20869;&#30340;&#32454;&#32990;&#36523;&#20221;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#12289;MLP&#21644;LSTM&#27169;&#22411;&#65292;&#23545;&#36328;&#36234;&#32986;&#32974;&#21457;&#32946;&#30340;&#21069;4&#20010;&#23567;&#26102;&#30340;3D&#26102;&#38388;&#24207;&#21015;&#20849;&#32858;&#28966;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#32990;&#20998;&#31867;&#20934;&#30830;&#24615;&#27979;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#20010;&#20307;&#32454;&#32990;&#30340;&#23569;&#37327;&#26102;&#31354;&#29305;&#24449;&#65292;&#21253;&#25324;&#32454;&#32990;&#36712;&#36857;&#21644;&#32454;&#32990;&#21629;&#36816;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#36129;&#29486;&#65292;&#24182;&#21487;&#20197;&#20174;&#29983;&#29289;&#23398;&#30693;&#35782;&#30340;&#35282;&#24230;&#35299;&#37322;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#30452;&#25509;&#20174;&#31616;&#21333;&#30340;&#26102;&#31354;&#29305;&#24449;&#20013;&#39044;&#27979;4D&#22270;&#20687;&#24207;&#21015;&#20013;&#30340;&#32454;&#32990;&#36523;&#20221;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining cell identities in imaging sequences is an important yet challenging task. The conventional method for cell identification is via cell tracking, which is complex and can be time-consuming. In this study, we propose an innovative approach to cell identification during early C. elegans embryogenesis using machine learning. We employed random forest, MLP, and LSTM models, and tested cell classification accuracy on 3D time-lapse confocal datasets spanning the first 4 hours of embryogenesis. By leveraging a small number of spatial-temporal features of individual cells, including cell trajectory and cell fate information, our models achieve an accuracy of over 90%, even with limited data. We also determine the most important feature contributions and can interpret these features in the context of biological knowledge. Our research demonstrates the success of predicting cell identities in 4D imaging sequences directly from simple spatio-temporal features.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06040</link><description>&lt;p&gt;
&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting. (arXiv:2401.06040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#26576;&#20123;&#33258;&#28982;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#65292;&#27604;&#22914;&#21253;&#21547;&#19981;&#21516;&#31890;&#24230;&#25110;&#23610;&#24230;&#19978;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65288;WavGCRN&#65289;&#65292;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#65288;MSA&#65289;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#22312;WavGCRN&#20013;&#65292;&#20132;&#36890;&#25968;&#25454;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#34987;&#20998;&#35299;&#20026;&#26102;&#39057;&#20998;&#37327;&#65292;&#26500;&#24314;&#20102;&#22810;&#27969;&#36755;&#20837;&#32467;&#26500;&#65307;&#28982;&#21518;&#20351;&#29992;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65288;GCRNs&#65289;&#20316;&#20026;&#32534;&#30721;&#22120;&#23545;&#27599;&#20010;&#27969;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21462;&#19981;&#21516;&#23610;&#24230;&#30340;&#26102;&#31354;&#29305;&#24449;&#65307;&#26368;&#21518;&#65292;&#21487;&#23398;&#20064;&#30340;&#36870;DWT&#21644;GCRN&#34987;&#32467;&#21512;&#20026;&#35299;&#30721;&#22120;&#65292;&#34701;&#21512;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the inf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;VI-PANN&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38899;&#39057;&#27169;&#24335;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05531</link><description>&lt;p&gt;
VI-PANN: &#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21464;&#20998;&#25512;&#29702;&#26469;&#25552;&#39640;&#38899;&#39057;&#27169;&#24335;&#35782;&#21035;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition. (arXiv:2401.05531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;VI-PANN&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38899;&#39057;&#27169;&#24335;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#22810;&#26679;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#33719;&#21462;&#30340;&#30693;&#35782;&#65292;&#24212;&#29992;&#20110;&#22312;&#21487;&#29992;&#39046;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#36739;&#23569;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#26377;&#35768;&#22810;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#20351;&#29992;&#30340;&#26159;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19981;&#32463;&#26657;&#20934;&#65292;&#20063;&#26080;&#27861;&#25552;&#20379;&#39044;&#27979;&#30340;&#35748;&#30693;&#65288;&#27169;&#22411;&#65289;&#19981;&#30830;&#23450;&#24230;&#12290;&#19982;&#30830;&#23450;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#33021;&#22815;&#24456;&#22909;&#22320;&#36827;&#34892;&#26657;&#20934;&#65292;&#25552;&#20379;&#39044;&#27979;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24230;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21464;&#20998;&#25512;&#29702;&#39044;&#35757;&#32451;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#65288;VI-PANNs&#65289;&#12290;VI-PANNs&#26159;&#22522;&#20110;&#27969;&#34892;&#30340;ResNet-54&#26550;&#26500;&#30340;&#21464;&#20998;&#25512;&#29702;&#21464;&#20307;&#65292;&#20854;&#22312;&#22823;&#35268;&#27169;&#38899;&#39057;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;AudioSet&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) is an increasingly popular approach to training deep learning (DL) models that leverages the knowledge gained by training a foundation model on diverse, large-scale datasets for use on downstream tasks where less domain- or task-specific data is available. The literature is rich with TL techniques and applications; however, the bulk of the research makes use of deterministic DL models which are often uncalibrated and lack the ability to communicate a measure of epistemic (model) uncertainty in prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models are often well-calibrated, provide access to epistemic uncertainty for a prediction, and are capable of achieving competitive predictive performance. In this study, we propose variational inference pre-trained audio neural networks (VI-PANNs). VI-PANNs are a variational inference variant of the popular ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio event detection da
&lt;/p&gt;</description></item><item><title>T-PRIME&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20256;&#36755;&#24103;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#38480;&#21046;&#19979;&#30340;&#23454;&#26102;&#21487;&#34892;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04837</link><description>&lt;p&gt;
T-PRIME: &#22522;&#20110;Transformer&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
T-PRIME: Transformer-based Protocol Identification for Machine-learning at the Edge. (arXiv:2401.04837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04837
&lt;/p&gt;
&lt;p&gt;
T-PRIME&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20256;&#36755;&#24103;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#38480;&#21046;&#19979;&#30340;&#23454;&#26102;&#21487;&#34892;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#35889;&#20849;&#20139;&#20801;&#35768;&#30456;&#21516;&#26631;&#20934;&#65288;&#20363;&#22914;802.11&#31995;&#21015;&#65289;&#25110;&#19981;&#21516;&#26631;&#20934;&#65288;&#20363;&#22914;LTE&#21644;DVB&#65289;&#30340;&#19981;&#21516;&#21327;&#35758;&#22312;&#37325;&#21472;&#30340;&#39057;&#27573;&#20013;&#20849;&#23384;&#12290;&#38543;&#30528;&#36825;&#31181;&#33539;&#24335;&#30340;&#25512;&#24191;&#65292;&#26080;&#32447;&#31995;&#32479;&#24517;&#39035;&#22312;&#25925;&#24847;&#25439;&#22351;&#21069;&#23548;&#30721;&#12289;&#26497;&#20302;&#20449;&#22122;&#27604;&#21644;&#25361;&#25112;&#24615;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#23454;&#26102;&#35782;&#21035;&#27963;&#21160;&#21457;&#23556;&#22120;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#27874;&#24418;&#12290;&#36890;&#36807;&#35774;&#35745;T-PRIME&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#21069;&#23548;&#30721;&#21305;&#37197;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;T-PRIME&#36890;&#36807;&#20854;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20256;&#36755;&#24103;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#26597;&#30475;&#36229;&#20986;&#21069;&#23548;&#30721;&#30340;&#24207;&#21015;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#23545;&#27604;Transformer&#27169;&#22411;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20854;&#27425;&#65292;&#20005;&#26684;&#20998;&#26512;&#20102;T-PRIME&#22312;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#38480;&#21046;&#19979;&#30340;&#23454;&#26102;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum sharing allows different protocols of the same standard (e.g., 802.11 family) or different standards (e.g., LTE and DVB) to coexist in overlapping frequency bands. As this paradigm continues to spread, wireless systems must also evolve to identify active transmitters and unauthorized waveforms in real time under intentional distortion of preambles, extremely low signal-to-noise ratios and challenging channel conditions. We overcome limitations of correlation-based preamble matching methods in such conditions through the design of T-PRIME: a Transformer-based machine learning approach. T-PRIME learns the structural design of transmitted frames through its attention mechanism, looking at sequence patterns that go beyond the preamble alone. The paper makes three contributions: First, it compares Transformer models and demonstrates their superiority over traditional methods and state-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's real-time feasibility on Deep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04331</link><description>&lt;p&gt;
&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20998;&#25968;&#38454;&#36830;&#32493;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65306;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20005;&#26684;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;(FDE)&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;Caputo&#23548;&#25968;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#25972;&#25968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#12290;&#21033;&#29992;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#19982;&#20256;&#32479;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#20013;&#30340;&#26080;&#35760;&#24518;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#19981;&#21516;&#12290;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30456;&#23545;&#20110;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#27809;&#26377;&#25915;&#20987;&#25110;&#25200;&#21160;&#30340;&#29615;&#22659;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#20248;&#21183;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#39564;&#35777;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#31283;&#23450;&#24615;&#21644;&#24377;&#24615;&#65292;&#20294;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.00873</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27969;&#34892;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#23545;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#36125;&#21494;&#26031;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#28508;&#22312;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#25512;&#23548;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#20998;&#26512;&#36824;&#34920;&#26126;&#20102;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#25972;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#33021;&#37327;&#27169;&#22411;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19979;&#30028;&#65292;&#32463;&#35777;&#26126;&#33021;&#21487;&#38752;&#22320;&#24809;&#32602;&#26368;&#37325;&#35201;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26032;&#25552;&#20986;&#30340;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35832;&#22914;&#20572;&#27490;&#26799;&#24230;&#12289;&#21160;&#37327;&#32534;&#30721;&#22120;&#25110;&#19987;&#38376;&#30340;&#32858;&#31867;&#31561;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives, elucidating the underlying probabilistic graphical models in each class and presenting a standardized methodology for their derivation from first principles. The analysis also indicates a natural means of integrating self-supervised learning with likelihood-based generative models. We instantiate this concept within the realm of cluster-based self-supervised learning and energy models, introducing a novel lower bound which is proven to reliably penalize the most important failure modes. Furthermore, this newly proposed lower bound enables the training of a standard backbone architecture without the necessity for asymmetric elements such as stop gradients, momentum encoders, or specialized clusteri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01047</link><description>&lt;p&gt;
&#36890;&#36807;&#20542;&#26012;&#25351;&#25968;&#23618;&#25913;&#21892;&#31283;&#20581;&#24615;&#65306;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective. (arXiv:2311.01047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#28145;&#24230;&#32593;&#32476;&#31283;&#20581;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#22823;&#22810;&#20381;&#36182;&#20110;&#21512;&#36866;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#20114;&#34917;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#30340;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#12290;&#38500;&#20102;&#26368;&#23567;&#21270;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#20195;&#20215;&#22806;&#65292;&#31070;&#32463;&#20803;&#36890;&#36807;&#26368;&#22823;&#21270;&#20542;&#26012;&#25351;&#25968;&#65288;TEXP&#65289;&#23618;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#31454;&#20105;&#20197;&#31232;&#30095;&#22320;&#34920;&#31034;&#23618;&#36755;&#20837;&#12290;TEXP&#23398;&#20064;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#25968;&#25454;&#22122;&#22768;&#30340;&#39640;&#26031;&#27169;&#22411;&#19979;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#21305;&#37197;&#28388;&#27874;&#22120;&#12290;&#22312;TEXP&#23618;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20542;&#26012;&#30340;softmax&#26367;&#20195;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#36827;&#34892;&#25512;&#29702;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#20195;&#34920;&#30340;&#31454;&#20105;&#20449;&#21495;&#20551;&#35774;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#36890;&#36807;&#31616;&#21270;&#27169;&#22411;&#25552;&#20379;&#27934;&#23519;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#23545;&#36793;&#32536;&#38598;&#21512;&#19978;&#30340;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Hodge&#20998;&#35299;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#30340;&#26080;&#25955;&#24230;&#21644;&#26080;&#26059;&#24230;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#23427;&#20204;&#26469;&#34920;&#31034;&#20219;&#24847;&#36793;&#32536;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#27969;&#21160;&#25968;&#25454;&#25512;&#26029;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.19450</link><description>&lt;p&gt;
Hodge-Compositional &#36793;&#32536;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hodge-Compositional Edge Gaussian Processes. (arXiv:2310.19450v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#23545;&#36793;&#32536;&#38598;&#21512;&#19978;&#30340;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Hodge&#20998;&#35299;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#30340;&#26080;&#25955;&#24230;&#21644;&#26080;&#26059;&#24230;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#23427;&#20204;&#26469;&#34920;&#31034;&#20219;&#24847;&#36793;&#32536;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#27969;&#21160;&#25968;&#25454;&#25512;&#26029;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#38598;&#21512;&#30340;2-&#22797;&#24418;&#32467;&#26500;&#65288;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20854;&#20013;&#36793;&#32536;&#21487;&#24418;&#25104;&#19977;&#35282;&#38754;&#65289;&#30340;&#20989;&#25968;&#24314;&#27169;&#30340;&#26377;&#21407;&#21017;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#23398;&#20064;&#32593;&#32476;&#19978;&#30340;&#27969;&#21160;&#31867;&#22411;&#25968;&#25454;&#65292;&#20854;&#20013;&#36793;&#32536;&#27969;&#21487;&#20197;&#36890;&#36807;&#31163;&#25955;&#30340;&#25955;&#24230;&#21644;&#26059;&#24230;&#26469;&#34920;&#24449;&#12290;&#20511;&#37492;Hodge&#20998;&#35299;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#30340;&#26080;&#25955;&#24230;&#21644;&#26080;&#26059;&#28216;&#30340;&#36793;&#32536;GPs&#12290;&#28982;&#21518;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#21019;&#24314;Hodge-&#32452;&#21512;&#36793;&#32536;GPs&#65292;&#36825;&#20123;GPs&#36275;&#22815;&#34920;&#36798;&#20219;&#20309;&#36793;&#32536;&#20989;&#25968;&#12290;&#36825;&#20123;GPs&#20415;&#20110;&#23545;&#36793;&#32536;&#20989;&#25968;&#30340;&#19981;&#21516;Hodge&#20998;&#37327;&#36827;&#34892;&#30452;&#25509;&#21644;&#29420;&#31435;&#30340;&#23398;&#20064;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#20013;&#25429;&#25417;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#31361;&#26174;&#23427;&#20204;&#30340;&#23454;&#38469;&#28508;&#21147;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#36135;&#24065;&#20817;&#25442;&#12289;&#28023;&#27915;&#27969;&#21160;&#21644;&#20379;&#27700;&#32593;&#32476;&#20013;&#30340;&#27969;&#21160;&#25968;&#25454;&#25512;&#26029;&#65292;&#24182;&#23558;&#20854;&#19982;&#26367;&#20195;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15516</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27169;&#22411;&#22312;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DRL&#27714;&#35299;&#22120;&#36890;&#24120;&#26159;&#29992;&#26469;&#35299;&#20915;&#33410;&#28857;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#24212;&#29992;&#31070;&#32463;&#26041;&#27861;&#26469;&#35299;&#20915;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#65288;CPP&#65289;&#65292;&#30340;&#30740;&#31350;&#21364;&#21313;&#20998;&#26377;&#38480;&#65292;&#22240;&#20026;&#19982;TSP&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#35299;&#31354;&#38388;&#36890;&#24120;&#26356;&#21152;&#19981;&#35268;&#21017;&#21644;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;DRL&#26694;&#26550;&#65292;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#65288;CPP-LC&#65289;&#30340;CPP&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36127;&#36733;&#32422;&#26463;&#30340;&#22797;&#26434;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21019;&#26032;&#28857;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;CPP-LC&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#39034;&#24207;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;DRL&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21363;Arc-DRL&#27169;&#22411;&#65292;&#23427;&#30001;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;CPP-LC&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#20351;&#24471;DRL&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep reinforcement learning (DRL) models have shown promising results in solving routing problems. However, most DRL solvers are commonly proposed to solve node routing problems, such as the Traveling Salesman Problem (TSP). Meanwhile, there has been limited research on applying neural methods to arc routing problems, such as the Chinese Postman Problem (CPP), since they often feature irregular and complex solution spaces compared to TSP. To fill these gaps, this paper proposes a novel DRL framework to address the CPP with load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc routing problem with load constraints. The novelty of our method is two-fold. First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential model. Subsequently, we introduce an autoregressive model based on DRL, namely Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge effectively. Such a framework allows the DRL model to work efficiently 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11984</link><description>&lt;p&gt;
&#20174;&#25554;&#20540;&#21040;&#22806;&#25512;&#65306;&#31639;&#26415;Transformer&#30340;&#23436;&#25972;&#38271;&#24230;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25552;&#20986;&#20197;&#26469;&#65292;Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#31639;&#27861;&#20219;&#21153;&#20013;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#26041;&#38754;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#21644;&#27880;&#24847;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#26368;&#20339;&#38271;&#24230;&#27867;&#21270;&#30340;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#30446;&#26631;&#25351;&#21521;&#20559;&#32622;&#26469;&#27867;&#21270;&#21040;&#38271;&#38271;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Attention Bias Calibration&#65288;ABC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26657;&#20934;&#38454;&#27573;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#20559;&#32622;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#26426;&#21046;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;ABC&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#20123;&#31639;&#26415;&#20219;&#21153;&#19978;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#23436;&#32654;&#38271;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20248;&#21270;&#36807;&#31243;&#20026;&#38024;&#23545;&#26131;&#20110;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26367;&#20195;&#20998;&#24067;&#30340;&#21442;&#25968;&#20248;&#21270;&#26469;&#35299;&#20915;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21487;&#24212;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#20998;&#24067;&#33539;&#22260;&#65292;&#36895;&#24230;&#24555;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11837</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26367;&#20195;&#21697;&#20248;&#21270;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Optimising Distributions with Natural Gradient Surrogates. (arXiv:2310.11837v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20248;&#21270;&#36807;&#31243;&#20026;&#38024;&#23545;&#26131;&#20110;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26367;&#20195;&#20998;&#24067;&#30340;&#21442;&#25968;&#20248;&#21270;&#26469;&#35299;&#20915;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21487;&#24212;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#20998;&#24067;&#33539;&#22260;&#65292;&#36895;&#24230;&#24555;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#20248;&#21270;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#65292;&#36890;&#24120;&#33021;&#24471;&#21040;&#24555;&#36895;&#25910;&#25947;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#65292;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#28041;&#21450;&#23558;&#20248;&#21270;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#20110;&#26367;&#20195;&#20998;&#24067;&#21442;&#25968;&#30340;&#20248;&#21270;&#65292;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#24456;&#23481;&#26131;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#21487;&#20197;&#35299;&#37322;&#20026;&#24212;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#29616;&#26377;&#26041;&#27861;&#30340;&#20363;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#21487;&#20197;&#26377;&#25928;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#20998;&#24067;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#23427;&#24555;&#36895;&#12289;&#26131;&#20110;&#29702;&#35299;&#65292;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#33258;&#21160;&#24494;&#20998;&#36719;&#20214;&#36827;&#34892;&#31616;&#21333;&#23454;&#29616;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20887;&#38271;&#30340;&#27169;&#22411;&#29305;&#23450;&#23548;&#25968;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#21464;&#20998;&#25512;&#26029;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and varia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11684</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37327;&#23376;&#21152;&#36895;&#22312;&#35299;&#20915;&#26080;&#38480;&#26102;&#22495;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#25552;&#39640;&#24179;&#22343;&#22870;&#21169;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#20195;&#29702;&#19982;&#26410;&#30693;MDP&#30340;&#20114;&#21160;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20132;&#20114;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#23548;&#30340;&#20855;&#26377;&#37327;&#23376;&#20449;&#21495;&#30340;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#33719;&#21462;&#20195;&#29702;&#33719;&#21462;&#30340;&#37327;&#23376;&#20449;&#21495;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#30340;&#20248;&#21183;&#33021;&#22815;&#22312;&#26080;&#38480;&#26102;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#23548;&#33268;&#36951;&#25022;&#20445;&#35777;&#30340;&#25351;&#25968;&#36827;&#23637;&#12290;&#20855;&#20307;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#20026;$\tilde{\mathcal{O}}(1)$&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30456;&#23545;&#20110;&#32463;&#20856;&#23545;&#24212;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;$\tilde{\mathcal{O}}(\sqrt{T})$&#30028;&#38480;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
&lt;/p&gt;</description></item><item><title>BayesDiff&#25552;&#20986;&#20102;&#19968;&#31181;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#36845;&#20195;&#21407;&#21017;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20272;&#35745;&#65292;&#24182;&#21487;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#36807;&#28388;&#20302;&#36136;&#37327;&#22270;&#20687;&#65292;&#22686;&#24378;&#25104;&#21151;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#32416;&#27491;&#22833;&#36133;&#29983;&#25104;&#32467;&#26524;&#20013;&#30340;&#20266;&#24433;&#12290;</title><link>http://arxiv.org/abs/2310.11142</link><description>&lt;p&gt;
BayesDiff: &#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20272;&#35745;&#25193;&#25955;&#20013;&#30340;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference. (arXiv:2310.11142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11142
&lt;/p&gt;
&lt;p&gt;
BayesDiff&#25552;&#20986;&#20102;&#19968;&#31181;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#36845;&#20195;&#21407;&#21017;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20272;&#35745;&#65292;&#24182;&#21487;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#36807;&#28388;&#20302;&#36136;&#37327;&#22270;&#20687;&#65292;&#22686;&#24378;&#25104;&#21151;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#32416;&#27491;&#22833;&#36133;&#29983;&#25104;&#32467;&#26524;&#20013;&#30340;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#36136;&#37327;&#36739;&#20302;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#26679;&#26412;&#24230;&#37327;&#65292;&#23545;&#20854;&#36827;&#34892;&#37492;&#21035;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BayesDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#36845;&#20195;&#21407;&#21017;&#26469;&#25551;&#36848;&#25193;&#25955;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#26368;&#21518;&#19968;&#23618;&#30340;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#20272;&#35745;&#30340;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#19981;&#20165;&#21487;&#20197;&#32858;&#21512;&#25104;&#26679;&#26412;&#32423;&#24230;&#37327;&#65292;&#20197;&#36807;&#28388;&#20986;&#36136;&#37327;&#36739;&#20302;&#30340;&#22270;&#20687;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#22312;&#25991;&#26412;&#36716;&#22270;&#20687;&#20219;&#21153;&#20013;&#22686;&#24378;&#25104;&#21151;&#30340;&#29983;&#25104;&#32467;&#26524;&#24182;&#32416;&#27491;&#22833;&#36133;&#30340;&#29983;&#25104;&#32467;&#26524;&#20013;&#30340;&#20266;&#24433;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BayesDiff&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#30340;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Ring-A-Bell&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#21487;&#20197;&#20107;&#20808;&#20934;&#22791;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.10012</link><description>&lt;p&gt;
&#21709;&#38083;&#65281;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?. (arXiv:2310.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#30340;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Ring-A-Bell&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#21487;&#20197;&#20107;&#20808;&#20934;&#22791;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#30340;&#25193;&#25955;(SD)&#65292;&#26368;&#36817;&#23637;&#31034;&#20986;&#20102;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#28508;&#22312;&#28389;&#29992;&#30340;&#20960;&#20010;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#21019;&#24314;&#21463;&#29256;&#26435;&#38480;&#21046;&#12289;&#31105;&#27490;&#21644;&#21463;&#38480;&#20869;&#23481;&#65292;&#25110;&#32773;&#19981;&#36866;&#23452;&#24037;&#20316;&#30340;(NSFW)&#22270;&#29255;&#26041;&#38754;&#12290;&#34429;&#28982;&#24050;&#32463;&#37319;&#21462;&#20102;&#19968;&#20123;&#25514;&#26045;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#35780;&#20272;&#38454;&#27573;&#23454;&#26045;&#23433;&#20840;&#36807;&#28388;&#22120;&#25110;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#28040;&#38500;&#19981;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#25110;&#39118;&#26684;&#65292;&#20294;&#36825;&#20123;&#23433;&#20840;&#25514;&#26045;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#26469;&#30740;&#31350;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Ring-A-Bell&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;T2I&#25193;&#25955;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#25972;&#20010;&#35780;&#20272;&#21487;&#20197;&#22312;&#27809;&#26377;&#30446;&#26631;&#27169;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#21069;&#20934;&#22791;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Ring-A-Bell&#39318;&#20808;&#23545;&#28608;&#21169;&#36827;&#34892;&#20998;&#35299;&#65292;&#28982;&#21518;&#36890;&#36807;&#21435;&#38500;&#20505;&#36873;&#27010;&#24565;&#21644;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#30456;&#20851;&#24230;&#26469;&#35774;&#35745;&#31579;&#36873;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first pe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08224</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#28508;&#22312;&#20108;&#36827;&#21046;&#32534;&#30721;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08224
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37197;&#22791;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#28508;&#22312;&#31354;&#38388;&#20013;&#22352;&#26631;$\vec{x}$&#30340;&#24179;&#26041;&#25351;&#25968;&#22686;&#38271;&#65292;&#35825;&#23548;&#20986;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#25105;&#20204;&#25551;&#36848;&#30340;&#29616;&#35937;&#26159;&#24050;&#30693;&#30340;&#19968;&#31181;&#34987;&#31216;&#20026;"&#31070;&#32463;&#23849;&#28291;"&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23427;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20986;&#29616;&#65292;&#24182;&#23548;&#33268;&#28508;&#22312;&#31867;&#22343;&#20540;&#23849;&#28291;&#20026;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#21152;&#36895;&#20102;&#25910;&#25947;&#21040;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#33410;&#28857;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#23558;&#22122;&#22768;&#26631;&#31614;&#32435;&#20837;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.08031</link><description>&lt;p&gt;
&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Local Graph Clustering with Noisy Labels. (arXiv:2310.08031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#33410;&#28857;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#23558;&#22122;&#22768;&#26631;&#31614;&#32435;&#20837;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23545;&#20110;&#24102;&#26377;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#26631;&#31614;&#65289;&#30340;&#22270;&#24418;&#30340;&#22686;&#21152;&#20852;&#36259;&#65292;&#20419;&#20351;&#20102;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#22788;&#29702;&#25972;&#20010;&#22270;&#24418;&#30340;&#26041;&#27861;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#24555;&#36895;&#23616;&#37096;&#26041;&#27861;&#65288;&#21363;&#19981;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#22270;&#24418;&#65289;&#30340;&#21457;&#23637;&#36824;&#24456;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22122;&#22768;&#33410;&#28857;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#33410;&#28857;&#26681;&#25454;&#25152;&#23646;&#31751;&#30340;&#32852;&#23646;&#20851;&#31995;&#25509;&#25910;&#21021;&#22987;&#20108;&#36827;&#21046;&#26631;&#31614;&#65306;&#22914;&#26524;&#23427;&#20204;&#23646;&#20110;&#30446;&#26631;&#31751;&#65292;&#21017;&#20026;1&#65307;&#21542;&#21017;&#20026;0&#12290;&#38543;&#21518;&#65292;&#36825;&#20123;&#26631;&#31614;&#30340;&#19968;&#37096;&#20998;&#20250;&#34987;&#32763;&#36716;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22122;&#22768;&#26631;&#31614;&#32435;&#20837;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#22909;&#22788;&#12290;&#36890;&#36807;&#26500;&#24314;&#24102;&#26377;&#36825;&#20123;&#26631;&#31614;&#30340;&#21152;&#26435;&#22270;&#24418;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;&#23616;&#37096;&#32858;&#31867;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#21644;&#21152;&#26435;&#22270;&#24418;&#19978;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#35282;&#24230;&#20986;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider
&lt;/p&gt;</description></item><item><title>"Unraveling the Single Tangent Space Fallacy"&#35770;&#25991;&#20998;&#26512;&#21644;&#28548;&#28165;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#35823;&#21306;&#65292;&#35813;&#35823;&#21306;&#26159;&#25351;&#23558;&#25968;&#25454;&#20165;&#25237;&#24433;&#21040;&#21333;&#19968;&#20999;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07902</link><description>&lt;p&gt;
&#25581;&#31034;&#21333;&#20999;&#24179;&#38754;&#35823;&#21306;&#65306;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#20998;&#26512;&#21644;&#28548;&#28165;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning. (arXiv:2310.07902v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07902
&lt;/p&gt;
&lt;p&gt;
"Unraveling the Single Tangent Space Fallacy"&#35770;&#25991;&#20998;&#26512;&#21644;&#28548;&#28165;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#35823;&#21306;&#65292;&#35813;&#35823;&#21306;&#26159;&#25351;&#23558;&#25968;&#25454;&#20165;&#25237;&#24433;&#21040;&#21333;&#19968;&#20999;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#35768;&#22810;&#21518;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#12289;&#24314;&#27169;&#25110;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#22266;&#20307;&#26041;&#21521;&#34920;&#31034;&#22235;&#20803;&#25968;&#30340;&#21333;&#20301;&#33539;&#25968;&#26465;&#20214;&#25110;&#21018;&#24230;&#21644;&#21487;&#25805;&#32437;&#24615;&#26925;&#29699;&#30340;&#27491;&#23450;&#24615;&#31561;&#20960;&#20309;&#32422;&#26463;&#12290;&#26377;&#25928;&#22788;&#29702;&#36825;&#26679;&#30340;&#20960;&#20309;&#32422;&#26463;&#38656;&#35201;&#23558;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21046;&#23450;&#20013;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#40654;&#26364;&#27969;&#24418;&#25104;&#20026;&#22788;&#29702;&#36825;&#31181;&#20960;&#20309;&#32422;&#26463;&#30340;&#24378;&#22823;&#25968;&#23398;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#23545;&#20854;&#37319;&#29992;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#25968;&#23398;&#19978;&#30340;&#32570;&#38519;&#21270;&#31616;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;&#8220;&#21333;&#20999;&#24179;&#38754;&#35823;&#21306;&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#20165;&#28041;&#21450;&#23558;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#21333;&#19968;&#20999;&#31354;&#38388;&#65288;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65289;&#19978;&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#25104;&#30340;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algor
&lt;/p&gt;</description></item><item><title>CrIBo&#36890;&#36807;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#23494;&#38598;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#33258;&#28982;&#29702;&#35299;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07855</link><description>&lt;p&gt;
CrIBo: &#36890;&#36807;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping. (arXiv:2310.07855v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07855
&lt;/p&gt;
&lt;p&gt;
CrIBo&#36890;&#36807;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#23494;&#38598;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#33258;&#28982;&#29702;&#35299;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#26368;&#36817;&#37051;&#26816;&#32034;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26102;&#65292;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#38480;&#21046;&#65292;&#22312;&#22270;&#20687;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#20165;&#22312;&#20840;&#23616;&#34920;&#31034;&#20013;&#34987;&#38544;&#21547;&#22320;&#25429;&#33719;&#12290;&#36825;&#31181;&#20840;&#23616;&#24341;&#23548;&#21487;&#33021;&#23548;&#33268;&#23545;&#35937;&#34920;&#31034;&#30340;&#19981;&#21487;&#21462;&#30340;&#32416;&#32544;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#24341;&#23548;&#26041;&#27861;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#22686;&#24378;&#23494;&#38598;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#23545;&#35937;&#32423;&#26368;&#36817;&#37051;&#24341;&#23548;&#65292;CrIBo&#25104;&#20026;&#19968;&#20010;&#26126;&#26174;&#24378;&#22823;&#21644;&#36866;&#24403;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26368;&#36817;&#37051;&#26816;&#32034;&#12290;CrIBo&#22312;&#21518;&#19968;&#20010;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26356;&#26631;&#20934;&#30340;&#19979;&#28216;&#33258;&#28982;&#29702;&#35299;&#24212;&#29992;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07171</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#24067;&#22810;&#26679;&#21270;&#23454;&#29616;&#32852;&#37030;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#22312;&#26080;&#38656;&#30452;&#25509;&#25968;&#25454;&#20849;&#20139;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#30340;&#33021;&#21147;&#32780;&#26085;&#30410;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25361;&#25112;&#65292;&#23545;FL&#30340;&#27867;&#21270;&#33021;&#21147;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#24403;&#24182;&#38750;&#25152;&#26377;&#23458;&#25143;&#31471;&#37117;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#36830;&#25509;&#25110;&#26377;&#38480;&#30340;&#35745;&#31639;&#33021;&#21147;&#32780;&#24120;&#35265;&#12290;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#22797;&#26434;&#21270;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#28041;&#21450;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#24046;&#36317;&#38382;&#39064;&#19978;&#65292;&#20294;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#38750;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIRE&#30340;&#20989;&#25968;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#19982;&#28176;&#36827;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#38646;&#23556;&#20987;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.04418</link><description>&lt;p&gt;
&#29992;&#20110;&#30456;&#23545;&#20301;&#32622;&#30340;&#20989;&#25968;&#25554;&#20540;&#25913;&#36827;&#20102;&#38271;&#19978;&#19979;&#25991;Transformer
&lt;/p&gt;
&lt;p&gt;
Functional Interpolation for Relative Positions Improves Long Context Transformers. (arXiv:2310.04418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04418
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIRE&#30340;&#20989;&#25968;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#19982;&#28176;&#36827;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#38646;&#23556;&#20987;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26102;&#65292;&#38450;&#27490;Transformer&#22312;&#35757;&#32451;&#20197;&#22806;&#26356;&#38271;&#36755;&#20837;&#19978;&#24615;&#33021;&#19979;&#38477;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;Transformer&#26550;&#26500;&#22312;&#21487;&#22788;&#29702;&#30340;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#19978;&#22522;&#26412;&#27809;&#26377;&#38480;&#21046;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;&#36873;&#25321;&#21487;&#33021;&#20250;&#38480;&#21046;&#36825;&#20123;&#27169;&#22411;&#22312;&#26356;&#38271;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20989;&#25968;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#19982;&#28176;&#36827;&#25554;&#20540;&#26041;&#27861;&#65288;FIRE&#65289;&#65292;&#20197;&#25913;&#36827;Transformer&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#34920;&#31034;&#20986;&#19968;&#20123;&#27969;&#34892;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#22914;T5&#30340;RPE&#12289;Alibi&#21644;Kerple&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#38646;&#23556;&#20987;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#19978;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;FIRE&#27169;&#22411;&#22312;&#26356;&#38271;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.01690</link><description>&lt;p&gt;
&#20351;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28909;&#24102;&#27668;&#26059;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#21464;&#21270;&#65292;&#39123;&#39118;&#21464;&#24471;&#26356;&#21152;&#24378;&#28872;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#27861;&#27604;&#22522;&#20110;&#25968;&#23398;&#27169;&#22411;&#30340;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#32463;&#27982;&#23454;&#24800;&#21644;&#26131;&#20110;&#33719;&#21462;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#21355;&#26143;&#25104;&#20687;&#12289;&#36965;&#24863;&#21644;&#22823;&#27668;&#25968;&#25454;&#65292;&#37319;&#29992;&#32423;&#32852;&#26041;&#27861;&#36827;&#34892;&#39123;&#39118;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#30340;&#39044;&#27979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20845;&#20010;&#20027;&#35201;&#30406;&#22320;&#30340;51&#20010;&#39123;&#39118;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32423;&#32852;&#27169;&#22411;&#30340;&#26368;&#32456;&#39044;&#27979;&#22312;36&#23567;&#26102;&#20869;&#26174;&#31034;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#25152;&#26377;&#19977;&#39033;&#20219;&#21153;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#21644;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#30340;&#20540;&#37117;&#36229;&#36807;&#20102;0.5&#21644;20dB&#12290;&#26412;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#25193;&#25955;&#27169;&#22411;&#31561;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#39640;&#24615;&#33021;&#38656;&#27714;&#65288;&#22914;&#39123;&#39118;&#39044;&#27979;&#65289;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#35745;&#31639;&#32463;&#27982;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16739</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33267;6G&#36793;&#32536;&#65306;&#35270;&#37326;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#26377;&#21487;&#33021;&#22609;&#36896;&#25105;&#20204;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#20113;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65306;1) &#21709;&#24212;&#26102;&#38388;&#38271;&#65307;2) &#39640;&#24102;&#23485;&#25104;&#26412;&#65307;&#20197;&#21450;3) &#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#36843;&#20999;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;6G&#36793;&#32536;&#37096;&#32626;LLMs&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30001;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#31361;&#20986;&#22312;&#32456;&#31471;&#29992;&#25143;&#38468;&#36817;&#37096;&#32626;LLMs&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;LLMs&#26102;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#35774;&#24819;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;6G MEC&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20004;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#21363;LLMs&#30340;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#12290;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#30340;&#22266;&#26377;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15135</link><description>&lt;p&gt;
&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Contrastive Continual Multi-view Clustering with Filtered Structural Fusion. (arXiv:2309.15135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#36866;&#29992;&#20110;&#20808;&#21069;&#25910;&#38598;&#35270;&#22270;&#24182;&#25552;&#21462;&#19968;&#33268;&#21644;&#20114;&#34917;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#35270;&#22270;&#25353;&#39034;&#24207;&#25910;&#38598;&#30340;&#23454;&#26102;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#26032;&#35270;&#22270;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering thrives in applications where views are collected in advance by extracting consistent and complementary information among views. However, it overlooks scenarios where data views are collected sequentially, i.e., real-time data. Due to privacy issues or memory burden, previous views are not available with time in these situations. Some methods are proposed to handle it but are trapped in a stability-plasticity dilemma. In specific, these methods undergo a catastrophic forgetting of prior knowledge when a new view is attained. Such a catastrophic forgetting problem (CFP) would cause the consistent and complementary information hard to get and affect the clustering performance. To tackle this, we propose a novel method termed Contrastive Continual Multi-view Clustering with Filtered Structural Fusion (CCMVC-FSF). Precisely, considering that data correlations play a vital role in clustering and prior knowledge ought to guide the clustering process of a new view, we de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#20687;&#32032;&#32423;&#24179;&#28369;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20108;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#28369;&#20998;&#24067;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#25928;&#29575;&#65292;&#24182;&#23436;&#20840;&#19978;&#30028;&#25237;&#24433;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.13150</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#24179;&#28369;&#29992;&#20110;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations. (arXiv:2309.13150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#20687;&#32032;&#32423;&#24179;&#28369;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20108;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#28369;&#20998;&#24067;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#25928;&#29575;&#65292;&#24182;&#23436;&#20840;&#19978;&#30028;&#25237;&#24433;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#24863;&#30693;&#27169;&#22411;&#22312;&#38754;&#23545;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#40065;&#26834;&#24615;&#30340;&#35748;&#35777;&#36807;&#31243;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#22240;&#20026;&#38656;&#35201;&#22312;&#19977;&#32500;&#30456;&#26426;&#36816;&#21160;&#31354;&#38388;&#20013;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#24471;&#21040;&#22823;&#37327;&#22270;&#20687;&#25237;&#24433;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35777;&#26126;3D-2D&#25237;&#24433;&#21464;&#25442;&#23545;&#25239;&#30456;&#26426;&#36816;&#21160;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#32500;&#20687;&#32032;&#31354;&#38388;&#32780;&#38750;&#19977;&#32500;&#29289;&#29702;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#28369;&#20998;&#24067;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#30456;&#26426;&#36816;&#21160;&#37319;&#26679;&#65292;&#24182;&#22823;&#22823;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#20687;&#32032;&#32423;&#24179;&#28369;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#19968;&#31181;&#22343;&#21248;&#20998;&#21306;&#30340;&#25216;&#26415;&#23436;&#20840;&#19978;&#30028;&#25237;&#24433;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, computer vision has made remarkable advancements in autonomous driving and robotics. However, it has been observed that deep learning-based visual perception models lack robustness when faced with camera motion perturbations. The current certification process for assessing robustness is costly and time-consuming due to the extensive number of image projections required for Monte Carlo sampling in the 3D camera motion space. To address these challenges, we present a novel, efficient, and practical framework for certifying the robustness of 3D-2D projective transformations against camera motion perturbations. Our approach leverages a smoothing distribution over the 2D pixel space instead of in the 3D physical space, eliminating the need for costly camera motion sampling and significantly enhancing the efficiency of robustness certifications. With the pixel-wise smoothed classifier, we are able to fully upper bound the projection errors using a technique of uniform partit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36951;&#20256;&#31639;&#27861;&#19982;&#35789;&#20856;&#24335;&#35780;&#20272;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#32452;&#21512;&#65292;&#20197;&#20811;&#26381;&#20197;&#24448;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35774;&#35745;&#31354;&#38388;&#19981;&#36275;&#21644;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27604;&#20197;&#24448;&#33539;&#22260;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08399</link><description>&lt;p&gt;
&#20248;&#21270;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#32452;&#21512;&#65306;&#19968;&#31181;&#35789;&#20856;&#36951;&#20256;&#31639;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Modular Robot Composition: A Lexicographic Genetic Algorithm Approach. (arXiv:2309.08399v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36951;&#20256;&#31639;&#27861;&#19982;&#35789;&#20856;&#24335;&#35780;&#20272;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#32452;&#21512;&#65292;&#20197;&#20811;&#26381;&#20197;&#24448;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35774;&#35745;&#31354;&#38388;&#19981;&#36275;&#21644;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27604;&#20197;&#24448;&#33539;&#22260;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#26426;&#22120;&#20154;&#34987;&#35774;&#35745;&#20026;&#36890;&#29992;&#30828;&#20214;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#36866;&#24212;&#20219;&#21153;&#38656;&#27714;&#25110;&#29615;&#22659;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#32780;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#21017;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#65292;&#21363;&#26426;&#22120;&#20154;&#30340;&#24418;&#24335;&#21644;&#32467;&#26500;&#65292;&#23545;&#20027;&#35201;&#24615;&#33021;&#25351;&#26631;--&#37319;&#36141;&#25104;&#26412;&#12289;&#21608;&#26399;&#26102;&#38388;&#21644;&#33021;&#28304;&#25928;&#29575;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22359;&#32452;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22312;&#24320;&#21457;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#20013;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#20805;&#20998;&#25506;&#32034;&#35774;&#35745;&#31354;&#38388;&#65292;&#35201;&#20040;&#26080;&#27861;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36951;&#20256;&#31639;&#27861;&#19982;&#35789;&#20856;&#24335;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#30340;&#32452;&#21512;&#65292;&#20197;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#21487;&#33021;&#32452;&#21512;&#30340;&#25968;&#37327;&#19978;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#33539;&#22260;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial robots are designed as general-purpose hardware, which limits their ability to adapt to changing task requirements or environments. Modular robots, on the other hand, offer flexibility and can be easily customized to suit diverse needs. The morphology, i.e., the form and structure of a robot, significantly impacts the primary performance metrics acquisition cost, cycle time, and energy efficiency. However, identifying an optimal module composition for a specific task remains an open problem, presenting a substantial hurdle in developing task-tailored modular robots. Previous approaches either lack adequate exploration of the design space or the possibility to adapt to complex tasks. We propose combining a genetic algorithm with a lexicographic evaluation of solution candidates to overcome this problem and navigate search spaces exceeding those in prior work by magnitudes in the number of possible compositions. We demonstrate that our approach outperforms a state-of-the-art b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.06634</link><description>&lt;p&gt;
$G$-Mapper&#65306;&#23398;&#20064;Mapper&#26500;&#36896;&#20013;&#30340;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
$G$-Mapper: Learning a Cover in the Mapper Construction. (arXiv:2309.06634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mapper&#31639;&#27861;&#26159;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#20013;&#19968;&#31181;&#21453;&#26144;&#32473;&#23450;&#25968;&#25454;&#38598;&#32467;&#26500;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;Mapper&#31639;&#27861;&#38656;&#35201;&#35843;&#25972;&#22810;&#20010;&#21442;&#25968;&#20197;&#29983;&#25104;&#19968;&#20010;"&#22909;&#30475;&#30340;"Mapper&#22270;&#12290;&#35813;&#35770;&#25991;&#20851;&#27880;&#20110;&#36873;&#25321;&#35206;&#30422;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26681;&#25454;&#27491;&#24577;&#24615;&#30340;&#32479;&#35745;&#26816;&#39564;&#21453;&#22797;&#20998;&#21106;&#35206;&#30422;&#26469;&#20248;&#21270;Mapper&#22270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;$G$-means&#32858;&#31867;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#36827;&#34892;Anderson-Darling&#26816;&#39564;&#26469;&#23547;&#25214;$k$-means&#20013;&#26368;&#20339;&#30340;&#31751;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#21106;&#36807;&#31243;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#26681;&#25454;&#32473;&#23450;&#25968;&#25454;&#30340;&#20998;&#24067;&#31934;&#24515;&#36873;&#25321;&#35206;&#30422;&#12290;&#23545;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#35206;&#30422;&#20351;Mapper&#22270;&#20445;&#30041;&#20102;&#25968;&#25454;&#38598;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.05019</link><description>&lt;p&gt;
SA-Solver&#65306;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#20122;&#24403;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#20174;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30456;&#24403;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#25913;&#36827;&#30340;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#32771;&#34385;&#35299;&#25193;&#25955;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#37319;&#26679;&#21487;&#20197;&#22312;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#26041;&#38754;&#25552;&#20379;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#23545;&#38543;&#26426;&#37319;&#26679;&#30340;&#32508;&#21512;&#20998;&#26512;&#65306;&#26041;&#24046;&#25511;&#21046;&#30340;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#32447;&#24615;&#22810;&#27493;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SA-Solver&#65292;&#23427;&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SA-Solver&#23454;&#29616;&#20102;&#65306;1&#65289;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#24615;&#33021;&#65307;2&#65289;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
&lt;/p&gt;</description></item><item><title>ArtiGrasp&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#27169;&#25311;&#30340;&#26041;&#24335;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#31574;&#30053;&#26469;&#21512;&#25104;&#21452;&#25163;&#28789;&#24039;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03891</link><description>&lt;p&gt;
ArtiGrasp&#65306;&#21452;&#25163;&#28789;&#24039;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#30340;&#29289;&#29702;&#21512;&#29702;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation. (arXiv:2309.03891v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03891
&lt;/p&gt;
&lt;p&gt;
ArtiGrasp&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#27169;&#25311;&#30340;&#26041;&#24335;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#31574;&#30053;&#26469;&#21512;&#25104;&#21452;&#25163;&#28789;&#24039;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ArtiGrasp&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#21253;&#25324;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#22312;&#20869;&#30340;&#21452;&#25163;&#25163;-&#29289;&#20307;&#20132;&#20114;&#12290;&#30001;&#20110;&#20840;&#23616;&#25163;&#33109;&#36816;&#21160;&#21644;&#31934;&#30830;&#30340;&#25163;&#25351;&#25511;&#21046;&#23545;&#20110;&#29289;&#20307;&#30340;&#20851;&#33410;&#34920;&#36798;&#26159;&#24517;&#35201;&#30340;&#65292;&#36825;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;ArtiGrasp&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#27169;&#25311;&#35757;&#32451;&#19968;&#20010;&#25511;&#21046;&#20840;&#23616;&#21644;&#23616;&#37096;&#25163;&#23039;&#24577;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#25163;&#23039;&#24577;&#21442;&#32771;&#19979;&#32479;&#19968;&#20102;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35757;&#32451;&#20851;&#33410;&#34920;&#36798;&#25152;&#38656;&#30340;&#31934;&#30830;&#25163;&#25351;&#25511;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#23398;&#20064;&#35838;&#31243;&#12290;&#23427;&#20174;&#21333;&#25163;&#25805;&#20316;&#38745;&#27490;&#29289;&#20307;&#24320;&#22987;&#65292;&#28982;&#21518;&#36827;&#34892;&#21253;&#25324;&#20004;&#21482;&#25163;&#21644;&#38750;&#38745;&#27490;&#29289;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#35757;&#32451;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#29289;&#20307;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#29289;&#20307;&#31227;&#21040;&#30446;&#26631;&#20851;&#33410;&#23039;&#24577;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#38656;&#35201;&#25235;&#25569;&#65292;&#20851;&#33410;&#34920;&#36798;&#21644;&#29289;&#20307;&#23039;&#24577;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand pose reference. Moreover, to facilitate the training of the precise finger control required for articulation, we present a learning curriculum with increasing difficulty. It starts with single-hand manipulation of stationary objects and continues with multi-agent training including both hands and non-stationary objects. To evaluate our method, we introduce Dynamic Object Grasping and Articulation, a task that involves bringing an object into a target articulated pose. This task requires grasping,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>SortedNet&#26159;&#19968;&#31181;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25490;&#24207;&#35757;&#32451;&#21644;&#27010;&#29575;&#26041;&#24335;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#32500;&#24230;&#19978;&#23454;&#29616;&#39640;&#25928;&#21160;&#24577;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#20013;&#28789;&#27963;&#36866;&#24212;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;</title><link>http://arxiv.org/abs/2309.00255</link><description>&lt;p&gt;
SortedNet&#65292;&#27599;&#20010;&#32593;&#32476;&#37117;&#26377;&#33258;&#24049;&#30340;&#20301;&#32622;&#65306;&#38754;&#21521;&#35757;&#32451;&#22810;&#23545;&#19968;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00255
&lt;/p&gt;
&lt;p&gt;
SortedNet&#26159;&#19968;&#31181;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25490;&#24207;&#35757;&#32451;&#21644;&#27010;&#29575;&#26041;&#24335;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#32500;&#24230;&#19978;&#23454;&#29616;&#39640;&#25928;&#21160;&#24577;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#20013;&#28789;&#27963;&#36866;&#24212;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#22823;&#65292;&#22914;&#20309;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#32422;&#26463;&#19979;&#25214;&#21040;&#26368;&#20248;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#32452;&#25104;&#37096;&#20998;&#36890;&#24120;&#20801;&#35768;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#19981;&#24847;&#35782;&#21040;&#36825;&#31181;&#27169;&#22359;&#21270;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#32570;&#20047;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#36866;&#24212;&#27169;&#22411;&#35745;&#31639;&#36127;&#36733;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SortedNet&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#20041;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#30340;&#20869;&#22312;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#21160;&#24577;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#23884;&#22871;&#32467;&#26500;&#30340;&#23376;&#27169;&#22411;&#21644;&#20027;&#27169;&#22411;&#20849;&#20139;&#21442;&#25968;&#30340;&#26041;&#24335;&#65292;&#24182;&#20197;&#25490;&#24207;&#21644;&#27010;&#29575;&#30340;&#26041;&#24335;&#35757;&#32451;&#23427;&#20204;&#12290;&#36825;&#31181;&#23376;&#32593;&#32476;&#30340;&#25490;&#24207;&#35757;&#32451;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19968;&#36718;&#35757;&#32451;&#20013;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26356;&#26032;&#26041;&#26696;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#23376;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#20989;&#25968;&#24615;&#25688;&#35201;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#23545;&#26080;&#38480;&#32500;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;&#36827;&#34892;&#25200;&#21160;&#65292;&#25918;&#23485;&#20102;&#23545;&#25968;&#25454;&#36712;&#36857;&#30340;&#20551;&#35774;&#65292;&#24182;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#26377;&#38480;&#32500;&#23376;&#31354;&#38388;&#23884;&#20837;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00125</link><description>&lt;p&gt;
&#36890;&#36807;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Functional Summaries via the Independent Component Laplace Process. (arXiv:2309.00125v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#20989;&#25968;&#24615;&#25688;&#35201;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#23545;&#26080;&#38480;&#32500;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;&#36827;&#34892;&#25200;&#21160;&#65292;&#25918;&#23485;&#20102;&#23545;&#25968;&#25454;&#36712;&#36857;&#30340;&#20551;&#35774;&#65292;&#24182;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#26377;&#38480;&#32500;&#23376;&#31354;&#38388;&#23884;&#20837;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#65288;ICLP&#65289;&#26426;&#21046;&#30340;&#24046;&#20998;&#38544;&#31169;&#20989;&#25968;&#24615;&#25688;&#35201;&#30340;&#26032;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;&#35270;&#20026;&#30495;&#27491;&#26080;&#38480;&#32500;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;ICLP&#22122;&#22768;&#26469;&#25200;&#21160;&#23427;&#20204;&#65292;&#35813;&#26032;&#26426;&#21046;&#25918;&#23485;&#20102;&#20851;&#20110;&#25968;&#25454;&#36712;&#36857;&#30340;&#20551;&#35774;&#65292;&#24182;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26377;&#38480;&#32500;&#23376;&#31354;&#38388;&#23884;&#20837;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20989;&#25968;&#31354;&#38388;&#20013;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36731;&#24494;&#36807;&#24179;&#28369;&#25688;&#35201;&#26469;&#35777;&#26126;&#38544;&#31169;&#25104;&#26412;&#19981;&#20250;&#20027;&#23548;&#32479;&#35745;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#21487;&#20197;&#24573;&#30053;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a new mechanism for releasing differentially private functional summaries called the Independent Component Laplace Process, or ICLP, mechanism. By treating the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over-smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13888</link><description>&lt;p&gt;
&#20154;&#33080;&#22270;&#20687;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#21464;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Morphing of Face Images. (arXiv:2308.13888v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#21464;&#24418;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#20247;&#22810;&#33402;&#26415;&#21644;&#21462;&#35777;&#24212;&#29992;&#12290;&#30001;&#20110;&#23039;&#24577;&#12289;&#20809;&#29031;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#21464;&#21270;&#65292;&#23427;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#36825;&#20010;&#20219;&#21153;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#30340;&#21464;&#24418;&#21644;&#26080;&#32541;&#36807;&#28193;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#20154;&#33080;&#22270;&#20687;&#30340;&#36825;&#31181;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31163;&#25955;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26102;&#38388;&#20381;&#36182;&#30340;&#65292;&#20801;&#35768;&#23545;&#30446;&#26631;&#22270;&#20687;&#36827;&#34892;&#36830;&#32493;&#30340;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#21464;&#24418;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#26102;&#38388;&#20381;&#36182;&#21464;&#24418;&#30340;&#30452;&#25509;&#21644;&#36870;&#21464;&#25442;&#12290;&#21069;&#32773;&#36127;&#36131;&#23558;&#30446;&#26631;&#22270;&#20687;&#21464;&#24418;&#20026;&#28304;&#22270;&#20687;&#65292;&#32780;&#21518;&#32773;&#21017;&#29992;&#20110;&#22312;&#30456;&#21453;&#26041;&#21521;&#36827;&#34892;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#21464;&#24418;&#32593;&#32476;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphing is one of the seminal problems in computer graphics, with numerous artistic and forensic applications. It is notoriously challenging due to pose, lighting, gender, and ethnicity variations. Generally, this task consists of a warping for feature alignment and a blending for a seamless transition between the warped images.  We propose to leverage coordinate-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks, by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping, and blending of the target images.  During warping inference, we need both direct and inverse transformations of the time-dependent warping. The first is responsible for morphing the target image into the source image, while the inverse is used for morphing in the opposite direction. Our neural warping sto
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12289;&#35838;&#31243;&#36827;&#24230;&#24494;&#35843;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11863</link><description>&lt;p&gt;
KinSPEAK: &#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11863
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12289;&#35838;&#31243;&#36827;&#24230;&#24494;&#35843;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#20855;&#22791;&#20102;&#22823;&#35268;&#27169;&#35760;&#24405;&#30340;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#35821;&#38899;&#25968;&#25454;&#65292;&#20294;&#26159;&#23454;&#29616;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#30340;&#24378;&#22823;&#35821;&#38899;&#35782;&#21035;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36981;&#24490;&#31616;&#21333;&#30340;&#35838;&#31243;&#36827;&#24230;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26469;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20851;&#27880;&#20351;&#29992;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;&#20844;&#20849;&#32593;&#31449;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#21046;&#20316;&#23460;&#32423;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#24178;&#20928;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#24178;&#20928;&#30340;&#22522;&#20934;&#27169;&#22411;&#23545;&#26469;&#33258;&#26356;&#22810;&#22810;&#26679;&#21644;&#22024;&#26434;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#23450;&#20041;&#19968;&#20010;&#31616;&#21333;&#30340;&#35838;&#31243;&#35757;&#32451;&#36827;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#36830;&#32493;&#22235;&#20195;&#23545;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;3.2&#65285;&#30340;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#22312;Mozilla Common Voice&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;15.9&#65285;&#30340;WER&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25512;&#24191;&#21040;&#38750;&#32039;&#33268;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#19968;&#33268;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#20195;&#25968;&#32467;&#26500;&#30340;&#24847;&#22806;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.03812</link><description>&lt;p&gt;
&#38750;&#32039;&#33268;&#32479;&#19968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Noncompact uniform universal approximation. (arXiv:2308.03812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03812
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25512;&#24191;&#21040;&#38750;&#32039;&#33268;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#19968;&#33268;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#20195;&#25968;&#32467;&#26500;&#30340;&#24847;&#22806;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25512;&#24191;&#21040;&#22312;&#65288;&#38750;&#32039;&#33268;&#65289;&#36755;&#20837;&#31354;&#38388; \(\mathbb R^n\) &#19978;&#30340;&#19968;&#33268;&#25910;&#25947;&#12290;&#25152;&#26377;&#22312;&#26080;&#31351;&#36828;&#22788;&#20026;&#38646;&#30340;&#36830;&#32493;&#20989;&#25968;&#37117;&#21487;&#20197;&#29992;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#65292;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;&#28176;&#36817;&#32447;&#24615;&#34892;&#20026;&#30340;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968; \(\varphi\neq0\)&#12290;&#24403; \(\varphi\) &#36824;&#34987;&#38480;&#21046;&#22312;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#20934;&#30830;&#30830;&#23450;&#20102;&#21738;&#20123;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#65292;&#24471;&#21040;&#20102;&#20197;&#19979;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;&#35753; \(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\) &#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377; \(l\) &#20010;&#38544;&#34255;&#23618;&#21644; \(n\) &#20010;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#30340;&#20989;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#12290;&#23545;&#20110;&#25152;&#26377;&#30340; \(n\) &#21644;&#25152;&#26377;&#30340; \(l\geq2\)&#65292;\(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\) &#22312;&#36880;&#28857;&#20056;&#31215;&#19979;&#26159;&#19968;&#20010;&#20195;&#25968;&#12290;&#22914;&#26524; \(\varphi\) &#30340;&#24038;&#26497;&#38480;&#19981;&#31561;&#20110;&#20854;&#21491;&#26497;&#38480;&#65288;&#20363;&#22914;&#65292;&#24403; \(\varphi\) &#26159;sigmoid&#20989;&#25968;&#26102;&#65289;&#65292;&#20195;&#25968; \(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\)&#65288;\(l\geq2\)&#65289;&#20250;&#20135;&#29983;&#19968;&#20123;&#24847;&#22806;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. When $\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\geq2$, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\varphi$ differs from its right limit (for instance, when $\varphi$ is sigmoidal) the algebra $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq2$) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13586</link><description>&lt;p&gt;
&#35299;&#20915;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling the Sample Complexity of Online Reinforcement Learning. (arXiv:2307.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25968;&#25454;&#25928;&#29575;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#28176;&#36817;&#26368;&#23567;&#30340;&#21518;&#24724;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#30340;&#26368;&#20248;&#24615;&#20165;&#22312;&#8220;&#22823;&#26679;&#26412;&#8221;&#24773;&#20917;&#19979;&#24471;&#21040;&#20445;&#35777;&#65292;&#20026;&#20102;&#20351;&#20854;&#31639;&#27861;&#36816;&#34892;&#26368;&#20339;&#65292;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#39044;&#29123;&#25104;&#26412;&#12290;&#22914;&#20309;&#22312;&#19981;&#20135;&#29983;&#20219;&#20309;&#39044;&#29123;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#21333;&#35843;&#20540;&#20256;&#25773;(MVP)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#30001;\cite{zhang2020reinforcement}&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20351;&#24471;&#21518;&#24724;&#30340;&#37327;&#32423;&#20026;(&#27169;&#38500;&#23545;&#25968;&#22240;&#23376;)\begin{equation *} \min\biggr\{ \sqrt{SAH^3K}&#65292;\&#65292;HK \biggr\}&#65292;\end{equation *}&#20854;&#20013;$S$&#26159;&#29366;&#24577;&#25968;&#65292;$A$&#26159;&#21160;&#20316;&#25968;&#65292;$H$&#26159;&#35268;&#21010;&#26102;&#22495;&#65292;$K$&#26159;&#24635;&#30340;&#22238;&#21512;&#25968;&#12290;&#36825;&#20010;&#21518;&#24724;&#30340;&#37327;&#32423;&#19982;&#26497;&#23567;&#21270;&#21518;&#24724;&#37327;&#32423;&#26159;&#30456;&#21305;&#37197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.  We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}  \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21152;&#26435;&#25130;&#26029;&#26680;&#33539;&#25968;&#20943;&#21435;&#25130;&#26029;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#65292;&#21033;&#29992;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#26469;&#25552;&#21462;&#30456;&#20284;&#30340;&#32467;&#26500;&#36827;&#34892;&#21435;&#22122;&#65292;&#27169;&#22411;&#20860;&#39038;&#20102;&#36328;&#36890;&#36947;&#24046;&#24322;&#21644;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.07932</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#30340;&#25130;&#26029;&#33539;&#25968;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising. (arXiv:2307.07932v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07932
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21152;&#26435;&#25130;&#26029;&#26680;&#33539;&#25968;&#20943;&#21435;&#25130;&#26029;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#65292;&#21033;&#29992;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#26469;&#25552;&#21462;&#30456;&#20284;&#30340;&#32467;&#26500;&#36827;&#34892;&#21435;&#22122;&#65292;&#27169;&#22411;&#20860;&#39038;&#20102;&#36328;&#36890;&#36947;&#24046;&#24322;&#21644;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20302;&#31209;&#36924;&#36817;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#22312;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#24573;&#30053;&#20102;&#36328;&#36890;&#36947;&#24046;&#24322;&#25110;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21152;&#26435;&#25130;&#26029;&#26680;&#33539;&#25968;&#20943;&#21435;&#25130;&#26029;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#65288;DtNFM&#65289;&#30340;&#26041;&#27861;&#26469;&#21435;&#22122;&#24425;&#33394;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22122;&#22768;&#22270;&#20687;&#30340;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#65292;&#30456;&#20284;&#32467;&#26500;&#34987;&#25910;&#38598;&#24182;&#26500;&#36896;&#20102;&#19968;&#31995;&#21015;&#30456;&#20284;&#30340;&#22359;&#30697;&#38453;&#12290;&#23545;&#20110;&#27599;&#20010;&#20998;&#32452;&#65292;&#23545;&#20854;&#36827;&#34892;DtNFM&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#21435;&#22122;&#29256;&#26412;&#12290;&#36890;&#36807;&#36830;&#25509;&#25152;&#26377;&#21435;&#22122;&#22359;&#30697;&#38453;&#65292;&#24471;&#21040;&#21435;&#22122;&#22270;&#20687;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;DtNFM&#27169;&#22411;&#26377;&#20004;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#36328;&#36890;&#36947;&#24046;&#24322;&#21644;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;&#36825;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high flexibility and remarkable performance, low-rank approximation methods has been widely studied for color image denoising. However, those methods mostly ignore either the cross-channel difference or the spatial variation of noise, which limits their capacity in real world color image denoising. To overcome those drawbacks, this paper is proposed to denoise color images with a double-weighted truncated nuclear norm minus truncated Frobenius norm minimization (DtNFM) method. Through exploiting the nonlocal self-similarity of the noisy image, the similar structures are gathered and a series of similar patch matrices are constructed. For each group, the DtNFM model is conducted for estimating its denoised version. The denoised image would be obtained by concatenating all the denoised patch matrices. The proposed DtNFM model has two merits. First, it models and utilizes both the cross-channel difference and the spatial variation of noise. This provides sufficient flexibility 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05435</link><description>&lt;p&gt;
One-Versus-Others Attention: &#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#36229;&#36234;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#25968;&#36890;&#24120;&#23569;&#20110;&#22235;&#20010;&#65288;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#39046;&#22495;&#65292;&#25968;&#25454;&#36755;&#20837;&#21487;&#33021;&#21253;&#25324;X&#23556;&#32447;&#12289;PET&#25195;&#25551;&#12289;MRI&#12289;&#36951;&#20256;&#31579;&#26597;&#12289;&#20020;&#24202;&#31508;&#35760;&#31561;&#65292;&#36825;&#23601;&#38656;&#35201;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#20004;&#20004;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#65292;&#20294;&#23545;&#20110;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#23545;&#20110;$n$&#20010;&#27169;&#24577;&#65292;&#35745;&#31639;&#27880;&#24847;&#21147;&#23558;&#23548;&#33268;$n \choose 2$&#30340;&#22797;&#26434;&#24230;&#65292;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#20013;&#31435;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#65292;&#35813;&#26426;&#21046;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02932</link><description>&lt;p&gt;
&#24403;&#25298;&#32477;&#23398;&#20064;&#23545;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#26368;&#20248;&#26102;
&lt;/p&gt;
&lt;p&gt;
When No-Rejection Learning is Optimal for Regression with Rejection. (arXiv:2307.02932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25298;&#32477;&#23398;&#20064;&#26159;&#30740;&#31350;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30456;&#20114;&#20316;&#29992;&#30340;&#20856;&#22411;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25298;&#32477;&#22120;&#12290;&#22312;&#26679;&#26412;&#21040;&#36798;&#26102;&#65292;&#25298;&#32477;&#22120;&#39318;&#20808;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#23427;&#65307;&#22914;&#26524;&#25509;&#21463;&#65292;&#39044;&#27979;&#22120;&#23436;&#25104;&#39044;&#27979;&#20219;&#21153;&#65307;&#22914;&#26524;&#34987;&#25298;&#32477;&#65292;&#21017;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#20154;&#31867;&#12290;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#39044;&#27979;&#22120;&#21644;&#25298;&#32477;&#22120;&#12290;&#36825;&#25913;&#21464;&#20102;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#38750;&#20984;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#24102;&#26377;&#25298;&#32477;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#19968;&#20123;&#30740;&#31350;&#24320;&#21457;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#24182;&#30740;&#31350;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.02799</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#65292;&#20445;&#30041;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information. (arXiv:2307.02799v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#21040;&#30697;&#38453;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#65288;PSM&#65289;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;&#19982;&#19968;&#33324;&#30340;&#26174;&#33879;&#24615;&#22270;&#30456;&#27604;&#65292;PSM&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#30340;&#26144;&#23556;&#25351;&#31034;&#20102;&#20010;&#20307;&#29305;&#23450;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#23545;&#20110;&#20174;&#20957;&#35270;&#21306;&#22495;&#30340;&#24322;&#36136;&#24615;&#20013;&#33719;&#21462;&#20010;&#20307;&#35270;&#35273;&#20559;&#22909;&#38750;&#24120;&#26377;&#29992;&#12290;PSM&#30340;&#39044;&#27979;&#26159;&#20026;&#20102;&#33719;&#21462;&#26410;&#35265;&#22270;&#20687;&#30340;PSM&#65292;&#20294;&#30001;&#20110;&#20010;&#20307;&#20957;&#35270;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20174;&#26377;&#38480;&#30340;&#30524;&#21160;&#25968;&#25454;&#20013;&#35782;&#21035;&#20010;&#20307;&#20957;&#35270;&#27169;&#24335;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#20010;&#20307;&#20043;&#38388;&#20957;&#35270;&#36235;&#21183;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#26041;&#27861;&#20013;&#65292;PSMs&#34987;&#21521;&#37327;&#21270;&#20197;&#36866;&#24212;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#32780;&#24573;&#35270;&#20102;&#19982;&#22270;&#20687;&#23545;&#24212;&#30340;PSMs&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#33258;&#21160;&#25581;&#31034;PSMs&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This paper presents a few-shot personalized saliency prediction using tensor-to-matrix regression for preserving the structural global information of personalized saliency maps (PSMs). In contrast to a general saliency map, a PSM has been great potential since its map indicates the person-specific visual attention that is useful for obtaining individual visual preferences from heterogeneity of gazed areas. The PSM prediction is needed for acquiring the PSM for the unseen image, but its prediction is still a challenging task due to the complexity of individual gaze patterns. For recognizing individual gaze patterns from the limited amount of eye-tracking data, the previous methods adopt the similarity of gaze tendency between persons. However, in the previous methods, the PSMs are vectorized for the prediction model. In this way, the structural global information of the PSMs corresponding to the image is ignored. For automatically revealing the relationship between PSMs, we focus on the
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#26041;&#27861;&#65288;GGS&#65289;&#20248;&#21270;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#65292;&#28040;&#38500;&#20102;&#31361;&#21464;&#36317;&#31163;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#21457;&#29616;&#39640;&#36866;&#24212;&#24615;&#34507;&#30333;&#36136;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.00494</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#20248;&#21270;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing. (arXiv:2307.00494v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00494
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#26041;&#27861;&#65288;GGS&#65289;&#20248;&#21270;&#34507;&#30333;&#36136;&#36866;&#24212;&#24615;&#65292;&#28040;&#38500;&#20102;&#31361;&#21464;&#36317;&#31163;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#21457;&#29616;&#39640;&#36866;&#24212;&#24615;&#34507;&#30333;&#36136;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35774;&#35745;&#20986;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#39640;&#36866;&#24212;&#24615;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#23545;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#26469;&#35828;&#37117;&#26159;&#38761;&#21629;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#28023;&#37327;&#24207;&#21015;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#23558;&#25628;&#32034;&#38480;&#21046;&#22312;&#20174;&#21442;&#32771;&#24207;&#21015;&#30340;&#23567;&#31361;&#21464;&#21322;&#24452;&#33539;&#22260;&#20869;&#65292;&#20294;&#36825;&#26679;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#35774;&#35745;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#28040;&#38500;&#31361;&#21464;&#36317;&#31163;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;Gibbs&#37319;&#26679;&#65288;GGS&#65289;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#24212;&#29992;&#24102;&#26377;&#26799;&#24230;&#30340;Gibbs&#26469;&#25552;&#20986;&#26377;&#21033;&#30340;&#31361;&#21464;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#24179;&#28369;&#30340;&#26041;&#27861;&#21435;&#38500;&#23548;&#33268;&#20551;&#38451;&#24615;&#30340;&#22122;&#22768;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#21457;&#29616;&#20102;&#39640;&#36866;&#24212;&#24615;&#34507;&#30333;&#36136;&#65292;&#26368;&#22810;&#20855;&#26377;8&#20010;&#31361;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;GFP&#21644;AAV&#35774;&#35745;&#38382;&#39064;&#12289;&#28040;&#34701;&#35797;&#39564;&#21644;&#22522;&#20934;&#27169;&#22411;&#26469;&#38416;&#26126;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS
&lt;/p&gt;</description></item><item><title>&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#32500;&#24230;&#35781;&#21650;&#65292;&#20294;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#21487;&#20197;&#20811;&#26381;&#32500;&#24230;&#35781;&#21650;&#12290;</title><link>http://arxiv.org/abs/2306.15924</link><description>&lt;p&gt;
&#36816;&#31639;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The curse of dimensionality in operator learning. (arXiv:2306.15924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15924
&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#32500;&#24230;&#35781;&#21650;&#65292;&#20294;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#21487;&#20197;&#20811;&#26381;&#32500;&#24230;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#65292;&#21487;&#20197;&#29992;&#20110;&#36890;&#36807;&#27169;&#25311;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#65292;&#25110;&#32773;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24341;&#21457;&#20102;&#31639;&#23376;&#23398;&#20064;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#33324;&#30340;&#21482;&#30001;&#20854; $C^r$ &#25110; Lipschitz &#27491;&#21017;&#24615;&#29305;&#24449;&#21270;&#30340;&#31639;&#23376;&#31867;&#65292;&#31639;&#23376;&#23398;&#20064;&#36973;&#21463;&#20102;&#32500;&#24230;&#35781;&#21650;&#65292;&#36825;&#37324;&#36890;&#36807;&#26080;&#31351;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#31354;&#38388;&#30340;&#34920;&#24449;&#26469;&#31934;&#30830;&#23450;&#20041;&#32500;&#24230;&#35781;&#21650;&#12290;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324; PCA-Net&#12289;DeepONet &#21644; FNO &#22312;&#20869;&#30340;&#22810;&#31181;&#29616;&#26377;&#31070;&#32463;&#31639;&#23376;&#12290;&#26412;&#25991;&#30340;&#31532;&#20108;&#39033;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#65292;&#21487;&#20197;&#20811;&#26381;&#19968;&#33324;&#30340;&#32500;&#24230;&#35781;&#21650;&#65307;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operator architectures employ neural networks to approximate operators mapping between Banach spaces of functions; they may be used to accelerate model evaluations via emulation, or to discover models from data. Consequently, the methodology has received increasing attention over recent years, giving rise to the rapidly growing field of operator learning. The first contribution of this paper is to prove that for general classes of operators which are characterized only by their $C^r$- or Lipschitz-regularity, operator learning suffers from a curse of dimensionality, defined precisely here in terms of representations of the infinite-dimensional input and output function spaces. The result is applicable to a wide variety of existing neural operators, including PCA-Net, DeepONet and the FNO. The second contribution of the paper is to prove that the general curse of dimensionality can be overcome for solution operators defined by the Hamilton-Jacobi equation; this is achieved by lev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#65288;GSD&#65289;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#21644;&#19981;&#31934;&#30830;&#27010;&#29575;&#27169;&#22411;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#38750;&#26631;&#20934;&#31354;&#38388;&#20013;&#22914;&#20309;&#27491;&#30830;&#21033;&#29992;&#20840;&#37096;&#20449;&#24687;&#26469;&#36827;&#34892;&#27604;&#36739;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12803</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#40065;&#26834;&#32479;&#35745;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement. (arXiv:2306.12803v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#65288;GSD&#65289;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#21644;&#19981;&#31934;&#30830;&#27010;&#29575;&#27169;&#22411;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#38750;&#26631;&#20934;&#31354;&#38388;&#20013;&#22914;&#20309;&#27491;&#30830;&#21033;&#29992;&#20840;&#37096;&#20449;&#24687;&#26469;&#36827;&#34892;&#27604;&#36739;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#30456;&#24403;&#26222;&#36941;&#30340;&#65292;&#27604;&#22914;&#35828;&#65292;&#20855;&#26377;&#19981;&#21516;&#32553;&#25918;&#32500;&#24230;&#30340;&#22810;&#32500;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#27491;&#30830;&#22320;&#21033;&#29992;&#36825;&#20123;&#31354;&#38388;&#20013;&#32534;&#30721;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#20173;&#28982;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#65288;&#38598;&#21512;&#65289;&#20559;&#24207;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#38543;&#26426;&#21464;&#37327;&#26144;&#23556;&#21040;&#36825;&#20123;&#38750;&#26631;&#20934;&#31354;&#38388;&#20013;&#12290;&#24403;&#27809;&#26377;&#25110;&#23436;&#20840;&#30340;&#22522;&#25968;&#32467;&#26500;&#26102;&#65292;&#36825;&#20010;&#20559;&#24207;&#20851;&#31995;&#21253;&#21547;&#38543;&#26426;&#20248;&#21183;&#21644;&#26399;&#26395;&#39034;&#24207;&#20316;&#20026;&#26497;&#31471;&#24773;&#20917;&#12290;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#23548;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#65288;GSD&#65289;&#39034;&#24207;&#30340;&#65288;&#27491;&#21017;&#21270;&#30340;&#65289;&#32479;&#35745;&#26816;&#39564;&#65292;&#24182;&#36890;&#36807;&#19981;&#31934;&#30830;&#27010;&#29575;&#27169;&#22411;&#20351;&#20854;&#26356;&#20026;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#29992;&#22810;&#32500;&#36139;&#22256;&#24230;&#37327;&#12289;&#37329;&#34701;&#21644;&#21307;&#23398;&#25968;&#25454;&#36827;&#34892;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spaces with locally varying scale of measurement, like multidimensional structures with differently scaled dimensions, are pretty common in statistics and machine learning. Nevertheless, it is still understood as an open question how to exploit the entire information encoded in them properly. We address this problem by considering an order based on (sets of) expectations of random variables mapping into such non-standard spaces. This order contains stochastic dominance and expectation order as extreme cases when no, or respectively perfect, cardinal structure is given. We derive a (regularized) statistical test for our proposed generalized stochastic dominance (GSD) order, operationalize it by linear optimization, and robustify it by imprecise probability models. Our findings are illustrated with data from multidimensional poverty measurement, finance, and medicine.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.08175</link><description>&lt;p&gt;
DCTX-Conformer&#65306;&#38024;&#23545;&#20302;&#24310;&#36831;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;Conformer&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer. (arXiv:2306.08175v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08175
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#29616;&#22312;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#12290;&#35832;&#22914;&#21452;&#27169;&#24335;&#21644;&#21160;&#24577;&#20998;&#22359;&#35757;&#32451;&#31561;&#25216;&#26415;&#26377;&#21161;&#20110;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#23436;&#25972;&#21644;&#26377;&#38480;&#30340;&#36807;&#21435;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27969;&#24335;&#35782;&#21035;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#32479;&#19968;ASR&#31995;&#32479;&#36827;&#34892;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#8212;&#8212;&#21160;&#24577;&#19978;&#19979;&#25991;Conformer&#65288;DCTX-Conformer&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;&#38750;&#37325;&#21472;&#30340;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19968;&#22359;&#30340;&#24038;&#19978;&#19979;&#25991;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#30001;&#20110;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#30446;&#21069;&#26368;&#20248;&#35299;&#25552;&#21319;&#20102;25.0%&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#32780;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#8220;&#25237;&#30707;&#26426;&#8221;&#20248;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#35777;&#26126;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#21487;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04815</link><description>&lt;p&gt;
SGD&#20013;&#30340;&#25237;&#30707;&#26426;&#65306;&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#21450;&#20854;&#36890;&#36807;&#29305;&#24449;&#23398;&#20064;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning. (arXiv:2306.04815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#8220;&#25237;&#30707;&#26426;&#8221;&#20248;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#35777;&#26126;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#21487;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#20026;&#20160;&#20040;&#32463;&#24120;&#20986;&#29616;&#35757;&#32451;&#25439;&#22833;&#23574;&#23792;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#26159;&#8220;&#25237;&#30707;&#26426;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#20248;&#21270;&#29616;&#35937;&#65292;&#26368;&#21021;&#22312;[Lewkowycz&#31561;&#20154;&#65292;2020&#24180;]&#30340;&#22823;&#23398;&#20064;&#29575;GD&#20013;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25237;&#30707;&#26426;&#20986;&#29616;&#22312;&#30001;&#27491;&#20999;&#20869;&#26680;&#30340;&#21069;&#20960;&#20010;&#29305;&#24449;&#21521;&#37327;&#25152;&#24352;&#25104;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#36866;&#29992;&#20110;GD&#21644;SGD&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#65292;&#21363;&#25237;&#30707;&#26426;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#65288;AGOP&#65289;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;SGD&#20013;&#65292;&#26356;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#20250;&#23548;&#33268;&#26356;&#22810;&#30340;&#25237;&#30707;&#26426;&#20986;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;AGOP&#23545;&#40784;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first present an explanation regarding the common occurrence of spikes in the training loss when neural networks are trained with stochastic gradient descent (SGD). We provide evidence that the spikes in the training loss of SGD are "catapults", an optimization phenomenon originally observed in GD with large learning rates in [Lewkowycz et al. 2020]. We empirically show that these catapults occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel, for both GD and SGD. Second, we posit an explanation for how catapults lead to better generalization by demonstrating that catapults promote feature learning by increasing alignment with the Average Gradient Outer Product (AGOP) of the true predictor. Furthermore, we demonstrate that a smaller batch size in SGD induces a larger number of catapults, thereby improving AGOP alignment and test performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.03928</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#35774;&#35745;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#36890;&#24120;&#34987;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#30340;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#39044;&#27979;&#24182;&#19981;&#23436;&#32654;&#65292;&#36825;&#20123;&#31995;&#32479;&#36824;&#38656;&#35201;&#35753;&#20154;&#31867;&#19987;&#23478;&#20102;&#35299;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#26469;&#26356;&#26032;&#33258;&#24049;&#30340;&#39044;&#27979;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26368;&#36817;&#26377;&#20154;&#35748;&#20026;&#65292;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#21487;&#33021;&#20250;&#36991;&#24320;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#31995;&#32479;&#19981;&#26159;&#25552;&#20379;&#21333;&#20010;&#26631;&#31614;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#19968;&#32452;&#26631;&#31614;&#39044;&#27979;&#20540;&#65292;&#21363;&#39044;&#27979;&#38598;&#65292;&#24182;&#24378;&#21046;&#35201;&#27714;&#19987;&#23478;&#20174;&#39044;&#27979;&#38598;&#20013;&#39044;&#27979;&#19968;&#20010;&#26631;&#31614;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#36804;&#20170;&#20173;&#20381;&#36182;&#20110;&#26679;&#24335;&#21270;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25215;&#35834;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#31181;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#27880;&#24847;&#21147;&#23618;&#21487;&#20197;&#35760;&#24518;&#937;(Hn)&#20010;&#31034;&#20363;&#24207;&#21015;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#35760;&#24518;&#23481;&#37327;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.02010</link><description>&lt;p&gt;
Transformers&#20013;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Memorization Capacity of Multi-Head Attention in Transformers. (arXiv:2306.02010v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#27880;&#24847;&#21147;&#23618;&#21487;&#20197;&#35760;&#24518;&#937;(Hn)&#20010;&#31034;&#20363;&#24207;&#21015;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#35760;&#24518;&#23481;&#37327;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#25104;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#39318;&#36873;&#26550;&#26500;&#65292;&#20294;&#20854;&#29702;&#35770;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#35760;&#24518;&#23481;&#37327;&#65292;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#33021;&#22815;&#35760;&#24518;&#22810;&#23569;&#20010;&#31034;&#20363;&#24207;&#21015;&#65292;&#20316;&#20026;&#22836;&#25968;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#20989;&#25968;&#12290;&#22312;&#23545;&#35270;&#35273;transformers&#30340;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#20110;&#36755;&#20837;&#25968;&#25454;&#32447;&#24615;&#29420;&#31435;&#24615;&#30340;&#26032;&#20551;&#35774;&#65292;&#19981;&#21516;&#20110;&#36890;&#24120;&#20351;&#29992;&#30340;&#19968;&#33324;&#20301;&#32622;&#20551;&#35774;&#12290;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;H&#20010;&#22836;&#65292;&#32500;&#24230;d&#65292;&#19978;&#19979;&#25991;&#22823;&#23567;n &lt; d&#30340;&#27880;&#24847;&#21147;&#23618;&#65292;&#20855;&#26377;&#920;(Hd^2)&#20010;&#21442;&#25968;&#65292;&#21487;&#20197;&#35760;&#20303;&#937;(Hn)&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#22836;&#22914;&#20309;&#22788;&#29702;&#19981;&#21516;&#30340;&#31034;&#20363;&#24207;&#21015;&#65292;&#21463;&#21040;softmax&#36816;&#31639;&#31526;&#30340;&#39281;&#21644;&#29305;&#24615;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can memorize, as a function of the number of heads and sequence length. Motivated by experimental findings on vision transformers, we introduce novel assumptions about the linear independence of input data, distinct from the commonly used general-position assumption. Under these assumptions, we demonstrate that an attention layer with $H$ heads, dimension $d$, and context size $n &lt; d$, featuring $\Theta(Hd^2)$ parameters, can memorize $\Omega(Hn)$ examples. Our analysis sheds light on how different attention heads handle various example sequences, aided by the softmax operator's saturation property. We validate our findings through experiments on synthetic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18436</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#23454;&#29616;&#26368;&#20248;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#22343;&#20540;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#26494;&#24347;&#26368;&#36817;&#34987;&#25552;&#20986;&#29992;&#20110;&#35299;&#20915;K&#22343;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#20294;&#23454;&#29616;SDP&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#36825;&#20123;&#20445;&#35777;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#34987;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#20047;&#22362;&#23454;&#30340;&#32479;&#35745;&#22522;&#30784;&#25110;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;NMF&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#20984;Burer-Monteiro&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#21322;&#23450;&#35268;&#21010;&#26494;&#24347;&#30340;K&#22343;&#20540;&#20844;&#24335;&#30340;&#38750;&#36127;&#20302;&#31209;&#38480;&#21046;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#65292;&#21516;&#26102;&#20063;&#20139;&#26377;&#19982;SDP&#30456;&#21516;&#30340;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;NMF&#31639;&#27861;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;SDP&#27714;&#35299;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
$K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18352</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#22810;&#35270;&#35282;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Genetic Algorithm for Multi-View Feature Selection. (arXiv:2305.18352v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18352
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#34917;&#20805;&#20449;&#24687;&#26469;&#22686;&#24378;&#39044;&#27979;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20250;&#23548;&#33268;&#39640;&#32500;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;&#36825;&#23545;&#21487;&#20197;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#39044;&#27979;&#27169;&#22411;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#19981;&#33391;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#21033;&#29992;&#36328;&#27169;&#24577;&#30340;&#20869;&#22312;&#20449;&#24687;&#12289;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#20110;&#29305;&#23450;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#19978;&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of feature
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17583</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26080;&#38480;&#26641;&#29366;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;&#27010;&#29575;&#22270;&#27169;&#22411;(PGMs)&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#26366;&#32463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#20026;&#26680;&#26426;&#22120;&#25110;&#26080;&#38480;&#22823;&#23567;&#30340;&#39640;&#26031;&#36807;&#31243;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#12290;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
&lt;/p&gt;</description></item><item><title>CoCoRL&#26159;&#19968;&#31181;&#20174;&#19981;&#30693;&#36947;&#22870;&#21169;&#30340;&#24050;&#30693;&#23433;&#20840;&#28436;&#31034;&#20013;&#25512;&#26029;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;Constrained Markov Decision Process&#65288;CMDP&#65289;&#65292;&#24182;&#19988;&#23545;&#20110;&#20960;&#20046;&#26368;&#20248;&#28436;&#31034;&#33021;&#22815;&#26080;&#35823;&#24046;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#23433;&#20840;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.16147</link><description>&lt;p&gt;
&#20174;&#26410;&#30693;&#22870;&#21169;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#23433;&#20840;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Learning Safety Constraints from Demonstrations with Unknown Rewards. (arXiv:2305.16147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16147
&lt;/p&gt;
&lt;p&gt;
CoCoRL&#26159;&#19968;&#31181;&#20174;&#19981;&#30693;&#36947;&#22870;&#21169;&#30340;&#24050;&#30693;&#23433;&#20840;&#28436;&#31034;&#20013;&#25512;&#26029;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;Constrained Markov Decision Process&#65288;CMDP&#65289;&#65292;&#24182;&#19988;&#23545;&#20110;&#20960;&#20046;&#26368;&#20248;&#28436;&#31034;&#33021;&#22815;&#26080;&#35823;&#24046;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#23433;&#20840;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Convex Constraint Learning for Reinforcement Learning (CoCoRL)&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#24050;&#30693;&#23433;&#20840;&#28436;&#31034;&#20013;&#25512;&#26029;Constrained Markov Decision Process (CMDP)&#30340;&#20849;&#20139;&#32422;&#26463;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#21482;&#38480;&#20110;&#24050;&#30693;&#22870;&#21169;&#25110;&#23436;&#20840;&#24050;&#30693;&#29615;&#22659;&#21160;&#24577;&#30340;&#28436;&#31034;&#30456;&#27604;&#65292;CoCoRL&#21487;&#20197;&#20174;&#20855;&#26377;&#19981;&#21516;&#26410;&#30693;&#22870;&#21169;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#29615;&#22659;&#21160;&#24577;&#12290;CoCoRL&#22522;&#20110;&#28436;&#31034;&#26500;&#24314;&#20102;&#19968;&#20010;&#20984;&#23433;&#20840;&#38598;&#65292;&#21363;&#20351;&#26159;&#28508;&#22312;&#30340;&#27425;&#20248;&#28436;&#31034;&#20063;&#33021;&#20445;&#35777;&#23433;&#20840;&#12290;&#23545;&#20110;&#20960;&#20046;&#26368;&#20248;&#28436;&#31034;&#65292;CoCoRL&#33021;&#22815;&#26080;&#35823;&#24046;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#23433;&#20840;&#38598;&#12290;&#25105;&#20204;&#22312;&#34920;&#26684;&#29615;&#22659;&#21644;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#32422;&#26463;&#30340;&#36830;&#32493;&#39550;&#39542;&#20223;&#30495;&#20013;&#35780;&#20272;CoCoRL&#12290;CoCoRL&#23398;&#20064;&#21040;&#30340;&#38480;&#21046;&#23548;&#33268;&#20102;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#24182;&#21487;&#20197;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#22522;&#20110;&#23398;&#20064;&#24050;&#30693;&#22238;&#25253;&#30340;&#26367;&#20195;&#26041;&#27861;&#26080;&#27861;&#25512;&#24191;&#21040;&#20855;&#26377;&#19981;&#21516;&#22238;&#25253;&#30340;&#26032;&#29615;&#22659;&#65292;&#31361;&#26174;&#20102;CoCoRL&#22312;&#19981;&#30693;&#36947;&#22238;&#25253;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32422;&#26463;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a novel approach for inferring shared constraints in a Constrained Markov Decision Process (CMDP) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, CoCoRL can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. CoCoRL constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL converges to the true safe set with no policy regret. We evaluate CoCoRL in tabular environments and a continuous driving simulation with multiple constraints. CoCoRL learns constraints that lead to safe driving behavior and that can be transferred to different tasks and environments. In contrast, alternative methods based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15086</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#23454;&#29616;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#20174;&#22122;&#22768;&#29983;&#25104;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#39640;&#26031;&#20808;&#39564;&#20551;&#35774;&#65292;&#23427;&#20204;&#22312;&#38750;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#34203;&#23450;&#35860;&#26725;&#26159;&#19968;&#31181;&#23398;&#20064; SDE &#20197;&#22312;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#34987;&#35270;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#34203;&#23450;&#35860;&#26725;&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20043;&#38388;&#30340;&#38750;&#37197;&#23545;&#36716;&#25442;&#26041;&#38754;&#24182;&#19981;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#65288;UNSB&#65289;&#65292;&#23427;&#23558;&#34203;&#23450;&#35860;&#26725;&#19982;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#23398;&#20064;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#30340; SDE&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; UNSB &#26159;&#21487;&#20280;&#32553;&#30340;&#65292;&#24182;&#19988;&#25104;&#21151;&#35299;&#20915;&#20102;&#21508;&#31181;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#38024;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#20102;&#26377;&#26465;&#20214;&#27169;&#25311;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13318</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#26041;&#27861;&#36827;&#34892;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A principled deep learning approach for geological facies generation. (arXiv:2305.13318v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#38024;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#20102;&#26377;&#26465;&#20214;&#27169;&#25311;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22320;&#29699;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#27169;&#25311;&#19981;&#21487;&#35266;&#27979;&#20307;&#31215;&#20013;&#30340;&#22320;&#36136;&#30456;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#32771;&#34385;&#21040;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#28145;&#24230;&#29983;&#25104;&#23398;&#20064;&#26159;&#20811;&#26381;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#23616;&#38480;&#24615;&#65288;&#29305;&#21035;&#26159;&#32570;&#20047;&#29289;&#29702;&#36924;&#30495;&#24615;&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#23545;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#36827;&#34892;&#24212;&#29992;&#65292;&#20197;&#20415;&#26377;&#26465;&#20214;&#22320;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#25239;&#24615;&#26041;&#27861;&#21644;&#26088;&#22312;&#20419;&#36827;&#20854;&#35757;&#32451;&#30340;&#31283;&#23450;&#21270;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20197;&#38543;&#26426;&#36807;&#31243;&#20026;&#22522;&#30784;&#30340;Flumy&#27169;&#22411;&#29983;&#25104;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#27169;&#25311;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#21033;&#29992;&#24418;&#24577;&#23398;&#25351;&#26631;&#27604;&#36739;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#36845;&#20195;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#31283;&#23450;&#25216;&#26415;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of geological facies in an unobservable volume is essential in various geoscience applications. Given the complexity of the problem, deep generative learning is a promising approach to overcome the limitations of traditional geostatistical simulation models, in particular their lack of physical realism. This research aims to investigate the application of generative adversarial networks and deep variational inference for conditionally simulating meandering channels in underground volumes. In this paper, we review the generative deep learning approaches, in particular the adversarial ones and the stabilization techniques that aim to facilitate their training. The proposed approach is tested on 2D and 3D simulations generated by the stochastic process-based model Flumy. Morphological metrics are utilized to compare our proposed method with earlier iterations of generative adversarial networks. The results indicate that by utilizing recent stabilization techniques, generati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10738</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#22270;&#32858;&#31867;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36866;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861; - &#21487;&#20197;&#25429;&#33719;&#20851;&#38190;&#30340;&#21160;&#24577;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#38754;&#21521;&#32858;&#31867;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#26469;&#22788;&#29702;&#12290;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#21160;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20063;&#24341;&#21457;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TGC&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#22270;&#28145;&#24230;&#32858;&#31867;&#65292;&#23427;&#35843;&#25972;&#20102;&#28145;&#24230;&#32858;&#31867;&#25216;&#26415;&#65288;&#32858;&#31867;&#20998;&#37197;&#20998;&#24067;&#21644;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#65289;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#22270;&#22522;&#20110;&#20132;&#20114;&#24207;&#21015;&#30340;&#25209;&#22788;&#29702;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#20102;&#26102;&#38388;&#22270;&#32858;&#31867;&#19982;&#29616;&#26377;&#38745;&#24577;&#22270;&#32858;&#31867;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;TGC&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#27599;&#20010;&#20132;&#36890;&#28857;&#30340;&#26102;&#38388;&#65292;&#35299;&#20915;&#20844;&#20132;&#36816;&#36755;&#20013;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19981;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15495</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#25353;&#38656;&#20844;&#20849;&#20132;&#36890;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit. (arXiv:2303.15495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#27599;&#20010;&#20132;&#36890;&#28857;&#30340;&#26102;&#38388;&#65292;&#35299;&#20915;&#20844;&#20132;&#36816;&#36755;&#20013;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19981;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#30340;&#20027;&#35201;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#65292;&#20844;&#20132;&#36816;&#36755;&#23384;&#22312;&#30528;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20110;&#20056;&#23458;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#30340;&#20272;&#35745;&#26356;&#21152;&#20934;&#30830;&#21644;&#21487;&#38752;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#24310;&#35823;&#21644;&#20943;&#23569;&#20056;&#23458;&#20154;&#25968;&#65292;&#23588;&#20854;&#26159;&#22312;&#20381;&#38752;&#20844;&#20849;&#20132;&#36890;&#30340;&#22478;&#24066;&#20013;&#26356;&#21152;&#20005;&#37325;&#12290;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19982;&#26102;&#38388;&#34920;&#19981;&#21305;&#37197;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#22266;&#23450;&#26102;&#21051;&#34920;&#30340;&#24310;&#36831;&#12290;&#26681;&#25454;&#26412;&#25991;&#22312;&#32445;&#32422;&#24066;&#20844;&#20132;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#30740;&#31350;&#65292;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#21644;&#23454;&#38469;&#35745;&#21010;&#26102;&#38388;&#20043;&#38388;&#23384;&#22312;&#24179;&#22343;&#32422;&#20843;&#20998;&#38047;&#25110;491&#31186;&#30340;&#24310;&#36831;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27599;&#20010;&#20132;&#36890;&#28857;&#65288;&#31449;&#65289;&#20844;&#20132;&#36710;&#30340;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#22823;&#37117;&#24066;&#21306;&#22495;&#20013;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20026;&#20272;&#31639;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the major public transportation systems in cities, bus transit has its problems, including more accuracy and reliability when estimating the bus arrival time for riders. This can lead to delays and decreased ridership, especially in cities where public transportation is heavily relied upon. A common issue is that the arrival times of buses do not match the schedules, resulting in latency for fixed schedules. According to the study in this paper on New York City bus data, there is an average delay of around eight minutes or 491 seconds mismatch between the bus arrivals and the actual scheduled time. This research paper presents a novel AI-based data-driven approach for estimating the arrival times of buses at each transit point (station). Our approach is based on a fully connected neural network and can predict the arrival time collectively across all bus lines in large metropolitan areas. Our neural-net data-driven approach provides a new way to estimate the arrival time of the b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#32447;&#24615;&#21028;&#21035;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#20004;&#20010;&#20551;&#35774;&#30340;&#20984;&#32452;&#21512;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#21333;&#20010;&#28304;&#20219;&#21153;&#30340;&#30452;&#25509;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26368;&#20248;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20110;EEG&#21644;ECG&#30340;&#20998;&#31867;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14186</link><description>&lt;p&gt;
&#24102;&#26377;Fisher&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#30340;&#36817;&#20284;&#26368;&#20248;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis. (arXiv:2302.14186v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#32447;&#24615;&#21028;&#21035;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#20004;&#20010;&#20551;&#35774;&#30340;&#20984;&#32452;&#21512;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#21333;&#20010;&#28304;&#20219;&#21153;&#30340;&#30452;&#25509;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26368;&#20248;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20110;EEG&#21644;ECG&#30340;&#20998;&#31867;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;Fisher&#32447;&#24615;&#21028;&#21035;&#65288;FLD&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#35813;&#31867;&#27169;&#22411;&#26159;&#20004;&#20010;&#20551;&#35774;&#30340;&#20984;&#32452;&#21512;&#65306;i&#65289;&#20195;&#34920;&#20808;&#21069;&#30475;&#21040;&#30340;&#28304;&#20219;&#21153;&#30340;&#24179;&#22343;&#20551;&#35774;&#21644;ii&#65289;&#22312;&#26032;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#20551;&#35774;&#12290;&#23545;&#20110;&#29305;&#23450;&#30340;&#29983;&#25104;&#35774;&#32622;&#65292;&#25105;&#20204;&#22312;0-1&#25439;&#22833;&#19979;&#23548;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35745;&#31639;&#30340;&#36924;&#36817;&#65292;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#23545;&#26368;&#20248;&#20551;&#35774;&#12289;&#20551;&#35774;i&#65289;&#21644;&#20551;&#35774;ii&#65289;&#20043;&#38388;&#30456;&#23545;&#39118;&#38505;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#22312;&#22522;&#20110;EEG&#21644;ECG&#30340;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35748;&#20026;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#21333;&#20010;&#28304;&#20219;&#21153;&#30340;&#30452;&#25509;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#26368;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#21487;&#33021;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a class of models based on Fisher's Linear Discriminant (FLD) in the context of domain adaptation. The class is the convex combination of two hypotheses: i) an average hypothesis representing previously seen source tasks and ii) a hypothesis trained on a new target task. For a particular generative setting we derive the optimal convex combination of the two models under 0-1 loss, propose a computable approximation, and study the effect of various parameter settings on the relative risks between the optimal hypothesis, hypothesis i), and hypothesis ii). We demonstrate the effectiveness of the proposed optimal classifier in the context of EEG- and ECG-based classification settings and argue that the optimal classifier can be computed without access to direct information from any of the individual source tasks. We conclude by discussing further applications, limitations, and possible future directions.
&lt;/p&gt;</description></item><item><title>PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13755</link><description>&lt;p&gt;
&#21452;&#20215;&#20540;&#32593;&#32476;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrosynthetic Planning with Dual Value Networks. (arXiv:2301.13755v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13755
&lt;/p&gt;
&lt;p&gt;
PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26088;&#22312;&#20174;&#21830;&#19994;&#19978;&#21487;&#24471;&#30340;&#36215;&#22987;&#26448;&#26009;&#20013;&#25214;&#21040;&#21512;&#25104;&#30446;&#26631;&#20998;&#23376;&#30340;&#36335;&#32447;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21333;&#27493;&#21453;&#24212;&#39044;&#27979;&#22120;&#19982;&#22810;&#27493;&#35268;&#21010;&#22120;&#30340;&#32452;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21333;&#27493;&#39044;&#27979;&#22120;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#31163;&#32447;&#35757;&#32451;&#30340;&#65292;&#21482;&#20248;&#21270;&#21333;&#27493;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#23436;&#25972;&#30340;&#36335;&#32447;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;PDVN&#65292;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;MDP&#26469;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25913;&#21892;&#21333;&#27493;&#39044;&#27979;&#22120;&#12290;&#22312;PDVN&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#21333;&#29420;&#30340;&#20215;&#20540;&#32593;&#32476;&#65292;&#20998;&#21035;&#39044;&#27979;&#20998;&#23376;&#30340;&#21487;&#21512;&#25104;&#24615;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#20445;&#25345;&#21333;&#27493;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of ML-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning (RL) to improve the single-step predictor, by using a tree-shaped MDP to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks (PDVN), which alternates between the planning phase and updating phase. In PDVN, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20309;&#29256;&#26412;&#30340;Weisfeiler-Leman&#27979;&#35797;(GWL)&#65292;&#21487;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20960;&#20309;GNN&#30340;&#34920;&#29616;&#21147;</title><link>http://arxiv.org/abs/2301.09308</link><description>&lt;p&gt;
&#35770;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Geometric Graph Neural Networks. (arXiv:2301.09308v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20309;&#29256;&#26412;&#30340;Weisfeiler-Leman&#27979;&#35797;(GWL)&#65292;&#21487;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20960;&#20309;GNN&#30340;&#34920;&#29616;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; Weisfeiler-Leman (WL) &#22270;&#21516;&#26500;&#27979;&#35797;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#30340;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340; GNNs &#21644; WL &#26694;&#26550;&#19981;&#36866;&#29992;&#20110;&#23884;&#20837;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20960;&#20309;&#22270;&#24418;&#65292;&#20363;&#22914;&#29983;&#29289;&#20998;&#23376;&#12289;&#26448;&#26009;&#21644;&#20854;&#20182;&#29289;&#29702;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; WL &#27979;&#35797;&#30340;&#20960;&#20309;&#29256;&#26412; (GWL)&#65292;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#65292;&#21516;&#26102;&#23562;&#37325;&#24213;&#23618;&#29289;&#29702;&#23545;&#31216;&#24615;&#65306;&#25490;&#21015;&#12289;&#26059;&#36716;&#12289;&#21453;&#23556;&#21644;&#24179;&#31227;&#12290;&#25105;&#20204;&#20351;&#29992; GWL &#26469;&#34920;&#24449;&#20855;&#26377;&#19981;&#21464;&#25110;&#31561;&#21464;&#20110;&#29289;&#29702;&#23545;&#31216;&#24615;&#30340;&#20960;&#20309; GNN &#30340;&#34920;&#29616;&#21147;&#65292;&#20197;&#21306;&#20998;&#20960;&#20309;&#22270;&#24418;&#12290;GWL &#25581;&#31034;&#20102;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20960;&#20309; GNN &#30340;&#34920;&#29616;&#21147;&#65306;(1) &#19981;&#21464;&#23618;&#34920;&#29616;&#21147;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21306;&#20998;&#19968;&#36339;&#30456;&#21516;&#30340;&#20960;&#20309;&#22270;&#24418;&#65307;(2) &#31561;&#21464;&#23618;&#36890;&#36807;&#20256;&#25773;&#23616;&#37096;&#37051;&#22495;&#20043;&#22806;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#21306;&#20998;&#26356;&#22823;&#31867;&#21035;&#30340;&#22270;&#24418;&#65307;(3)
&lt;/p&gt;
&lt;p&gt;
The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3)
&lt;/p&gt;</description></item><item><title>FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.07160</link><description>&lt;p&gt;
FedTracker&#65306;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model. (arXiv:2211.07160v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07160
&lt;/p&gt;
&lt;p&gt;
FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#38656;&#35201;&#23558;&#27169;&#22411;&#26292;&#38706;&#32473;&#21508;&#31181;&#21442;&#19982;&#32773;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24694;&#24847;&#23458;&#25143;&#31471;&#26410;&#32463;&#25480;&#26435;&#22320;&#20998;&#21457;&#25110;&#36716;&#21806;&#27169;&#22411;&#65292;&#20174;&#32780;&#25439;&#23475;FL&#22242;&#38431;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#20026;&#20102;&#38459;&#27490;&#36825;&#31181;&#19981;&#24403;&#34892;&#20026;&#65292;&#24314;&#31435;&#19968;&#31181;&#39564;&#35777;&#27169;&#22411;&#25152;&#26377;&#26435;&#24182;&#36861;&#28335;&#27844;&#38706;&#32773;&#30340;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedTracker&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;FL&#27169;&#22411;&#20445;&#25252;&#26694;&#26550;&#12290;FedTracker&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#21253;&#25324;&#20840;&#23616;&#27700;&#21360;&#26426;&#21046;&#21644;&#26412;&#22320;&#25351;&#32441;&#26426;&#21046;&#12290;&#21069;&#32773;&#29992;&#20110;&#39564;&#35777;&#20840;&#23616;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#65292;&#32780;&#21518;&#32773;&#29992;&#20110;&#35782;&#21035;&#35813;&#27169;&#22411;&#26469;&#33258;&#21738;&#20010;&#23458;&#25143;&#31471;&#12290;FedTracker&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#21407;&#21017;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) princ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#20013;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Fisher&#20960;&#20309;&#21644;Bures-Wasserstein&#20960;&#20309;&#30340;&#20960;&#20309;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#32447;&#24615;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.00617</link><description>&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems. (arXiv:2211.00617v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#20013;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Fisher&#20960;&#20309;&#21644;Bures-Wasserstein&#20960;&#20309;&#30340;&#20960;&#20309;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#32447;&#24615;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#36830;&#32493;&#26102;&#38388;&#25506;&#32034;&#24615;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#20013;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#35813;&#35774;&#32622;&#21253;&#25324;&#20855;&#26377;&#38750;&#30830;&#23450;&#24615;&#25104;&#26412;&#21644;&#20801;&#35768;&#30446;&#26631;&#20013;&#28155;&#21152;&#39069;&#22806;&#30340;&#29109;&#27491;&#21017;&#21270;&#39033;&#30340;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#30340;&#39640;&#26031;&#31574;&#30053;&#65292;&#20854;&#22343;&#20540;&#26159;&#29366;&#24577;&#21464;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#65292;&#26041;&#24046;&#19982;&#29366;&#24577;&#26080;&#20851;&#12290;&#19982;&#31163;&#25955;&#26102;&#38388;&#38382;&#39064;&#30456;&#21453;&#65292;&#31574;&#30053;&#20013;&#30340;&#25104;&#26412;&#20989;&#25968;&#19981;&#26159;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#24182;&#19988;&#24182;&#38750;&#25152;&#26377;&#30340;&#19979;&#38477;&#26041;&#21521;&#37117;&#23548;&#33268;&#26377;&#30028;&#30340;&#36845;&#20195;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Fisher&#20960;&#20309;&#21644;Bures-Wasserstein&#20960;&#20309;&#30340;&#31574;&#30053;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30340;&#20960;&#20309;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#31574;&#30053;&#36845;&#20195;&#34987;&#35777;&#26126;&#28385;&#36275;&#20808;&#39564;&#30028;&#65292;&#24182;&#20197;&#32447;&#24615;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#31163;&#25955;&#26102;&#38388;&#31574;&#30053;&#30340;PG&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#36830;&#32493;&#26102;&#38388;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the global linear convergence of policy gradient (PG) methods for finite-horizon continuous-time exploratory linear-quadratic control (LQC) problems. The setting includes stochastic LQC problems with indefinite costs and allows additional entropy regularisers in the objective. We consider a continuous-time Gaussian policy whose mean is linear in the state variable and whose covariance is state-independent. Contrary to discrete-time problems, the cost is noncoercive in the policy and not all descent directions lead to bounded iterates. We propose geometry-aware gradient descents for the mean and covariance of the policy using the Fisher geometry and the Bures-Wasserstein geometry, respectively. The policy iterates are shown to satisfy an a-priori bound, and converge globally to the optimal policy with a linear rate. We further propose a novel PG method with discrete-time policies. The algorithm leverages the continuous-time analysis, and achieves a robust linear convergence acr
&lt;/p&gt;</description></item><item><title>AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.09475</link><description>&lt;p&gt;
AMPNet: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AMPNet: Attention as Message Passing for Graph Neural Networks. (arXiv:2210.09475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09475
&lt;/p&gt;
&lt;p&gt;
AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20256;&#32479;GNNs&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#20851;&#20110;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#30340;&#22797;&#26434;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;AMPNet&#65292;&#29992;&#20110;GNNs&#65292;&#23427;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#22312;&#28040;&#24687;&#20256;&#36882;&#27493;&#39588;&#20013;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#29983;&#29289;&#31995;&#32479;&#65288;&#22914;fMRI&#33041;&#27963;&#21160;&#35760;&#24405;&#21644;&#31354;&#38388;&#22522;&#22240;&#32452;&#25968;&#25454;&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;AMPNet&#30340;&#33021;&#21147;&#65292;&#23427;&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#22522;&#20934;&#25552;&#39640;&#20102;20&#65285;&#65292;&#22312;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#21518;&#21448;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;8&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#29983;&#29289;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;AMPNet&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful representation learning framework for graph-structured data. A key limitation of conventional GNNs is their representation of each node with a singular feature vector, potentially overlooking intricate details about individual node features. Here, we propose an Attention-based Message-Passing layer for GNNs (AMPNet) that encodes individual features per node and models feature-level interactions through cross-node attention during message-passing steps. We demonstrate the abilities of AMPNet through extensive benchmarking on real-world biological systems such as fMRI brain activity recordings and spatial genomic data, improving over existing baselines by 20% on fMRI signal reconstruction, and further improving another 8% with positional embedding added. Finally, we validate the ability of AMPNet to uncover meaningful feature-level interactions through case studies on biological systems. We anticipate that our architecture will be h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#36807;&#31243;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#65288;GPEC&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23545;&#22797;&#26434;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.02419</link><description>&lt;p&gt;
&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Boundary-Aware Uncertainty for Feature Attribution Explainers. (arXiv:2210.02419v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#36807;&#31243;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#65288;GPEC&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23545;&#22797;&#26434;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;&#30340;&#35299;&#37322;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#36890;&#24120;&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#36793;&#30028;&#21608;&#22260;&#23637;&#29616;&#20986;&#22797;&#26434;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#33030;&#24369;&#25110;&#35823;&#23548;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#37327;&#21270;&#36825;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#20102;&#35299;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#65288;GPEC&#65289;&#26694;&#26550;&#65292;&#23427;&#29983;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23558;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#19982;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#19981;&#30830;&#23450;&#24615;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27979;&#22320;&#32447;&#30340;&#26680;&#65292;&#23427;&#25429;&#25417;&#30446;&#26631;&#40657;&#30418;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26680;&#30456;&#20284;&#24230;&#38543;&#30528;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#36882;&#22686;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#40657;&#30418;&#20998;&#31867;&#22120;&#21644;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;GPEC&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#65292;&#24182;&#23548;&#33268;&#26356;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-hoc explanation methods have become a critical tool for understanding black-box classifiers in high-stakes applications. However, high-performing classifiers are often highly nonlinear and can exhibit complex behavior around the decision boundary, leading to brittle or misleading local explanations. Therefore there is an impending need to quantify the uncertainty of such explanation methods in order to understand when explanations are trustworthy. In this work we propose the Gaussian Process Explanation unCertainty (GPEC) framework, which generates a unified uncertainty estimate combining decision boundary-aware uncertainty with explanation function approximation uncertainty. We introduce a novel geodesic-based kernel, which captures the complexity of the target black-box decision boundary. We show theoretically that the proposed kernel similarity increases with decision boundary complexity. The proposed framework is highly flexible; it can be used with any black-box classifier an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2209.06356</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#31616;&#21333;&#29366;&#24577;-&#21160;&#20316;&#25277;&#35937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#36805;&#36895;&#25512;&#26029;&#20986;&#31561;&#20215;&#22870;&#21169;&#21644;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#21453;&#22797;&#35797;&#38169;&#26469;&#23398;&#20064;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#30340;&#20540;&#31561;&#20215;&#24615;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#23558;&#29615;&#22659;&#30340;&#35266;&#23519;MDP&#31616;&#21270;&#20026;&#25277;&#35937;MDP&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#24403;&#21487;&#20197;&#20107;&#20808;&#26500;&#24314;&#36866;&#24403;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#26102;&#65292;&#21487;&#20197;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#65292;&#36890;&#24120;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#21516;&#24577;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#21738;&#20123;&#29366;&#24577;&#21160;&#20316;&#23545;&#23548;&#33268;&#30456;&#21516;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#21457;&#29616;&#30446;&#21069;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#65292;&#35813;&#25351;&#26631;&#28385;&#36275;&#26356;&#22810;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#23569;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2209.06177</link><description>&lt;p&gt;
&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65306;&#21516;&#36136;&#24615;-&#24322;&#36136;&#24615;&#20108;&#20998;&#27861;&#21450;&#20854;&#24310;&#20280;
&lt;/p&gt;
&lt;p&gt;
Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond. (arXiv:2209.06177v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#21457;&#29616;&#30446;&#21069;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#65292;&#35813;&#25351;&#26631;&#28385;&#36275;&#26356;&#22810;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#23569;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#36136;&#24615;&#26159;&#25551;&#36848;&#36793;&#36830;&#25509;&#30456;&#20284;&#33410;&#28857;&#20542;&#21521;&#30340;&#22270;&#24418;&#23646;&#24615;&#65307;&#30456;&#21453;&#30340;&#27010;&#24565;&#20026;&#24322;&#36136;&#24615;&#12290;&#20154;&#20204;&#36890;&#24120;&#35748;&#20026;&#24322;&#36136;&#24615;&#22270;&#23545;&#20110;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#20184;&#20986;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#24320;&#21457;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#26222;&#36941;&#34987;&#25509;&#21463;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;&#27491;&#30830;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#24418;&#24335;&#21270;&#20102;&#26399;&#26395;&#30340;&#29305;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#21738;&#20123;&#25351;&#26631;&#28385;&#36275;&#21738;&#20123;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#30340;&#25351;&#26631;&#28385;&#36275;&#27604;&#20854;&#20182;&#27969;&#34892;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#26356;&#22810;&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#32780;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#21516;&#36136;&#24615;-&#24322;&#36136;&#24615;&#20108;&#20998;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#65292;&#20351;&#24471;...
&lt;/p&gt;
&lt;p&gt;
Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. It is often believed that heterophilous graphs are challenging for standard message-passing graph neural networks (GNNs), and much effort has been put into developing efficient methods for this setting. However, there is no universally agreed-upon measure of homophily in the literature. In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. For this, we formalize desirable properties for a proper homophily measure and verify which measures satisfy which properties. In particular, we show that a measure that we call adjusted homophily satisfies more desirable properties than other popular homophily measures while being rarely used in graph machine learning literature. Then, we go beyond the homophily-heterophily dichotomy and propose a new characteristic that allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.07827</link><description>&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(M-LSTF)&#26159;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19981;&#21516;&#65292;M-LSTF&#20219;&#21153;&#20174;&#20004;&#20010;&#26041;&#38754;&#26356;&#20855;&#25361;&#25112;&#24615;&#65306;1) M-LSTF&#27169;&#22411;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#65307;2)&#22312;&#28378;&#21160;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#20004;&#20010;&#36830;&#32493;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#38543;&#30528;&#39044;&#27979;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26356;&#26131;&#20110;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;M-LSTF&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23618;&#38754;&#30340;&#35760;&#24518;&#32452;&#20214;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#24335;&#30340;&#26041;&#24335;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#36880;&#27493;&#22312;&#35757;&#32451;&#26679;&#26412;&#20013;&#24341;&#20837;&#20271;&#21162;&#21033;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate long sequence time-series forecasting (M-LSTF) is a practical but challenging problem. Unlike traditional timer-series forecasting tasks, M-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to learn time-series patterns both within and between multiple time features; 2) Under the rolling forecasting setting, the similarity between two consecutive training samples increases with the increasing prediction length, which makes models more prone to overfitting. In this paper, we propose a generalizable memory-driven Transformer to target M-LSTF problems. Specifically, we first propose a global-level memory component to drive the forecasting procedure by integrating multiple time-series features. In addition, we adopt a progressive fashion to train our model to increase its generalizability, in which we gradually introduce Bernoulli noises to training samples. Extensive experiments have been performed on five different datasets across multiple fields. Exper
&lt;/p&gt;</description></item></channel></rss>