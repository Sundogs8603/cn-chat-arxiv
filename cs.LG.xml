<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;Follmer&#27969;&#26469;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#36716;&#21270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01460</link><description>&lt;p&gt;
&#28145;&#24230;&#26465;&#20214;&#29983;&#25104;&#23398;&#20064;&#65306;&#27169;&#22411;&#19982;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Conditional Generative Learning: Model and Error Analysis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;Follmer&#27969;&#26469;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#36716;&#21270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#31216;&#20026;&#26465;&#20214;Follmer&#27969;&#12290;&#20174;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#24320;&#22987;&#65292;&#25152;&#25552;&#20986;&#30340;&#27969;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;&#20854;&#36716;&#21270;&#20026;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#65292;&#22312;&#26102;&#38388;1&#22788;&#36798;&#21040;&#31283;&#23450;&#12290;&#20026;&#20102;&#26377;&#25928;&#23454;&#29616;&#65292;&#25105;&#20204;&#20351;&#29992;&#27431;&#25289;&#26041;&#27861;&#23545;&#27969;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38750;&#21442;&#25968;&#21270;&#20272;&#35745;&#36895;&#24230;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23398;&#20064;&#26679;&#26412;&#30340;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#22312;&#26465;&#20214;&#20998;&#24067;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#23427;&#22312;&#19968;&#31995;&#21015;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#26631;&#20934;&#30340;&#38750;&#21442;&#25968;&#21270;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#21040;&#28041;&#21450;&#22270;&#20687;&#25968;&#25454;&#30340;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#35828;&#26126;&#23427;&#20248;&#20110;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an Ordinary Differential Equation (ODE) based deep generative method for learning a conditional distribution, named the Conditional Follmer Flow. Starting from a standard Gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1. For effective implementation, we discretize the flow with Euler's method where we estimate the velocity field nonparametrically using a deep neural network. Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ODE flow. Our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing condition
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.20233</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Bilevel Optimization for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20233
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#30340;&#20989;&#25968;&#35270;&#35282;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#34987;&#26368;&#23567;&#21270;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#22312;&#21442;&#25968;&#35774;&#32622;&#19979;&#24320;&#21457;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#23545;&#20110;&#39044;&#27979;&#20989;&#25968;&#30340;&#21442;&#25968;&#24378;&#20984;&#12290;&#20989;&#25968;&#35270;&#35282;&#19981;&#20381;&#36182;&#20110;&#27492;&#20551;&#35774;&#65292;&#29305;&#21035;&#20801;&#35768;&#20351;&#29992;&#36229;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20869;&#37096;&#39044;&#27979;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#36866;&#21512;&#33258;&#28982;&#20989;&#25968;&#21452;&#23618;&#32467;&#26500;&#30340;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12995</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Protein Language Model for Unified Molecular Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#24037;&#31243;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#22312;&#27531;&#22522;&#32423;&#21035;&#19978;&#36816;&#34892;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21407;&#23376;&#27700;&#24179;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#38480;&#21046;&#38459;&#30861;&#20102;&#25105;&#20204;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ms-ESM&#65288;&#22810;&#23610;&#24230;ESM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#12290;ms-ESM&#36890;&#36807;&#39044;&#35757;&#32451;&#22810;&#23610;&#24230;&#20195;&#30721;&#20999;&#25442;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#33719;&#27531;&#22522;&#21644;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ms-ESM&#22312;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25581;&#31034;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#65292;ms-ESM&#19981;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12995v1 Announce Type: cross  Abstract: Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ms-ESM (multi-scale ESM), a novel approach that enables multi-scale unified molecular modeling. ms-ESM achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ms-ESM not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11432</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#30340;&#25581;&#31192;
&lt;/p&gt;
&lt;p&gt;
Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20986;&#29616;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#25968;&#37327;&#28608;&#22686;&#12290;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#24050;&#25104;&#20026;&#20854;&#20013;&#19968;&#39033;&#20027;&#35201;&#24212;&#29992;&#65292;&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#25110;&#39640;&#38454;&#36816;&#21160;&#23398;&#21464;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#31163;&#25955;&#36873;&#25321;&#25110;&#36830;&#32493;&#25511;&#21046;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;DRL&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36830;&#32493;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;DRL&#31639;&#27861;&#20316;&#20026;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20998;&#26512;&#25216;&#26415;&#26469;&#35752;&#35770;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11432v1 Announce Type: cross  Abstract: With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment. We provide some analytical techniques for discussing the interpretability of the trained mode
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25512;&#26029;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#26368;&#22823;&#21270;&#19982;&#25512;&#26029;&#27169;&#22411;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#25512;&#26029;&#27169;&#22411;&#36817;&#20284;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08941</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#26080;&#20851;&#21518;&#39564;&#36924;&#36817;&#30340;&#24555;&#36895;&#20934;&#30830;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08941
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25512;&#26029;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#26368;&#22823;&#21270;&#19982;&#25512;&#26029;&#27169;&#22411;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#25512;&#26029;&#27169;&#22411;&#36817;&#20284;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#25512;&#26029;&#21253;&#25324;&#23398;&#20064;&#20004;&#20010;&#27169;&#22411;&#65306;&#65288;1&#65289;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#28508;&#22312;&#31354;&#38388;&#19978;&#30340;&#31616;&#21333;&#20998;&#24067;&#36716;&#25442;&#20026;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25512;&#26029;&#27169;&#22411;&#65292;&#36817;&#20284;&#32473;&#23450;&#25968;&#25454;&#30340;&#28508;&#22312;&#32534;&#30721;&#21518;&#39564;&#12290;&#36825;&#20004;&#20010;&#32452;&#20214;&#36890;&#36807;&#23545;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#36793;&#38469;&#20284;&#28982;&#30340;&#19979;&#30028;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#12290;&#22312;&#32852;&#21512;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#25512;&#26029;&#27169;&#22411;&#24456;&#24046;&#22320;&#36817;&#20284;&#20102;&#28508;&#22312;&#32534;&#30721;&#21518;&#39564;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20248;&#21270;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23545;&#23398;&#20064;&#21040;&#30340;&#29983;&#25104;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25512;&#26029;&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#27599;&#27425;&#26356;&#26032;&#20043;&#21069;&#26368;&#22823;&#21270;&#19982;&#25512;&#26029;&#27169;&#22411;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36845;&#20195;&#35757;&#32451;&#25928;&#29575;&#20302;&#65292;&#38656;&#35201;&#21551;&#21457;&#24335;&#26631;&#20934;&#26469;&#20174;&#36845;&#20195;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08941v1 Announce Type: cross  Abstract: Inference for Variational Autoencoders (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data. The two components are learned jointly via a lower bound to the generative model's log marginal likelihood. In early phases of joint training, the inference model poorly approximates the latent code posteriors. Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model. As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model. Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative
&lt;/p&gt;</description></item><item><title>&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#65292;&#21487;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06009</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#21487;&#38752;LLM&#30340;&#26816;&#27979;&#22120;&#65306;&#23454;&#29616;&#12289;&#29992;&#36884;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06009
&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#65292;&#21487;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#20174;&#36755;&#20986;&#19981;&#24544;&#23454;&#21040;&#26377;&#20559;&#35265;&#21644;&#26377;&#27602;&#30340;&#29983;&#25104;&#12290;&#30001;&#20110;&#22260;&#32469;LLMs&#23384;&#22312;&#30340;&#20960;&#20010;&#38480;&#21046;&#24615;&#22240;&#32032;&#65288;&#35757;&#32451;&#25104;&#26412;&#12289;API&#35775;&#38382;&#12289;&#25968;&#25454;&#21487;&#29992;&#24615;&#31561;&#65289;&#65292;&#22312;&#37096;&#32626;&#27169;&#22411;&#26102;&#21487;&#33021;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#26045;&#21152;&#30452;&#25509;&#23433;&#20840;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27491;&#22312;&#21162;&#21147;&#21019;&#24314;&#21644;&#37096;&#32626;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65306;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#12290;&#38500;&#20102;&#26816;&#27979;&#22120;&#26412;&#36523;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#27169;&#22411;&#30340;&#24191;&#27867;&#29992;&#36884;&#8212;&#8212;&#20174;&#20805;&#24403;&#38450;&#25252;&#26639;&#21040;&#20419;&#36827;&#26377;&#25928;&#30340;AI&#27835;&#29702;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24320;&#21457;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#24037;&#20316;&#65292;&#26088;&#22312;&#20351;&#26816;&#27979;&#22120;&#26356;&#21487;&#38752;&#24182;&#25299;&#23637;&#20854;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06009v1 Announce Type: new  Abstract: Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model. Therefore, an efficient and reliable alternative is required. To this end, we present our ongoing efforts to create and deploy a library of detectors: compact and easy-to-build classification models that provide labels for various harms. In addition to the detectors themselves, we discuss a wide range of uses for these detector models - from acting as guardrails to enabling effective AI governance. We also deep dive into inherent challenges in their development and discuss future work aimed at making the detectors more reliable and broadening their scope.
&lt;/p&gt;</description></item><item><title>Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03304</link><description>&lt;p&gt;
&#25152;&#38656;&#21482;&#26159; Mad Libs: &#22686;&#24378;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03304
&lt;/p&gt;
&lt;p&gt;
Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25552;&#21462;&#65288;DocEAE&#65289;&#26159;&#19968;&#20010;&#26497;&#20854;&#22256;&#38590;&#30340;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Mad Lib Aug&#65288;MLA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335; DocEAE &#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102; Mad Libs &#30340;&#30452;&#35273;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#28909;&#38376;&#28216;&#25103;&#20013;&#20351;&#29992;&#30340;&#20998;&#31867;&#25513;&#30721;&#25991;&#26723;&#21487;&#20197;&#34987; LLMs &#29983;&#25104;&#24182;&#35299;&#31572;&#65292;&#20174;&#32780;&#20026; DocEAE &#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992; MLA&#65292;&#25105;&#20204;&#30340;&#25972;&#20307; F1 &#20998;&#25968;&#24179;&#22343;&#25913;&#36827;&#20102; 2.6 &#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#19982;&#26080;&#22686;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03304v1 Announce Type: new  Abstract: Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;</title><link>https://arxiv.org/abs/2403.03071</link><description>&lt;p&gt;
&#35770;Brenier&#30340;&#26497;&#20998;&#35299;&#30340;&#31070;&#32463;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
On a Neural Implementation of Brenier's Polar Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1991&#24180;&#65292;Brenier&#35777;&#26126;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#23558;$QR$&#20998;&#35299;&#65288;&#20998;&#20026;&#21322;&#27491;&#23450;&#30697;&#38453;$\times$&#37193;&#30697;&#38453;&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#30690;&#37327;&#22330;$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#20219;&#24847;&#22330;$F$&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#20984;&#20989;&#25968;$u$&#30340;&#26799;&#24230;&#19982;&#20445;&#27979;&#24230;&#26144;&#23556;$M$&#30340;&#22797;&#21512;&#65292;&#21363;$F=\nabla u \circ M$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20855;&#26377;&#28145;&#36828;&#29702;&#35770;&#24847;&#20041;&#30340;&#32467;&#26524;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#23450;&#29702;&#19982;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#28508;&#22312;&#20989;&#25968;$u$&#21442;&#25968;&#21270;&#20026;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#12290;&#26144;&#23556;$M$&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;$u^*$&#65292;&#21363;$u$&#30340;&#20984;&#20849;&#36717;&#65292;&#36880;&#28857;&#35745;&#31639;&#24471;&#21040;&#65292;&#21363;$M=\nabla u^* \circ F$&#65292;&#25110;&#32773;&#20316;&#20026;&#36741;&#21161;&#32593;&#32476;&#23398;&#20064;&#24471;&#21040;&#12290;&#22240;&#20026;$M$&#22312;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03071v1 Announce Type: cross  Abstract: In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\times$ unitary -- to any vector field $F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary network. Because $M$ is, in gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15734</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#35265;&#35777;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#29289;&#29702;&#39046;&#22495;&#29305;&#23450;&#27934;&#23519;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#31185;&#23398;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;PDE&#25968;&#25454;&#12290; &#36825;&#37325;&#26032;&#24341;&#20837;&#20102;&#23545;&#26114;&#36149;&#30340;&#25968;&#20540;PDE&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#37096;&#20998;&#21066;&#24369;&#20102;&#36991;&#20813;&#36825;&#20123;&#26114;&#36149;&#27169;&#25311;&#30340;&#21407;&#22987;&#30446;&#26631;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#23547;&#27714;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290; &#20026;&#20102;&#20943;&#23569;&#23545;&#24102;&#26377;&#27169;&#25311;&#35299;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#37325;&#26500;&#30340;&#20195;&#29702;&#20219;&#21153;&#22312;&#26410;&#26631;&#35760;&#30340;PDE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#36816;&#31639;&#31526;&#12290; &#20026;&#20102;&#25552;&#39640;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24110;&#21161;&#31070;&#32463;&#36816;&#31639;&#31526;&#28789;&#27963;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#25110;&#35774;&#35745;&#12290; &#22312;&#21508;&#31181;PD&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15734v1 Announce Type: new  Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insight for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining and in-context learning methods for PDE operator learning. To reduce the need for training data with simulated solutions, we pretrain neural operators on unlabeled PDE data using reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging in-context learning methods, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PD
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#30340;&#33021;&#37327;&#20989;&#25968;&#21487;&#20197;&#34987;&#35270;&#20026;&#27010;&#29575;&#24314;&#27169;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#65292;&#21487;&#28789;&#27963;&#36866;&#24212;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.10202</link><description>&lt;p&gt;
&#26500;&#24314;&#32852;&#24819;&#35760;&#24518;&#19982;&#27010;&#29575;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Bridging Associative Memory and Probabilistic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10202
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#30340;&#33021;&#37327;&#20989;&#25968;&#21487;&#20197;&#34987;&#35270;&#20026;&#27010;&#29575;&#24314;&#27169;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#65292;&#21487;&#28789;&#27963;&#36866;&#24212;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#32852;&#24819;&#35760;&#24518;&#21644;&#27010;&#29575;&#24314;&#27169;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#20004;&#20010;&#22522;&#26412;&#30340;&#20027;&#39064;&#12290;&#31532;&#19968;&#20010;&#30740;&#31350;&#35774;&#35745;&#29992;&#20110;&#21435;&#22122;&#12289;&#23436;&#25104;&#21644;&#26816;&#32034;&#25968;&#25454;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#31532;&#20108;&#20010;&#30740;&#31350;&#23398;&#20064;&#21644;&#20174;&#27010;&#29575;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#30340;&#33021;&#37327;&#20989;&#25968;&#21487;&#20197;&#34987;&#35270;&#20026;&#27010;&#29575;&#24314;&#27169;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#24231;&#26725;&#26753;&#65292;&#20351;&#24471;&#24819;&#27861;&#33021;&#22312;&#20004;&#20010;&#26041;&#21521;&#19978;&#26377;&#30410;&#30340;&#27969;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22235;&#20010;&#20363;&#23376;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#20197;&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#36866;&#24212;&#26032;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#8221;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#65306;&#19968;&#31181;&#26159;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#35201;&#21160;&#24577;&#21019;&#24314;&#26032;&#30340;&#35760;&#24518;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#26126;&#30830;&#35745;&#31639;&#27604;&#20363;&#35760;&#24518;&#20998;&#37197;&#65292;&#20351;&#29992;e&#20316;&#20026;&#27010;&#29575;&#20989;&#25968;&#20998;&#37197;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10202v1 Announce Type: new  Abstract: Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#20351;&#29992;&#25152;&#23398;&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.10036</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Predictive Linear Online Tracking for Unknown Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#20351;&#29992;&#25152;&#23398;&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#36861;&#36394;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36319;&#38543;&#19968;&#20010;&#31227;&#21160;&#30340;&#30446;&#26631;&#12290;&#19982;&#32463;&#20856;&#30340;&#36861;&#36394;&#25511;&#21046;&#19981;&#21516;&#65292;&#30446;&#26631;&#26159;&#26410;&#30693;&#30340;&#12289;&#38750;&#24179;&#31283;&#30340;&#65292;&#24182;&#19988;&#23427;&#30340;&#29366;&#24577;&#36880;&#27493;&#25581;&#31034;&#65292;&#22240;&#27492;&#36866;&#21512;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#27425;&#25104;&#26412;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#12290;&#25152;&#23398;&#27169;&#22411;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PLOT&#30340;&#21160;&#24577;&#36951;&#25022;&#19982;$\mathcal{O}(\sqrt{TV_T})$&#25104;&#27604;&#20363;&#65292;&#20854;&#20013;$V_T$&#26159;&#30446;&#26631;&#21160;&#21147;&#23398;&#30340;&#24635;&#21464;&#21270;&#37327;&#65292;$T$&#26159;&#26102;&#38388;&#38271;&#24230;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#22235;&#26059;&#32764;&#26426;&#19978;&#23454;&#29616;&#20102;PLOT&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10036v1 Announce Type: cross  Abstract: In this paper, we study the problem of online tracking in linear control systems, where the objective is to follow a moving target. Unlike classical tracking control, the target is unknown, non-stationary, and its state is revealed sequentially, thus, fitting the framework of online non-stochastic control. We consider the case of quadratic costs and propose a new algorithm, called predictive linear online tracking (PLOT). The algorithm uses recursive least squares with exponential forgetting to learn a time-varying dynamic model of the target. The learned model is used in the optimal policy under the framework of receding horizon control. We show the dynamic regret of PLOT scales with $\mathcal{O}(\sqrt{TV_T})$, where $V_T$ is the total variation of the target dynamics and $T$ is the time horizon. Unlike prior work, our theoretical results hold for non-stationary targets. We implement PLOT on a real quadrotor and provide open-source so
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.07586</link><description>&lt;p&gt;
&#25581;&#31034;&#29305;&#23450;&#32676;&#20307;&#30340;&#20998;&#24067;&#24335;&#27010;&#24565;&#28418;&#31227;: &#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#30830;&#20445;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#25512;&#21160;&#20102;&#24320;&#21457;&#26088;&#22312;&#20943;&#23569;&#20915;&#31574;&#36807;&#31243;&#20013;&#27495;&#35270;&#32467;&#26524;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#22312;&#36825;&#26041;&#38754;&#30340;&#24320;&#25299;&#24615;&#21162;&#21147;&#12290;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#26159;&#25351;&#19968;&#20010;&#32676;&#20307;&#38543;&#26102;&#38388;&#32463;&#21382;&#27010;&#24565;&#28418;&#31227;&#65292;&#32780;&#21478;&#19968;&#20010;&#32676;&#20307;&#21364;&#27809;&#26377;&#65292;&#23548;&#33268;&#20844;&#24179;&#24615;&#19979;&#38477;&#65292;&#21363;&#20351;&#20934;&#30830;&#24615;&#20445;&#25345;&#30456;&#23545;&#31283;&#23450;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#65292;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20998;&#24067;&#24335;&#24615;&#36136;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#29420;&#31435;&#32463;&#21382;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#65292;&#21516;&#26102;&#20173;&#20849;&#20139;&#30456;&#21516;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20174;&#32780;&#21019;&#36896;&#20102;&#19968;&#20010;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#29615;&#22659;&#26469;&#32500;&#25345;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#20043;&#19968;&#26159;&#23545;&#32676;&#20307;&#29305;&#23450;&#30340;&#27010;&#24565;&#28418;&#31227;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#20869;&#37096;&#21270;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and intr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06784</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transfer learning with generative models for object detection on limited datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#26159;&#26377;&#38480;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#38656;&#35201;&#27491;&#30830;&#26631;&#35760;&#27599;&#20010;&#30446;&#26631;&#21608;&#22260;&#30340;&#36793;&#30028;&#26694;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;&#22312;&#28023;&#27915;&#29983;&#29289;&#23398;&#39046;&#22495;&#65292;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#26816;&#27979;&#28023;&#27915;&#29289;&#31181;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38480;&#21046;&#38382;&#39064;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#37319;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#20855;&#20307;&#30340;&#39046;&#22495;&#12290;&#31532;&#20108;&#31181;&#31574;&#30053;&#26159;&#20351;&#29992;copy-paste&#25216;&#26415;&#25110;ad-hoc&#27169;&#25311;&#22120;&#31561;&#26041;&#27861;&#21019;&#24314;&#29305;&#23450;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#30340;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#35774;&#35745;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#36890;&#29992;&#24773;&#26223;&#19979;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#26469;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06160</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improved Evidential Deep Learning via a Mixture of Dirichlet Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#26469;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#23567;&#21270;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20197;&#23398;&#20064;&#39044;&#27979;&#20998;&#24067;&#19978;&#30340;&#20803;&#20998;&#24067;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#32463;&#39564;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;Bengs&#31561;&#20154;&#30340;&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#26681;&#26412;&#32570;&#38519;&#65306;&#21363;&#20351;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#65292;&#23398;&#20064;&#21040;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#12290;&#36890;&#36807;&#25552;&#20379;&#25991;&#29486;&#20013;&#19968;&#31867;&#24191;&#27867;&#20351;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36825;&#20010;&#35266;&#23519;&#30340;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;EDL&#26041;&#27861;&#26412;&#36136;&#19978;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#24067;&#19982;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#29305;&#23450;&#24046;&#24322;&#24230;&#37327;&#26469;&#35757;&#32451;&#20803;&#20998;&#24067;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#29702;&#35770;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#20854;&#24314;&#27169;&#20026;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#28151;&#21512;&#29289;&#26469;&#23398;&#20064;&#19968;&#33268;&#30446;&#26631;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;EDL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores a modern predictive uncertainty estimation approach, called evidential deep learning (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. Despite their strong empirical performance, recent studies by Bengs et al. identify a fundamental pitfall of the existing methods: the learned epistemic uncertainty may not vanish even in the infinite-sample limit. We corroborate the observation by providing a unifying view of a class of widely used objectives from the literature. Our analysis reveals that the EDL methods essentially train a meta distribution by minimizing a certain divergence measure between the distribution and a sample-size-independent target distribution, resulting in spurious epistemic uncertainty. Grounded in theoretical principles, we propose learning a consistent target distribution by modeling it with a mixture of Dirichlet distributions and lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#23545;&#26041;&#24046;&#20135;&#29983;&#24433;&#21709;&#12290;&#24403;&#20551;&#35774;&#31867;&#21035;&#30340;&#25299;&#25169;&#32467;&#26500;&#31526;&#21512;&#26576;&#20123;&#26465;&#20214;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#30340;&#24615;&#33021;&#19982;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05928</link><description>&lt;p&gt;
&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65306;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#30340;&#24179;&#26041;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#23545;&#26041;&#24046;&#20135;&#29983;&#24433;&#21709;&#12290;&#24403;&#20551;&#35774;&#31867;&#21035;&#30340;&#25299;&#25169;&#32467;&#26500;&#31526;&#21512;&#26576;&#20123;&#26465;&#20214;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#30340;&#24615;&#33021;&#19982;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20381;&#36182;&#24615;&#65288;&#946;-&#28151;&#21512;&#65289;&#25968;&#25454;&#21644;&#24179;&#26041;&#25439;&#22833;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#22312;&#19968;&#20010;&#20551;&#35774;&#31867;&#21035;&#934;_p&#30340;&#23376;&#38598;F&#20013;&#65292;&#20854;&#20013;&#934;_p&#26159;&#33539;&#25968;&#8741;f&#8741;_&#934;_p&#8801;sup_m&#8805;1 m^{-1/p}&#8741;f&#8741;_L^m&#65292;&#20854;&#20013;p&#8712;[2&#65292;&#8734;]&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#22312;&#20855;&#26377;&#20381;&#36182;&#24615;&#25968;&#25454;&#30340;&#23398;&#20064;&#20013;&#23547;&#25214;&#23574;&#38160;&#30340;&#22122;&#22768;&#20132;&#20114;&#39033;&#25110;&#26041;&#24046;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20856;&#22411;&#30340;&#38750;&#28176;&#36817;&#32467;&#26524;&#26174;&#31034;&#20986;&#26041;&#24046;&#20195;&#29702;&#36890;&#36807;&#24213;&#23618;&#21327;&#21464;&#37327;&#36807;&#31243;&#30340;&#28151;&#21512;&#26102;&#38388;&#36827;&#34892;&#20102;&#20056;&#31215;&#32553;&#20943;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#22312;&#25105;&#20204;&#30340;&#20551;&#35774;&#31867;&#21035;F&#19978;&#65292;L^2&#21644;&#934;_p&#30340;&#25299;&#25169;&#26159;&#21487;&#27604;&#36739;&#30340;&#65292;&#21363;&#934;_p&#26159;&#19968;&#20010;&#24369;&#20122;&#39640;&#26031;&#31867;&#21035;&#65306;&#8741;f&#8741;_&#934;_p&#8818;&#8741;f&#8741;_L^2^&#951;&#65292;&#20854;&#20013;&#951;&#8712;(0&#65292;1]&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#22312;&#20854;&#20027;&#23548;&#39033;&#20013;&#21482;&#23454;&#29616;&#20102;&#19968;&#31181;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#35768;&#22810;&#20381;&#36182;&#24615;&#25968;&#25454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31574;&#30053;&#31867;&#21035;&#20855;&#26377;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05689</link><description>&lt;p&gt;
Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#36275;&#20197;&#20445;&#35777;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#30446;&#26631;&#30340;&#28176;&#36827;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31574;&#30053;&#31867;&#21035;&#20855;&#26377;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#25955;&#26102;&#38388;&#19979;&#30340;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#30340;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#26088;&#22312;&#23558;&#36880;&#28176;&#25193;&#22823;&#30340;&#33218;&#23376;&#38598;&#21521;&#26368;&#20339;&#20998;&#24067;&#26041;&#21521;&#25512;&#36827;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;N&#33218;&#38382;&#39064;&#20013;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#65292;&#22914;&#26524;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#65292;&#37027;&#20040;&#23601;&#20250;&#26377;&#19968;&#20010;$O(1/\sqrt{N})$&#30340;&#26368;&#20248;&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#25351;&#25968;&#25110;&#20248;&#20808;&#32423;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#20381;&#36182;&#20110;&#32479;&#19968;&#20840;&#23616;&#21560;&#24341;&#23376;&#23646;&#24615;&#65288;UGAP&#65289;&#26469;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35201;&#27714;&#36981;&#24490;&#21516;&#27493;&#20551;&#35774;&#65288;SA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the infinite-horizon, average-reward restless bandit problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;QASE-net&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05482</link><description>&lt;p&gt;
&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#24212;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;QASE-net&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#27979;&#37327;&#32908;&#32905;&#34920;&#38754;&#32908;&#30005;&#65288;sEMG&#65289;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#23588;&#20854;&#26159;&#38752;&#36817;&#24515;&#33039;&#30340;&#21306;&#22495;&#65292;&#20027;&#35201;&#30340;&#27745;&#26579;&#28304;&#20043;&#19968;&#26159;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;sEMG&#25968;&#25454;&#36136;&#37327;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;QASE-net&#65292;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;sEMG&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#12290;QASE-net&#23558;CNN-BLSTM&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26694;&#26550;&#21033;&#29992;&#20102;&#20004;&#20010;&#24320;&#25918;&#35775;&#38382;&#25968;&#25454;&#24211;&#30340;&#23454;&#38469;&#19990;&#30028;sEMG&#21644;ECG&#25968;&#25454;&#65292;&#20998;&#21035;&#26159;&#38750;&#20405;&#20837;&#24615;&#36866;&#24212;&#24615;&#20551;&#32930;&#25968;&#25454;&#24211;&#21644;MIT-BIH&#27491;&#24120;&#31398;&#24615;&#24515;&#24459;&#25968;&#25454;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QASE-net&#20248;&#20110;&#20808;&#21069;&#30340;&#35780;&#20272;&#27169;&#22411;&#65292;&#20855;&#26377;&#26174;&#33879;&#38477;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26126;&#26174;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26174;&#31034;&#20102;QASE-net&#22312;&#25552;&#39640;&#21487;&#38752;&#24615;&#21644;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04655</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Calibration for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; (VLM) &#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#22788;&#29702;&#22270;&#20687;&#35782;&#21035;&#12289;&#25991;&#26412;&#39537;&#21160;&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#12289;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#24320;&#25918;&#35789;&#27719;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#25552;&#39640; VLM &#19979;&#28216;&#24615;&#33021;&#30340;&#36866;&#24212;&#26041;&#27861;&#19978;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;&#22914;&#25552;&#31034;&#23398;&#20064;&#65289;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#22823;&#22823;&#24573;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#22312;&#24494;&#35843;&#30340; VLM &#20013;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#20250;&#22823;&#22823;&#38477;&#20302;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#25552;&#31034;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#24320;&#25918;&#35789;&#27719;&#30340;&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; "Distance-Aware Ca"
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Ca
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;(GPWNO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30005;&#23376;&#23494;&#24230;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#38754;&#27874;&#21644;&#39640;&#26031;&#22411;&#36712;&#36947;&#22522;&#24213;&#22312;&#26080;&#38480;&#32500;&#21151;&#33021;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#23494;&#24230;&#30340;&#39640;&#39057;&#21644;&#20302;&#39057;&#25104;&#20998;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04278</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#23376;&#23494;&#24230;&#20272;&#35745;&#30340;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Gaussian Plane-Wave Neural Operator for Electron Density Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;(GPWNO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30005;&#23376;&#23494;&#24230;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#38754;&#27874;&#21644;&#39640;&#26031;&#22411;&#36712;&#36947;&#22522;&#24213;&#22312;&#26080;&#38480;&#32500;&#21151;&#33021;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#23494;&#24230;&#30340;&#39640;&#39057;&#21644;&#20302;&#39057;&#25104;&#20998;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#30005;&#23376;&#23494;&#24230;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21270;&#23398;&#31995;&#32479;&#21644;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;(DFT)&#27169;&#25311;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;(GPWNO)&#65292;&#23427;&#22312;&#26080;&#38480;&#32500;&#21151;&#33021;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#38754;&#27874;&#21644;&#39640;&#26031;&#22411;&#36712;&#36947;&#22522;&#24213;&#36827;&#34892;&#25805;&#20316;&#65292;&#22312;DFT&#30340;&#32972;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#12290;&#29305;&#21035;&#22320;&#65292;&#30001;&#20110;&#20004;&#31181;&#22522;&#24213;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#23494;&#24230;&#30340;&#39640;&#39057;&#21644;&#20302;&#39057;&#25104;&#20998;&#37117;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#34920;&#31034;&#12290;&#23545;QM9&#12289;MD&#21644;&#26448;&#26009;&#39033;&#30446;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;GPWNO&#30456;&#27604;&#20854;&#20182;&#21313;&#31181;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies machine learning for electron density prediction, which is fundamental for understanding chemical systems and density functional theory (DFT) simulations. To this end, we introduce the Gaussian plane-wave neural operator (GPWNO), which operates in the infinite-dimensional functional space using the plane-wave and Gaussian-type orbital bases, widely recognized in the context of DFT. In particular, both high- and low-frequency components of the density can be effectively represented due to the complementary nature of the two bases. Extensive experiments on QM9, MD, and material project datasets demonstrate GPWNO's superior performance over ten baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#25351;&#26631;&#19982;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#30340;&#32479;&#35745;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.03915</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#21152;&#36895;A/B&#27979;&#35797;&#30340;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Metrics that Maximise Power for Accelerated A/B-Tests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#25351;&#26631;&#19982;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#30340;&#32479;&#35745;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25216;&#26415;&#20844;&#21496;&#20013;&#65292;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23454;&#29616;&#33258;&#20449;&#30340;&#20915;&#31574;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;&#38271;&#26399;&#25910;&#20837;&#25110;&#29992;&#25143;&#20445;&#30041;&#65289;&#65292;&#22312;A/B&#27979;&#35797;&#20013;&#65292;&#33021;&#22815;&#22312;&#36825;&#20010;&#25351;&#26631;&#19978;&#26377;&#32479;&#35745;&#26174;&#33879;&#25552;&#21319;&#30340;&#31995;&#32479;&#21464;&#20307;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#36234;&#30340;&#12290;&#28982;&#32780;&#65292;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#36890;&#24120;&#20855;&#26377;&#26102;&#24310;&#21644;&#19981;&#25935;&#24863;&#24615;&#12290;&#22240;&#27492;&#65292;&#23454;&#39564;&#30340;&#25104;&#26412;&#24456;&#39640;&#65306;&#23454;&#39564;&#38656;&#35201;&#38271;&#26102;&#38388;&#36816;&#34892;&#65292;&#21363;&#20351;&#22914;&#27492;&#65292;&#20108;&#31867;&#38169;&#35823;&#65288;&#21363;&#20551;&#38452;&#24615;&#65289;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25351;&#26631;&#30452;&#25509;&#26368;&#22823;&#21270;&#23427;&#20204;&#30456;&#23545;&#20110;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#25152;&#20855;&#26377;&#30340;&#32479;&#35745;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#23481;&#26131;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65292;&#21363;&#26356;&#39640;&#30340;&#24179;&#22343;&#24230;&#37327;&#25935;&#24863;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#25913;&#36827;&#20102;&#20108;&#31867;&#38169;&#35823;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#26368;&#23567;&#21270;&#25351;&#26631;&#22312;&#36807;&#21435;&#23454;&#39564;&#30340;$log$&#19978;&#20135;&#29983;&#30340;$p$-value&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#31243;&#24207;&#20013;&#25910;&#38598;&#20102;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03293</link><description>&lt;p&gt;
Flora: &#20302;&#31209;&#36866;&#37197;&#22120;&#26159;&#24708;&#24708;&#30340;&#26799;&#24230;&#21387;&#32553;&#22120;
&lt;/p&gt;
&lt;p&gt;
Flora: Low-Rank Adapters Are Secretly Gradient Compressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#30340;&#26174;&#30528;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#26469;&#23384;&#20648;&#35757;&#32451;&#30340;&#20248;&#21270;&#29366;&#24577;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#26469;&#36890;&#36807;&#35757;&#32451;&#26356;&#23569;&#30340;&#21442;&#25968;&#26469;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;LoRA&#23558;&#25972;&#20307;&#26435;&#37325;&#26356;&#26032;&#30697;&#38453;&#38480;&#21046;&#20026;&#20302;&#31209;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#30830;&#23450;&#23427;&#21487;&#20197;&#36817;&#20284;&#20026;&#38543;&#26426;&#25237;&#24433;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Flora&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20139;&#21463;&#20248;&#21270;&#29366;&#24577;&#30340;&#27425;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26041;&#24046;&#30340;&#35268;&#27169;&#26159;&#24433;&#21709;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#30340;&#20027;&#35201;&#21442;&#25968;&#65292;&#36739;&#22823;&#30340;&#26041;&#24046;&#21487;&#22686;&#21152;&#22122;&#22768;&#25233;&#21046;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00811</link><description>&lt;p&gt;
&#25193;&#25955;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#26041;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of the Variance of Diffusion-based Speech Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26041;&#24046;&#30340;&#35268;&#27169;&#26159;&#24433;&#21709;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#30340;&#20027;&#35201;&#21442;&#25968;&#65292;&#36739;&#22823;&#30340;&#26041;&#24046;&#21487;&#22686;&#21152;&#22122;&#22768;&#25233;&#21046;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#29992;&#20110;&#29983;&#25104;&#35821;&#38899;&#22686;&#24378;&#30340;&#24378;&#22823;&#27169;&#22411;&#12290;&#22312;&#26368;&#36817;&#30340;SGMSE+&#26041;&#27861;&#20013;&#65292;&#35757;&#32451;&#28041;&#21450;&#21040;&#25511;&#21046;&#28436;&#21270;&#36807;&#31243;&#20013;&#22343;&#20540;&#21644;&#26041;&#24046;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#36880;&#28176;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#21644;&#29615;&#22659;&#22122;&#22768;&#21040;&#24178;&#20928;&#35821;&#38899;&#20449;&#21495;&#20013;&#12290;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#21462;&#20915;&#20110;&#36873;&#25321;&#29992;&#26469;&#25511;&#21046;&#28155;&#21152;&#29615;&#22659;&#21644;&#39640;&#26031;&#22122;&#22768;&#19979;&#28436;&#21270;&#36807;&#31243;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#26041;&#24046;&#30340;&#35268;&#27169;&#26159;&#24433;&#21709;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#30340;&#20027;&#35201;&#21442;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#23427;&#25511;&#21046;&#30528;&#22122;&#22768;&#25233;&#21046;&#21644;&#35821;&#38899;&#22833;&#30495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36739;&#22823;&#30340;&#26041;&#24046;&#22686;&#21152;&#20102;&#22122;&#22768;&#25233;&#21046;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#22240;&#20026;&#20135;&#29983;&#20272;&#35745;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20943;&#23569;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models proved to be powerful models for generative speech enhancement. In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually. The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise. In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions. More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.11963</link><description>&lt;p&gt;
&#36328;&#36234;&#36827;&#21270;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11963
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23558;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#36827;&#34892;&#20248;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;ERL&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;ERL&#20013;&#19981;&#21516;&#30740;&#31350;&#20998;&#25903;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65306;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#65292;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27599;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#32452;&#32455;&#20102;&#22810;&#20010;&#30740;&#31350;&#20998;&#25903;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#20998;&#25903;&#33268;&#21147;&#20110;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#25972;&#21512;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2311.03099</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23601;&#20687;&#36229;&#32423;&#39532;&#37324;&#22885;&#65306;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;DARE&#26469;&#23558;&#22823;&#22810;&#25968;delta&#21442;&#25968;&#65288;&#21363;&#24494;&#35843;&#21644;&#39044;&#35757;&#32451;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65289;&#35774;&#32622;&#20026;&#38646;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#30417;&#30563;&#24494;&#35843;(SFT) LMs&#30340;&#33021;&#21147;&#65292;DARE&#36890;&#36807;&#38543;&#26426;&#21024;&#38500;&#27604;&#29575;&#20026;p&#30340;delta&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;1/(1 - p)&#37325;&#26032;&#32553;&#25918;&#21097;&#20313;&#21442;&#25968;&#26469;&#36817;&#20284;&#21407;&#22987;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;DARE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#22810;&#20010;SFT&#21516;&#28304;&#27169;&#22411;&#30340;delta&#21442;&#25968;&#65292;&#20197;&#20943;&#36731;&#21442;&#25968;&#24178;&#25200;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#34701;&#21512;&#23558;&#23427;&#20204;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20026;&#22522;&#30784;&#30340;LM&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;SFT delta&#21442;&#25968;&#20540;&#33539;&#22260;&#36890;&#24120;&#24456;&#23567;&#65288;&#22312;0.005&#20197;&#20869;&#65289;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#20887;&#20313;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;90%&#29978;&#33267;99%&#30340;&#21442;&#25968;&#12290;&#65288;2&#65289;DARE&#21487;&#20197;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;LM&#21512;&#24182;&#20026;&#19968;&#20010;LM&#65292;&#24182;&#26377;&#39550;&#39542;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with dive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Consistent3D&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#23481;&#26131;&#20986;&#29616;&#20960;&#20309;&#23849;&#28291;&#21644;&#36136;&#37327;&#24046;&#32441;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09050</link><description>&lt;p&gt;
Consistent3D: &#23454;&#29616;&#19968;&#33268;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior. (arXiv:2401.09050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Consistent3D&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#23481;&#26131;&#20986;&#29616;&#20960;&#20309;&#23849;&#28291;&#21644;&#36136;&#37327;&#24046;&#32441;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#21450;&#20854;&#21464;&#31181;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20294;&#23481;&#26131;&#20986;&#29616;&#20960;&#20309;&#23849;&#28291;&#21644;&#36136;&#37327;&#24046;&#30340;&#32441;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#20998;&#26512;&#20102;SDS&#65292;&#24182;&#21457;&#29616;&#23427;&#30340;&#33976;&#39311;&#37319;&#26679;&#36807;&#31243;&#23454;&#38469;&#19978;&#23545;&#24212;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#36712;&#36857;&#37319;&#26679;&#65306;SDS&#27839;&#30528;SDE&#36712;&#36857;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#23569;&#24102;&#22122;&#22768;&#30340;&#26679;&#26412;&#65292;&#36825;&#20010;&#26679;&#26412;&#21017;&#20316;&#20026;&#20248;&#21270;3D&#27169;&#22411;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;SDE&#37319;&#26679;&#20013;&#30340;&#38543;&#26426;&#24615;&#32463;&#24120;&#23548;&#33268;&#26679;&#26412;&#22810;&#26679;&#19988;&#19981;&#21487;&#39044;&#27979;&#65292;&#19981;&#24635;&#26159;&#26356;&#23569;&#24102;&#22122;&#22768;&#65292;&#22240;&#27492;&#19981;&#26159;&#19968;&#20010;&#19968;&#33268;&#27491;&#30830;&#30340;&#25351;&#23548;&#65292;&#36825;&#35299;&#37322;&#20102;SDS&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#30001;&#20110;&#23545;&#20110;&#20219;&#20309;SDE&#65292;&#24635;&#26159;&#23384;&#22312;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#20854;&#36712;&#36857;&#37319;&#26679;&#21487;&#20197;&#30830;&#23450;&#24615;&#21644;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#25152;&#38656;&#30446;&#26631;&#28857;&#20316;&#20026;SDE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;Consistent3D&#8221;&#26041;&#27861;&#65292;&#25506;&#32034;ODE&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score distillation sampling (SDS) and its variants have greatly boosted the development of text-to-3D generation, but are vulnerable to geometry collapse and poor textures yet. To solve this issue, we first deeply analyze the SDS and find that its distillation sampling process indeed corresponds to the trajectory sampling of a stochastic differential equation (SDE): SDS samples along an SDE trajectory to yield a less noisy sample which then serves as a guidance to optimize a 3D model. However, the randomness in SDE sampling often leads to a diverse and unpredictable sample which is not always less noisy, and thus is not a consistently correct guidance, explaining the vulnerability of SDS. Since for any SDE, there always exists an ordinary differential equation (ODE) whose trajectory sampling can deterministically and consistently converge to the desired target point as the SDE, we propose a novel and effective "Consistent3D" method that explores the ODE deterministic sampling prior for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>Redco&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs&#65292;&#24182;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2310.16355</link><description>&lt;p&gt;
Redco:&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#21487;&#22312;&#20219;&#20309;GPU/TPUs&#19978;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs
&lt;/p&gt;
&lt;p&gt;
Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs. (arXiv:2310.16355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16355
&lt;/p&gt;
&lt;p&gt;
Redco&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs&#65292;&#24182;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#20869;&#23384;&#38656;&#27714;&#32473;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#21306;&#20197;&#20998;&#24067;&#22312;&#22810;&#20010;GPU&#25110;TPU&#19978;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#29616;&#26377;&#27169;&#22411;&#24182;&#34892;&#24037;&#20855;&#65288;&#22914;Megatron-LM&#12289;DeepSpeed&#21644;Alpa&#65289;&#36827;&#34892;&#30456;&#24403;&#30340;&#32534;&#30721;&#21644;&#22797;&#26434;&#30340;&#37197;&#32622;&#24037;&#20316;&#12290;&#36825;&#20123;&#24037;&#20855;&#38656;&#35201;&#29992;&#25143;&#20855;&#22791;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65288;MLSys&#65289;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#32473;LLM&#24320;&#21457;&#24102;&#26469;&#20102;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;MLSys&#32972;&#26223;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Redco&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;LLMs&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20197;&#21450;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;Redco&#30340;&#35774;&#35745;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#33258;&#21160;&#21270;&#27169;&#22411;&#24182;&#34892;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#35268;&#21017;&#65292;&#29992;&#20110;&#20026;&#20219;&#20309;GPU / TPU&#29983;&#25104;&#24352;&#37327;&#24182;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#36870;&#38382;&#39064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23450;&#20041;&#27491;&#21017;&#21270;&#22120;&#24182;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#22788;&#29702;&#26410;&#30693;&#30340;&#36864;&#21270;&#25805;&#20316;&#31526;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#32534;&#30721;&#22120;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.02897</link><description>&lt;p&gt;
&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#20013;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#65306;&#36870;&#38382;&#39064;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Recovery of Training Data from Overparameterized Autoencoders: An Inverse Problem Perspective. (arXiv:2310.02897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#36870;&#38382;&#39064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23450;&#20041;&#27491;&#21017;&#21270;&#22120;&#24182;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#22788;&#29702;&#26410;&#30693;&#30340;&#36864;&#21270;&#25805;&#20316;&#31526;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#32534;&#30721;&#22120;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20013;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#36864;&#21270;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#26679;&#26412;&#30340;&#24674;&#22797;&#23450;&#20041;&#20026;&#19968;&#20010;&#36870;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#36870;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#38544;&#24335;&#22320;&#23450;&#20041;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#20174;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#12290;&#25105;&#20204;&#23558;&#22797;&#26434;&#30340;&#20248;&#21270;&#20219;&#21153;&#24320;&#21457;&#25104;&#19968;&#20010;&#23454;&#38469;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36845;&#20195;&#22320;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#35745;&#31639;&#26469;&#20272;&#35745;&#21644;&#22788;&#29702;&#26410;&#30693;&#30340;&#36864;&#21270;&#25805;&#20316;&#31526;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30450;&#30446;&#20462;&#34917;&#65292;&#30446;&#26631;&#26159;&#20174;&#35768;&#22810;&#32570;&#22833;&#30340;&#20687;&#32032;&#20013;&#24674;&#22797;&#35757;&#32451;&#22270;&#20687;&#65292;&#32780;&#36825;&#20123;&#32570;&#22833;&#30340;&#20687;&#32032;&#26159;&#25353;&#29031;&#26410;&#30693;&#30340;&#27169;&#24335;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#21508;&#31181;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#22914;&#20840;&#36830;&#25509;&#21644;U-Net&#65288;&#20855;&#26377;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#21644;&#22810;&#26679;&#30340;&#35757;&#32451;&#25439;&#22833;&#20540;&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#33258;&#32534;&#30721;&#22120;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the recovery of training data from overparameterized autoencoder models. Given a degraded training sample, we define the recovery of the original sample as an inverse problem and formulate it as an optimization task. In our inverse problem, we use the trained autoencoder to implicitly define a regularizer for the particular training dataset that we aim to retrieve from. We develop the intricate optimization task into a practical method that iteratively applies the trained autoencoder and relatively simple computations that estimate and address the unknown degradation operator. We evaluate our method for blind inpainting where the goal is to recover training images from degradation of many missing pixels in an unknown pattern. We examine various deep autoencoder architectures, such as fully connected and U-Net (with various nonlinearities and at diverse train loss values), and show that our method significantly outperforms previous methods for training data recovery from autoen
&lt;/p&gt;</description></item><item><title>Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.02409</link><description>&lt;p&gt;
Nugget 2D&#65306;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02409
&lt;/p&gt;
&lt;p&gt;
Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#32553;&#25918;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;Qin&#65286;Van Durme&#65288;2023&#24180;&#65289;&#30340;Nugget&#26041;&#27861;&#20174;BERT&#31867;&#26694;&#26550;&#25193;&#23637;&#21040;&#20165;&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#24314;&#27169;&#20026;&#21387;&#32553;&#30340;&#8220;nuggets&#8221;&#65292;&#36825;&#20123;&#8220;nuggets&#8221;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36827;&#34892;&#37325;&#24314;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#35832;&#22914;LLaMA&#20043;&#31867;&#30340;&#29616;&#25104;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Nugget2D&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#20445;&#30041;&#20102;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#32534;&#30721;&#23454;&#39564;&#20013;&#65292;Nugget2D&#21487;&#20197;&#20197;20&#20493;&#30340;&#21387;&#32553;&#27604;&#25910;&#32553;&#19978;&#19979;&#25991;&#65292;&#37325;&#24314;&#26102;&#30340;BLEU&#24471;&#20998;&#20026;98&#65285;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#33410;&#32422;&#25968;&#25454;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#22240;&#20026;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#25915;&#20987;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#23545;&#25915;&#20987;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.01959</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;&#31070;&#35861;&#65306;&#20160;&#20040;&#26159;&#27169;&#22411;&#31363;&#21462;&#30340;&#21547;&#20041;&#65311;
&lt;/p&gt;
&lt;p&gt;
Beyond Labeling Oracles: What does it mean to steal ML models?. (arXiv:2310.01959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#33410;&#32422;&#25968;&#25454;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#22240;&#20026;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#25915;&#20987;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#23545;&#25915;&#20987;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#38480;&#26469;&#31363;&#21462;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#36890;&#36807;ML-as-a-Service&#25552;&#20379;&#30340;API&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;&#25968;&#25454;&#38590;&#20197;&#33719;&#21462;&#65292;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#27492;&#27169;&#22411;&#25552;&#21462;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#22312;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26356;&#23569;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#33719;&#21462;&#27169;&#22411;&#12290;&#20851;&#20110;&#27169;&#22411;&#25552;&#21462;&#30340;&#25991;&#29486;&#26222;&#36941;&#22768;&#31216;&#25110;&#20551;&#35774;&#25915;&#20987;&#32773;&#33021;&#22815;&#33410;&#32422;&#25968;&#25454;&#33719;&#21462;&#21644;&#26631;&#27880;&#25104;&#26412;&#12290;&#28982;&#32780;&#25105;&#20204;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#24403;&#21069;&#30340;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#25915;&#20987;&#32773;&#33021;&#22815;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#23545;&#24433;&#21709;&#27169;&#22411;&#25552;&#21462;&#25104;&#21151;&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#23545;&#21463;&#23475;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#21363;&#23545;&#20998;&#24067;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#27604;&#25915;&#20987;&#31574;&#30053;&#65288;&#20915;&#23450;&#21521;&#21463;&#23475;&#32773;&#27169;&#22411;API&#21457;&#20986;&#21738;&#20123;&#26597;&#35810;&#65289;&#31561;&#20854;&#20182;&#22240;&#32032;&#26356;&#20026;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#24076;&#26395;&#24320;&#21457;&#21516;&#31561;&#27700;&#24179;&#30340;&#25915;&#20987;&#32773;&#26356;&#37325;&#35201;&#30340;&#26159;&#33719;&#21462;&#23545;&#20998;&#24067;&#25968;&#25454;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction attacks are designed to steal trained models with only query access, as is often provided through APIs that ML-as-a-Service providers offer. ML models are expensive to train, in part because data is hard to obtain, and a primary incentive for model extraction is to acquire a model while incurring less cost than training from scratch. Literature on model extraction commonly claims or presumes that the attacker is able to save on both data acquisition and labeling costs. We show that the attacker often does not. This is because current attacks implicitly rely on the adversary being able to sample from the victim model's data distribution. We thoroughly evaluate factors influencing the success of model extraction. We discover that prior knowledge of the attacker, i.e. access to in-distribution data, dominates other factors like the attack policy the adversary follows to choose which queries to make to the victim model API. Thus, an adversary looking to develop an equally 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;</title><link>http://arxiv.org/abs/2309.12701</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21457;&#29616;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming. (arXiv:2309.12701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20915;&#31574;&#26641;&#30001;&#20110;&#21487;&#20197;&#34987;&#20154;&#31867;&#26816;&#26597;&#21644;&#35299;&#37322;&#32780;&#20855;&#26377;&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30828;&#20214;&#30340;&#36827;&#27493;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#30340;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#27604;&#36890;&#24120;&#30340;&#36138;&#23146;&#26041;&#27861;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#26641;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20248;&#31639;&#27861;&#36820;&#22238;&#30340;&#26159;&#19968;&#20010;&#20248;&#21270;&#25163;&#21160;&#23450;&#20041;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#21333;&#20010;&#26641;&#65292;&#36890;&#36807;&#25351;&#23450;&#26368;&#22823;&#20915;&#31574;&#33410;&#28857;&#25968;&#37327;&#26469;&#33719;&#24471;&#65292;&#23545;&#20110;&#36825;&#20010;&#26435;&#34913;&#30340;&#36136;&#37327;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65288;MDP&#65289;&#24418;&#24335;&#26469;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21333;&#19968;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#35745;&#31639;&#20986;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35753;&#29992;&#25143;&#20107;&#21518;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#26641;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and run
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09924</link><description>&lt;p&gt;
&#22522;&#20110;&#28909;&#21644;&#27874;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#22270;&#25299;&#25169;&#23646;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Graph topological property recovery with heat and wave dynamics-based features on graphsD. (arXiv:2309.09924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#19978;&#30340;PDE&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33719;&#24471;&#36830;&#32493;&#30340;&#33410;&#28857;&#21644;&#22270;&#32423;&#34920;&#31034;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#19982;&#22270;&#30340;&#35889;&#29305;&#24615;&#20197;&#21450;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#28216;&#36208;&#22312;&#22270;&#19978;&#34892;&#20026;&#20043;&#38388;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#38543;&#26426;&#22270;&#29983;&#25104;&#21442;&#25968;&#12289;Ricci&#26354;&#29575;&#21644;&#25345;&#20037;&#21516;&#35843;&#31561;&#26041;&#24335;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#33021;&#22815;&#25429;&#25417;&#21040;&#22270;&#24418;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#26174;&#33879;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;GDeNet&#22312;&#21253;&#25324;&#24341;&#29992;&#22270;&#12289;&#33647;&#29289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#22312;&#20869;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08630</link><description>&lt;p&gt;
PCN&#65306;&#19968;&#31181;&#21033;&#29992;&#26032;&#39062;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21943;&#27880;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21943;&#27880;&#26631;&#35760;&#26159;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#31890;&#23376;&#30896;&#25758;&#20135;&#29983;&#30340;&#38181;&#29366;&#21943;&#27880;&#65292;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#21457;&#23556;&#31890;&#23376;&#12290;&#21943;&#27880;&#26631;&#35760;&#30340;&#36827;&#23637;&#20026;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#30340;&#26032;&#29289;&#29702;&#25628;&#32034;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#22797;&#26434;&#30896;&#25758;&#25968;&#25454;&#20013;&#23547;&#25214;&#38544;&#34255;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23558;&#21943;&#27880;&#34920;&#31034;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#30340;&#26041;&#27861;&#22810;&#31181;&#22810;&#26679;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#21521;&#27169;&#22411;&#38544;&#34255;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#22320;&#32534;&#30721;&#26368;&#22810;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#34920;&#31034;&#20013;&#26368;&#22909;&#22320;&#23398;&#20064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Particle Chebyshev Network&#65288;PCN&#65289;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#12290;ChebConv&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;GNN&#20013;&#30340;&#19968;&#31181;&#26377;&#25928;&#26367;&#20195;&#20256;&#32479;&#22270;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#21943;&#27880;&#26631;&#35760;&#20013;&#36824;&#27809;&#26377;&#34987;&#25506;&#32034;&#36807;&#12290;PCN&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#38750;&#21516;&#27493;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07115</link><description>&lt;p&gt;
&#38754;&#21521;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification. (arXiv:2309.07115v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#38750;&#21516;&#27493;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#38024;&#23545;&#24320;&#25918;&#38598;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#20010;&#20154;&#34920;&#31034;&#12290;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#36890;&#24120;&#22312;&#35813;&#38382;&#39064;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#22312;&#26032;&#30340;&#21644;&#26410;&#35265;&#36807;&#30340;&#31867;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;DML&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#26377;&#24369;&#26631;&#31614;&#30340;&#36741;&#21161;&#20219;&#21153;&#21487;&#20197;&#22686;&#21152;&#23398;&#20064;&#21040;&#30340;&#35828;&#35805;&#20154;&#34920;&#31034;&#30340;&#32039;&#20945;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#24191;&#20041;&#31471;&#21040;&#31471;&#25439;&#22833;&#65288;GE2E&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#38899;&#35270;&#39057;&#31354;&#38388;&#20013;&#23454;&#29616;&#31454;&#20105;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;&#21516;&#27493;&#38899;&#35270;&#39057;&#37319;&#26679;&#38543;&#26426;&#31574;&#30053;&#65292;&#24050;&#32463;&#26174;&#31034;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25253;&#21578;&#20102;VoxCeleb1-O/E&#30340;&#19977;&#20010;&#23448;&#26041;&#35797;&#39564;&#21015;&#34920;&#19978;&#30340;0.244&#65285;&#65292;0.252&#65285;&#65292;0.441&#65285;&#30340;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a methodology for achieving robust multimodal person representations optimized for open-set audio-visual speaker verification. Distance Metric Learning (DML) approaches have typically dominated this problem space, owing to strong performance on new and unseen classes. In our work, we explored multitask learning techniques to further boost performance of the DML approach and show that an auxiliary task with weak labels can increase the compactness of the learned speaker representation. We also extend the Generalized end-to-end loss (GE2E) to multimodal inputs and demonstrate that it can achieve competitive performance in an audio-visual space. Finally, we introduce a non-synchronous audio-visual sampling random strategy during training time that has shown to improve generalization. Our network achieves state of the art performance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three official trial lists of VoxCeleb1-O/E
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#32422;&#26463;&#29627;&#23572;&#20857;&#26364;&#26426;&#20316;&#20026;&#35797;&#39564;&#27874;&#20989;&#25968;&#21442;&#25968;&#21270;&#65292;&#30740;&#31350;&#20102;&#20855;&#26377;&#20195;&#25968;&#34928;&#20943;&#38271;&#31243;&#21453;&#38081;&#30913;&#30456;&#20114;&#20316;&#29992;&#30340;&#27178;&#21521;&#22330;&#20234;&#36763;&#38142;&#20013;&#30340;&#37327;&#23376;&#30456;&#21464;&#12290;&#36890;&#36807;&#26377;&#38480;&#22823;&#23567;&#32553;&#25918;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20013;&#24515;&#33655;&#30340;&#20540;&#19982;&#34928;&#21464;&#25351;&#25968;$\alpha_\mathrm{LR}$&#19981;&#21516;&#65292;&#32780;&#20020;&#30028;&#25351;&#25968;&#21017;&#20445;&#25345;&#25509;&#36817;&#30701;&#31243;&#20234;&#36763;&#27169;&#22411;&#30340;&#20540;&#65292;&#36825;&#25903;&#25345;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#20849;&#24418;&#19981;&#21464;&#24615;&#30772;&#32570;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.09709</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#30740;&#31350;&#38271;&#31243;&#21453;&#38081;&#30913;&#20234;&#36763;&#38142;
&lt;/p&gt;
&lt;p&gt;
Neural-network quantum state study of the long-range antiferromagnetic Ising chain. (arXiv:2308.09709v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09709
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#32422;&#26463;&#29627;&#23572;&#20857;&#26364;&#26426;&#20316;&#20026;&#35797;&#39564;&#27874;&#20989;&#25968;&#21442;&#25968;&#21270;&#65292;&#30740;&#31350;&#20102;&#20855;&#26377;&#20195;&#25968;&#34928;&#20943;&#38271;&#31243;&#21453;&#38081;&#30913;&#30456;&#20114;&#20316;&#29992;&#30340;&#27178;&#21521;&#22330;&#20234;&#36763;&#38142;&#20013;&#30340;&#37327;&#23376;&#30456;&#21464;&#12290;&#36890;&#36807;&#26377;&#38480;&#22823;&#23567;&#32553;&#25918;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20013;&#24515;&#33655;&#30340;&#20540;&#19982;&#34928;&#21464;&#25351;&#25968;$\alpha_\mathrm{LR}$&#19981;&#21516;&#65292;&#32780;&#20020;&#30028;&#25351;&#25968;&#21017;&#20445;&#25345;&#25509;&#36817;&#30701;&#31243;&#20234;&#36763;&#27169;&#22411;&#30340;&#20540;&#65292;&#36825;&#25903;&#25345;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#20849;&#24418;&#19981;&#21464;&#24615;&#30772;&#32570;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30740;&#31350;&#20855;&#26377;&#20195;&#25968;&#34928;&#20943;&#38271;&#31243;&#21453;&#38081;&#30913;&#30456;&#20114;&#20316;&#29992;&#30340;&#27178;&#21521;&#22330;&#20234;&#36763;&#38142;&#20013;&#30340;&#37327;&#23376;&#30456;&#21464;&#65292;&#37319;&#29992;&#32422;&#26463;&#29627;&#23572;&#20857;&#26364;&#26426;&#20316;&#20026;&#35797;&#39564;&#27874;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#22312;&#26377;&#38480;&#22823;&#23567;&#32553;&#25918;&#20998;&#26512;&#20013;&#65292;&#36890;&#36807;&#24207;&#21442;&#25968;&#21644;&#31532;&#20108;&#20010;Renyi&#29109;&#65292;&#25105;&#20204;&#21457;&#29616;&#20013;&#24515;&#33655;&#19982;&#23567;&#30340;&#34928;&#21464;&#25351;&#25968;$\alpha_\mathrm{LR}$&#19981;&#21516;&#65292;&#20559;&#31163;&#20102;1/2&#65292;&#32780;&#20020;&#30028;&#25351;&#25968;&#21017;&#26080;&#35770;&#21463;&#21040;$\alpha_\mathrm{LR}$&#30340;&#24433;&#21709;&#22914;&#20309;&#65292;&#37117;&#20445;&#25345;&#38750;&#24120;&#25509;&#36817;&#30701;&#31243;&#65288;SR&#65289;&#20234;&#36763;&#27169;&#22411;&#30340;&#20540;&#65292;&#25903;&#25345;&#20808;&#21069;&#25552;&#20986;&#30340;&#20849;&#24418;&#19981;&#21464;&#24615;&#30772;&#32570;&#22330;&#26223;&#12290;&#20026;&#20102;&#30830;&#23450;&#20234;&#36763;&#23431;&#23449;&#24615;&#21644;&#20849;&#24418;&#23545;&#31216;&#24615;&#30340;&#20020;&#30028;&#28857;&#65292;&#25105;&#20204;&#23545;&#26222;&#36866;Binder&#27604;&#21644;&#30456;&#20851;&#20989;&#25968;&#30340;&#20849;&#24418;&#22330;&#35770;&#65288;CFT&#65289;&#25551;&#36848;&#36827;&#34892;&#20102;&#20004;&#39033;&#38468;&#21152;&#27979;&#35797;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;$\alpha_\mat
&lt;/p&gt;
&lt;p&gt;
We investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions by using the variational Monte Carlo method with the restricted Boltzmann machine being employed as a trial wave function ansatz. In the finite-size scaling analysis with the order parameter and the second R\'enyi entropy, we find that the central charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$ in contrast to the critical exponents staying very close to the short-range (SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting the previously proposed scenario of conformal invariance breakdown. To identify the threshold of the Ising universality and the conformal symmetry, we perform two additional tests for the universal Binder ratio and the conformal field theory (CFT) description of the correlation function. It turns out that both indicate a noticeable deviation from the SR Ising class at $\alpha_\mat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04539</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#20445;&#35777;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications. (arXiv:2306.04539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#29702;&#35299;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26412;&#36136;&#65306;&#22312;&#20174;&#20004;&#20010;&#37117;&#27809;&#26377;&#30340;&#27169;&#24577;&#23398;&#20064;&#26102;&#20986;&#29616;&#20102;&#26032;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#36825;&#19968;&#20132;&#20114;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#21482;&#20351;&#29992;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26080;&#26631;&#31614;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65292;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#65289;&#12290;&#21033;&#29992;&#31934;&#30830;&#30340;&#20449;&#24687;&#35770;&#20132;&#20114;&#23450;&#20041;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25512;&#23548;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#37327;&#21270;&#36825;&#31181;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#37327;&#21644;&#21333;&#29420;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#21040;&#36817;&#20284;&#31639;&#27861;&#26469;&#25512;&#23548;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#34394;&#25311;&#20581;&#24247;&#20013;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#21644;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03833</link><description>&lt;p&gt;
&#34394;&#25311;&#20581;&#24247;&#20013;&#30340;&#24739;&#32773;&#39044;&#27979;&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#22270;&#35889;&#21644;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic Knowledge Graph and Text Mining Approach. (arXiv:2306.03833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#34394;&#25311;&#20581;&#24247;&#20013;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#21644;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#20581;&#24247;&#34987;&#35465;&#20026;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#20013;&#30340;&#25913;&#21464;&#24615;&#21147;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#36864;&#20986;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#20581;&#24247;&#32467;&#26524;&#65292;&#22686;&#21152;&#20581;&#24247;&#12289;&#31038;&#20250;&#21644;&#32463;&#27982;&#25104;&#26412;&#12290;&#21450;&#26102;&#39044;&#27979;&#24739;&#32773;&#30340;&#36864;&#20986;&#20351;&#32929;&#19996;&#33021;&#22815;&#37319;&#21462;&#31215;&#26497;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#24739;&#32773;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#25552;&#39640;&#20445;&#30041;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22312;&#32447;&#21644;&#31163;&#32447;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#30340;&#21307;&#29983;&#24739;&#32773;&#23545;&#35805;&#12289;&#21508;&#20010;&#32929;&#19996;&#30340;&#21160;&#24577;&#21644;&#22797;&#26434;&#32593;&#32476;&#20013;&#23398;&#20064;&#38544;&#24335;&#21644;&#26174;&#24335;&#30693;&#35782;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#20013;&#22269;&#26368;&#22823;&#30340;&#34394;&#25311;&#20581;&#24247;&#24179;&#21488;&#20043;&#19968;&#21512;&#20316;&#26469;&#35780;&#20272;MDKDP&#12290;MDKDP&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual health has been acclaimed as a transformative force in healthcare delivery. Yet, its dropout issue is critical that leads to poor health outcomes, increased health, societal, and economic costs. Timely prediction of patient dropout enables stakeholders to take proactive steps to address patients' concerns, potentially improving retention rates. In virtual health, the information asymmetries inherent in its delivery format, between different stakeholders, and across different healthcare delivery systems hinder the performance of existing predictive methods. To resolve those information asymmetries, we propose a Multimodal Dynamic Knowledge-driven Dropout Prediction (MDKDP) framework that learns implicit and explicit knowledge from doctor-patient dialogues and the dynamic and complex networks of various stakeholders in both online and offline healthcare delivery systems. We evaluate MDKDP by partnering with one of the largest virtual health platforms in China. MDKDP improves the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#65292;&#25512;&#32763;&#20102;&#29616;&#26377;&#25216;&#26415;&#23545;&#36890;&#20449;&#22270;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;D-SGD&#22312;&#20984;&#35774;&#32622;&#20013;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#27867;&#21270;&#30028;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.02939</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#19982;&#27867;&#21270;&#20998;&#26512;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Stability and Generalization Analysis of the Decentralized SGD Algorithm. (arXiv:2306.02939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#65292;&#25512;&#32763;&#20102;&#29616;&#26377;&#25216;&#26415;&#23545;&#36890;&#20449;&#22270;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;D-SGD&#22312;&#20984;&#35774;&#32622;&#20013;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#27867;&#21270;&#30028;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(D-SGD)&#31639;&#27861;&#30340;&#26032;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;&#26041;&#27861;&#12290;&#24471;&#21040;&#30340;&#32467;&#26524;&#22823;&#22823;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#25512;&#32763;&#20102;&#23427;&#20204;&#20851;&#20110;&#36890;&#20449;&#22270;&#23545;&#27867;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#20363;&#22914;&#65292;&#22312;&#20984;&#35774;&#32622;&#20013;&#65292;&#26080;&#35770;&#22270;&#30340;&#36873;&#25321;&#22914;&#20309;&#65292;D-SGD&#20855;&#26377;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#30456;&#21516;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#26469;&#33258;&#20110;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36825;&#20250;&#38544;&#34255;&#19968;&#20010;&#19982;&#20998;&#24067;&#24335;&#22330;&#26223;&#19981;&#20860;&#23481;&#30340;&#26368;&#32456;&#20840;&#23616;&#24179;&#22343;&#21270;&#27493;&#39588;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#20513;&#23548;&#20998;&#26512;&#26412;&#22320;&#21442;&#25968;&#30340;&#19978;&#30830;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22270;&#30830;&#23454;&#23545;&#27867;&#21270;&#20135;&#29983;&#24433;&#21709;&#12290;&#19982;&#20043;&#21069;&#30340;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21363;&#20351;&#23545;&#20110;&#38750;&#36830;&#25509;&#22270;&#20063;&#33021;&#20135;&#29983;&#38750;&#24179;&#20961;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new generalization error analysis for the Decentralized Stochastic Gradient Descent (D-SGD) algorithm based on algorithmic stability. The obtained results largely improve upon state-of-the-art results, and even invalidate their claims that the communication graph has a detrimental effect on generalization. For instance, we show that in convex settings, D-SGD has the same generalization bounds as the classical SGD algorithm, no matter the choice of graph. We exhibit that this counter-intuitive result comes from considering the average of local parameters, which hides a final global averaging step incompatible with the decentralized scenario. In light of this observation, we advocate to analyze the supremum over local parameters and show that in this case, the graph does have an impact on the generalization. Unlike prior results, our analysis yields non-vacuous bounds even for non-connected graphs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#33324;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#30340;&#26080;&#22122;&#22768;&#22238;&#24402;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#21457;&#29616;&#21253;&#21547;&#22823;&#37327;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.12883</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#33324;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#26469;&#30740;&#31350;&#26080;&#22122;&#22768;&#22238;&#24402;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Mean Squared Error of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors. (arXiv:2305.12883v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#33324;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#30340;&#26080;&#22122;&#22768;&#22238;&#24402;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#21457;&#29616;&#21253;&#21547;&#22823;&#37327;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26368;&#23567;$\ell_2$&#33539;&#25968;&#65288;&#26080;&#23725;&#65289;&#25554;&#20540;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#30740;&#31350;&#26041;&#20852;&#26410;&#33406;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#26512;&#37117;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#22238;&#24402;&#35823;&#24046;&#32467;&#26500;&#65292;&#20551;&#35774;&#35823;&#24046;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20855;&#26377;&#38646;&#22343;&#20540;&#21644;&#30456;&#21516;&#30340;&#26041;&#24046;&#65292;&#19982;&#29305;&#24449;&#21521;&#37327;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29702;&#35770;&#20998;&#26512;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#26679;&#26412;&#22806;&#39044;&#27979;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;&#26080;&#23725;&#25554;&#20540;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#20801;&#35768;&#26356;&#19968;&#33324;&#30340;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#65292;&#25171;&#30772;&#20102;&#29616;&#26377;&#25991;&#29486;&#30340;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#36890;&#36807;&#25551;&#32472;&#26377;&#38480;&#26679;&#26412;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#26469;&#34920;&#24449;&#22343;&#26041;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26679;&#26412;&#37327;&#65292;&#21253;&#21547;&#22823;&#37327;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a significant growth in research focusing on minimum $\ell_2$ norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses have been limited to a simple regression error structure, assuming independent and identically distributed errors with zero mean and common variance, independent of the feature vectors. Additionally, the main focus of these theoretical analyses has been on the out-of-sample prediction risk. This paper breaks away from the existing literature by examining the mean squared error of the ridgeless interpolation least squares estimator, allowing for more general assumptions about the regression errors. Specifically, we investigate the potential benefits of overparameterization by characterizing the mean squared error in a finite sample. Our findings reveal that including a large number of unimportant parameters relative to the sample size can effectively reduce the mean squared error of the estimator. N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26126;&#30830;&#23545;&#25239;&#29615;&#22659;&#19979;&#38656;&#35201;&#39069;&#22806;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#24265;&#20215;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#26696;&#38477;&#20302;&#35843;&#33410;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.02497</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#40065;&#26834;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter Tuning for Adversarially Robust Models. (arXiv:2304.02497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26126;&#30830;&#23545;&#25239;&#29615;&#22659;&#19979;&#38656;&#35201;&#39069;&#22806;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#24265;&#20215;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#26696;&#38477;&#20302;&#35843;&#33410;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26088;&#22312;&#30830;&#23450;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#21738;&#20123;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26159;&#38656;&#35201;&#35843;&#33410;&#30340;&#65292;&#21516;&#26102;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#35843;&#33410;&#25104;&#26412;&#12290;&#36890;&#36807;&#23545;3&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#20808;&#21069;&#26377;&#20851;&#23545;&#25239;&#40065;&#26834;&#24615;&#25991;&#29486;&#20013;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#65292;&#24182;&#21457;&#29616;&#35813;&#38382;&#39064;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#30340;&#22797;&#26434;&#24615;&#20027;&#35201;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#38656;&#35201;&#35843;&#25972;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26469;&#24179;&#34913;&#26631;&#20934;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#65307;&#38656;&#35201;&#29420;&#31435;&#35843;&#25972;&#26631;&#20934;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#38454;&#27573;&#30340;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#21033;&#29992;&#24265;&#20215;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26469;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#36229;&#21442;&#25968;&#35843;&#33410;&#25104;&#26412;&#30340;&#26032;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the problem of hyper-parameter tuning (HPT) for robust (i.e., adversarially trained) models, with the twofold goal of i) establishing which additional HPs are relevant to tune in adversarial settings, and ii) reducing the cost of HPT for robust models. We pursue the first goal via an extensive experimental study based on 3 recent models widely adopted in the prior literature on adversarial robustness. Our findings show that the complexity of the HPT problem, already notoriously expensive, is exacerbated in adversarial settings due to two main reasons: i) the need of tuning additional HPs which balance standard and adversarial training; ii) the need of tuning the HPs of the standard and adversarial training phases independently. Fortunately, we also identify new opportunities to reduce the cost of HPT for robust models. Specifically, we propose to leverage cheap adversarial training methods to obtain inexpensive, yet highly correlated, estimations of the quality ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#23884;&#20837;&#21644;&#39044;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#23376;&#22270; GNN &#19978;&#39640;&#25928;&#35745;&#25968;&#23376;&#32467;&#26500;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.10576</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#22312;&#23376;&#22270;&#19978;&#36816;&#34892; GNN&#65292;&#20351;&#29992;&#23376;&#22270; GNN &#39640;&#25928;&#35745;&#25968;&#23376;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficiently Counting Substructures by Subgraph GNNs without Running GNN on Subgraphs. (arXiv:2303.10576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#23884;&#20837;&#21644;&#39044;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#23376;&#22270; GNN &#19978;&#39640;&#25928;&#35745;&#25968;&#23376;&#32467;&#26500;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22312;&#22270;&#23398;&#20064;&#20013;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26469;&#36817;&#20284;&#35745;&#31639;&#29305;&#23450;&#20989;&#25968;&#65292;&#22914;&#35745;&#25968;&#22270;&#30340;&#23376;&#32467;&#26500;&#65292;&#26159;&#19968;&#20010;&#28909;&#38376;&#36235;&#21183;&#12290;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23376;&#22270; GNN&#65292;&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#22270;&#65292;&#24182;&#36890;&#36807;&#23545;&#27599;&#20010;&#23376;&#22270;&#24212;&#29992; GNN &#26469;&#22686;&#24378;&#22270;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#23376;&#22270; GNN &#33021;&#22815;&#35745;&#25968;&#22797;&#26434;&#30340;&#23376;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#20250;&#36973;&#21463;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#35268;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992; GNN &#39640;&#25928;&#22320;&#35745;&#25968;&#23376;&#32467;&#26500;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23376;&#22270;&#20013;&#21040;&#26681;&#33410;&#28857;&#30340;&#36317;&#31163;&#26159;&#25552;&#39640;&#23376;&#22270; GNN &#35745;&#25968;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20449;&#24687;&#32534;&#30721;&#20026;&#32467;&#26500;&#23884;&#20837;&#65292;&#24182;&#39044;&#20808;&#35745;&#31639;&#36825;&#20123;&#23884;&#20837;&#65292;&#20197;&#36991;&#20813;&#36890;&#36807; GNN &#21453;&#22797;&#25552;&#21462;&#25152;&#26377;&#23376;&#22270;&#20013;&#30340;&#20449;&#24687;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#20445;&#25345;&#23376;&#22270; GNN &#30340;&#35745;&#25968;&#33021;&#21147;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using graph neural networks (GNNs) to approximate specific functions such as counting graph substructures is a recent trend in graph learning. Among these works, a popular way is to use subgraph GNNs, which decompose the input graph into a collection of subgraphs and enhance the representation of the graph by applying GNN to individual subgraphs. Although subgraph GNNs are able to count complicated substructures, they suffer from high computational and memory costs. In this paper, we address a non-trivial question: can we count substructures efficiently with GNNs? To answer the question, we first theoretically show that the distance to the rooted nodes within subgraphs is key to boosting the counting power of subgraph GNNs. We then encode such information into structural embeddings, and precompute the embeddings to avoid extracting information over all subgraphs via GNNs repeatedly. Experiments on various benchmarks show that the proposed model can preserve the counting power of subgra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23558;&#24037;&#19994;&#29289;&#32852;&#32593;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.09068</link><description>&lt;p&gt;
VFP&#65306;&#32771;&#34385;&#23646;&#24615;&#30456;&#20851;&#24615;&#23558;&#24037;&#19994;&#29289;&#32852;&#32593;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#20197;&#20379;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
VFP: Converting Tabular Data for IIoT into Images Considering Correlations of Attributes for Convolutional Neural Networks. (arXiv:2303.09068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23558;&#24037;&#19994;&#29289;&#32852;&#32593;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#29983;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20256;&#32479;&#22522;&#20110;&#20915;&#31574;&#26641;&#31639;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#24050;&#34987;&#37319;&#29992;&#12290; &#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#30495;&#23454;&#25968;&#23383;&#23646;&#24615;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#34920;&#26684;&#25968;&#25454;&#26102;&#23384;&#22312;&#38480;&#21046;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;DeepInsight&#65292;REFINED&#21644;IGTD&#23558;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#20197;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290; &#20182;&#20204;&#22312;&#22270;&#20687;&#30340;&#26576;&#20123;&#29305;&#23450;&#20301;&#32622;&#25910;&#38598;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#20351;&#36716;&#25442;&#21518;&#30340;&#22270;&#20687;&#30475;&#36215;&#26469;&#20687;&#26159;&#23454;&#38469;&#22270;&#20687;&#12290; &#25910;&#38598;&#31867;&#20284;&#30340;&#29305;&#24449;&#19982;&#20256;&#32479;&#30340;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;ML&#25216;&#26415;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#21024;&#38500;&#19968;&#20123;&#39640;&#24230;&#30456;&#20851;&#30340;&#23646;&#24615;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290; &#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#36716;&#25442;&#26041;&#27861;&#22266;&#23450;&#20102;&#22270;&#20687;&#22823;&#23567;&#65292;&#26681;&#25454;&#34920;&#26684;&#25968;&#25454;&#30340;&#23646;&#24615;&#25968;&#37327;&#65292;&#20250;&#36896;&#25104;&#28010;&#36153;&#25110;&#19981;&#36275;&#30340;&#20687;&#32032;&#12290; &#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;Vortex&#29305;&#24449;&#23450;&#20301;&#65288;VFP&#65289;&#12290; VFP&#32771;&#34385;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#24182;&#25918;&#32622;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
For tabular data generated from IIoT devices, traditional machine learning (ML) techniques based on the decision tree algorithm have been employed. However, these methods have limitations in processing tabular data where real number attributes dominate. To address this issue, DeepInsight, REFINED, and IGTD were proposed to convert tabular data into images for utilizing convolutional neural networks (CNNs). They gather similar features in some specific spots of an image to make the converted image look like an actual image. Gathering similar features contrasts with traditional ML techniques for tabular data, which drops some highly correlated attributes to avoid overfitting. Also, previous converting methods fixed the image size, and there are wasted or insufficient pixels according to the number of attributes of tabular data. Therefore, this paper proposes a new converting method, Vortex Feature Positioning (VFP). VFP considers the correlation of features and places similar features fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2302.05534</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#35774;&#32622;&#65292;&#36825;&#26159;&#19968;&#20010;&#24182;&#34892;&#20256;&#36755;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#26159;&#23558;&#30693;&#35782;&#20174;&#20302;&#23618;&#65288;&#28304;&#65289;&#20219;&#21153;&#20256;&#36755;&#21040;&#39640;&#23618;&#65288;&#30446;&#26631;&#65289;&#20219;&#21153;&#65292;&#20197;&#20943;&#23569;&#21518;&#32773;&#30340;&#25506;&#32034;&#39118;&#38505;&#65292;&#21516;&#26102;&#24182;&#34892;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#20302;&#23618;&#21644;&#39640;&#23618;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#24577;&#25110;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26368;&#20248;&#20540;&#25903;&#37197;&#8221;&#30340;&#33258;&#28982;&#32780;&#24517;&#35201;&#30340;&#26465;&#20214;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#23545;&#20110;&#39640;&#23618;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#29366;&#24577;&#19978;&#21487;&#20197;&#23454;&#29616;&#24658;&#23450;&#30340;&#36951;&#25022;&#65292;&#36825;&#21462;&#20915;&#20110;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19981;&#30456;&#20284;&#26102;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#36951;&#25022;&#65307;&#32780;&#23545;&#20110;&#20302;&#23618;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#20570;&#20986;&#29306;&#29298;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#20302;&#23618;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks
&lt;/p&gt;</description></item></channel></rss>