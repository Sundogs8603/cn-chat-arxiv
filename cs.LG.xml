<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02066</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;Transformer&#36827;&#34892;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#38656;&#35201;&#21516;&#26102;&#29983;&#25104;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#20854;&#30446;&#26631;&#23646;&#24615;&#65292;&#36825;&#23545;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#39033;&#33392;&#24040;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#65292;&#23427;&#23558;Transformer&#30340;&#35299;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;&#21644;&#39044;&#27979;&#22120;&#32467;&#21512;&#20026;&#19968;&#20010;&#20855;&#26377;&#20849;&#20139;&#26435;&#37325;&#30340;&#32852;&#21512;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#26469;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;Transformer&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30446;&#26631;&#23646;&#24615;&#30340;&#26032;&#39062;&#20998;&#23376;&#65292;&#30456;&#27604;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
&lt;/p&gt;</description></item><item><title>VENOM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;V:N:M&#26684;&#24335;&#65292;&#21487;&#20197;&#22312;NVIDIA&#30340;&#31232;&#30095;&#24352;&#37327;&#26680;&#24515;&#19978;&#23454;&#29616;&#20219;&#24847;N:M&#27604;&#29575;&#12290;&#20351;&#29992;Spatha&#31232;&#30095;&#24211;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;37&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#19968;&#31181;&#20108;&#38454;&#20462;&#21098;&#25216;&#26415;&#26469;&#23454;&#29616;&#39640;&#31232;&#30095;&#27604;&#29575;&#30340;V:N:M&#24418;&#24335;&#65292;&#24182;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.02065</link><description>&lt;p&gt;
VENOM&#65306;&#19968;&#31181;&#29992;&#20110;&#37322;&#25918;&#31232;&#30095;&#24352;&#37327;&#26680;&#24515;&#24040;&#22823;&#28508;&#21147;&#30340;&#30690;&#37327;&#21270;N:M&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores. (arXiv:2310.02065v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02065
&lt;/p&gt;
&lt;p&gt;
VENOM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;V:N:M&#26684;&#24335;&#65292;&#21487;&#20197;&#22312;NVIDIA&#30340;&#31232;&#30095;&#24352;&#37327;&#26680;&#24515;&#19978;&#23454;&#29616;&#20219;&#24847;N:M&#27604;&#29575;&#12290;&#20351;&#29992;Spatha&#31232;&#30095;&#24211;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;37&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#19968;&#31181;&#20108;&#38454;&#20462;&#21098;&#25216;&#26415;&#26469;&#23454;&#29616;&#39640;&#31232;&#30095;&#27604;&#29575;&#30340;V:N:M&#24418;&#24335;&#65292;&#24182;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#21644;&#35268;&#27169;&#25193;&#22823;&#35201;&#27714;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21151;&#29575;&#12290;&#31232;&#30095;&#21270;&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#26356;&#23567;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#21152;&#36895;&#30828;&#20214;&#12290;&#28982;&#32780;&#65292;&#35201;&#26377;&#25928;&#22320;&#21033;&#29992;&#31232;&#30095;&#21521;&#37327;&#21333;&#20803;&#30340;&#30828;&#20214;&#25903;&#25345;&#65292;&#38656;&#35201;&#36827;&#34892;&#26680;&#24515;&#23454;&#29616;&#12289;&#20462;&#21098;&#31639;&#27861;&#21644;&#23384;&#20648;&#26684;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;NVIDIA&#30340;&#31232;&#30095;&#24352;&#37327;&#26680;&#24515;&#65288;SPTC&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;2&#20493;&#30340;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;SPTC&#21482;&#25903;&#25345;2:4&#26684;&#24335;&#65292;&#38480;&#21046;&#20102;&#21487;&#23454;&#29616;&#30340;&#31232;&#30095;&#27604;&#29575;&#20026;50%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;V:N:M&#26684;&#24335;&#65292;&#21487;&#20197;&#22312;SPTC&#19978;&#25191;&#34892;&#20219;&#24847;N:M&#27604;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#26032;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Spatha&#65292;&#19968;&#20010;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20363;&#31243;&#30340;&#39640;&#24615;&#33021;&#31232;&#30095;&#24211;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Spatha&#27604;cuBLAS&#23454;&#29616;&#20102;&#39640;&#36798;37&#20493;&#30340;&#21152;&#36895;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#20108;&#38454;&#20462;&#21098;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#31232;&#30095;&#27604;&#29575;&#30340;V:N:M&#24418;&#24335;&#65292;&#24182;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no los
&lt;/p&gt;</description></item><item><title>&#26412;&#25216;&#26415;&#25253;&#21578;&#24635;&#32467;&#20102;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;&#21307;&#30103;&#19987;&#23478;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#24515;&#35299;&#37322;&#23545;&#20110;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#30340;&#25968;&#25454;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02063</link><description>&lt;p&gt;
&#26469;&#33258;EXMOS&#29992;&#25143;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#65306;&#24635;&#32467;&#35780;&#20272;EXMOS&#24179;&#21488;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#25910;&#33719;&#30340;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform. (arXiv:2310.02063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#24635;&#32467;&#20102;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;&#21307;&#30103;&#19987;&#23478;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#24515;&#35299;&#37322;&#23545;&#20110;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#30340;&#25968;&#25454;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#39046;&#22495;&#65292;&#35299;&#37322;&#30340;&#25552;&#20379;&#22312;&#35843;&#35797;&#21644;&#22686;&#24378;&#39044;&#27979;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#36741;&#21161;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#24515;&#21644;&#25968;&#25454;&#20013;&#24515;&#35299;&#37322;&#22312;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#25968;&#25454;&#30456;&#20851;&#38382;&#39064;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#30446;&#30340;&#19978;&#30340;&#26377;&#25928;&#24615;&#33267;&#20170;&#23578;&#26410;&#24471;&#21040;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#12290;&#22312;&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#23545;&#22522;&#20110;&#25968;&#25454;&#20013;&#24515;&#21644;&#27169;&#22411;&#20013;&#24515;&#35270;&#35282;&#30340;&#20840;&#23616;&#35299;&#37322;&#22312;&#25903;&#25345;&#21307;&#30103;&#19987;&#23478;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#25968;&#25454;&#37197;&#32622;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#30740;&#31350;&#36825;&#20123;&#21160;&#24577;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#21253;&#25324;&#28041;&#21450;70&#20301;&#21307;&#30103;&#19987;&#23478;&#30340;&#23450;&#37327;&#20998;&#26512;&#21644;&#28041;&#21450;30&#20301;&#21307;&#30103;&#19987;&#23478;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of interactive machine-learning systems, the provision of explanations serves as a vital aid in the processes of debugging and enhancing prediction models. However, the extent to which various global model-centric and data-centric explanations can effectively assist domain experts in detecting and resolving potential data-related issues for the purpose of model improvement has remained largely unexplored. In this technical report, we summarise the key findings of our two user studies. Our research involved a comprehensive examination of the impact of global explanations rooted in both data-centric and model-centric perspectives within systems designed to support healthcare experts in optimising machine learning models through both automated and manual data configurations. To empirically investigate these dynamics, we conducted two user studies, comprising quantitative analysis involving a sample size of 70 healthcare experts and qualitative assessments involving 30 healthc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"Inhibitor"&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;ReLU&#21644;&#21152;&#27861;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25110;&#26367;&#20195;&#31639;&#27861;&#31995;&#32479;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#21644;&#25903;&#25345;&#26356;&#22823;&#30340;&#37327;&#21270;Transformer&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30456;&#27604;&#65292;&#35813;&#26426;&#21046;&#22312;&#39044;&#27979;&#24471;&#20998;&#19978;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;&#36825;&#19968;&#21019;&#26032;&#21487;&#33021;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.02041</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;Transformer&#30340;"Inhibitor"&#65306;ReLU&#21644;&#21152;&#27861;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers. (arXiv:2310.02041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"Inhibitor"&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;ReLU&#21644;&#21152;&#27861;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25110;&#26367;&#20195;&#31639;&#27861;&#31995;&#32479;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#21644;&#25903;&#25345;&#26356;&#22823;&#30340;&#37327;&#21270;Transformer&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30456;&#27604;&#65292;&#35813;&#26426;&#21046;&#22312;&#39044;&#27979;&#24471;&#20998;&#19978;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;&#36825;&#19968;&#21019;&#26032;&#21487;&#33021;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#37327;&#21270;Transformer&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#29992;&#21482;&#28041;&#21450;&#21152;&#27861;&#21644;ReLU&#28608;&#27963;&#30340;&#26367;&#20195;&#26426;&#21046;&#26469;&#21462;&#20195;&#22522;&#20110;&#28857;&#31215;&#21644;Softmax&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#36825;&#26679;&#21487;&#20197;&#36991;&#20813;&#30697;&#38453;&#20056;&#27861;&#20013;&#24120;&#38656;&#35201;&#30340;&#21452;&#31934;&#24230;&#25193;&#23637;&#21644;&#26114;&#36149;&#30340;Softmax&#35745;&#31639;&#65292;&#20294;&#20173;&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26680;&#24515;&#21151;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25110;&#21516;&#24577;&#21152;&#23494;&#31561;&#26367;&#20195;&#31639;&#27861;&#31995;&#32479;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#21644;&#25903;&#25345;&#26356;&#22823;&#30340;&#37327;&#21270;Transformer&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#35757;&#32451;&#23454;&#39564;&#26174;&#31034;&#65292;&#27979;&#35797;&#38598;&#30340;&#39044;&#27979;&#24471;&#20998;&#19982;&#37319;&#29992;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#20256;&#32479;Transformer&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#32553;&#25918;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#22312;&#26126;&#25991;&#21644;&#21152;&#23494;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30456;&#20449;&#26412;&#25991;&#20171;&#32461;&#30340;&#22522;&#20110;ReLU&#21644;&#21152;&#27861;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#33021;&#20250;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;A
&lt;/p&gt;
&lt;p&gt;
To enhance the computational efficiency of quantized Transformers, we replace the dot-product and Softmax-based attention with an alternative mechanism involving addition and ReLU activation only. This side-steps the expansion to double precision often required by matrix multiplication and avoids costly Softmax evaluations but maintains much of the core functionality of conventional dot-product attention. It can enable more efficient execution and support larger quantized Transformer models on resource-constrained hardware or alternative arithmetic systems like homomorphic encryption. Training experiments on four common benchmark tasks show test set prediction scores comparable to those of conventional Transformers with dot-product attention. Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption. In particular, we believe that the ReLU and addition-based attention mechanism introduced in this paper may enable privacy-preserving A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;aSAGA&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20020;&#24202;&#21644;&#23478;&#24237;&#30561;&#30496;&#30740;&#31350;&#20013;&#26377;&#25928;&#25191;&#34892;&#12290;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#26144;&#23556;&#26469;&#35782;&#21035;&#28784;&#33394;&#21306;&#22495;&#65292;&#38656;&#35201;&#36827;&#34892;&#25163;&#21160;&#37325;&#26032;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02032</link><description>&lt;p&gt;
aSAGA: &#24102;&#28784;&#21306;&#22495;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
aSAGA: Automatic Sleep Analysis with Gray Areas. (arXiv:2310.02032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;aSAGA&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20020;&#24202;&#21644;&#23478;&#24237;&#30561;&#30496;&#30740;&#31350;&#20013;&#26377;&#25928;&#25191;&#34892;&#12290;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#26144;&#23556;&#26469;&#35782;&#21035;&#28784;&#33394;&#21306;&#22495;&#65292;&#38656;&#35201;&#36827;&#34892;&#25163;&#21160;&#37325;&#26032;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#19982;&#25163;&#21160;&#30561;&#30496;&#20998;&#26399;&#30456;&#24403;&#30340;&#21487;&#38752;&#24615;&#21644;&#26356;&#39640;&#30340;&#26102;&#38388;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23436;&#20840;&#33258;&#21160;&#30340;&#40657;&#30418;&#35299;&#20915;&#26041;&#26696;&#24456;&#38590;&#36866;&#24212;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#26041;&#27861;&#21644;&#30561;&#30496;&#25216;&#26415;&#24072;&#30340;&#24037;&#20316;&#20043;&#38388;&#30340;&#20132;&#20114;&#20173;&#19981;&#34987;&#20805;&#20998;&#30740;&#31350;&#21644;&#27010;&#24565;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#30561;&#30496;&#20998;&#26512;&#30340;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;aSAGA&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20020;&#24202;&#22810;&#39033;&#24335;&#30417;&#27979;&#35760;&#24405;&#21644;&#23478;&#24237;&#30561;&#30496;&#30740;&#31350;&#20013;&#26377;&#25928;&#25191;&#34892;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#27169;&#22411;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#37319;&#29992;&#19977;&#20010;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#30340;&#20020;&#24202;&#21069;&#39564;&#35777;&#26041;&#27861;&#36827;&#34892;&#65292;&#21253;&#25324;&#24320;&#25918;&#33719;&#21462;&#12289;&#20020;&#24202;&#21644;&#30740;&#31350;&#39537;&#21160;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26144;&#23556;&#35782;&#21035;&#33258;&#21160;&#30561;&#30496;&#20998;&#26512;&#20013;&#30340;&#27169;&#31946;&#21306;&#22495;&#65292;&#27010;&#24565;&#21270;&#20026;&#28784;&#33394;&#21306;&#22495;&#65292;&#38656;&#35201;&#36827;&#34892;&#25163;&#21160;&#37325;&#26032;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art automatic sleep staging methods have already demonstrated comparable reliability and superior time efficiency to manual sleep staging. However, fully automatic black-box solutions are difficult to adapt into clinical workflow and the interaction between explainable automatic methods and the work of sleep technologists remains underexplored and inadequately conceptualized. Thus, we propose a human-in-the-loop concept for sleep analysis, presenting an automatic sleep staging model (aSAGA), that performs effectively with both clinical polysomnographic recordings and home sleep studies. To validate the model, extensive testing was conducted, employing a preclinical validation approach with three retrospective datasets; open-access, clinical, and research-driven. Furthermore, we validate the utilization of uncertainty mapping to identify ambiguous regions, conceptualized as gray areas, in automatic sleep analysis that warrants manual re-evaluation. The results demonstrate t
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;AI/ML&#30028;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36807;&#20110;&#20851;&#27880;&#29366;&#24577;&#27010;&#29575;&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#23545;&#25928;&#29992;&#30340;&#20934;&#30830;&#21487;&#38752;&#20272;&#35745;&#65292;&#23548;&#33268;&#20102;&#26399;&#26395;&#21644;&#23454;&#38469;&#24433;&#21709;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.02029</link><description>&lt;p&gt;
&#22312;&#20934;&#30830;&#39044;&#27979;&#19982;&#31967;&#31957;&#20915;&#31574;&#20043;&#38388;&#65306;AI/ML&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Between accurate prediction and poor decision making: the AI/ML gap. (arXiv:2310.02029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;AI/ML&#30028;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36807;&#20110;&#20851;&#27880;&#29366;&#24577;&#27010;&#29575;&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#23545;&#25928;&#29992;&#30340;&#20934;&#30830;&#21487;&#38752;&#20272;&#35745;&#65292;&#23548;&#33268;&#20102;&#26399;&#26395;&#21644;&#23454;&#38469;&#24433;&#21709;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#20381;&#36182;&#20110;AI/ML&#21151;&#33021;&#26469;&#39044;&#27979;&#21487;&#33021;&#34892;&#21160;&#30340;&#21518;&#26524;&#24182;&#20248;&#21270;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#30028;&#22312;&#35299;&#20915;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#21162;&#21147;&#38750;&#24120;&#24378;&#28872;&#65288;&#19988;&#25104;&#21151;&#65289;&#65292;&#20197;&#33267;&#20110;&#20135;&#29983;&#20102;&#36825;&#26679;&#30340;&#24187;&#35273;&#65306;&#23398;&#20064;&#32773;&#39044;&#27979;&#65288;&#25110;&#20998;&#31867;&#65289;&#36234;&#20934;&#30830;&#65292;&#26368;&#32456;&#20915;&#31574;&#23601;&#20250;&#36234;&#22909;&#12290;&#20294;&#36825;&#31181;&#20551;&#35774;&#21482;&#26377;&#22312;&#65288;&#20154;&#31867;&#25110;&#20154;&#24037;&#65289;&#20915;&#31574;&#32773;&#23545;&#21487;&#33021;&#34892;&#21160;&#30340;&#25928;&#29992;&#26377;&#23436;&#20840;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#25165;&#25104;&#31435;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;AI/ML&#30028;&#36804;&#20170;&#20026;&#27490;&#37319;&#21462;&#20102;&#36807;&#20110;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#36807;&#20998;&#20851;&#27880;&#29366;&#24577;&#65288;&#25110;&#30446;&#26631;&#65289;&#27010;&#29575;&#30340;&#20272;&#35745;&#65292;&#32780;&#24573;&#35270;&#20102;&#23545;&#25928;&#29992;&#30340;&#20934;&#30830;&#21487;&#38752;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#24456;&#23569;&#26377;&#35777;&#25454;&#35777;&#26126;&#38169;&#35823;&#30340;&#25928;&#29992;&#35780;&#20272;&#23545;&#20915;&#31574;&#31574;&#30053;&#30340;&#39044;&#26399;&#25928;&#29992;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#24773;&#20917;&#23548;&#33268;&#20102;&#26399;&#26395;&#21644;&#23454;&#38469;&#24433;&#21709;&#20043;&#38388;&#30340;&#23454;&#36136;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents rely on AI/ML functionalities to predict the consequence of possible actions and optimise the policy. However, the effort of the research community in addressing prediction accuracy has been so intense (and successful) that it created the illusion that the more accurate the learner prediction (or classification) the better would have been the final decision. Now, such an assumption is valid only if the (human or artificial) decision maker has complete knowledge of the utility of the possible actions. This paper argues that AI/ML community has taken so far a too unbalanced approach by devoting excessive attention to the estimation of the state (or target) probability to the detriment of accurate and reliable estimations of the utility. In particular, few evidence exists about the impact of a wrong utility assessment on the resulting expected utility of the decision strategy. This situation is creating a substantial gap between the expectations and the effective impact
&lt;/p&gt;</description></item><item><title>DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;</title><link>http://arxiv.org/abs/2310.02027</link><description>&lt;p&gt;
DeepHGCN&#65306;&#26397;&#30528;&#26356;&#28145;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks. (arXiv:2310.02027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02027
&lt;/p&gt;
&lt;p&gt;
DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HGCN&#65289;&#22312;&#25552;&#21462;&#20998;&#23618;&#22270;&#20449;&#24687;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#21452;&#26354;&#25805;&#20316;&#21644;&#38543;&#30528;&#28145;&#24230;&#22686;&#21152;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;HGCN&#21463;&#38480;&#20110;&#27973;&#23618;&#26550;&#26500;&#12290;&#23613;&#31649;&#22312;GCNs&#20013;&#24050;&#32463;&#24212;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20294;&#26159;&#24320;&#21457;&#21452;&#26354;&#27835;&#30103;&#26041;&#27861;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25805;&#20316;&#24517;&#39035;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#20197;&#36866;&#24212;&#21452;&#26354;&#24615;&#36136;&#12290;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DeepHGCN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22823;&#22823;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#25928;&#26524;&#30340;&#28145;&#23618;&#22810;&#23618;HGCN&#26550;&#26500;&#12290;DeepHGCN&#20855;&#26377;&#20004;&#20010;&#28145;&#23618;HGCN&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#32447;&#24615;&#26144;&#23556;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26377;&#25928;&#30340;&#21452;&#26354;&#27531;&#24046;&#36830;&#25509;&#21644;&#26435;&#37325;&#21644;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#20419;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic graph convolutional networks (HGCN) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures, due to the expensive hyperbolic operations and the over-smoothing issue as depth increases. Although in GCNs, treatments have been applied to alleviate over-smoothing, developing a hyperbolic therapy presents distinct challenges since operations should be carefully designed to fit the hyperbolic nature. Addressing the above challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN architecture with dramatically improved computational efficiency and substantially alleviated over-smoothing effect. DeepHGCN presents two key enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer that enables fast and accurate linear maps; and (2) Techniques such as hyperbolic residual connections and regularization for both weights and features facilitated by an effic
&lt;/p&gt;</description></item><item><title>DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02025</link><description>&lt;p&gt;
DeepZero: &#23558;&#38646;&#38454;&#20248;&#21270;&#24212;&#29992;&#20110;&#28145;&#24230;&#27169;&#22411;&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02025
&lt;/p&gt;
&lt;p&gt;
DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#27861;&#33719;&#21462;&#19968;&#38454;&#20449;&#24687;&#26102;&#65292;&#38646;&#38454;&#20248;&#21270;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20854;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#22312;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;DeepZero&#65292;&#19968;&#20010;&#22522;&#20110;&#38646;&#38454;&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#23558;&#38646;&#38454;&#20248;&#21270;&#25193;&#23637;&#21040;&#20174;&#38646;&#24320;&#22987;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#38543;&#26426;&#32447;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24378;&#21270;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#31216;&#20026;&#32435;&#20160;&#36951;&#25022;&#65292;&#23427;&#36890;&#36807;&#23558;&#31639;&#27861;&#30340;&#34920;&#29616;&#37327;&#21270;&#20026;&#20854;&#22312;&#21508;&#20010;&#22238;&#21512;&#20013;&#25152;&#29983;&#25104;&#30340;&#38598;&#20307;&#31119;&#21033;&#26469;&#25552;&#20379;&#19968;&#20010;&#22522;&#20110;&#20844;&#24179;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.02023</link><description>&lt;p&gt;
&#23545;&#20110;&#32447;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#65292;&#32435;&#20160;&#36951;&#25022;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Nash Regret Guarantees for Linear Bandits. (arXiv:2310.02023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#38543;&#26426;&#32447;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24378;&#21270;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#31216;&#20026;&#32435;&#20160;&#36951;&#25022;&#65292;&#23427;&#36890;&#36807;&#23558;&#31639;&#27861;&#30340;&#34920;&#29616;&#37327;&#21270;&#20026;&#20854;&#22312;&#21508;&#20010;&#22238;&#21512;&#20013;&#25152;&#29983;&#25104;&#30340;&#38598;&#20307;&#31119;&#21033;&#26469;&#25552;&#20379;&#19968;&#20010;&#22522;&#20110;&#20844;&#24179;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38543;&#26426;&#32447;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#20013;&#33719;&#24471;&#20102;&#23545;&#36951;&#25022;&#30340;&#19968;&#20010;&#26356;&#24378;&#21270;&#30340;&#19978;&#30028;&#65292;&#31216;&#20026;&#32435;&#20160;&#36951;&#25022;&#65292;&#20854;&#23450;&#20041;&#20026;&#32447;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#31215;&#32047;&#30340;&#39044;&#26399;&#22870;&#21169;&#30340;&#20960;&#20309;&#24179;&#22343;&#19982;&#65288;&#20808;&#39564;&#26410;&#30693;&#30340;&#65289;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30001;&#20110;&#20960;&#20309;&#24179;&#22343;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#65288;NSW&#65289;&#20989;&#25968;&#65292;&#22240;&#27492;&#35813;&#20844;&#24335;&#23558;&#31639;&#27861;&#30340;&#34920;&#29616;&#37327;&#21270;&#20026;&#20854;&#22312;&#21508;&#20010;&#22238;&#21512;&#20013;&#25152;&#29983;&#25104;&#30340;&#38598;&#20307;&#31119;&#21033;&#12290;&#24050;&#30693;NSW&#28385;&#36275;&#20844;&#24179;&#20844;&#29702;&#65292;&#22240;&#27492;&#23545;&#32435;&#20160;&#36951;&#25022;&#30340;&#19978;&#30028;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#20844;&#24179;&#20445;&#35777;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26102;&#38388;&#36328;&#24230;&#20026;$T$&#22238;&#21512;&#12289;&#33218;&#38598;&#21512;&#20026;${X}$&#12289;&#32500;&#24230;&#20026;$d$&#30340;&#38543;&#26426;&#32447;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20851;&#27880;&#19982;&#27599;&#20010;&#33218;&#30456;&#20851;&#30340;&#38543;&#26426;&#22870;&#21169;&#26159;&#38750;&#36127;&#30340;&#12289;$\nu$-&#27425;&#27850;&#26494;&#38543;&#26426;&#21464;&#37327;&#30340;&#35774;&#32622;&#12290;&#23545;&#20110;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening -- referred to as Nash regret -- is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.  We consider the stochastic linear bandits problem over a horizon of $T$ rounds and with set of arms ${X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward -- associated with each arm in ${X}$ -- is a non-negative, $\nu$-sub-Poisson random variable. For this setting, we develop an algorithm th
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;QUITE&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#24037;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#23545;&#35937;&#30340;&#36136;&#37327;&#65292;&#20174;&#19968;&#32452;&#30001;&#24322;&#26500;&#24037;&#20154;&#25552;&#20379;&#30340;&#22122;&#22768;&#25104;&#23545;&#27604;&#36739;&#24320;&#22987;&#65292;&#23545;&#19968;&#32452;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;QUITE&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#20351;&#20854;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.02016</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#36136;&#24037;&#20154;&#23545;&#19968;&#32452;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#65306;&#19968;&#20010;&#30456;&#24403;&#31616;&#21333;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ranking a Set of Objects using Heterogeneous Workers: QUITE an Easy Problem. (arXiv:2310.02016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02016
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;QUITE&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#24037;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#23545;&#35937;&#30340;&#36136;&#37327;&#65292;&#20174;&#19968;&#32452;&#30001;&#24322;&#26500;&#24037;&#20154;&#25552;&#20379;&#30340;&#22122;&#22768;&#25104;&#23545;&#27604;&#36739;&#24320;&#22987;&#65292;&#23545;&#19968;&#32452;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;QUITE&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#20351;&#20854;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#20174;&#19968;&#32452;&#19981;&#24179;&#31561;&#24037;&#20154;&#25552;&#20379;&#30340;&#22122;&#22768;&#25104;&#23545;&#27604;&#36739;&#24320;&#22987;&#65292;&#23545;$N$&#20010;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#24037;&#20154;&#37117;&#20855;&#26377;&#29305;&#23450;&#30340;&#21487;&#38752;&#24615;&#31243;&#24230;&#65292;&#36825;&#21453;&#26144;&#20102;&#22905;&#23545;&#23545;&#35937;&#23545;&#36827;&#34892;&#25490;&#24207;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20551;&#35774;&#23545;&#35937;&#20855;&#26377;&#20869;&#22312;&#36136;&#37327;&#65292;&#19968;&#20010;&#23545;&#35937;&#34987;&#20248;&#36873;&#21478;&#19968;&#20010;&#30340;&#27010;&#29575;&#21462;&#20915;&#20110;&#20004;&#20010;&#31454;&#20105;&#32773;&#30340;&#36136;&#37327;&#24046;&#24322;&#21644;&#24037;&#20154;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;QUITE&#65292;&#19968;&#20010;&#38750;&#33258;&#36866;&#24212;&#30340;&#25490;&#24207;&#31639;&#27861;&#65292;&#23427;&#21516;&#26102;&#20272;&#35745;&#24037;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#23545;&#35937;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;QUITE&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#19982;&#20043;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;QUITE&#22914;&#20309;&#33258;&#28982;&#22320;&#21464;&#24471;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of ranking $N$ objects starting from a set of noisy pairwise comparisons provided by a crowd of unequal workers, each worker being characterized by a specific degree of reliability, which reflects her ability to rank pairs of objects. More specifically, we assume that objects are endowed with intrinsic qualities and that the probability with which an object is preferred to another depends both on the difference between the qualities of the two competitors and on the reliability of the worker. We propose QUITE, a non-adaptive ranking algorithm that jointly estimates workers' reliabilities and qualities of objects. Performance of QUITE is compared in different scenarios against previously proposed algorithms. Finally, we show how QUITE can be naturally made adaptive.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20809;&#35889;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#25968;&#25454;&#20381;&#36182;&#30340;&#21442;&#25968;&#21270;PDE&#27714;&#35299;&#65292;&#21487;&#20197;&#20934;&#30830;&#23398;&#20064;&#21644;&#39044;&#27979;&#22797;&#26434;&#30340;PDE&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.02013</link><description>&lt;p&gt;
&#26080;&#38656;&#25968;&#25454;&#20381;&#36182;&#30340;&#21442;&#25968;PDE&#30340;&#20809;&#35889;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spectral operator learning for parametric PDEs without data reliance. (arXiv:2310.02013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20809;&#35889;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#25968;&#25454;&#20381;&#36182;&#30340;&#21442;&#25968;&#21270;PDE&#27714;&#35299;&#65292;&#21487;&#20197;&#20934;&#30830;&#23398;&#20064;&#21644;&#39044;&#27979;&#22797;&#26434;&#30340;PDE&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCLON&#30340;&#22522;&#20110;&#31639;&#23376;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#32780;&#26080;&#38656;&#21033;&#29992;&#25968;&#25454;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#37319;&#29992;&#20809;&#35889;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#20132;&#20989;&#25968;&#65288;&#22914;&#20613;&#37324;&#21494;&#32423;&#25968;&#21644;Legendre&#22810;&#39033;&#24335;&#65289;&#36827;&#34892;&#23637;&#24320;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#32593;&#26684;&#28857;&#19978;&#23454;&#29616;&#20934;&#30830;&#30340;PDE&#35299;&#12290;&#36890;&#36807;&#23558;&#20809;&#35889;&#26041;&#27861;&#30340;&#20248;&#28857;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;SCLON&#25552;&#20379;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#28040;&#38500;&#20102;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#20540;&#35745;&#31639;&#30340;&#25104;&#23545;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#36824;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;&#22797;&#26434;&#21442;&#25968;&#21270;PDE&#30340;&#35299;&#65292;&#20174;&#22855;&#24322;&#25668;&#21160;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#21040;Navier-Stokes&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the Spectral Coefficient Learning via Operator Network (SCLON), a novel operator learning-based approach for solving parametric partial differential equations (PDEs) without the need for data harnessing. The cornerstone of our method is the spectral methodology that employs expansions using orthogonal functions, such as Fourier series and Legendre polynomials, enabling accurate PDE solutions with fewer grid points. By merging the merits of spectral methods - encompassing high accuracy, efficiency, generalization, and the exact fulfillment of boundary conditions with the prowess of deep neural networks, SCLON offers a transformative strategy. Our approach not only eliminates the need for paired input-output training data, which typically requires extensive numerical computations, but also effectively learns and predicts solutions of complex parametric PDEs, ranging from singularly perturbed convection-diffusion equations to the Navier-Stokes equations. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#28145;&#24230;&#38480;&#21046;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;&#12290;&#30740;&#31350;&#32473;&#20986;&#20102;&#19968;&#31181;&#20855;&#20307;&#26500;&#36896;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#22312;&#20219;&#20309;&#28145;&#24230;&#37117;&#33021;&#20445;&#25345;&#20248;&#21270;&#30340;&#20449;&#21495;&#20256;&#25773;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#26377;&#30028;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02012</link><description>&lt;p&gt;
&#36229;&#36234;&#28145;&#24230;&#38480;&#21046;&#30340;&#35757;&#32451;&#65306;&#25209;&#24402;&#19968;&#21270;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;
&lt;/p&gt;
&lt;p&gt;
Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion. (arXiv:2310.02012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#28145;&#24230;&#38480;&#21046;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;&#12290;&#30740;&#31350;&#32473;&#20986;&#20102;&#19968;&#31181;&#20855;&#20307;&#26500;&#36896;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#22312;&#20219;&#20309;&#28145;&#24230;&#37117;&#33021;&#20445;&#25345;&#20248;&#21270;&#30340;&#20449;&#21495;&#20256;&#25773;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#26377;&#30028;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#23618;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#12290;&#19968;&#20123;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25209;&#24402;&#19968;&#21270;&#25913;&#21892;&#20102;&#20449;&#21495;&#20256;&#25773;&#65292;&#36890;&#36807;&#36991;&#20813;&#34920;&#31034;&#22312;&#23618;&#20043;&#38388;&#21464;&#24471;&#20849;&#32447;&#12290;&#28982;&#32780;&#65292;&#25209;&#24402;&#19968;&#21270;&#30340;&#22343;&#22330;&#29702;&#35770;&#20063;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#31181;&#22909;&#22788;&#26159;&#20197;&#28145;&#24230;&#26799;&#24230;&#29190;&#28856;&#30340;&#20195;&#20215;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#25209;&#24402;&#19968;&#21270;&#30340;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#8220;&#25209;&#24402;&#19968;&#21270;&#32593;&#32476;&#33021;&#21542;&#20445;&#25345;&#26368;&#20248;&#30340;&#20449;&#21495;&#20256;&#25773;&#29305;&#24615;&#65292;&#20294;&#36991;&#20813;&#26799;&#24230;&#29190;&#28856;&#65311;&#8221;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#32473;&#20986;&#19968;&#31181;&#20855;&#20307;&#30340;&#26500;&#36896;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#32447;&#24615;&#28608;&#27963;&#21644;&#25209;&#24402;&#19968;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#20219;&#20309;&#28145;&#24230;&#37117;&#20855;&#26377;&#26377;&#30028;&#26799;&#24230;&#12290;&#22522;&#20110;Weingarten&#24494;&#31215;&#20998;&#65292;&#25105;&#20204;&#20026;&#35813;&#26500;&#36896;&#30340;MLP&#24320;&#21457;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#38750;&#28176;&#36817;&#29702;&#35770;&#65292;&#32473;&#20986;&#20102;&#21069;&#21521;&#20449;&#21495;&#20256;&#25773;&#30340;&#31934;&#30830;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question: "Can a batch-normalized network keep the optimal signal propagation properties, but avoid exploding gradients?" We answer this question in the affirmative by giving a particular construction of an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization that provably has bounded gradients at any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.02011</link><description>&lt;p&gt;
&#35299;&#30721;&#20154;&#31867;&#34892;&#20026;&#65306;&#20998;&#26512;&#21487;&#31359;&#25140;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#25968;&#25454;&#36827;&#34892;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition. (arXiv:2310.02011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#36816;&#21160;&#25110;&#30456;&#23545;&#23450;&#20301;&#26377;&#25928;&#22320;&#20135;&#29983;&#20102;&#21487;&#20197;&#34987;&#35745;&#31639;&#26426;&#35835;&#21462;&#30340;&#21407;&#22987;&#30005;&#20449;&#21495;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25805;&#20316;&#25216;&#26415;&#26469;&#23545;&#19981;&#21516;&#30340;&#20154;&#31867;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#19982;&#27531;&#24046;MobileNet&#36827;&#34892;&#21512;&#22863;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#31216;&#20026;FusionActNet&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#27531;&#24046;&#22359;&#20998;&#21035;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#26126;&#26174;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#32593;&#32476;&#29420;&#31435;&#35757;&#32451;&#65292;&#24471;&#21040;&#20004;&#20010;&#19987;&#19994;&#30340;&#39640;&#31934;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26550;&#26500;&#35843;&#25972;&#30340;&#31639;&#27861;&#20248;&#21183;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#36229;&#31867;&#20013;&#20248;&#31168;&#22320;&#35782;&#21035;&#27963;&#21160;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#27531;&#24046;&#32593;&#32476;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#30340;&#27531;&#24046;MobileNet&#36827;&#34892;&#20256;&#36882;&#12290;&#38543;&#21518;&#65292;&#36825;&#20010;&#21512;&#22863;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19968;&#20123;&#29305;&#23450;&#30340;&#23376;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a sp
&lt;/p&gt;</description></item><item><title>fmeffects&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#30340;R&#36719;&#20214;&#21253;&#12290;</title><link>http://arxiv.org/abs/2310.02008</link><description>&lt;p&gt;
fmeffects: &#19968;&#20010;&#29992;&#20110;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
fmeffects: An R Package for Forward Marginal Effects. (arXiv:2310.02008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02008
&lt;/p&gt;
&lt;p&gt;
fmeffects&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#30340;R&#36719;&#20214;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#26377;&#25928;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#35299;&#37322;&#26041;&#27861;&#26368;&#36817;&#34987;&#24341;&#20837;&#12290;&#23427;&#20204;&#20197;&#8220;&#22914;&#26524;&#25105;&#20204;&#23558;$x$&#25913;&#21464;$h$&#65292;&#37027;&#20040;&#39044;&#27979;&#32467;&#26524;$\widehat{y}$&#20250;&#21457;&#29983;&#20160;&#20040;&#21464;&#21270;&#65311;&#8221;&#30340;&#24418;&#24335;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;fmeffects&#36719;&#20214;&#21253;&#65292;&#36825;&#26159;FMEs&#30340;&#31532;&#19968;&#20010;&#36719;&#20214;&#23454;&#29616;&#12290;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#29702;&#35770;&#32972;&#26223;&#12289;&#36719;&#20214;&#21253;&#21151;&#33021;&#21644;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#21450;&#36719;&#20214;&#35774;&#35745;&#21644;&#26410;&#26469;&#25193;&#23637;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forward marginal effects (FMEs) have recently been introduced as a versatile and effective model-agnostic interpretation method. They provide comprehensible and actionable model explanations in the form of: If we change $x$ by an amount $h$, what is the change in predicted outcome $\widehat{y}$? We present the R package fmeffects, the first software implementation of FMEs. The relevant theoretical background, package functionality and handling, as well as the software design and options for future extensions are discussed in this paper.
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>MUSCLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#65288;MUSCLE&#65289;&#30340;&#39044;&#35757;&#32451;&#27969;&#31243;&#65292;&#29992;&#20110;&#22810;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;X-ray&#22270;&#20687;&#30340;&#28145;&#24230;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#27719;&#38598;&#26469;&#33258;&#22810;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;X-ray&#22270;&#20687;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#22810;&#20010;X-ray&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02000</link><description>&lt;p&gt;
MUSCLE: &#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#29992;&#20110;&#22810;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;X-ray&#22270;&#20687;&#30340;&#28145;&#24230;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts. (arXiv:2310.02000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02000
&lt;/p&gt;
&lt;p&gt;
MUSCLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#65288;MUSCLE&#65289;&#30340;&#39044;&#35757;&#32451;&#27969;&#31243;&#65292;&#29992;&#20110;&#22810;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;X-ray&#22270;&#20687;&#30340;&#28145;&#24230;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#27719;&#38598;&#26469;&#33258;&#22810;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;X-ray&#22270;&#20687;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#22810;&#20010;X-ray&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#24050;&#24191;&#27867;&#29992;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#30528;&#30524;&#20110;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#39640;X-ray&#22270;&#20687;&#20998;&#26512;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27969;&#31243;&#65292;&#21517;&#20026;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#65288;MUSCLE&#65289;&#65292;&#29992;&#20110;&#22810;&#31181;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#21644;&#20998;&#21106;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#36523;&#20307;&#37096;&#20301;&#65288;&#21253;&#25324;&#22836;&#37096;&#12289;&#32954;&#37096;&#21644;&#39592;&#39612;&#65289;&#30340;X-ray&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MUSCLE&#36890;&#36807;&#27719;&#38598;&#26469;&#33258;&#22810;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;X-ray&#22270;&#20687;&#36827;&#34892;&#22522;&#20110;MoCo&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#20849;&#21516;&#39044;&#35757;&#32451;&#29992;&#20110;&#21508;&#31181;X-ray&#20998;&#26512;&#20219;&#21153;&#30340;&#39592;&#24178;&#32593;&#32476;&#12290;MUSCLE&#20013;&#20351;&#29992;&#20102;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#23398;&#20064;&#35745;&#21010;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;/&#25968;&#25454;&#38598;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;MUSCLE&#22312;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;X-ray&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-supervised learning (SSL) algorithms have been widely used to pre-train deep models, few efforts [11] have been done to improve representation learning of X-ray image analysis with SSL pre-trained models. In this work, we study a novel self-supervised pre-training pipeline, namely Multi-task Self-super-vised Continual Learning (MUSCLE), for multiple medical imaging tasks, such as classification and segmentation, using X-ray images collected from multiple body parts, including heads, lungs, and bones. Specifically, MUSCLE aggregates X-rays collected from multiple body parts for MoCo-based representation learning, and adopts a well-designed continual learning (CL) procedure to further pre-train the backbone subject various X-ray analysis tasks jointly. Certain strategies for image pre-processing, learning schedules, and regularization have been used to solve data heterogeneity, overfitting, and catastrophic forgetting problems for multi-task/dataset learning in MUSCLE.We evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01991</link><description>&lt;p&gt;
&#22635;&#31354;&#39064;&#65306;&#25506;&#32034;&#24182;&#22686;&#24378;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#26399;&#30340;&#25991;&#29486;&#20013;&#24191;&#27867;&#25506;&#35752;&#20102;&#27491;&#21521;&#25512;&#29702;&#65288;&#21363;&#32473;&#23450;&#38382;&#39064;&#25214;&#31572;&#26696;&#65289;&#65292;&#20294;&#36870;&#21521;&#25512;&#29702;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#32473;&#23450;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#20854;&#31572;&#26696;&#65292;&#22312;&#38382;&#39064;&#20013;&#26377;&#20123;&#32454;&#33410;&#34987;&#30465;&#30053;&#20102;&#65292;LLM&#33021;&#21542;&#26377;&#25928;&#22320;&#36824;&#21407;&#20986;&#32570;&#22833;&#30340;&#20449;&#24687;&#65311;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#20462;&#25913;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#36825;&#19968;&#20219;&#21153;&#65306;GSM8k&#12289;SVAMP&#21644;MultiArith&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27491;&#21521;&#25512;&#29702;&#30456;&#27604;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#65288;GPT4&#12289;GPT3.5&#12289;PaLM-2&#21644;LLaMa-2&#65289;&#22312;&#36870;&#21521;&#25512;&#29702;&#19978;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#21033;&#29992;&#35813;&#20219;&#21153;&#30340;&#29305;&#23450;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#65306;Rephrase&#23558;&#32473;&#23450;&#30340;&#38382;&#39064;&#37325;&#36848;&#20026;&#19968;&#20010;&#27491;&#21521;&#25512;&#29702;&#38382;&#39064;&#65292;PAL-Tools&#32467;&#21512;&#20102;&#31243;&#24207;&#36741;&#21161;&#30340;LLM&#24605;&#24819;&#65292;&#29983;&#25104;&#19968;&#32452;&#26041;&#31243;&#24335;&#21487;&#20197;&#35299;&#20915;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;XOR&#25968;&#25454;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;ReLU&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01975</link><description>&lt;p&gt;
&#20004;&#23618;ReLU&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;XOR&#25968;&#25454;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data. (arXiv:2310.01975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;XOR&#25968;&#25454;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;ReLU&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#39640;&#24230;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#65292;&#20197;&#20415;&#33021;&#22815;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#31181;&#36807;&#25311;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20173;&#28982;&#33021;&#22815;&#36798;&#21040;&#24456;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#29616;&#35937;&#65292;&#26368;&#36817;&#19968;&#31995;&#21015;&#30340;&#24037;&#20316;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#26512;&#22823;&#22810;&#20173;&#38480;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#20026;&#32447;&#24615;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;XOR&#31867;&#22411;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#26631;&#31614;&#32763;&#36716;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#20449;&#22122;&#27604;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;ReLU&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#32467;&#26524;&#65292;&#34920;&#26126;&#24403;&#28385;&#36275;&#21069;&#36848;&#26465;&#20214;&#19981;&#25104;&#31435;&#26102;&#65292;&#25152;&#33719;&#24471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#30456;&#24046;&#19968;&#20010;&#32477;&#23545;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning models are usually highly over-parameterized so that they can overfit the training data. Surprisingly, such overfitting neural networks can usually still achieve high prediction accuracy. To study this "benign overfitting" phenomenon, a line of recent works has theoretically studied the learning of linear models and two-layer neural networks. However, most of these analyses are still limited to the very simple learning problems where the Bayes-optimal classifier is linear. In this work, we investigate a class of XOR-type classification tasks with label-flipping noises. We show that, under a certain condition on the sample complexity and signal-to-noise ratio, an over-parameterized ReLU CNN trained by gradient descent can achieve near Bayes-optimal accuracy. Moreover, we also establish a matching lower bound result showing that when the previous condition is not satisfied, the prediction accuracy of the obtained CNN is an absolute constant away from the Bayes-optima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#26041;&#24335;&#19979;&#35745;&#31639;&#20004;&#20010;&#20998;&#24067;&#30340;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#20960;&#20309;&#24615;&#36136;&#21644;&#27979;&#22320;&#32447;&#24615;&#36136;&#65292;&#36890;&#36807;&#25805;&#20316;&#21644;&#20132;&#25442;&#27979;&#22320;&#32447;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#26469;&#36924;&#36817;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2310.01973</link><description>&lt;p&gt;
&#32852;&#21512;&#20998;&#24067;&#24335;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Federated Wasserstein Distance. (arXiv:2310.01973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#26041;&#24335;&#19979;&#35745;&#31639;&#20004;&#20010;&#20998;&#24067;&#30340;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#20960;&#20309;&#24615;&#36136;&#21644;&#27979;&#22320;&#32447;&#24615;&#36136;&#65292;&#36890;&#36807;&#25805;&#20316;&#21644;&#20132;&#25442;&#27979;&#22320;&#32447;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#26469;&#36924;&#36817;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#26041;&#24335;&#19979;&#35745;&#31639;&#20004;&#20010;&#20998;&#24067;&#30340;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19981;&#35775;&#38382;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20013;&#22830;&#23454;&#20307;/&#26381;&#21153;&#22120;&#21327;&#35843;&#35745;&#31639;&#65292;&#20272;&#35745;&#23384;&#20648;&#22312;&#19981;&#21516;&#35774;&#22791;/&#23458;&#25143;&#31471;&#19978;&#30340;&#20004;&#20010;&#26679;&#26412;&#30340;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#23588;&#20854;&#26159;&#19977;&#35282;&#19981;&#31561;&#24335;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#27979;&#22320;&#32447;&#24615;&#36136;&#65306;&#25105;&#20204;&#30340;&#31639;&#27861;FedWad&#65288;&#32852;&#21512;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#65289;&#36890;&#36807;&#25805;&#20316;&#21644;&#20132;&#25442;&#27979;&#22320;&#32447;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#26469;&#36845;&#20195;&#36924;&#36817;&#29926;&#22622;&#26031;&#22374;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#36755;&#20837;&#26679;&#26412;&#12290;&#38500;&#20102;&#24314;&#31435;FedWad&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22312;&#32852;&#21512;&#26680;&#24515;&#38598;&#21644;&#32852;&#21512;&#26368;&#20248;&#20256;&#36755;&#25968;&#25454;&#38598;&#36317;&#31163;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20998;&#21035;&#29992;&#20110;&#26500;&#24314;&#26032;&#30340;&#32852;&#21512;&#27169;&#22411;&#21644;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a principled way of computing the Wasserstein distance between two distributions in a federated manner. Namely, we show how to estimate the Wasserstein distance between two samples stored and kept on different devices/clients whilst a central entity/server orchestrates the computations (again, without having access to the samples). To achieve this feat, we take advantage of the geometric properties of the Wasserstein distance -- in particular, the triangle inequality -- and that of the associated {\em geodesics}: our algorithm, FedWad (for Federated Wasserstein Distance), iteratively approximates the Wasserstein distance by manipulating and exchanging distributions from the space of geodesics in lieu of the input samples. In addition to establishing the convergence properties of FedWad, we provide empirical results on federated coresets and federate optimal transport dataset distance, that we respectively exploit for building a novel federated model and for boosting perfor
&lt;/p&gt;</description></item><item><title>&#27969;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#21270;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01972</link><description>&lt;p&gt;
&#27969;&#34892;&#23398;&#20064;&#65306;&#36890;&#36807;&#38543;&#26426;&#36890;&#20449;&#22686;&#24378;&#20998;&#25955;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Epidemic Learning: Boosting Decentralized Learning with Randomized Communication. (arXiv:2310.01972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01972
&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#21270;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27969;&#34892;&#23398;&#20064;&#65288;EL&#65289;&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#19981;&#26029;&#21464;&#21270;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#27604;&#20256;&#32479;&#30340;DL&#26041;&#27861;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;EL&#30340;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#23558;&#20854;&#27169;&#22411;&#26356;&#26032;&#21457;&#36865;&#32473;&#19968;&#20010;&#38543;&#26426;&#26679;&#26412;&#30340;$s$&#20010;&#20854;&#20182;&#33410;&#28857;&#65288;&#22312;$n$&#20010;&#33410;&#28857;&#30340;&#31995;&#32479;&#20013;&#65289;&#12290;&#25105;&#20204;&#23545;EL&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#21464;&#21270;&#30340;&#25299;&#25169;&#32467;&#26500;&#23548;&#33268;&#20102;&#27604;&#29616;&#26377;&#30340;&#65288;&#38745;&#24577;&#21644;&#21160;&#24577;&#65289;&#25299;&#25169;&#32467;&#26500;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23545;&#20110;&#24179;&#28369;&#30340;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;EL&#30340;&#26242;&#24577;&#36845;&#20195;&#27425;&#25968;&#65292;&#21363;&#36798;&#21040;&#28176;&#36817;&#32447;&#24615;&#21152;&#36895;&#25152;&#38656;&#30340;&#36718;&#25968;&#65292;&#26159;$\mathcal{O}(\frac{n^3}{s^2})$&#65292;&#36229;&#36807;&#20102;&#24050;&#30693;&#30340;&#26368;&#20339;&#30028;&#38480;$\mathcal{O}({n^3})$&#65292;&#22686;&#21152;&#20102;$s^2$&#20493;&#65292;&#34920;&#26126;&#20102;&#38543;&#26426;&#36890;&#20449;&#22312;DL&#20013;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;96&#20010;&#33410;&#28857;&#30340;&#32593;&#32476;&#20013;&#23545;EL&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;DL&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;EL&#36798;&#21040;&#20102;&#26356;&#24555;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $\mathcal{O}(\frac{n^3}{s^2})$ which outperforms the best-known bound $\mathcal{O}({n^3})$ by a factor of $ s^2 $, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#33410;&#32422;&#25968;&#25454;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#22240;&#20026;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#25915;&#20987;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#23545;&#25915;&#20987;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.01959</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;&#31070;&#35861;&#65306;&#20160;&#20040;&#26159;&#27169;&#22411;&#31363;&#21462;&#30340;&#21547;&#20041;&#65311;
&lt;/p&gt;
&lt;p&gt;
Beyond Labeling Oracles: What does it mean to steal ML models?. (arXiv:2310.01959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#33410;&#32422;&#25968;&#25454;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#22240;&#20026;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#25915;&#20987;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#23545;&#25915;&#20987;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#38480;&#26469;&#31363;&#21462;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#36890;&#36807;ML-as-a-Service&#25552;&#20379;&#30340;API&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;&#25968;&#25454;&#38590;&#20197;&#33719;&#21462;&#65292;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#27492;&#27169;&#22411;&#25552;&#21462;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#22312;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26356;&#23569;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#33719;&#21462;&#27169;&#22411;&#12290;&#20851;&#20110;&#27169;&#22411;&#25552;&#21462;&#30340;&#25991;&#29486;&#26222;&#36941;&#22768;&#31216;&#25110;&#20551;&#35774;&#25915;&#20987;&#32773;&#33021;&#22815;&#33410;&#32422;&#25968;&#25454;&#33719;&#21462;&#21644;&#26631;&#27880;&#25104;&#26412;&#12290;&#28982;&#32780;&#25105;&#20204;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#24403;&#21069;&#30340;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#25915;&#20987;&#32773;&#33021;&#22815;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#23545;&#24433;&#21709;&#27169;&#22411;&#25552;&#21462;&#25104;&#21151;&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#23545;&#21463;&#23475;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#21363;&#23545;&#20998;&#24067;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#27604;&#25915;&#20987;&#31574;&#30053;&#65288;&#20915;&#23450;&#21521;&#21463;&#23475;&#32773;&#27169;&#22411;API&#21457;&#20986;&#21738;&#20123;&#26597;&#35810;&#65289;&#31561;&#20854;&#20182;&#22240;&#32032;&#26356;&#20026;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#24076;&#26395;&#24320;&#21457;&#21516;&#31561;&#27700;&#24179;&#30340;&#25915;&#20987;&#32773;&#26356;&#37325;&#35201;&#30340;&#26159;&#33719;&#21462;&#23545;&#20998;&#24067;&#25968;&#25454;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction attacks are designed to steal trained models with only query access, as is often provided through APIs that ML-as-a-Service providers offer. ML models are expensive to train, in part because data is hard to obtain, and a primary incentive for model extraction is to acquire a model while incurring less cost than training from scratch. Literature on model extraction commonly claims or presumes that the attacker is able to save on both data acquisition and labeling costs. We show that the attacker often does not. This is because current attacks implicitly rely on the adversary being able to sample from the victim model's data distribution. We thoroughly evaluate factors influencing the success of model extraction. We discover that prior knowledge of the attacker, i.e. access to in-distribution data, dominates other factors like the attack policy the adversary follows to choose which queries to make to the victim model API. Thus, an adversary looking to develop an equally 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#21040;&#36798;-&#36991;&#20813;&#27010;&#29575;&#21644;&#21512;&#25104;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#38388;&#20256;&#25773;&#21644;&#21521;&#21518;&#36882;&#24402;&#25216;&#26415;&#65292;&#35745;&#31639;&#20986;&#20102;&#27010;&#29575;&#30340;&#19979;&#30028;&#20316;&#20026;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.01951</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#24615;&#21040;&#36798;-&#36991;&#20813;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Reach-Avoid for Bayesian Neural Networks. (arXiv:2310.01951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#21040;&#36798;-&#36991;&#20813;&#27010;&#29575;&#21644;&#21512;&#25104;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#38388;&#20256;&#25773;&#21644;&#21521;&#21518;&#36882;&#24402;&#25216;&#26415;&#65292;&#35745;&#31639;&#20986;&#20102;&#27010;&#29575;&#30340;&#19979;&#30028;&#20316;&#20026;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21516;&#26102;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#24182;&#32508;&#21512;&#20986;&#36866;&#29992;&#20110;&#20854;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#30830;&#20445;&#36890;&#36807;&#31574;&#30053;&#20316;&#20986;&#30340;&#24207;&#21015;&#20915;&#31574;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#26159;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#38382;&#39064;&#65306;&#31532;&#19968;&#65292;&#23545;&#30001;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#25551;&#36848;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#39044;&#27979;&#30340;&#21040;&#36798;-&#36991;&#20813;&#27010;&#29575;&#30340;&#35745;&#31639;&#65307;&#31532;&#20108;&#65292;&#21512;&#25104;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20197;&#28385;&#36275;&#32473;&#23450;&#30340;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#65288;&#36798;&#21040;&#8220;&#30446;&#26631;&#8221;&#29366;&#24577;&#65292;&#21516;&#26102;&#36991;&#20813;&#19968;&#32452;&#8220;&#19981;&#23433;&#20840;&#8221;&#29366;&#24577;&#65289;&#21644;&#23398;&#20064;&#30340;BNN&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#21306;&#38388;&#20256;&#25773;&#21644;&#21521;&#21518;&#36882;&#24402;&#25216;&#26415;&#26469;&#35745;&#31639;&#31574;&#30053;&#21160;&#20316;&#24207;&#21015;&#28385;&#36275;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#19979;&#30028;&#12290;&#36825;&#26679;&#35745;&#31639;&#20986;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning seeks to simultaneously learn the dynamics of an unknown stochastic environment and synthesise an optimal policy for acting in it. Ensuring the safety and robustness of sequential decisions made through a policy in such an environment is a key challenge for policies intended for safety-critical scenarios. In this work, we investigate two complementary problems: first, computing reach-avoid probabilities for iterative predictions made with dynamical models, with dynamics described by Bayesian neural network (BNN); second, synthesising control policies that are optimal with respect to a given reach-avoid specification (reaching a "target" state, while avoiding a set of "unsafe" states) and a learned BNN model. Our solution leverages interval propagation and backward recursion techniques to compute lower bounds for the probability that a policy's sequence of actions leads to satisfying the reach-avoid specification. Such computed lower bounds provide saf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Supervised Contrastive&#65288;SupCon&#65289;&#23398;&#20064;&#30340;&#24378;&#22823;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;OOD&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;SupCon&#25439;&#22833;&#30340;&#23545;&#27604;&#39033;&#26469;&#23454;&#29616;&#12290;&#24403;&#36741;&#21161;OOD&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#28151;&#21512;&#25216;&#26415;&#26469;&#29983;&#25104;&#20266;OOD&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01942</link><description>&lt;p&gt;
OOD&#24863;&#30693;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OOD Aware Supervised Contrastive Learning. (arXiv:2310.01942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Supervised Contrastive&#65288;SupCon&#65289;&#23398;&#20064;&#30340;&#24378;&#22823;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;OOD&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;SupCon&#25439;&#22833;&#30340;&#23545;&#27604;&#39033;&#26469;&#23454;&#29616;&#12290;&#24403;&#36741;&#21161;OOD&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#28151;&#21512;&#25216;&#26415;&#26469;&#29983;&#25104;&#20266;OOD&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#36827;&#34892;&#35782;&#21035;&#30340;&#23433;&#20840;&#37096;&#32626;&#20013;&#65292;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;OOD&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#29992;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#35757;&#32451;&#30340;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#35797;&#22270;&#35299;&#20915;&#20854;&#20013;&#22266;&#26377;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29992;Supervised Contrastive&#65288;SupCon&#65289;&#35757;&#32451;&#23398;&#21040;&#30340;&#24378;&#22823;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;OOD&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#39069;&#22806;&#30340;&#23545;&#27604;&#39033;&#25193;&#23637;&#20102;SupCon&#25439;&#22833;&#12290;&#31532;&#19968;&#20010;&#39033;&#23558;&#36741;&#21161;OOD&#34920;&#31034;&#19982;ID&#34920;&#31034;&#20998;&#31163;&#65292;&#32780;&#19981;&#23545;&#36741;&#21161;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26045;&#21152;&#20219;&#20309;&#32422;&#26463;&#12290;&#31532;&#20108;&#20010;&#39033;&#23558;OOD&#29305;&#24449;&#36828;&#31163;&#29616;&#26377;&#30340;&#31867;&#21035;&#21407;&#22411;&#65292;&#21516;&#26102;&#23558;ID&#34920;&#31034;&#25512;&#21521;&#20854;&#30456;&#23545;&#24212;&#30340;&#31867;&#21035;&#21407;&#22411;&#12290;&#24403;&#36741;&#21161;OOD&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#28151;&#21512;&#25216;&#26415;&#26469;&#39640;&#25928;&#29983;&#25104;&#20266;OOD&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26465;&#20214;&#21069;&#38376;&#35843;&#25972;&#21644;&#21487;&#36776;&#35782;&#24615;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25918;&#26494;&#38480;&#21046;&#21644;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20445;&#35777;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01937</link><description>&lt;p&gt;
Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder
&lt;/p&gt;
&lt;p&gt;
Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder. (arXiv:2310.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26465;&#20214;&#21069;&#38376;&#35843;&#25972;&#21644;&#21487;&#36776;&#35782;&#24615;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25918;&#26494;&#38480;&#21046;&#21644;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20445;&#35777;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#24403;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#21069;&#38376;&#35843;&#25972;&#26159;&#22788;&#29702;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#21069;&#38376;&#35843;&#25972;&#30340;&#38480;&#21046;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#28385;&#36275;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#26465;&#20214;&#21069;&#38376;&#35843;&#25972;&#30340;&#27010;&#24565;&#24182;&#21457;&#23637;&#20445;&#35777;&#26465;&#20214;&#21069;&#38376;&#35843;&#25972;&#30340;&#22240;&#26524;&#25928;&#24212;&#21487;&#36776;&#35782;&#24615;&#30340;&#23450;&#29702;&#65292;&#25918;&#26494;&#20102;&#19968;&#20123;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26080;&#27861;&#32473;&#23450;&#19968;&#20010;&#26465;&#20214;&#21069;&#38376;&#21464;&#37327;&#65292;&#22240;&#27492;&#24076;&#26395;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#35813;&#21464;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CFDiVAE&#26469;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26465;&#20214;&#21069;&#38376;&#35843;&#25972;&#21464;&#37327;&#30340;&#34920;&#31034;&#65292;&#24182;&#27491;&#24335;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
An essential and challenging problem in causal inference is causal effect estimation from observational data. The problem becomes more difficult with the presence of unobserved confounding variables. The front-door adjustment is a practical approach for dealing with unobserved confounding variables. However, the restriction for the standard front-door adjustment is difficult to satisfy in practice. In this paper, we relax some of the restrictions by proposing the concept of conditional front-door (CFD) adjustment and develop the theorem that guarantees the causal effect identifiability of CFD adjustment. Furthermore, as it is often impossible for a CFD variable to be given in practice, it is desirable to learn it from data. By leveraging the ability of deep generative models, we propose CFDiVAE to learn the representation of the CFD adjustment variable directly from data with the identifiable Variational AutoEncoder and formally prove the model identifiability. Extensive experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>RoFormer&#27169;&#22359;&#21487;&#20197;&#20026;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#24863;&#30693;&#22810;&#23454;&#20363;&#23398;&#20064;&#25552;&#20379;&#20934;&#30830;&#33258;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#23545;&#34917;&#19969;&#29420;&#31435;&#24615;&#21644;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#24037;&#20316;&#37327;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01924</link><description>&lt;p&gt;
RoFormer&#23545;&#20110;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20301;&#32622;&#24863;&#30693;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification. (arXiv:2310.01924v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01924
&lt;/p&gt;
&lt;p&gt;
RoFormer&#27169;&#22359;&#21487;&#20197;&#20026;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#24863;&#30693;&#22810;&#23454;&#20363;&#23398;&#20064;&#25552;&#20379;&#20934;&#30830;&#33258;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#23545;&#34917;&#19969;&#29420;&#31435;&#24615;&#21644;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#24037;&#20316;&#37327;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22270;&#20687;&#30340;&#20960;&#20010;&#21313;&#20159;&#20687;&#32032;&#35268;&#27169;&#23545;&#20110;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20855;&#26377;&#20923;&#32467;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#12290;&#37492;&#20110;&#27599;&#20010;&#22270;&#20687;&#20013;&#23454;&#20363;&#30340;&#25968;&#37327;&#36739;&#39640;&#65292;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#34917;&#19969;&#30340;&#29420;&#31435;&#24615;&#21644;&#25490;&#24207;&#19981;&#21464;&#24615;&#65292;&#24573;&#30053;&#20102;&#32452;&#32455;&#32467;&#26500;&#21644;&#34917;&#19969;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#30740;&#31350;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#36825;&#31181;&#22823;&#37327;&#20196;&#29260;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#12290;&#23588;&#20854;&#26159;&#34917;&#19969;&#30340;&#30456;&#23545;&#20301;&#32622;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#30452;&#25509;&#30340;&#32534;&#30721;&#27169;&#22359;&#65292;&#21363;RoFormer&#23618;&#65292;&#20381;&#38752;&#20869;&#23384;&#39640;&#25928;&#30340;&#20934;&#30830;&#33258;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#12290;&#35813;&#27169;&#22359;&#21487;&#20197;&#22312;&#22823;&#22411;&#21644;&#20219;&#24847;&#24418;&#29366;&#30340;&#24187;&#28783;&#29255;&#34917;&#19969;&#19978;&#36827;&#34892;&#23436;&#20840;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whole slide image (WSI) classification is a critical task in computational pathology. However, the gigapixel-size of such images remains a major challenge for the current state of deep-learning. Current methods rely on multiple-instance learning (MIL) models with frozen feature extractors. Given the the high number of instances in each image, MIL methods have long assumed independence and permutation-invariance of patches, disregarding the tissue structure and correlation between patches. Recent works started studying this correlation between instances but the computational workload of such a high number of tokens remained a limiting factor. In particular, relative position of patches remains unaddressed. We propose to apply a straightforward encoding module, namely a RoFormer layer , relying on memory-efficient exact self-attention and relative positional encoding. This module can perform full self-attention with relative position encoding on patches of large and arbitrary shaped WSIs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;UWF-CFP&#21644;OCTA&#22270;&#20687;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#26469;&#25913;&#36827;&#33258;&#21160;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65292;&#23558;2D UWF-CFP&#22270;&#20687;&#21644;3D&#39640;&#20998;&#36776;&#29575;6x6 mm^3 OCTA&#22270;&#20687;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#37319;&#29992;ResNet50&#21644;3D-ResNet50&#27169;&#22411;&#30340;&#34701;&#21512;&#20197;&#21450;Squeeze-and-Excitation&#65288;SE&#65289;&#22359;&#26469;&#22686;&#24378;&#30456;&#20851;&#29305;&#24449;&#65292;&#21516;&#26102;&#37319;&#29992;&#22810;&#27169;&#24577;Manifold Mixup&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01912</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;UWF-CFP&#21644;OCTA&#22270;&#20687;&#25913;&#36827;&#20102;&#33258;&#21160;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improved Automatic Diabetic Retinopathy Severity Classification Using Deep Multimodal Fusion of UWF-CFP and OCTA Images. (arXiv:2310.01912v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;UWF-CFP&#21644;OCTA&#22270;&#20687;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#26469;&#25913;&#36827;&#33258;&#21160;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65292;&#23558;2D UWF-CFP&#22270;&#20687;&#21644;3D&#39640;&#20998;&#36776;&#29575;6x6 mm^3 OCTA&#22270;&#20687;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#37319;&#29992;ResNet50&#21644;3D-ResNet50&#27169;&#22411;&#30340;&#34701;&#21512;&#20197;&#21450;Squeeze-and-Excitation&#65288;SE&#65289;&#22359;&#26469;&#22686;&#24378;&#30456;&#20851;&#29305;&#24449;&#65292;&#21516;&#26102;&#37319;&#29992;&#22810;&#27169;&#24577;Manifold Mixup&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24120;&#35265;&#19988;&#20005;&#37325;&#30340;&#24182;&#21457;&#30151;&#65292;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#65292;&#22240;&#27492;&#38656;&#35201;&#20934;&#30830;&#21450;&#26102;&#30340;&#35786;&#26029;&#12290;&#26368;&#36817;&#30340;&#25104;&#20687;&#25216;&#26415;&#36827;&#23637;&#65292;&#22914;&#36229;&#24191;&#35282;&#24425;&#33394;&#30524;&#24213;&#29031;&#30456;&#65288;UWF-CFP&#65289;&#21644;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#65288;OCTA&#65289;&#65292;&#20026;&#26089;&#26399;&#26816;&#27979;DR&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#25968;&#25454;&#19981;&#19968;&#33268;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#25104;&#20687;&#27169;&#24335;&#26174;&#33879;&#25552;&#39640;DR&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;ResNet50&#21644;3D-ResNet50&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#20351;&#29992;Squeeze-and-Excitation&#65288;SE&#65289;&#22359;&#26469;&#22686;&#24378;&#30456;&#20851;&#29305;&#24449;&#65292;&#23558;2D UWF-CFP&#22270;&#20687;&#21644;3D&#39640;&#20998;&#36776;&#29575;6x6 mm^3 OCTA&#65288;&#32467;&#26500;&#21644;&#27969;&#65289;&#22270;&#20687;&#36827;&#34892;&#25972;&#21512;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#22686;&#21152;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#23558;&#22810;&#27169;&#24577;Manifold Mixup&#24212;&#29992;&#20110;&#36830;&#25509;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetic Retinopathy (DR), a prevalent and severe complication of diabetes, affects millions of individuals globally, underscoring the need for accurate and timely diagnosis. Recent advancements in imaging technologies, such as Ultra-WideField Color Fundus Photography (UWF-CFP) imaging and Optical Coherence Tomography Angiography (OCTA), provide opportunities for the early detection of DR but also pose significant challenges given the disparate nature of the data they produce. This study introduces a novel multimodal approach that leverages these imaging modalities to notably enhance DR classification. Our approach integrates 2D UWF-CFP images and 3D high-resolution 6x6 mm$^3$ OCTA (both structure and flow) images using a fusion of ResNet50 and 3D-ResNet50 models, with Squeeze-and-Excitation (SE) blocks to amplify relevant features. Additionally, to increase the model's generalization capabilities, a multimodal extension of Manifold Mixup, applied to concatenated multimodal features, i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36229;&#36234;&#22522;&#20934;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;HMDB-AD&#21644;HMDB-Violence&#65292;&#25361;&#25112;&#20855;&#26377;&#22810;&#26679;&#21270;&#21160;&#20316;&#24322;&#24120;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22810;&#24103;&#24322;&#24120;&#26816;&#27979;&#65288;MFAD&#65289;&#65292;&#23427;&#24314;&#31435;&#22312;AI-VAD&#26694;&#26550;&#19978;&#65292;&#32467;&#21512;&#20102;&#21333;&#24103;&#21644;&#20004;&#24103;&#29305;&#24449;&#65292;&#20197;&#31639;&#27861;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.01904</link><description>&lt;p&gt;
&#36229;&#36234;&#22522;&#20934;&#65306;&#26816;&#27979;&#35270;&#39057;&#20013;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Beyond the Benchmark: Detecting Diverse Anomalies in Videos. (arXiv:2310.01904v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36229;&#36234;&#22522;&#20934;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;HMDB-AD&#21644;HMDB-Violence&#65292;&#25361;&#25112;&#20855;&#26377;&#22810;&#26679;&#21270;&#21160;&#20316;&#24322;&#24120;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22810;&#24103;&#24322;&#24120;&#26816;&#27979;&#65288;MFAD&#65289;&#65292;&#23427;&#24314;&#31435;&#22312;AI-VAD&#26694;&#26550;&#19978;&#65292;&#32467;&#21512;&#20102;&#21333;&#24103;&#21644;&#20004;&#24103;&#29305;&#24449;&#65292;&#20197;&#31639;&#27861;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#65288;VAD&#65289;&#22312;&#29616;&#20195;&#30417;&#25511;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26088;&#22312;&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#24773;&#20917;&#20013;&#30340;&#21508;&#31181;&#24322;&#24120;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20027;&#35201;&#24378;&#35843;&#31616;&#21333;&#30340;&#21333;&#24103;&#24322;&#24120;&#65292;&#22914;&#26032;&#29289;&#20307;&#26816;&#27979;&#12290;&#36825;&#31181;&#29421;&#31364;&#30340;&#28966;&#28857;&#38480;&#21046;&#20102;VAD&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#25193;&#22823;VAD&#30740;&#31350;&#33539;&#22260;&#65292;&#21253;&#25324;&#36229;&#36234;&#20256;&#32479;&#22522;&#20934;&#36793;&#30028;&#30340;&#22797;&#26434;&#24322;&#24120;&#29616;&#35937;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;HMDB-AD&#21644;HMDB-Violence&#65292;&#29992;&#20110;&#25361;&#25112;&#20855;&#26377;&#22810;&#26679;&#21270;&#21160;&#20316;&#24322;&#24120;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#20174;HMDB51&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#27966;&#29983;&#32780;&#26469;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22810;&#24103;&#24322;&#24120;&#26816;&#27979;&#65288;MFAD&#65289;&#65292;&#23427;&#24314;&#31435;&#22312;AI-VAD&#26694;&#26550;&#19978;&#12290;AI-VAD&#21033;&#29992;&#21333;&#24103;&#29305;&#24449;&#65292;&#22914;&#23039;&#21183;&#20272;&#35745;&#21644;&#28145;&#24230;&#22270;&#20687;&#32534;&#30721;&#65292;&#20197;&#21450;&#20004;&#24103;&#29305;&#24449;&#65292;&#22914;&#29289;&#20307;&#36895;&#24230;&#12290;&#28982;&#21518;&#24212;&#29992;&#23494;&#24230;&#20272;&#35745;&#31639;&#27861;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Anomaly Detection (VAD) plays a crucial role in modern surveillance systems, aiming to identify various anomalies in real-world situations. However, current benchmark datasets predominantly emphasize simple, single-frame anomalies such as novel object detection. This narrow focus restricts the advancement of VAD models. In this research, we advocate for an expansion of VAD investigations to encompass intricate anomalies that extend beyond conventional benchmark boundaries. To facilitate this, we introduce two datasets, HMDB-AD and HMDB-Violence, to challenge models with diverse action-based anomalies. These datasets are derived from the HMDB51 action recognition dataset. We further present Multi-Frame Anomaly Detection (MFAD), a novel method built upon the AI-VAD framework. AI-VAD utilizes single-frame features such as pose estimation and deep image encoding, and two-frame features such as object velocity. They then apply a density estimation algorithm to compute anomaly scores. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01892</link><description>&lt;p&gt;
FiGURe: &#20351;&#29992;&#36807;&#28388;&#22120;&#22686;&#24378;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations. (arXiv:2310.01892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#25311;&#20302;&#36890;&#28388;&#27874;&#22120;&#30340;&#22686;&#24378;&#65292;&#38480;&#21046;&#20102;&#22312;&#38656;&#35201;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#36807;&#28388;&#22120;&#30340;&#22686;&#24378;&#26041;&#27861;&#26469;&#25429;&#25417;&#29305;&#24449;&#39057;&#35889;&#30340;&#19981;&#21516;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#26435;&#37325;&#26159;&#21487;&#33021;&#30340;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#38656;&#35201;&#39640;&#32500;&#34920;&#31034;&#12290;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#25968;&#25454;&#26102;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#22810;&#20010;&#22686;&#24378;&#26041;&#27861;&#26102;&#65292;&#22686;&#21152;&#20102;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#24674;&#22797;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; FiGURe &#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe achieves an ave
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.01886</link><description>&lt;p&gt;
&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#32447;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#31181;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20063;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#32791;&#26102;&#19988;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20250;&#32473;&#23384;&#20648;&#21644;&#26381;&#21153;&#24102;&#26469;&#24040;&#22823;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#26080;&#38656;&#35757;&#32451;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#23558;&#22810;&#20010;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#37325;&#22797;&#20351;&#29992;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#12290;&#38024;&#23545;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PERU-FFT&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#20219;&#21153;&#21521;&#37327;&#27880;&#20837;&#21040;&#19968;&#20010;mer&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20307;&#21487;&#36870;&#32593;&#32476;&#21644;&#21464;&#37327;&#22686;&#24378;&#31574;&#30053;&#32467;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#34928;&#20943;&#26657;&#27491;&#30340;PET&#22270;&#20687;&#29983;&#25104;&#36830;&#32493;&#20540;CT&#22270;&#20687;&#65292;&#20197;&#23454;&#29616;&#20840;&#25968;&#23383;&#33041;PET&#34928;&#20943;&#26657;&#27491;&#12290;</title><link>http://arxiv.org/abs/2310.01885</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20307;&#21487;&#36870;&#32593;&#32476;&#29983;&#25104;&#20840;&#25968;&#23383;&#33041;PET&#34928;&#20943;&#26657;&#27491;&#30340;&#21512;&#25104;CT
&lt;/p&gt;
&lt;p&gt;
Synthetic CT Generation via Variant Invertible Network for All-digital Brain PET Attenuation Correction. (arXiv:2310.01885v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20307;&#21487;&#36870;&#32593;&#32476;&#21644;&#21464;&#37327;&#22686;&#24378;&#31574;&#30053;&#32467;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#34928;&#20943;&#26657;&#27491;&#30340;PET&#22270;&#20687;&#29983;&#25104;&#36830;&#32493;&#20540;CT&#22270;&#20687;&#65292;&#20197;&#23454;&#29616;&#20840;&#25968;&#23383;&#33041;PET&#34928;&#20943;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34928;&#20943;&#26657;&#27491;&#65288;AC&#65289;&#23545;&#20110;&#29983;&#25104;&#26080;&#20266;&#36857;&#21644;&#23450;&#37327;&#20934;&#30830;&#30340;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#22270;&#20687;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;PET&#30340;AC&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21253;&#25324;&#25195;&#25551;&#38388;&#30340;&#36816;&#21160;&#21644;&#23558;&#32467;&#26500;&#24615;&#20307;&#32032;&#24378;&#24230;&#38169;&#35823;&#36716;&#25442;&#20026;PET&#34928;&#20943;&#26657;&#27491;&#22240;&#23376;&#12290;&#22914;&#20170;&#65292;&#22312;PET&#19982;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#30456;&#32467;&#21512;&#30340;&#35774;&#22791;&#21830;&#19994;&#21270;&#20043;&#21518;&#65292;&#29992;&#20110;&#23450;&#37327;PET&#30340;AC&#38382;&#39064;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#26080;&#35299;&#21078;&#25104;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;PET AC&#30340;&#21487;&#34892;&#24615;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;PET AC&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#38750;&#34928;&#20943;&#26657;&#27491;&#30340;PET&#22270;&#20687;&#20013;&#29983;&#25104;&#36830;&#32493;&#20540;CT&#22270;&#20687;&#20197;&#36827;&#34892;&#33041;PET&#25104;&#20687;&#30340;AC&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21464;&#37327;&#22686;&#24378;&#31574;&#30053;&#30340;&#21487;&#36870;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#21452;&#21521;&#25512;&#26029;&#36807;&#31243;&#65292;&#29992;&#20110;&#21512;&#25104;CT&#29983;&#25104;&#65288;IVNAC&#65289;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#35758;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attenuation correction (AC) is essential for the generation of artifact-free and quantitatively accurate positron emission tomography (PET) images. However, AC of PET faces challenges including inter-scan motion and erroneous transformation of structural voxel-intensities to PET attenuation-correction factors. Nowadays, the problem of AC for quantitative PET have been solved to a large extent after the commercial availability of devices combining PET with computed tomography (CT). Meanwhile, considering the feasibility of a deep learning approach for PET AC without anatomical imaging, this paper develops a PET AC method, which uses deep learning to generate continuously valued CT images from non-attenuation corrected PET images for AC on brain PET imaging. Specifically, an invertible network combined with the variable augmentation strategy that can achieve the bidirectional inference processes is proposed for synthetic CT generation (IVNAC). To evaluate the performance of the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01884</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#30340;&#25913;&#36827;VMD&#21644;&#22534;&#21472;Informer&#22312;&#22686;&#24378;&#32929;&#24066;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer. (arXiv:2310.01884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#65288;VMD&#65289;&#12289;&#29305;&#24449;&#24037;&#31243;&#65288;FE&#65289;&#21644;&#22534;&#21472;Informer&#65292;&#24182;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;VMGCformer&#65288;Adam+GC+enhanced informer&#65289;&#65292;&#22312;&#22788;&#29702;&#32929;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#27874;&#21160;&#24615;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#24471;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30456;&#23545;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#20248;&#21270;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#24182;&#20171;&#32461;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#27979;&#24314;&#27169;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative adaptive hybrid model for stock market predictions, leveraging the capabilities of an enhanced Variational Mode Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated with an adaptive loss function. Through rigorous experimentation, the proposed model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates significant proficiency in addressing the intricate dynamics and volatile nature of stock market data. Experimental results, derived from multiple benchmark datasets, underscore the model's superiority in terms of prediction accuracy, responsiveness, and generalization capabilities over traditional and other hybrid models. The research further highlights potential avenues for optimization and introduces future directions to enhance predictive modeling, especially for small enterprises and feature engineering.
&lt;/p&gt;</description></item><item><title>AutoCast++&#26159;&#19968;&#20010;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#30340;&#26032;&#38395;&#25991;&#26723;&#38598;&#21512;&#20013;&#36827;&#34892;&#20107;&#20214;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.01880</link><description>&lt;p&gt;
AutoCast++&#65306;&#21033;&#29992;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#19990;&#30028;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval. (arXiv:2310.01880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01880
&lt;/p&gt;
&lt;p&gt;
AutoCast++&#26159;&#19968;&#20010;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#30340;&#26032;&#38395;&#25991;&#26723;&#38598;&#21512;&#20013;&#36827;&#34892;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#30340;&#23454;&#26102;&#20107;&#20214;&#39044;&#27979;&#22240;&#20854;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#33021;&#25552;&#20379;&#30693;&#24773;&#20915;&#31574;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#22914;&#26102;&#38388;&#24207;&#21015;&#65292;&#32780;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AutoCast++&#65292;&#23427;&#26159;&#19968;&#20010;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#31995;&#32479;&#65292;&#26088;&#22312;&#38024;&#23545;&#24191;&#27867;&#30340;&#26032;&#38395;&#25991;&#26723;&#38598;&#21512;&#36827;&#34892;&#20107;&#20214;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#26681;&#25454;&#38646;-shot&#30340;&#38382;&#39064;-&#27573;&#33853;&#30456;&#20851;&#24615;&#23545;&#25991;&#31456;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#26032;&#38395;&#12290;&#20043;&#21518;&#65292;&#25152;&#36873;&#30340;&#25991;&#31456;&#23558;&#36827;&#34892;&#38646;-shot&#22788;&#29702;&#29992;&#20110;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. Whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. In particular, (Zou et al., 2022) unveils AutoCast, a new benchmark that employs news articles for answering forecasting queries. Nevertheless, existing methods still trail behind human performance. The cornerstone of accurate forecasting, we argue, lies in identifying a concise, yet rich subset of news snippets from a vast corpus. With this motivation, we introduce AutoCast++, a zero-shot ranking-based context retrieval system, tailored to sift through expansive news document collections for event forecasting. Our approach first re-ranks articles based on zero-shot question-passage relevance, honing in on semantically pertinent news. Following this, the chosen articles are subjected to zero-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01875</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#28418;&#31227;&#35843;&#25972;&#23454;&#29616;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#19968;&#23567;&#32452;&#35757;&#32451;&#26679;&#26412;&#26469;&#24694;&#24847;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#29305;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24494;&#35843;&#24320;&#22987;&#65292;&#36890;&#36807;&#23545;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#30340;&#20840;&#38754;&#35780;&#20272;&#26469;&#25506;&#32034;&#26368;&#24120;&#35265;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#19982;&#39640;&#27745;&#26579;&#29575;&#30340;&#26377;&#24076;&#26395;&#30340;&#38450;&#24481;&#32467;&#26524;&#30456;&#27604;&#65292;&#26222;&#36890;&#30340;&#35843;&#25972;&#26041;&#27861;&#22312;&#20302;&#27745;&#26579;&#29575;&#22330;&#26223;&#19979;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#30772;&#22351;&#20102;&#22522;&#20110;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based 
&lt;/p&gt;</description></item><item><title>DeepDecipher&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#27979;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;API&#21644;&#25509;&#21475;&#65292;&#23427;&#36890;&#36807;&#25552;&#20379;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#20351;&#24471;&#23545;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#30340;&#20998;&#26512;&#26356;&#21152;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.01870</link><description>&lt;p&gt;
DeepDecipher: &#35775;&#38382;&#21644;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models. (arXiv:2310.01870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01870
&lt;/p&gt;
&lt;p&gt;
DeepDecipher&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#27979;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;API&#21644;&#25509;&#21475;&#65292;&#23427;&#36890;&#36807;&#25552;&#20379;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#20351;&#24471;&#23545;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#30340;&#20998;&#26512;&#26356;&#21152;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#23545;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#24037;&#20855;&#30340;&#32039;&#36843;&#38656;&#27714;&#36880;&#28176;&#22686;&#21152;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#65292;&#24182;&#19988;&#32570;&#20047;&#20998;&#26512;&#27169;&#22411;&#20869;&#37096;&#30340;&#21487;&#35775;&#38382;&#24037;&#20855;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepDecipher - &#19968;&#31181;&#29992;&#20110;&#25506;&#27979;&#36716;&#25442;&#22120;&#27169;&#22411;MLP&#23618;&#20013;&#31070;&#32463;&#20803;&#30340;API&#21644;&#25509;&#21475;&#12290;DeepDecipher&#23558;LLMs&#30340;&#20808;&#36827;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#36755;&#20986;&#21464;&#24471;&#23481;&#26131;&#33719;&#21462;&#12290;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#36824;&#20351;&#24471;&#23545;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#30340;&#26816;&#26597;&#26356;&#21152;&#30452;&#35266;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;DeepDecipher&#30340;&#35774;&#35745;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20998;&#26512;&#31070;&#32463;&#20803;&#65292;&#27604;&#36739;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#26377;&#20851;&#27169;&#22411;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23558;DeepDecipher&#30340;&#21151;&#33021;&#19982;&#31867;&#20284;&#30340;&#24037;&#20855;&#22914;Neuroscope&#21644;OpenAI&#30340;Neuron Explainer&#36827;&#34892;&#23545;&#27604;&#12290;DeepDecipher&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#20998;&#26512;&#12290;&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;DeepDecipher&#20351;LLMs&#21464;&#24471;&#26356;&#21152;&#36879;&#26126;&#12289;&#21487;&#20449;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, there is an urgent need for interpretable and transparent tools. Current methods are difficult to implement, and accessible tools to analyze model internals are lacking. To bridge this gap, we present DeepDecipher - an API and interface for probing neurons in transformer models' MLP layers. DeepDecipher makes the outputs of advanced interpretability techniques for LLMs readily available. The easy-to-use interface also makes inspecting these complex models more intuitive. This paper outlines DeepDecipher's design and capabilities. We demonstrate how to analyze neurons, compare models, and gain insights into model behavior. For example, we contrast DeepDecipher's functionality with similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher enables efficient, scalable analysis of LLMs. By granting access to state-of-the-art interpretability methods, DeepDecipher makes LLMs more transparent, trustworthy, and safe. Research
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#22312;&#26080;&#38656;&#32447;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.01865</link><description>&lt;p&gt;
&#24102;&#26377;&#34920;&#31034;&#23398;&#20064;&#30340;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Conditional Instrumental Variable Regression with Representation Learning for Causal Inference. (arXiv:2310.01865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#22312;&#26080;&#38656;&#32447;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#23384;&#22312;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20004;&#38454;&#27573;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;TSLS&#65289;&#26041;&#27861;&#21450;&#20854;&#20855;&#26377;&#26631;&#20934;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#30340;&#21464;&#31181;&#24120;&#29992;&#20110;&#28040;&#38500;&#28151;&#28102;&#20559;&#24046;&#65292;&#21253;&#25324;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#32447;&#24615;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26631;&#20934;IV&#26041;&#27861;&#25552;&#20986;&#30340;&#26080;&#28151;&#28102;&#24037;&#20855;&#30340;&#20005;&#26684;&#26465;&#20214;&#23545;&#20110;&#23454;&#36341;&#26469;&#35828;&#22826;&#24378;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#20934;IV&#26041;&#27861;&#65288;&#32447;&#24615;&#20551;&#35774;&#21644;&#20005;&#26684;&#26465;&#20214;&#65289;&#30340;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;IV&#65288;CIV&#65289;&#25918;&#26494;&#26631;&#20934;IV&#30340;&#26080;&#28151;&#28102;&#24037;&#20855;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;CIV&#22238;&#24402;&#19982;&#28151;&#28102;&#22343;&#34913;&#34920;&#31034;&#23398;&#20064;&#65288;CBRL.CIV&#65289;&#65292;&#29992;&#20110;&#21516;&#26102;&#28040;&#38500;&#30001;&#26410;&#35266;&#27979;&#28508;&#21464;&#37327;&#24341;&#36215;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#24179;&#34913;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square (TSLS) method and its variants with a standard instrumental variable (IV) are commonly used to eliminate confounding bias, including the bias caused by unobserved confounders, but they rely on the linearity assumption. Besides, the strict condition of unconfounded instruments posed on a standard IV is too strong to be practical. To address these challenging and practical problems of the standard IV method (linearity assumption and the strict condition), in this paper, we use a conditional IV (CIV) to relax the unconfounded instrument condition of standard IV and propose a non-linear CIV regression with Confounding Balancing Representation Learning, CBRL.CIV, for jointly eliminating the confounding bias from unobserved confounders and balancing the observed confounders, without the linearity assumption. We theoretically de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#35009;&#26799;&#24230;&#24046;&#20540;&#23454;&#29616;&#20102;&#32039;&#33268;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.01860</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#19979;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#38543;&#26426;&#26368;&#23567;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise. (arXiv:2310.01860v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01860
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#35009;&#26799;&#24230;&#24046;&#20540;&#23454;&#29616;&#20102;&#32039;&#33268;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20855;&#26377;&#36731;&#24494;&#22122;&#22768;&#20551;&#35774;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#20998;&#26512;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#24403;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#26102;&#20505;&#65292;&#26799;&#24230;&#21098;&#35009;&#26159;&#25512;&#23548;&#20986;&#33391;&#22909;&#30340;&#39640;&#27010;&#29575;&#20445;&#35777;&#30340;&#20851;&#38190;&#31639;&#27861;&#35201;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#21152;&#20197;&#22788;&#29702;&#22320;&#23454;&#29616;&#65292;&#21098;&#35009;&#25805;&#20316;&#20250;&#30772;&#22351;&#24120;&#29992;&#30340;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;Prox-SGD/Parallel SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#39640;&#27010;&#29575;&#20998;&#26512;&#30340;&#24037;&#20316;&#20165;&#32771;&#34385;&#26080;&#32422;&#26463;&#30340;&#38750;&#20998;&#24067;&#24335;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22797;&#21512;&#24335;/&#20998;&#24067;&#24335;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#24182;&#19981;&#21253;&#25324;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65288;&#22914;&#24378;&#20984;&#38382;&#39064;&#65289;&#65292;&#20063;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#24046;&#20540;&#21098;&#35009;&#30340;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#26032;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#32039;&#33268;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#32467;&#26524;&#65288;&#21253;&#25324;&#20960;&#20046;&#25152;&#26377;&#30340;&#22330;&#26223;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-probability analysis of stochastic first-order optimization methods under mild assumptions on the noise has been gaining a lot of attention in recent years. Typically, gradient clipping is one of the key algorithmic ingredients to derive good high-probability guarantees when the noise is heavy-tailed. However, if implemented na\"ively, clipping can spoil the convergence of the popular methods for composite and distributed optimization (Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason, many works on high-probability analysis consider only unconstrained non-distributed problems, and the existing results for composite/distributed problems do not include some important special cases (like strongly convex problems) and are not optimal. To address this issue, we propose new stochastic methods for composite and distributed optimization based on the clipping of stochastic gradient differences and prove tight high-probability convergence results (including nearly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#39640;&#26031;&#36924;&#36817;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;Kushner&#26041;&#31243;&#65292;&#36890;&#36807;&#20256;&#25773;&#21644;&#36125;&#21494;&#26031;&#26356;&#26032;&#27010;&#29575;&#23494;&#24230;&#30456;&#20851;&#30340;&#20004;&#20010;&#25509;&#36817;&#25439;&#22833;&#65292;&#21033;&#29992;Wasserstein&#24230;&#37327;&#21644;Fisher&#24230;&#37327;&#65292;&#36890;&#36807;&#38544;&#24335;&#26356;&#26032;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#26368;&#21518;&#30340;&#25509;&#36817;&#25439;&#22833;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#28385;&#36275;&#39640;&#26031;&#27969;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25193;&#23637;&#20102;&#32447;&#24615;&#24773;&#20917;&#19979;&#30340;Kalman-Bucy&#21644;Riccati&#27969;&#12290;</title><link>http://arxiv.org/abs/2310.01859</link><description>&lt;p&gt;
&#21464;&#20998;&#39640;&#26031;&#36924;&#36817;Kushner&#26368;&#20248;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Variational Gaussian approximation of the Kushner optimal filter. (arXiv:2310.01859v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#39640;&#26031;&#36924;&#36817;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;Kushner&#26041;&#31243;&#65292;&#36890;&#36807;&#20256;&#25773;&#21644;&#36125;&#21494;&#26031;&#26356;&#26032;&#27010;&#29575;&#23494;&#24230;&#30456;&#20851;&#30340;&#20004;&#20010;&#25509;&#36817;&#25439;&#22833;&#65292;&#21033;&#29992;Wasserstein&#24230;&#37327;&#21644;Fisher&#24230;&#37327;&#65292;&#36890;&#36807;&#38544;&#24335;&#26356;&#26032;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#26368;&#21518;&#30340;&#25509;&#36817;&#25439;&#22833;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#28385;&#36275;&#39640;&#26031;&#27969;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25193;&#23637;&#20102;&#32447;&#24615;&#24773;&#20917;&#19979;&#30340;Kalman-Bucy&#21644;Riccati&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20272;&#35745;&#29702;&#35770;&#20013;&#65292;Kushner&#26041;&#31243;&#25552;&#20379;&#20102;&#32473;&#23450;&#36830;&#32493;&#26102;&#38388;&#35266;&#27979;&#30340;&#21160;&#24577;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#23494;&#24230;&#30340;&#28436;&#21270;&#12290;&#22312;&#25105;&#20204;&#26368;&#36817;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#22788;&#29702;&#30340;&#21464;&#20998;&#39640;&#26031;&#36924;&#36817;&#26469;&#36817;&#20284;&#35299;&#20915;Kushner&#26041;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19982;&#27010;&#29575;&#23494;&#24230;&#30340;&#20256;&#25773;&#21644;&#36125;&#21494;&#26031;&#26356;&#26032;&#30456;&#20851;&#30340;&#20004;&#20010;&#25509;&#36817;&#36817;&#20284;&#25439;&#22833;&#12290;&#31532;&#19968;&#20010;&#26159;&#22522;&#20110;Wasserstein&#24230;&#37327;&#30340;&#25509;&#36817;&#25439;&#22833;&#65292;&#31532;&#20108;&#20010;&#26159;&#22522;&#20110;Fisher&#24230;&#37327;&#30340;&#25509;&#36817;&#25439;&#22833;&#12290;&#36825;&#20010;&#26368;&#21518;&#25509;&#36817;&#25439;&#22833;&#30340;&#35299;&#20915;&#26041;&#26696;&#30001;&#25105;&#20204;&#20043;&#21069;&#25552;&#20986;&#30340;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30340;&#38544;&#24335;&#26356;&#26032;&#32473;&#20986;&#12290;&#36825;&#20004;&#20010;&#21464;&#20998;&#26356;&#26032;&#21487;&#20197;&#34701;&#21512;&#24182;&#35777;&#26126;&#28385;&#36275;&#39640;&#26031;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#19968;&#32452;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#20010;&#39640;&#26031;&#27969;&#19982;&#32447;&#24615;&#24773;&#20917;&#19979;&#30340;Kalman-Bucy&#21644;Riccati&#27969;&#19968;&#33268;&#65292;&#24182;&#22312;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#25512;&#24191;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
In estimation theory, the Kushner equation provides the evolution of the probability density of the state of a dynamical system given continuous-time observations. Building upon our recent work, we propose a new way to approximate the solution of the Kushner equation through tractable variational Gaussian approximations of two proximal losses associated with the propagation and Bayesian update of the probability density. The first is a proximal loss based on the Wasserstein metric and the second is a proximal loss based on the Fisher metric. The solution to this last proximal loss is given by implicit updates on the mean and covariance that we proposed earlier. These two variational updates can be fused and shown to satisfy a set of stochastic differential equations on the Gaussian's mean and covariance matrix. This Gaussian flow is consistent with the Kalman-Bucy and Riccati flows in the linear case and generalize them in the nonlinear one.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#39640;&#32500;&#24230;&#30340;&#22320;&#29699;&#29289;&#29702;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#22312;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01853</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#22312;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model. (arXiv:2310.01853v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#39640;&#32500;&#24230;&#30340;&#22320;&#29699;&#29289;&#29702;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#22312;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#35299;&#20915;&#20102;&#22312;&#32473;&#23450;&#22024;&#26434;&#25110;&#19981;&#23436;&#25972;&#35266;&#27979;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#21160;&#21147;&#31995;&#32479;&#21487;&#34892;&#29366;&#24577;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#65292;&#30001;&#20110;&#22320;&#29699;&#29289;&#29702;&#21160;&#21147;&#31995;&#32479;&#30340;&#39640;&#32500;&#24230;&#24615;&#65292;&#24448;&#24448;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#32500;&#24230;&#65292;&#22240;&#27492;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#65288;SDA&#65289;&#36825;&#19968;&#26032;&#39062;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#27492;&#31867;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#35780;&#20998;&#32593;&#32476;&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data assimilation addresses the problem of identifying plausible state trajectories of dynamical systems given noisy or incomplete observations. In geosciences, it presents challenges due to the high-dimensionality of geophysical dynamical systems, often exceeding millions of dimensions. This work assesses the scalability of score-based data assimilation (SDA), a novel data assimilation method, in the context of such systems. We propose modifications to the score network architecture aimed at significantly reducing memory consumption and execution time. We demonstrate promising results for a two-layer quasi-geostrophic model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#29983;&#25104;&#21644;&#39564;&#35777;&#20043;&#38388;&#19968;&#33268;&#24615;&#30340;&#26694;&#26550;&#65292;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#20165;&#22312;76%&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32463;&#36807;&#31579;&#36873;&#30340;&#29983;&#25104;&#22120;&#21644;&#39564;&#35777;&#22120;&#31572;&#26696;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#19968;&#33268;&#24615;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#23558;Alpaca-30B&#30340;&#19968;&#33268;&#24615;&#20174;60%&#25552;&#39640;&#21040;93%&#65292;&#24182;&#19988;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#20063;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#38500;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#22806;&#65292;&#19968;&#33268;&#24615;&#24494;&#35843;&#36824;&#24102;&#26469;&#20102;&#20854;&#20182;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.01846</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#21644;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22120;&#39564;&#35777;&#22120;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Improving Generator-Validator Consistency of Language Models. (arXiv:2310.01846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#29983;&#25104;&#21644;&#39564;&#35777;&#20043;&#38388;&#19968;&#33268;&#24615;&#30340;&#26694;&#26550;&#65292;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#20165;&#22312;76%&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32463;&#36807;&#31579;&#36873;&#30340;&#29983;&#25104;&#22120;&#21644;&#39564;&#35777;&#22120;&#31572;&#26696;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#19968;&#33268;&#24615;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#23558;Alpaca-30B&#30340;&#19968;&#33268;&#24615;&#20174;60%&#25552;&#39640;&#21040;93%&#65292;&#24182;&#19988;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#20063;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#38500;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#22806;&#65292;&#19968;&#33268;&#24615;&#24494;&#35843;&#36824;&#24102;&#26469;&#20102;&#20854;&#20182;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25130;&#33267;2023&#24180;9&#26376;&#65292;ChatGPT&#22312;&#34987;&#38382;&#21040;"7+8=15&#65292;&#23545;&#36824;&#26159;&#38169;"&#26102;&#65292;&#22238;&#31572;"&#38169;"&#65292;&#20294;&#22312;&#34987;&#38382;&#21040;"7+8"&#31561;&#20110;&#22810;&#23569;&#26102;&#65292;&#22238;&#31572;"15"&#12290;&#36825;&#31181;&#29983;&#25104;&#21644;&#39564;&#35777;&#31572;&#26696;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24456;&#24120;&#35265;&#65292;&#30772;&#22351;&#20102;&#20154;&#20204;&#30340;&#20449;&#20219;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#29983;&#25104;&#21644;&#39564;&#35777;&#20043;&#38388;&#19968;&#33268;&#24615;&#30340;&#26694;&#26550;&#65288;&#31216;&#20026;&#29983;&#25104;&#22120;&#39564;&#35777;&#22120;&#19968;&#33268;&#24615;&#65289;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT-4&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#22312;76%&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32463;&#36807;&#29983;&#25104;&#22120;&#21644;&#39564;&#35777;&#22120;&#31572;&#26696;&#31579;&#36873;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#31216;&#20043;&#20026;&#19968;&#33268;&#24615;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;Alpaca-30B&#30340;&#19968;&#33268;&#24615;&#20174;60%&#25552;&#39640;&#21040;&#20102;93%&#65292;&#24182;&#19988;&#36825;&#31181;&#25913;&#36827;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#31215;&#26497;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39118;&#26684;&#65292;&#22914;&#24189;&#40664;&#65289;&#12290;&#38500;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#22806;&#65292;&#19968;&#33268;&#24615;&#24494;&#35843;&#36824;&#25913;&#21892;&#20102;&#20854;&#20182;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
As of September 2023, ChatGPT correctly answers "what is 7+8" with 15, but when asked "7+8=15, True or False" it responds with "False". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01845</link><description>&lt;p&gt;
&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#24120;&#35268;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#22270;&#20687;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#31934;&#30830;&#30340;&#24314;&#31569;&#29289;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#24050;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#20247;&#22810;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;Segment Anything Model&#65288;SAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#20854;&#25797;&#38271;&#26080;&#31867;&#21035;&#22270;&#20687;&#20998;&#21106;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;SAM&#30340;&#23616;&#38480;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#22312;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;SAM&#19981;&#20855;&#22791;&#35782;&#21035;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#23545;&#23450;&#20301;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.01837</link><description>&lt;p&gt;
&#25193;&#23637;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#26080;&#27861;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;/&#25512;&#29702;&#25805;&#20316;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21482;&#33021;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#19987;&#23478;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#34892;&#20026;&#21644;&#22522;&#30784;&#20915;&#31574;&#36807;&#31243;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#30830;&#20445;AI&#27169;&#22411;&#31283;&#20581;&#12289;&#23454;&#29992;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#30340;&#25163;&#27573;&#12290;&#24050;&#26377;&#19968;&#20123;XAI&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#35299;&#37322;&#21017;&#22522;&#26412;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#26368;&#26032;&#30340;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#20854;&#36866;&#29992;&#20110;&#22810;&#31867;&#22270;&#20687;&#20998;&#21106;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
&lt;/p&gt;</description></item><item><title>EMBERSim&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#25552;&#39640;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#20013;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#23427;&#36890;&#36807;&#20174;&#26368;&#22823;&#30340;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;EMBER&#24320;&#22987;&#65292;&#24182;&#23558;&#30456;&#20284;&#24615;&#20449;&#24687;&#21644;&#24694;&#24847;&#36719;&#20214;&#31867;&#21035;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#30456;&#20284;&#24615;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.01835</link><description>&lt;p&gt;
EMBERSim: &#29992;&#20110;&#25552;&#39640;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis. (arXiv:2310.01835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01835
&lt;/p&gt;
&lt;p&gt;
EMBERSim&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#25552;&#39640;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#20013;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#23427;&#36890;&#36807;&#20174;&#26368;&#22823;&#30340;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;EMBER&#24320;&#22987;&#65292;&#24182;&#23558;&#30456;&#20284;&#24615;&#20449;&#24687;&#21644;&#24694;&#24847;&#36719;&#20214;&#31867;&#21035;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#30456;&#20284;&#24615;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#21551;&#21457;&#24335;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#36716;&#21464;&#65292;&#35777;&#26126;&#22312;&#24403;&#21069;&#20805;&#28385;&#25932;&#23545;&#24615;&#23041;&#32961;&#30340;&#29615;&#22659;&#19979;&#26356;&#21152;&#24378;&#22823;&#12290;&#34429;&#28982;&#25105;&#20204;&#25215;&#35748;&#26426;&#22120;&#23398;&#20064;&#22312;&#25366;&#25496;&#26085;&#30410;&#22686;&#22810;&#30340;&#30456;&#20284;&#25991;&#20214;&#20013;&#30340;&#27169;&#24335;&#26041;&#38754;&#26356;&#21152;&#26377;&#20248;&#21183;&#65292;&#20294;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#22312;&#30456;&#20284;&#24615;&#30740;&#31350;&#39046;&#22495;&#20013;&#21487;&#29992;&#25968;&#25454;&#30340;&#26126;&#26174;&#32570;&#20047;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23569;&#25968;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#37325;&#28857;&#25918;&#22312;&#37327;&#21270;&#24694;&#24847;&#36719;&#20214;&#30340;&#30456;&#20284;&#24615;&#19978;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#28165;&#27905;&#25968;&#25454;&#12290;&#22312;&#26816;&#27979;&#32469;&#36807;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#21333;&#26041;&#38754;&#30340;&#37327;&#21270;&#23588;&#20854;&#21361;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#35299;&#20915;&#20108;&#36827;&#21046;&#25991;&#20214;&#30456;&#20284;&#24615;&#30740;&#31350;&#31354;&#30333;&#39046;&#22495;&#30340;&#19981;&#36275;&#65292;&#20174;&#26368;&#22823;&#30340;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;EMBER&#24320;&#22987;&#12290;&#25105;&#20204;&#23558;EMBER&#19982;&#30456;&#20284;&#24615;&#20449;&#24687;&#20197;&#21450;&#24694;&#24847;&#36719;&#20214;&#31867;&#21035;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#20197;&#25903;&#25345;&#30456;&#20284;&#24615;&#31354;&#38388;&#20013;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1)&#25105;&#20204;&#21457;&#24067;EMBERSim&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years there has been a shift from heuristics-based malware detection towards machine learning, which proves to be more robust in the current heavily adversarial threat landscape. While we acknowledge machine learning to be better equipped to mine for patterns in the increasingly high amounts of similar-looking files, we also note a remarkable scarcity of the data available for similarity-targeted research. Moreover, we observe that the focus in the few related works falls on quantifying similarity in malware, often overlooking the clean data. This one-sided quantification is especially dangerous in the context of detection bypass. We propose to address the deficiencies in the space of similarity research on binary files, starting from EMBER - one of the largest malware classification data sets. We enhance EMBER with similarity information as well as malware class tags, to enable further research in the similarity space. Our contribution is threefold: (1) we publish EMBERSim, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.01828</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65306;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#26102;&#30340;&#24517;&#22791;&#35201;&#27714;&#65292;&#30830;&#20445;&#25152;&#20351;&#29992;&#30340;&#40657;&#30418;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;XAI&#30340;&#37325;&#35201;&#24615;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#24448;&#24448;&#26159;&#40657;&#30418;&#23376;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#23427;&#20204;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;XAI&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;XAI&#31639;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01825</link><description>&lt;p&gt;
&#20908;&#23567;&#40614;&#20998;&#21106;&#30340;PEFT&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#26368;&#36817;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#21644;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#28508;&#22312;&#30340;PEFT&#24212;&#29992;&#12290;&#19981;&#21516;&#22320;&#21306;&#30340;&#27668;&#20505;&#22810;&#26679;&#24615;&#21644;&#23545;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#32473;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31181;&#26893;&#23395;&#33410;&#30340;&#20316;&#29289;&#31867;&#22411;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20351;&#29992;&#22269;&#20869;&#39046;&#20808;&#30340;&#20908;&#23567;&#40614;&#20316;&#29289;&#30417;&#27979;&#27169;&#22411;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;PEFT&#26041;&#27861;&#22312;&#20316;&#29289;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;PEFT&#26041;&#27861;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
&lt;/p&gt;</description></item><item><title>Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.01824</link><description>&lt;p&gt;
Mini-BEHAVIOR&#65306;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#38271;&#26399;&#20915;&#31574;&#21046;&#23450;&#30340;&#36807;&#31243;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01824
&lt;/p&gt;
&lt;p&gt;
Mini-BEHAVIOR&#26159;&#19968;&#20010;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;&#26234;&#33021;&#20307;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#36807;&#31243;&#29983;&#25104;&#23454;&#29616;&#20102;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Mini-BEHAVIOR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#65292;&#25361;&#25112;&#26234;&#33021;&#20307;&#21033;&#29992;&#25512;&#29702;&#21644;&#20915;&#31574;&#25216;&#33021;&#35299;&#20915;&#31867;&#20284;&#20110;&#26085;&#24120;&#20154;&#31867;&#25361;&#25112;&#30340;&#22797;&#26434;&#27963;&#21160;&#12290;Mini-BEHAVIOR&#29615;&#22659;&#26159;&#19968;&#20010;&#24555;&#36895;&#65292;&#29616;&#23454;&#30340;Gridworld&#29615;&#22659;&#65292;&#26082;&#20855;&#26377;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20445;&#30041;&#20102;&#22797;&#26434;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#22522;&#20934;&#20013;&#31526;&#21495;&#32423;&#30340;&#29289;&#29702;&#29616;&#23454;&#24863;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#38190;&#29305;&#24615;&#65292;&#22914;&#36807;&#31243;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#26080;&#38480;&#30340;&#20219;&#21153;&#21464;&#21270;&#21644;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#25903;&#25345;&#12290;Mini-BEHAVIOR&#25552;&#20379;&#20102;&#21407;&#22987;BEHAVIOR&#22522;&#20934;&#20013;&#21508;&#31181;&#23478;&#21153;&#20219;&#21153;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#35757;&#32451;&#30340;&#20837;&#38376;&#20195;&#30721;&#12290;&#24635;&#20043;&#65292;Mini-BEHAVIOR&#20026;&#35780;&#20272;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#20010;&#24555;&#36895;&#12289;&#24320;&#25918;&#24335;&#30340;&#22522;&#20934;&#12290;&#23427;&#20316;&#20026;&#30740;&#31350;&#30340;&#29992;&#25143;&#21451;&#22909;&#30340;&#20837;&#21475;&#28857;&#65292;&#20419;&#36827;&#20102;&#35780;&#20272;&#21644;&#21457;&#23637;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and devel
&lt;/p&gt;</description></item><item><title>MIMO-NeRF&#26159;&#19968;&#31181;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;MIMO MLP&#24182;&#36827;&#34892;&#20998;&#32452;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20943;&#36731;&#20102;&#37096;&#20998;&#27169;&#31946;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01821</link><description>&lt;p&gt;
MIMO-NeRF: &#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#24555;&#36895;&#31070;&#32463;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields. (arXiv:2310.01821v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01821
&lt;/p&gt;
&lt;p&gt;
MIMO-NeRF&#26159;&#19968;&#31181;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;MIMO MLP&#24182;&#36827;&#34892;&#20998;&#32452;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20943;&#36731;&#20102;&#37096;&#20998;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;SISO MLP&#65289;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#23558;3D&#22352;&#26631;&#21644;&#35270;&#35282;&#26144;&#23556;&#21040;&#39068;&#33394;&#21644;&#20307;&#31215;&#23494;&#24230;&#65292;&#36825;&#20250;&#23548;&#33268;&#28210;&#26579;&#36895;&#24230;&#21464;&#24930;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;MIMO-NeRF&#65289;&#65292;&#36890;&#36807;&#23558;SISO MLP&#26367;&#25442;&#20026;MIMO MLP&#24182;&#36827;&#34892;&#20998;&#32452;&#26144;&#23556;&#26469;&#20943;&#23569;&#36816;&#34892;&#30340;MLP&#25968;&#37327;&#12290;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#26159;&#65292;&#27599;&#20010;&#28857;&#30340;&#39068;&#33394;&#21644;&#20307;&#31215;&#23494;&#24230;&#21487;&#20197;&#26681;&#25454;&#32452;&#20869;&#36755;&#20837;&#22352;&#26631;&#30340;&#36873;&#25321;&#26377;&#25152;&#19981;&#21516;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19968;&#20123;&#26126;&#26174;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24555;&#36895;&#37325;&#26500;&#30340;MLP&#23545;MIMO MLP&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#20943;&#36731;&#27492;&#27169;&#31946;&#24615;&#32780;&#26080;&#38656;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21253;&#25324;&#27604;&#36739;&#21644;&#28040;&#34701;&#30740;&#31350;&#22312;&#20869;&#30340;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) have shown impressive results for novel view synthesis. However, they depend on the repetitive use of a single-input single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and view direction to the color and volume density in a sample-wise manner, which slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF) that reduces the number of MLPs running by replacing the SISO MLP with a MIMO MLP and conducting mappings in a group-wise manner. One notable challenge with this approach is that the color and volume density of each point can differ according to a choice of input coordinates in a group, which can lead to some notable ambiguity. We also propose a self-supervised learning method that regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this ambiguity without using pretrained models. The results of a comprehensive experimental evaluation including comparative and ablation studies are presented to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01820</link><description>&lt;p&gt;
&#36827;&#21521;&#40065;&#26834;&#24230;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks. (arXiv:2310.01820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#25968;&#25454;&#20013;&#30340;&#20381;&#36182;&#32467;&#26500;&#36890;&#36807;&#33410;&#28857;&#20043;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#36827;&#34892;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GNN&#24050;&#32463;&#25104;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20851;&#38190;&#26550;&#26500;&#65292;&#22312;&#25935;&#24863;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#35201;&#27714;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#26377;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#36825;&#23601;&#38656;&#35201;&#19968;&#20010;GNN&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;GNN&#35299;&#37322;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#38656;&#35201;&#25552;&#20379;&#21487;&#38752;&#30340;&#20445;&#30495;&#24230;&#24230;&#37327;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19968;&#22522;&#30784;&#24615;&#25361;&#25112;&#65292;&#37325;&#28857;&#22312;&#29616;&#26377;&#30340;&#20445;&#30495;&#24230;&#24230;&#37327;&#26041;&#27861;&#20013;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;Fid_+&#65292;Fid_-&#21644;Fid_&#916;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#20449;&#24687;&#35770;&#35299;&#37322;&#24615;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metri
&lt;/p&gt;</description></item><item><title>AutoLoRa&#26159;&#19968;&#20010;&#33258;&#21160;&#40065;&#26834;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#20998;&#25903;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#40065;&#26834;&#24494;&#35843;&#23384;&#22312;&#30340;&#26799;&#24230;&#26041;&#21521;&#20998;&#27495;&#21644;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01818</link><description>&lt;p&gt;
AutoLoRa&#65306;&#21442;&#25968;&#20813;&#35843;&#33258;&#21160;&#40065;&#26834;&#24494;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework. (arXiv:2310.01818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01818
&lt;/p&gt;
&lt;p&gt;
AutoLoRa&#26159;&#19968;&#20010;&#33258;&#21160;&#40065;&#26834;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#20998;&#25903;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#40065;&#26834;&#24494;&#35843;&#23384;&#22312;&#30340;&#26799;&#24230;&#26041;&#21521;&#20998;&#27495;&#21644;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24494;&#35843;&#65288;RFT&#65289;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#33719;&#24471;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;RFT&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#65288;FE&#65289;&#20248;&#21270;&#23545;&#25239;&#24615;&#21644;&#33258;&#28982;&#30446;&#26631;&#20250;&#23548;&#33268;&#26174;&#33879;&#19981;&#21516;&#30340;&#26799;&#24230;&#26041;&#21521;&#12290;&#36825;&#31181;&#20998;&#27495;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#23454;&#29616;&#65292;&#24182;&#20351;RFT&#23545;&#36229;&#21442;&#25968;&#39640;&#24230;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#31209;&#65288;LoRa&#65289;&#20998;&#25903;&#65292;&#23558;RFT&#20998;&#35299;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#32452;&#20214;&#65306;&#36890;&#36807;LoRa&#20998;&#25903;&#20248;&#21270;&#33258;&#28982;&#30446;&#26631;&#21644;&#36890;&#36807;FE&#20248;&#21270;&#23545;&#25239;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#33258;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#21644;&#25439;&#22833;&#39033;&#30340;&#26631;&#37327;&#12290;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#21160;&#21270;RFT&#36890;&#36807;LoRa&#20998;&#35299;&#24471;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;OpenSea&#20132;&#26131;&#30340;NFT&#25910;&#34255;&#30340;&#38142;&#19978;&#21644;&#38142;&#19979;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;NFT&#20215;&#26684;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#23613;&#31649;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#21487;&#20197;&#35299;&#37322;&#25910;&#34255;&#20869;&#37096;&#20215;&#26684;&#30340;&#21464;&#21270;&#65292;&#20294;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#25910;&#34255;&#21697;&#12290;</title><link>http://arxiv.org/abs/2310.01815</link><description>&lt;p&gt;
NFT&#30340;&#20215;&#26684;&#26159;&#30001;&#20160;&#20040;&#20915;&#23450;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Determines the Price of NFTs?. (arXiv:2310.01815v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01815
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;OpenSea&#20132;&#26131;&#30340;NFT&#25910;&#34255;&#30340;&#38142;&#19978;&#21644;&#38142;&#19979;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;NFT&#20215;&#26684;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#23613;&#31649;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#21487;&#20197;&#35299;&#37322;&#25910;&#34255;&#20869;&#37096;&#20215;&#26684;&#30340;&#21464;&#21270;&#65292;&#20294;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#25910;&#34255;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#33402;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#31361;&#30772;&#24615;&#30340;&#24179;&#21488;&#65292;&#26725;&#25509;&#20102;&#33402;&#26415;&#21644;&#25216;&#26415;&#39046;&#22495;&#12290;NFT&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#26694;&#26550;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#23383;&#33402;&#26415;&#24066;&#22330;&#65292;&#20351;&#33402;&#26415;&#23478;&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#23637;&#31034;&#21644;&#21464;&#29616;&#33258;&#24049;&#30340;&#20316;&#21697;&#12290;NFT&#23558;&#23384;&#20648;&#22312;&#21306;&#22359;&#38142;&#19978;&#30340;&#20803;&#25968;&#25454;&#19982;&#31163;&#38142;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23383;&#25152;&#26377;&#26435;&#24418;&#24335;&#12290;&#30446;&#21069;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#20849;&#21516;&#20915;&#23450;NFT&#30340;&#20215;&#26684;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;OpenSea&#20132;&#26131;&#30340;NFT&#25910;&#34255;&#30340;&#38142;&#19978;&#21644;&#38142;&#19979;&#25968;&#25454;&#65292;&#20197;&#20102;&#35299;&#20160;&#20040;&#24433;&#21709;NFT&#30340;&#23450;&#20215;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;NFT&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#21487;&#20197;&#29992;&#26469;&#35299;&#37322;&#25910;&#34255;&#20013;&#20215;&#26684;&#30340;&#21464;&#21270;&#65292;&#20294;&#36825;&#20123;&#25552;&#21462;&#30340;&#29305;&#24449;&#19981;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25910;&#34255;&#21697;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;NFT&#25910;&#34255;&#30340;&#20132;&#26131;&#37327;&#24448;&#24448;&#19982;&#20854;&#22312;&#32447;&#23384;&#22312;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#20851;&#27880;&#32773;&#25968;&#37327;&#65289;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of digital art, Non-Fungible Tokens (NFTs) have emerged as a groundbreaking platform, bridging the realms of art and technology. NFTs serve as the foundational framework that has revolutionized the market for digital art, enabling artists to showcase and monetize their creations in unprecedented ways. NFTs combine metadata stored on the blockchain with off-chain data, such as images, to create a novel form of digital ownership. It is not fully understood how these factors come together to determine NFT prices. In this study, we analyze both on-chain and off-chain data of NFT collections trading on OpenSea to understand what influences NFT pricing. Our results show that while text and image data of the NFTs can be used to explain price variations within collections, the extracted features do not generalize to new, unseen collections. Furthermore, we find that an NFT collection's trading volume often relates to its online presence, like social media followers an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;Kullback-Leibler&#25955;&#24230;&#30340;&#20223;&#30495;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#23558;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#19982;&#31070;&#32463;&#27604;&#20540;&#20272;&#35745;&#32467;&#21512;&#20026;&#19968;&#20010;&#30446;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#26469;&#23454;&#29616;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01808</link><description>&lt;p&gt;
&#22522;&#20110;&#24191;&#20041;Kullback-Leibler&#25955;&#24230;&#30340;&#20223;&#30495;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simulation-based Inference with the Generalized Kullback-Leibler Divergence. (arXiv:2310.01808v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;Kullback-Leibler&#25955;&#24230;&#30340;&#20223;&#30495;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#23558;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#19982;&#31070;&#32463;&#27604;&#20540;&#20272;&#35745;&#32467;&#21512;&#20026;&#19968;&#20010;&#30446;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#26469;&#23454;&#29616;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#20013;&#65292;&#30446;&#26631;&#26159;&#22312;&#20284;&#28982;&#20989;&#25968;&#21482;&#38544;&#24335;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#36890;&#24120;&#20351;&#29992;&#24402;&#19968;&#21270;&#23494;&#24230;&#20272;&#35745;&#22120;&#20316;&#20026;&#21518;&#39564;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#30001;&#20110;&#20248;&#21270;&#30340;&#26159;Kullback-Leibler&#25955;&#24230;&#65292;&#36825;&#31181;&#24418;&#24335;&#24456;&#38590;&#36866;&#24212;&#38750;&#24402;&#19968;&#21270;&#20195;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#24191;&#20041;Kullback-Leibler&#25955;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#12290;&#24403;&#27169;&#22411;&#31867;&#34987;&#24402;&#19968;&#21270;&#26102;&#65292;&#35813;&#30446;&#26631;&#24674;&#22797;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#32479;&#19968;&#21040;&#20102;&#31070;&#32463;&#27604;&#20540;&#20272;&#35745;&#20013;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#20026;&#19968;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24402;&#19968;&#21270;&#22522;&#30784;&#20998;&#24067;&#21644;&#23398;&#20064;&#27604;&#20540;&#26469;&#23454;&#29616;&#26368;&#20339;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Simulation-based Inference, the goal is to solve the inverse problem when the likelihood is only known implicitly. Neural Posterior Estimation commonly fits a normalized density estimator as a surrogate model for the posterior. This formulation cannot easily fit unnormalized surrogates because it optimizes the Kullback-Leibler divergence. We propose to optimize a generalized Kullback-Leibler divergence that accounts for the normalization constant in unnormalized distributions. The objective recovers Neural Posterior Estimation when the model class is normalized and unifies it with Neural Ratio Estimation, combining both into a single objective. We investigate a hybrid model that offers the best of both worlds by learning a normalized base distribution and a learned ratio. We also present benchmark results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#26469;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#22320;&#23454;&#29616;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01807</link><description>&lt;p&gt;
&#36890;&#36807;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#23454;&#29616;&#31163;&#25955;&#12289;&#32452;&#21512;&#21644;&#31526;&#21495;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Discrete, compositional, and symbolic representations through attractor dynamics. (arXiv:2310.01807v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#26469;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#22320;&#23454;&#29616;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24615;&#26159;&#31163;&#25955;&#31526;&#21495;&#31995;&#32479;&#65288;&#22914;&#35821;&#35328;&#21644;&#31243;&#24207;&#65289;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#23427;&#20351;&#24471;&#36825;&#20123;&#31995;&#32479;&#23613;&#31649;&#20351;&#29992;&#26377;&#38480;&#30340;&#31526;&#21495;&#38598;&#21512;&#65292;&#20294;&#20173;&#20855;&#26377;&#26080;&#38480;&#30340;&#23481;&#37327;&#12290;&#23427;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25512;&#29702;&#20013;&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#25277;&#35937;&#24615;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#21644;&#31526;&#21495;&#22788;&#29702;&#20043;&#38388;&#30340;&#30028;&#38754;&#36890;&#24120;&#26159;&#36890;&#36807;&#31639;&#27861;&#32423;&#21035;&#19978;&#30340;&#37327;&#21270;&#25110;softmax&#37319;&#26679;&#27493;&#39588;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#23558;&#31163;&#25955;&#21270;&#23454;&#29616;&#24471;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#22312;&#21560;&#24341;&#23376;&#32593;&#32476;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#21487;&#20197;&#20135;&#29983;&#32452;&#21512;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#31181;&#20449;&#24687;&#22686;&#38271;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositionality is an important feature of discrete symbolic systems, such as language and programs, as it enables them to have infinite capacity despite a finite symbol set. It serves as a useful abstraction for reasoning in both cognitive science and in AI, yet the interface between continuous and symbolic processing is often imposed by fiat at the algorithmic level, such as by means of quantization or a softmax sampling step. In this work, we explore how discretization could be implemented in a more neurally plausible manner through the modeling of attractor dynamics that partition the continuous representation space into basins that correspond to sequences of symbols. Building on established work in attractor networks and introducing novel training methods, we show that imposing structure in the symbolic space can produce compositionality in the attractor-supported representation space of rich sensory inputs. Lastly, we argue that our model exhibits the process of an information b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01794</link><description>&lt;p&gt;
GNNX-BENCH: &#36890;&#36807;&#28145;&#24230;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#22120;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking. (arXiv:2310.01794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#25581;&#31034;GNN&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#25152;&#26377;&#25552;&#20986;&#30340;&#31639;&#27861;&#37117;&#21253;&#21547;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#30340;&#35810;&#38382;&#26041;&#38754;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;GNN&#35299;&#37322;&#24615;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#23545;&#20107;&#23454;&#27714;&#35777;&#25512;&#29702;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;&#12289;&#23427;&#20204;&#23545;&#19981;&#21516;GNN&#26550;&#26500;&#12289;&#22122;&#22768;&#12289;&#38750;&#20984;&#25439;&#22833;&#34920;&#38754;&#20013;&#30340;&#38543;&#26426;&#24615;&#12289;&#22312;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#21487;&#34892;&#24615;&#31561;&#31561;&#65292;&#23578;&#26410;&#24471;&#21040;&#27491;&#24335;&#30340;&#30740;&#31350;&#12290;&#21463;&#27492;&#38656;&#27714;&#30340;&#28608;&#21457;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#21644;&#27604;&#36739;&#21508;&#31181;&#35299;&#37322;&#24615;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#20851;&#38190;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25152;&#26377;&#31639;&#27861;&#37117;&#21463;&#21040;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stabi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01783</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#23545;&#30740;&#31350;&#35770;&#25991;&#26377;&#29992;&#30340;&#21453;&#39304;&#21527;&#65311;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models provide useful feedback on research papers? A large-scale empirical analysis. (arXiv:2310.01783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#30340;&#21453;&#39304;&#26159;&#20005;&#35880;&#30740;&#31350;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#20135;&#20986;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#22797;&#26434;&#30340;&#19987;&#19994;&#30693;&#35782;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#31185;&#23398;&#21453;&#39304;&#26426;&#21046;&#12290;&#36234;&#26469;&#36234;&#38590;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21516;&#34892;&#35780;&#23457;&#24847;&#35265;&#12290;&#21021;&#32423;&#30740;&#31350;&#20154;&#21592;&#25110;&#26469;&#33258;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#23588;&#20854;&#38590;&#20197;&#21450;&#26102;&#33719;&#24471;&#21453;&#39304;&#12290;&#38543;&#30528;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#21453;&#39304;&#24341;&#36215;&#20102;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#23436;&#25972;PDF&#25552;&#20379;&#35780;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#21453;&#39304;&#30340;&#36136;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;15&#26412;Nature&#31867;&#26399;&#21002;&#65288;&#24635;&#20849;3096&#31687;&#35770;&#25991;&#65289;&#21644;ICLR m &#19978;&#23450;&#37327;&#27604;&#36739;&#20102;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01777</link><description>&lt;p&gt;
&#37319;&#29992;&#20272;&#35745;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#65288;SEA&#65289;
&lt;/p&gt;
&lt;p&gt;
SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;transformer&#26550;&#26500;&#22312;&#38656;&#35201;&#23545;&#24207;&#21015;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#24314;&#27169;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#32447;&#24615;&#36924;&#36817;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#20174;&#25945;&#24072;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#31232;&#30095;&#21644;&#32447;&#24615;&#26041;&#27861;&#22914;&#26524;&#19981;&#33021;&#20135;&#29983;&#23436;&#20840;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#36824;&#21487;&#33021;&#22833;&#21435;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEA&#65306;&#37319;&#29992;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;SEA&#36890;&#36807;&#22522;&#20110;&#26680;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#20272;&#35745;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#23545;&#23436;&#25972;&#27880;&#24847;&#21147;&#30697;&#38453;&#36827;&#34892;&#31232;&#30095;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture has made breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, transformers struggle with long sequences due to the quadratic complexity of the attention operation, and previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches may also lose interpretability if they do not produce full quadratic attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01769</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#20943;&#32531;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#65306;&#23545;&#31216;&#24615;&#21644;&#21021;&#22987;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#25913;&#21464;&#26799;&#24230;&#19979;&#38477;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#22312;&#23545;&#31216;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#26410;&#30693;&#30340;&#21322;&#27491;&#23450;&#30697;&#38453;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65288;$k&gt;r$&#65289;&#38543;&#26426;&#21021;&#22987;&#21270;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#22411;$\Omega (1/T^2)$&#19979;&#30028;&#65292;&#19982;&#31934;&#30830;&#21442;&#25968;&#21270;&#24773;&#20917;&#65288;$k=r$&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;$\exp (-\Omega (T))$&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#23545;&#31216;&#35774;&#32622;&#65292;&#20854;&#20013;$M^* \in \mathbb{R}^{n_1 \times n_2}$&#26159;&#26410;&#30693;&#30697;&#38453;&#65292;&#37319;&#29992;&#38750;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a positive semi-definite unknown matrix of rank $r \ll n$, and one uses a symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n \times k}$ with $k &gt; r$ is the factor matrix. We give a novel $\Omega (1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k &gt;r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\exp (-\Omega (T))$. Next, we study asymmetric setting where $M^* \in \mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll \min\{n_1,n_2\}$, and one uses an 
&lt;/p&gt;</description></item><item><title>BackDiff&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#21644;&#20960;&#20309;&#34920;&#31034;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21508;&#31181;&#31895;&#31890;&#21270;&#27169;&#22411;&#21644;&#34507;&#30333;&#36136;&#20013;&#30340;&#24191;&#20041;&#21270;&#21644;&#21487;&#38752;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.01768</link><description>&lt;p&gt;
BackDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#24191;&#20041;&#21487;&#20256;&#36882;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Backdiff: a diffusion model for generalized transferable protein backmapping. (arXiv:2310.01768v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01768
&lt;/p&gt;
&lt;p&gt;
BackDiff&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#21644;&#20960;&#20309;&#34920;&#31034;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21508;&#31181;&#31895;&#31890;&#21270;&#27169;&#22411;&#21644;&#34507;&#30333;&#36136;&#20013;&#30340;&#24191;&#20041;&#21270;&#21644;&#21487;&#38752;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31895;&#31890;&#21270;&#65288;CG&#65289;&#27169;&#22411;&#22312;&#30740;&#31350;&#34507;&#30333;&#36136;&#32467;&#26500;&#12289;&#28909;&#21147;&#23398;&#24615;&#36136;&#21644;&#26500;&#35937;&#21160;&#21147;&#23398;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#31895;&#31890;&#21270;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#22312;&#35768;&#22810;&#34507;&#30333;&#36136;&#35774;&#35745;&#21644;&#33647;&#29289;&#21457;&#29616;&#24212;&#29992;&#20013;&#38656;&#35201;&#35814;&#32454;&#30340;&#21407;&#23376;&#34920;&#31034;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#26102;&#65292;&#20174;CG&#26144;&#23556;&#22238;&#21407;&#23376;&#32423;&#26500;&#22411;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#32972;&#26144;&#23556;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#26222;&#36941;&#24212;&#29992;&#20110;&#19981;&#21516;CG&#27169;&#22411;&#21644;&#34507;&#30333;&#36136;&#30340;&#32972;&#26144;&#23556;&#26041;&#27861;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackDiff&#65292;&#19968;&#31181;&#26088;&#22312;&#23454;&#29616;&#34507;&#30333;&#36136;&#32972;&#26144;&#23556;&#38382;&#39064;&#30340;&#27867;&#21270;&#21644;&#21487;&#38752;&#24615;&#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;BackDiff&#21033;&#29992;&#20855;&#26377;&#20960;&#20309;&#34920;&#31034;&#30340;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;CG&#27169;&#22411;&#21487;&#20197;&#21253;&#21547;&#19981;&#21516;&#30340;&#31895;&#31890;&#21270;&#20301;&#28857;&#65292;&#20854;&#20013;&#21253;&#25324;&#36873;&#23450;&#30340;&#21407;&#23376;&#65288;CG&#21407;&#23376;&#65289;&#21644;&#21407;&#23376;&#22352;&#26631;&#30340;&#31616;&#21333;CG&#36741;&#21161;&#20989;&#25968;&#65288;CG&#36741;&#21161;&#65289;
&lt;/p&gt;
&lt;p&gt;
Coarse-grained (CG) models play a crucial role in the study of protein structures, protein thermodynamic properties, and protein conformation dynamics. Due to the information loss in the coarse-graining process, backmapping from CG to all-atom configurations is essential in many protein design and drug discovery applications when detailed atomic representations are needed for in-depth studies. Despite recent progress in data-driven backmapping approaches, devising a backmapping method that can be universally applied across various CG models and proteins remains unresolved. In this work, we propose BackDiff, a new generative model designed to achieve generalization and reliability in the protein backmapping problem. BackDiff leverages the conditional score-based diffusion model with geometric representations. Since different CG models can contain different coarse-grained sites which include selected atoms (CG atoms) and simple CG auxiliary functions of atomistic coordinates (CG auxiliar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20445;&#35777;&#21453;&#20107;&#23454;&#29983;&#25104;&#24402;&#22240;&#30340;&#29305;&#24449;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.01766</link><description>&lt;p&gt;
&#25506;&#32034;&#38024;&#23545;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21453;&#20107;&#23454;&#23545;&#40784;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Exploring Counterfactual Alignment Loss towards Human-centered AI. (arXiv:2310.01766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20445;&#35777;&#21453;&#20107;&#23454;&#29983;&#25104;&#24402;&#22240;&#30340;&#29305;&#24449;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#20154;&#20204;&#38590;&#20197;&#20449;&#20219;&#23427;&#20204;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;-&#25209;&#35780;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#35299;&#37322;&#24341;&#23548;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#19982;&#20154;&#31867;&#19987;&#23478;&#26631;&#27880;&#30340;&#22270;&#20687;&#21306;&#22495;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#20010;&#26412;&#36136;&#19978;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#22522;&#20110;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#21487;&#33021;&#26080;&#27861;&#22240;&#26524;&#22320;&#24402;&#22240;&#20110;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#23545;&#20854;&#23545;&#40784;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#26032;&#22411;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#22240;&#26524;&#24402;&#22240;&#33021;&#21147;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#65292;&#31216;&#20026;&#21453;&#20107;&#23454;&#23545;&#40784;&#25439;&#22833;&#65288;CF-Align&#65289;&#12290;&#36825;&#20010;&#25439;&#22833;&#20445;&#35777;&#20102;&#20998;&#31867;&#22120;&#30001;&#21453;&#20107;&#23454;&#29983;&#25104;&#24402;&#22240;&#30340;&#29305;&#24449;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have demonstrated impressive accuracy in supervised learning tasks. However, their lack of transparency makes it hard for humans to trust their results, especially in safe-critic domains such as healthcare. To address this issue, recent explanation-guided learning approaches proposed to align the gradient-based attention map to image regions annotated by human experts, thereby obtaining an intrinsically human-centered model. However, the attention map these methods are based on may fail to causally attribute the model predictions, thus compromising their validity for alignment. To address this issue, we propose a novel human-centered framework based on counterfactual generation. In particular, we utilize the counterfactual generation's ability for causal attribution to introduce a novel loss called the CounterFactual Alignment (CF-Align) loss. This loss guarantees that the features attributed by the counterfactual generation for the classifier align with the human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#25968;&#25454;&#28165;&#27927;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;ML&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#23384;&#22312;&#30528;&#20114;&#30456;&#20419;&#36827;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.01765</link><description>&lt;p&gt;
&#25968;&#25454;&#28165;&#27927;&#19982;&#26426;&#22120;&#23398;&#20064;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Cleaning and Machine Learning: A Systematic Literature Review. (arXiv:2310.01765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#25968;&#25454;&#28165;&#27927;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;ML&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#23384;&#22312;&#30528;&#20114;&#30456;&#20419;&#36827;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34987;&#25972;&#21512;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#31995;&#32479;&#20013;&#65292;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;&#30001;&#20110;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#23545;&#20110;&#26816;&#27979;&#21644;&#20462;&#22797;&#25968;&#25454;&#38169;&#35823;&#65288;&#21363;&#25968;&#25454;&#28165;&#27927;&#65289;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30740;&#31350;&#20154;&#21592;&#36824;&#22312;&#25506;&#32034;&#22914;&#20309;&#20351;&#29992;ML&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;&#65292;&#20174;&#32780;&#22312;ML&#21644;&#25968;&#25454;&#28165;&#27927;&#20043;&#38388;&#26500;&#24314;&#20102;&#21452;&#37325;&#20851;&#31995;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26080;&#23545;&#36825;&#31181;&#20851;&#31995;&#36827;&#34892;&#20840;&#38754;&#32508;&#36848;&#30340;&#30740;&#31350;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#30340;&#30446;&#26631;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#21253;&#25324;ML&#29992;&#20110;&#25968;&#25454;&#28165;&#27927;&#21644;&#25968;&#25454;&#28165;&#27927;&#29992;&#20110;ML&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#24037;&#20316;&#24314;&#35758;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#23545;2016&#24180;&#33267;2022&#24180;&#26399;&#38388;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#28165;&#27927;&#27963;&#21160;&#65292;&#21253;&#25324;&#29305;&#24449;&#28165;&#27927;&#12289;&#26631;&#31614;&#28165;&#27927;&#12289;&#23454;&#20307;&#21305;&#37197;&#12289;&#24322;&#24120;&#20540;&#26816;&#27979;&#12289;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. Objective: This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. Method: We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;&#32463;&#39564;&#20998;&#24067;&#19978;&#21021;&#22987;&#21270;&#24182;&#36816;&#34892;&#20351;&#29992;&#26089;&#26399;&#20572;&#27490;&#30340;Langevin&#25193;&#25955;&#30340;Vanilla Score&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22810;&#27169;&#24577;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2310.01762</link><description>&lt;p&gt;
&#20351;&#29992;Vanilla Score&#23545;&#22810;&#27169;&#24577;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65306;&#22522;&#20110;&#25968;&#25454;&#21021;&#22987;&#21270;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization. (arXiv:2310.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01762
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;&#32463;&#39564;&#20998;&#24067;&#19978;&#21021;&#22987;&#21270;&#24182;&#36816;&#34892;&#20351;&#29992;&#26089;&#26399;&#20572;&#27490;&#30340;Langevin&#25193;&#25955;&#30340;Vanilla Score&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22810;&#27169;&#24577;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#21644;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#65288;&#20998;&#24067;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#23548;&#25968;&#65289;&#30340;&#26041;&#27861;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#21644;&#26368;&#36817;&#30340;&#20852;&#36259;&#29190;&#21457;&#12290;&#22312;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#20013;&#65292;Hyv&#228;rinen&#25552;&#20986;&#20102;Vanilla Score Matching&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#35745;&#31639;&#24213;&#23618;&#30495;&#23454;&#25968;&#25454;&#30340;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#26469;&#23398;&#20064;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#19982;&#23545;&#27604;&#20998;&#27495;&#21644;&#20266;&#20284;&#28982;&#20272;&#35745;&#31561;&#24050;&#26377;&#25216;&#26415;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#29616;&#22312;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;Vanilla Score Matching&#22312;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#26102;&#23384;&#22312;&#26174;&#33879;&#22256;&#38590;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#26041;&#24335;&#21487;&#20197;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#20294;&#20197;&#19979;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#31572;&#26696; - &#26159;&#21542;&#26377;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#21487;&#20197;&#20165;&#20351;&#29992;Vanilla Score&#23545;&#22810;&#27169;&#24577;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65311;&#21463;&#19968;&#31995;&#21015;&#30456;&#20851;&#23454;&#39564;&#24615;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#26089;&#26399;&#20572;&#27490;&#30340;Langevin&#25193;&#25955;&#65292;&#20197;&#32463;&#39564;&#20998;&#24067;&#20026;&#21021;&#22987;&#21270;&#65292;&#24182;&#22312;...
&lt;/p&gt;
&lt;p&gt;
There is a long history, as well as a recent explosion of interest, in statistical and generative modeling approaches based on score functions -derivatives of the log-likelihood of a distribution. In seminal works, Hyv\"arinen proposed vanilla score matching as a way to learn distributions from data by computing an estimate of the score function of the underlying ground truth, and established connections between this method and established techniques like Contrastive Divergence and Pseudolikelihood estimation. It is by now well-known that vanilla score matching has significant difficulties learning multimodal distributions. Although there are various ways to overcome this difficulty, the following question has remained unanswered -- is there a natural way to sample multimodal distributions using just the vanilla score? Inspired by a long line of related experimental works, we prove that the Langevin diffusion with early stopping, initialized at the empirical distribution, and run on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#24182;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#23450;&#21046;&#30340;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01758</link><description>&lt;p&gt;
ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21270;&#65306;&#26368;&#20339;&#26085;&#21069;&#33021;&#37327;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:Optimal Day-Ahead Energy Scheduling. (arXiv:2310.01758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#24182;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#23450;&#21046;&#30340;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#31995;&#32479;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;&#36755;&#20837;&#20449;&#24687;&#65292;&#24182;&#20197;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23545;&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#30005;&#32593;&#26085;&#21069;&#33021;&#37327;&#35843;&#24230;&#20013;&#65292;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#26159;&#22312;&#24378;&#21046;&#38480;&#21046;&#21516;&#19968;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#30340;&#20248;&#21270;&#27169;&#22411;&#20013;&#35299;&#20915;&#30340;&#21464;&#37327;&#12290;&#36825;&#23558;&#20250;&#20135;&#29983;&#19968;&#20010;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#65307;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#23558;&#20351;&#36825;&#31867;&#38382;&#39064;&#24322;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#26080;&#27861;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#26032;&#20852;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32447;&#24615;&#21270;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#24191;&#27867;&#20351;&#29992;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#26412;&#25991;&#24320;&#21457;&#12289;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22235;&#31181;&#36866;&#29992;&#20110;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been widely applied in the power system area. They can be used for better predicting input information and modeling system performance with increased accuracy. In some applications such as battery degradation neural network-based microgrid day-ahead energy scheduling, the input features of the trained learning model are variables to be solved in optimization models that enforce limits on the output of the same learning model. This will create a neural network-embedded optimization problem; the use of nonlinear activation functions in the neural network will make such problems extremely hard to solve if not unsolvable. To address this emerging challenge, this paper investigated different methods for linearizing the nonlinear activation functions with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU activation function are developed, analyzed and compared in this paper. Each method employs a
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#30340;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#19988;&#26080;&#38656;&#32479;&#19968;&#25506;&#32034;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#26080;&#30028;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01756</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;&#26080;&#30028;&#25439;&#22833;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improved Algorithms for Adversarial Bandits with Unbounded Losses. (arXiv:2310.01756v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01756
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#30340;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#19988;&#26080;&#38656;&#32479;&#19968;&#25506;&#32034;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#26080;&#30028;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#26080;&#30028;&#25439;&#22833;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#31639;&#27861;&#23545;&#25439;&#22833;&#30340;&#22823;&#23567;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UMAB-NN&#21644;UMAB-G&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#38750;&#36127;&#21644;&#19968;&#33324;&#30340;&#26080;&#30028;&#25439;&#22833;&#12290;&#23545;&#20110;&#38750;&#36127;&#26080;&#30028;&#25439;&#22833;&#65292;UMAB-NN&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33258;&#36866;&#24212;&#19988;&#26080;&#38656;&#32479;&#19968;&#25506;&#32034;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;UMAB-G&#65292;&#21487;&#20197;&#23398;&#20064;&#20219;&#24847;&#26080;&#30028;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;MAB&#38382;&#39064;&#20013;&#27491;&#36127;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#26469;&#37197;&#21512;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#31639;&#27861;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#22788;&#29702;&#26080;&#30028;&#25439;&#22833;&#30340;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the Adversarial Multi-Armed Bandits (MAB) problem with unbounded losses, where the algorithms have no prior knowledge on the sizes of the losses. We present UMAB-NN and UMAB-G, two algorithms for non-negative and general unbounded loss respectively. For non-negative unbounded loss, UMAB-NN achieves the first adaptive and scale free regret bound without uniform exploration. Built up on that, we further develop UMAB-G that can learn from arbitrary unbounded loss. Our analysis reveals the asymmetry between positive and negative losses in the MAB problem and provide additional insights. We also accompany our theoretical findings with extensive empirical evaluations, showing that our algorithms consistently out-performs all existing algorithms that handles unbounded losses.
&lt;/p&gt;</description></item><item><title>CausalTime&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#25104;&#36924;&#30495;&#26102;&#38388;&#24207;&#21015;&#30340;&#27969;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#30495;&#23454;&#25968;&#25454;&#26497;&#20854;&#30456;&#20284;&#19988;&#24102;&#26377;&#22522;&#20934;&#22240;&#26524;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#29992;&#20110;&#23450;&#37327;&#24615;&#33021;&#35780;&#20272;&#12290;&#35813;&#27969;&#31243;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#27491;&#24577;&#27969;&#25429;&#25417;&#36924;&#30495;&#30340;&#21160;&#24577;&#65292;&#25552;&#21462;&#20551;&#35774;&#30340;&#22240;&#26524;&#22270;&#65292;&#24182;&#29983;&#25104;&#36866;&#21512;&#31639;&#27861;&#35780;&#20272;&#30340;&#22810;&#26679;&#21270;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2310.01753</link><description>&lt;p&gt;
CausalTime&#65306;&#29992;&#20110;&#22240;&#26524;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#30340;&#36924;&#30495;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery. (arXiv:2310.01753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01753
&lt;/p&gt;
&lt;p&gt;
CausalTime&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#25104;&#36924;&#30495;&#26102;&#38388;&#24207;&#21015;&#30340;&#27969;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#30495;&#23454;&#25968;&#25454;&#26497;&#20854;&#30456;&#20284;&#19988;&#24102;&#26377;&#22522;&#20934;&#22240;&#26524;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#29992;&#20110;&#23450;&#37327;&#24615;&#33021;&#35780;&#20272;&#12290;&#35813;&#27969;&#31243;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#27491;&#24577;&#27969;&#25429;&#25417;&#36924;&#30495;&#30340;&#21160;&#24577;&#65292;&#25552;&#21462;&#20551;&#35774;&#30340;&#22240;&#26524;&#22270;&#65292;&#24182;&#29983;&#25104;&#36866;&#21512;&#31639;&#27861;&#35780;&#20272;&#30340;&#22810;&#26679;&#21270;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65288;TSCD&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#25110;&#39044;&#27979;&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CausalTime&#27969;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#31867;&#20284;&#30495;&#23454;&#25968;&#25454;&#19988;&#24102;&#26377;&#22522;&#20934;&#22240;&#26524;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20197;&#36827;&#34892;&#23450;&#37327;&#24615;&#33021;&#35780;&#20272;&#12290;&#35813;&#27969;&#31243;&#20174;&#29305;&#23450;&#22330;&#26223;&#30340;&#30495;&#23454;&#35266;&#27979;&#25968;&#25454;&#24320;&#22987;&#65292;&#29983;&#25104;&#30456;&#21305;&#37197;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#27491;&#24577;&#27969;&#26469;&#20934;&#30830;&#25429;&#25417;&#36924;&#30495;&#30340;&#21160;&#24577;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37325;&#35201;&#24615;&#20998;&#26512;&#25110;&#21033;&#29992;&#20808;&#21069;&#30693;&#35782;&#65292;&#25552;&#21462;&#20551;&#35774;&#30340;&#22240;&#26524;&#22270;&#12290;&#31532;&#19977;&#65292;&#22312;&#22240;&#26524;&#27169;&#22411;&#20013;&#23558;&#22240;&#26524;&#39033;&#12289;&#27531;&#24046;&#39033;&#21644;&#22122;&#22768;&#39033;&#25286;&#20998;&#65292;&#24471;&#21040;&#22522;&#26412;&#30495;&#23454;&#30340;&#22240;&#26524;&#22270;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#25311;&#21512;&#30340;&#32593;&#32476;&#21644;&#27966;&#29983;&#30340;&#22240;&#26524;&#22270;&#65292;&#25105;&#20204;&#29983;&#25104;&#36866;&#21512;&#31639;&#27861;&#35780;&#20272;&#30340;&#22810;&#26679;&#21270;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series causal discovery (TSCD) is a fundamental problem of machine learning. However, existing synthetic datasets cannot properly evaluate or predict the algorithms' performance on real data. This study introduces the CausalTime pipeline to generate time-series that highly resemble the real data and with ground truth causal graphs for quantitative performance evaluation. The pipeline starts from real observations in a specific scenario and produces a matching benchmark dataset. Firstly, we harness deep neural networks along with normalizing flow to accurately capture realistic dynamics. Secondly, we extract hypothesized causal graphs by performing importance analysis on the neural network or leveraging prior knowledge. Thirdly, we derive the ground truth causal graphs by splitting the causal model into causal term, residual term, and noise term. Lastly, using the fitted network and the derived causal graph, we generate corresponding versatile time-series proper for algorithm asses
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20110;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#12289;&#32447;&#24615;&#21028;&#21035;&#27169;&#22411;&#12289;k&#26368;&#36817;&#37051;&#27169;&#22411;&#12289;&#20915;&#31574;&#26641;&#27169;&#22411;&#12289;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#12289;SVC BernoulliNB&#27169;&#22411;&#21644;GaussianNB&#27169;&#22411;&#37117;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#32593;&#32476;&#20999;&#29255;&#65292;&#24182;&#20026;&#21160;&#24577;&#35843;&#25972;&#32593;&#32476;&#20999;&#29255;&#26469;&#28385;&#36275;&#19981;&#21516;&#26381;&#21153;&#38656;&#27714;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01747</link><description>&lt;p&gt;
5G&#32593;&#32476;&#20999;&#29255;&#65306;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
5G Network Slicing: Analysis of Multiple Machine Learning Classifiers. (arXiv:2310.01747v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20110;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#12289;&#32447;&#24615;&#21028;&#21035;&#27169;&#22411;&#12289;k&#26368;&#36817;&#37051;&#27169;&#22411;&#12289;&#20915;&#31574;&#26641;&#27169;&#22411;&#12289;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#12289;SVC BernoulliNB&#27169;&#22411;&#21644;GaussianNB&#27169;&#22411;&#37117;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#32593;&#32476;&#20999;&#29255;&#65292;&#24182;&#20026;&#21160;&#24577;&#35843;&#25972;&#32593;&#32476;&#20999;&#29255;&#26469;&#28385;&#36275;&#19981;&#21516;&#26381;&#21153;&#38656;&#27714;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#20010;&#29289;&#29702;5G&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#20998;&#21106;&#25104;&#20960;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#34394;&#25311;&#32593;&#32476;&#20999;&#29255;&#65292;&#22914;&#24102;&#23485;&#12289;&#24310;&#36831;&#12289;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#26381;&#21153;&#36136;&#37327;&#65292;&#36825;&#23601;&#26159;5G&#32593;&#32476;&#20999;&#29255;&#12290;&#27599;&#20010;&#20999;&#29255;&#26159;&#19968;&#20010;&#29420;&#31435;&#30340;&#36923;&#36753;&#32593;&#32476;&#65292;&#28385;&#36275;&#29305;&#23450;&#26381;&#21153;&#25110;&#29992;&#20363;&#30340;&#35201;&#27714;&#65292;&#22914;&#34394;&#25311;&#29616;&#23454;&#12289;&#28216;&#25103;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25110;&#24037;&#19994;&#33258;&#21160;&#21270;&#12290;&#32593;&#32476;&#20999;&#29255;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20197;&#28385;&#36275;&#26381;&#21153;&#30340;&#21464;&#21270;&#38656;&#27714;&#65292;&#20174;&#32780;&#20197;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#22312;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#19978;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#26381;&#21153;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#12289;&#32447;&#24615;&#21028;&#21035;&#27169;&#22411;&#12289;k&#26368;&#36817;&#37051;&#27169;&#22411;&#12289;&#20915;&#31574;&#26641;&#27169;&#22411;&#12289;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#12289;SVC BernoulliNB&#27169;&#22411;&#21644;GaussianNB&#27169;&#22411;&#65292;&#20197;&#30740;&#31350;&#27599;&#20010;&#27169;&#22411;&#22312;&#26816;&#27979;&#32593;&#32476;&#20999;&#29255;&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#35813;&#25253;&#21578;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;...
&lt;/p&gt;
&lt;p&gt;
The division of one physical 5G communications infrastructure into several virtual network slices with distinct characteristics such as bandwidth, latency, reliability, security, and service quality is known as 5G network slicing. Each slice is a separate logical network that meets the requirements of specific services or use cases, such as virtual reality, gaming, autonomous vehicles, or industrial automation. The network slice can be adjusted dynamically to meet the changing demands of the service, resulting in a more cost-effective and efficient approach to delivering diverse services and applications over a shared infrastructure. This paper assesses various machine learning techniques, including the logistic regression model, linear discriminant model, k-nearest neighbor's model, decision tree model, random forest model, SVC BernoulliNB model, and GaussianNB model, to investigate the accuracy and precision of each model on detecting network slices. The report also gives an overview
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#30697;&#38453;&#25277;&#26679;&#8221;&#30340;&#24555;&#36895;&#38543;&#26426;&#20302;&#31209;&#20998;&#35299;&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#30697;&#38453;&#30340;&#32500;&#24230;&#38477;&#20302;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01739</link><description>&lt;p&gt;
&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#38543;&#26426;&#32500;&#24230;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Randomized Dimension Reduction with Statistical Guarantees. (arXiv:2310.01739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#30697;&#38453;&#25277;&#26679;&#8221;&#30340;&#24555;&#36895;&#38543;&#26426;&#20302;&#31209;&#20998;&#35299;&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#30697;&#38453;&#30340;&#32500;&#24230;&#38477;&#20302;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#21644;&#24222;&#22823;&#25968;&#25454;&#26159;&#29616;&#20195;&#31639;&#27861;&#21462;&#24471;&#21069;&#25152;&#26410;&#26377;&#25104;&#21151;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#31185;&#23398;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#22686;&#38271;&#30340;&#32500;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#24037;&#20316;&#37327;&#20063;&#32473;&#35745;&#31639;&#21644;&#25968;&#25454;&#32858;&#21512;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25104;&#26412;&#12290;&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#25918;&#32531;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#20174;&#30828;&#20214;&#23618;&#38754;&#38477;&#20302;&#30340;&#20943;&#36895;&#65292;&#24555;&#36895;&#22823;&#22411;&#32463;&#20856;&#20363;&#31243;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#39640;&#25928;&#31639;&#27861;&#23545;&#20110;&#25512;&#21160;&#31639;&#27861;&#28508;&#21147;&#30340;&#26497;&#38480;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#12290;&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#20123;&#24555;&#36895;&#25191;&#34892;&#21644;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;&#20174;&#35745;&#31639;&#25928;&#29575;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#22522;&#20110;&#8220;&#30697;&#38453;&#25277;&#26679;&#8221;&#30340;&#22823;&#22411;&#30697;&#38453;&#30340;&#24555;&#36895;&#38543;&#26426;&#20302;&#31209;&#20998;&#35299;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#32500;&#24230;&#38477;&#20302;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models and enormous data are essential driving forces of the unprecedented successes achieved by modern algorithms, especially in scientific computing and machine learning. Nevertheless, the growing dimensionality and model complexity, as well as the non-negligible workload of data pre-processing, also bring formidable costs to such successes in both computation and data aggregation. As the deceleration of Moore's Law slackens the cost reduction of computation from the hardware level, fast heuristics for expensive classical routines and efficient algorithms for exploiting limited data are increasingly indispensable for pushing the limit of algorithm potency. This thesis explores some of such algorithms for fast execution and efficient data utilization.  From the computational efficiency perspective, we design and analyze fast randomized low-rank decomposition algorithms for large matrices based on "matrix sketching", which can be regarded as a dimension reduction strategy in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01737</link><description>&lt;p&gt;
&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#40065;&#26834;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#19987;&#23478;&#31034;&#33539;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#22810;&#31181;&#40657;&#30418;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
&lt;/p&gt;</description></item><item><title>Nugget&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23558;&#35821;&#35328;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#65292;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#65292;&#22312;&#35821;&#20041;&#27604;&#36739;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;</title><link>http://arxiv.org/abs/2310.01732</link><description>&lt;p&gt;
Nugget: &#25991;&#26412;&#30340;&#31070;&#32463;&#32858;&#21512;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Nugget: Neural Agglomerative Embeddings of Text. (arXiv:2310.01732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01732
&lt;/p&gt;
&lt;p&gt;
Nugget&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23558;&#35821;&#35328;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#65292;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#65292;&#22312;&#35821;&#20041;&#27604;&#36739;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#23884;&#20837;&#25991;&#26412;&#24207;&#21015;&#26159;&#19968;&#20010;&#24191;&#27867;&#38656;&#27714;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20391;&#37325;&#20110;&#24658;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#36890;&#24120;&#38543;&#36755;&#20837;&#30340;&#38271;&#24230;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Nugget&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23558;&#35821;&#35328;&#32534;&#30721;&#20026;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23398;&#20064;&#36825;&#20123;nuggets&#65292;&#24182;&#30452;&#35266;&#22320;&#23558;&#35821;&#35328;&#20998;&#21106;&#25104;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Nugget&#22312;&#28041;&#21450;&#35821;&#20041;&#27604;&#36739;&#30340;&#20219;&#21153;&#20013;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#32039;&#20945;&#21333;&#20803;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#26032;&#30340;LM&#21487;&#33021;&#20250;&#23545;&#26356;&#22823;&#37327;&#30340;&#20869;&#23481;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding text sequences is a widespread requirement in modern language understanding. Existing approaches focus largely on constant-size representations. This is problematic, as the amount of information contained in text often varies with the length of the input. We propose a solution called Nugget, which encodes language into a representation based on a dynamically selected subset of input tokens. These nuggets are learned through tasks like autoencoding and machine translation, and intuitively segment language into meaningful units. We demonstrate Nugget outperforms related approaches in tasks involving semantic comparison. Finally, we illustrate these compact units allow for expanding the contextual window of a language model (LM), suggesting new future LMs that can condition on significantly larger amounts of content.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.01728</link><description>&lt;p&gt;
Time-LLM: &#36890;&#36807;&#37325;&#26032;&#32534;&#31243;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01728
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#21160;&#24577;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#19981;&#21516;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#22823;&#22411;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#19987;&#38376;&#21270;&#30340;&#65292;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#24212;&#29992;&#35774;&#35745;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;NLP&#21644;CV&#39046;&#22495;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#30340;&#24207;&#21015;&#26631;&#35760;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#20197;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Time-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#37325;&#29992;LLMs&#26469;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#65292;&#31216;&#20026;LLMAO&#65292;&#20855;&#26377;&#26080;&#38656;&#27979;&#35797;&#35206;&#30422;&#20449;&#24687;&#23601;&#33021;&#23450;&#20301;&#26377;&#38382;&#39064;&#30340;&#20195;&#30721;&#34892;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01726</link><description>&lt;p&gt;
&#29992;&#20110;&#26080;&#27979;&#35797;&#25925;&#38556;&#23450;&#20301;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Test-Free Fault Localization. (arXiv:2310.01726v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#65292;&#31216;&#20026;LLMAO&#65292;&#20855;&#26377;&#26080;&#38656;&#27979;&#35797;&#35206;&#30422;&#20449;&#24687;&#23601;&#33021;&#23450;&#20301;&#26377;&#38382;&#39064;&#30340;&#20195;&#30721;&#34892;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#23450;&#20301;&#65288;FL&#65289;&#26088;&#22312;&#33258;&#21160;&#23450;&#20301;&#26377;&#38382;&#39064;&#30340;&#20195;&#30721;&#34892;&#65292;&#36825;&#26159;&#35768;&#22810;&#25163;&#21160;&#21644;&#33258;&#21160;&#35843;&#35797;&#20219;&#21153;&#30340;&#20851;&#38190;&#31532;&#19968;&#27493;&#12290;&#20197;&#21069;&#30340;FL&#25216;&#26415;&#20551;&#35774;&#25552;&#20379;&#36755;&#20837;&#27979;&#35797;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#31243;&#24207;&#20998;&#26512;&#12289;&#31243;&#24207;&#25554;&#26729;&#25110;&#25968;&#25454;&#39044;&#22788;&#29702;&#12290;&#20197;&#24448;&#30340;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#38469;&#31243;&#24207;&#19978;&#20135;&#29983;&#26377;&#38480;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24456;&#23569;&#31034;&#20363;&#19978;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#34892;&#32423;&#25925;&#38556;&#23450;&#20301;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#22312;LLMs&#23398;&#20064;&#30340;&#34920;&#31034;&#20043;&#19978;&#24494;&#35843;&#19968;&#23567;&#32452;&#21452;&#21521;&#36866;&#37197;&#22120;&#23618;&#26469;&#20811;&#26381;LLMs&#30340;&#33258;&#24038;&#21521;&#21491;&#29305;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;LLMAO&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#27979;&#35797;&#35206;&#30422;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#26377;&#38382;&#39064;&#30340;&#20195;&#30721;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;3.5&#20159;&#12289;60&#20159;&#21644;160&#20159;&#27573;&#26469;&#24494;&#35843;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion para
&lt;/p&gt;</description></item><item><title>PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction.</title><link>http://arxiv.org/abs/2310.01720</link><description>&lt;p&gt;
PrACTiS: Perceiver-Attentional Copulas for Time Series&#65288;&#26102;&#38388;&#24207;&#21015;&#30340;&#24863;&#30693;-&#27880;&#24847;&#21147;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65289;
&lt;/p&gt;
&lt;p&gt;
PrACTiS: Perceiver-Attentional Copulas for Time Series. (arXiv:2310.01720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01720
&lt;/p&gt;
&lt;p&gt;
PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#32852;&#21512;&#20998;&#24067;&#32467;&#26500;&#30340;Transformer&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36807;&#20110;&#20381;&#36182;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24863;&#30693;&#22120;&#26550;&#26500;&#19982;&#32852;&#21512;&#20998;&#24067;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#24863;&#30693;&#22120;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#22797;&#26434;&#30340;&#39640;&#32500;&#22810;&#27169;&#24577;&#25968;&#25454;&#36716;&#25442;&#20026;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#38656;&#27714;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#28857;&#25512;&#26029;&#21644;&#23616;&#37096;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25554;&#34917;&#26679;&#26412;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#32852;&#21512;&#20998;&#24067;&#30340;&#27880;&#24847;&#21147;&#21644;&#36755;&#20986;&#26041;&#24046;&#27979;&#35797;&#26426;&#21046;&#26469;&#25429;&#25417;&#32570;&#22833;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21516;&#26102;&#20943;&#23569;&#39044;&#27979;&#36807;&#31243;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers incorporating copula structures have demonstrated remarkable performance in time series prediction. However, their heavy reliance on self-attention mechanisms demands substantial computational resources, thus limiting their practical utility across a wide range of tasks. In this work, we present a model that combines the perceiver architecture with a copula structure to enhance time-series forecasting. By leveraging the perceiver as the encoder, we efficiently transform complex, high-dimensional, multimodal data into a compact latent space, thereby significantly reducing computational demands. To further reduce complexity, we introduce midpoint inference and local attention mechanisms, enabling the model to capture dependencies within imputed samples effectively. Subsequently, we deploy the copula-based attention and output variance testing mechanism to capture the joint distribution of missing data, while simultaneously mitigating error propagation during prediction. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01717</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#30340;&#38598;&#25104;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#23558;&#21477;&#23376;&#30340;&#35789;&#21644;&#30701;&#35821;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#19981;&#20351;&#29992;&#35821;&#35328;&#23398;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#25429;&#25417;&#21040;&#20102;&#35299;&#26512;&#32467;&#26500;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26641;&#24179;&#22343;&#8221;&#30340;&#27010;&#24565;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65307;&#36825;&#31181;&#38598;&#25104;-&#33976;&#39311;&#30340;&#36807;&#31243;&#26159;&#32531;&#35299;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#34920;&#29616;&#20986;&#20854;&#22312;&#19981;&#21516;&#38598;&#25104;&#32452;&#20214;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01714</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31867;&#27604;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Analogical Reasoners. (arXiv:2310.01714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#26377;&#26631;&#35760;&#30340;&#25512;&#29702;&#36807;&#31243;&#31034;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#26088;&#22312;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#21463;&#31867;&#27604;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#31867;&#27604;&#25512;&#29702;&#26159;&#19968;&#31181;&#35748;&#30693;&#36807;&#31243;&#65292;&#20154;&#31867;&#20174;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#20013;&#33719;&#21462;&#30693;&#35782;&#26469;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19978;&#19979;&#25991;&#20013;&#30340;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#28982;&#21518;&#35299;&#20915;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20248;&#28857;&#65306;&#23427;&#30465;&#21435;&#20102;&#26631;&#35760;&#25110;&#26816;&#32034;&#23454;&#20363;&#30340;&#38656;&#27714;&#65292;&#25552;&#20379;&#20102;&#26222;&#36866;&#24615;&#21644;&#20415;&#21033;&#24615;&#65307;&#23427;&#36824;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#38382;&#39064;&#23450;&#21046;&#29983;&#25104;&#30340;&#31034;&#20363;&#21644;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#20248;&#20110;0-shot CoT&#21644;&#25163;&#21160;few-shot CoT&#65292;&#21253;&#25324;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, Analogical Prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#29420;&#29305;&#30340;&#38543;&#26426;&#20002;&#24323;&#27169;&#24335;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#21482;&#20381;&#38752;&#37325;&#26500;&#35823;&#24046;&#26469;&#25552;&#20379;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;DCGAN&#30456;&#23218;&#32654;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.01712</link><description>&lt;p&gt;
&#20002;&#24323;&#27169;&#24335;&#30340;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Autoencoding of Dropout Patterns. (arXiv:2310.01712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#29420;&#29305;&#30340;&#38543;&#26426;&#20002;&#24323;&#27169;&#24335;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#21482;&#20381;&#38752;&#37325;&#26500;&#35823;&#24046;&#26469;&#25552;&#20379;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;DCGAN&#30456;&#23218;&#32654;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#38543;&#26426;&#20002;&#24323;&#27169;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#27169;&#24335;&#20316;&#20026;&#34987;&#32534;&#30721;&#30340;&#20449;&#24687;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#30456;&#24212;&#30340;&#25968;&#25454;&#28857;&#12290;&#30001;&#20110;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#35757;&#32451;&#20165;&#20381;&#36182;&#20110;&#37325;&#26500;&#35823;&#24046;&#65292;&#25152;&#20197;&#30456;&#27604;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#19982;DCGAN&#30456;&#23218;&#32654;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generative model termed Deciphering Autoencoders. In this model, we assign a unique random dropout pattern to each data point in the training dataset and then train an autoencoder to reconstruct the corresponding data point using this pattern as information to be encoded. Since the training of Deciphering Autoencoders relies solely on reconstruction error, it offers more stable training than other generative models. Despite its simplicity, Deciphering Autoencoders show comparable sampling quality to DCGAN on the CIFAR-10 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26576;&#20123;MDP&#21487;&#20197;&#29992;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#34920;&#31034;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#30005;&#36335;&#22797;&#26434;&#24230;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#24120;&#27604;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01706</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Representation Complexity of Model-based and Model-free Reinforcement Learning. (arXiv:2310.01706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26576;&#20123;MDP&#21487;&#20197;&#29992;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#34920;&#31034;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#30005;&#36335;&#22797;&#26434;&#24230;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#24120;&#27604;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#23384;&#22312;&#19968;&#31867;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#23427;&#20204;&#30340;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#29992;&#20855;&#26377;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#34920;&#31034;&#65292;&#32780;&#26368;&#20248;&#30340;$Q$-&#20989;&#25968;&#22312;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#20013;&#36973;&#21463;&#25351;&#25968;&#32423;&#30005;&#36335;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#20851;&#27880;&#36924;&#36817;&#35823;&#24046;&#24182;&#24314;&#31435;&#21040;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20174;&#26032;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#35282;&#24230;&#20026;&#20160;&#20040;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#24120;&#27604;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35265;&#35299;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29615;&#22659;&#30340;&#30495;&#23454;&#35268;&#21017;&#65288;&#27169;&#22411;&#65289;&#26131;&#20110;&#34920;&#31034;&#65292;&#32780;&#20854;&#20182;&#25968;&#37327;&#65292;&#22914;$Q$-&#20989;&#25968;&#65292;&#20284;&#20046;&#24456;&#22797;&#26434;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36716;&#31227;&#26680;&#20989;&#25968;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SubFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#22270;&#36716;&#25442;&#22120;&#65292;&#36890;&#36807;&#22312;&#23376;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26469;&#38477;&#20302;&#26631;&#35760;&#25968;&#37327;&#24182;&#22686;&#24378;&#23398;&#20064;&#38271;&#31243;&#20132;&#20114;&#12290;&#22312;&#21270;&#23398;&#32467;&#26500;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#65292;SubFormer&#22312;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#22270;&#36716;&#25442;&#22120;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#20960;&#20998;&#38047;&#30340;&#26102;&#38388;&#36827;&#34892;&#35757;&#32451;&#12290;&#27880;&#24847;&#26435;&#37325;&#30340;&#35299;&#37322;&#26174;&#31034;SubFormer&#23637;&#29616;&#20986;&#26377;&#38480;&#30340;&#20851;&#27880;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2310.01704</link><description>&lt;p&gt;
Transformers&#26159;&#39640;&#25928;&#30340;&#20998;&#23618;&#21270;&#21270;&#23398;&#22270;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transformers are efficient hierarchical chemical graph learners. (arXiv:2310.01704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SubFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#22270;&#36716;&#25442;&#22120;&#65292;&#36890;&#36807;&#22312;&#23376;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26469;&#38477;&#20302;&#26631;&#35760;&#25968;&#37327;&#24182;&#22686;&#24378;&#23398;&#20064;&#38271;&#31243;&#20132;&#20114;&#12290;&#22312;&#21270;&#23398;&#32467;&#26500;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#65292;SubFormer&#22312;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#22270;&#36716;&#25442;&#22120;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#20960;&#20998;&#38047;&#30340;&#26102;&#38388;&#36827;&#34892;&#35757;&#32451;&#12290;&#27880;&#24847;&#26435;&#37325;&#30340;&#35299;&#37322;&#26174;&#31034;SubFormer&#23637;&#29616;&#20986;&#26377;&#38480;&#30340;&#20851;&#27880;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25913;&#32534;&#32780;&#26469;&#65292;&#27491;&#22312;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#24403;&#20195;&#30340;&#22270;&#36716;&#25442;&#22120;&#36890;&#24120;&#23558;&#33410;&#28857;&#25110;&#36793;&#35270;&#20026;&#29420;&#31435;&#30340;&#26631;&#35760;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#21363;&#20351;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#24418;&#20063;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#65292;&#22240;&#20026;&#33258;&#25105;&#27880;&#24847;&#22797;&#26434;&#24230;&#38543;&#26631;&#35760;&#25968;&#37327;&#30340;&#24179;&#26041;&#32423;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SubFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#22312;&#23376;&#22270;&#19978;&#25805;&#20316;&#30340;&#22270;&#36716;&#25442;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20943;&#23569;&#20102;&#26631;&#35760;&#25968;&#37327;&#65292;&#24182;&#22686;&#24378;&#20102;&#23398;&#20064;&#38271;&#31243;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#21270;&#23398;&#32467;&#26500;&#19978;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#20013;&#23637;&#31034;&#20102;SubFormer&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#22270;&#36716;&#25442;&#22120;&#30456;&#31454;&#20105;&#65292;&#20351;&#29992;&#28040;&#36153;&#32423;&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#30340;&#26102;&#38388;&#32422;&#20026;&#20960;&#20998;&#38047;&#12290;&#25105;&#20204;&#20351;&#29992;&#21270;&#23398;&#32467;&#26500;&#35299;&#37322;&#20102;&#27880;&#24847;&#26435;&#37325;&#12290;&#25105;&#20204;&#34920;&#26126;SubFormer&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#20851;&#27880;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, adapted from natural language processing, are emerging as a leading approach for graph representation learning. Contemporary graph transformers often treat nodes or edges as separate tokens. This approach leads to computational challenges for even moderately-sized graphs due to the quadratic scaling of self-attention complexity with token count. In this paper, we introduce SubFormer, a graph transformer that operates on subgraphs that aggregate information by a message-passing mechanism. This approach reduces the number of tokens and enhances learning long-range interactions. We demonstrate SubFormer on benchmarks for predicting molecular properties from chemical structures and show that it is competitive with state-of-the-art graph transformers at a fraction of the computational cost, with training times on the order of minutes on a consumer-grade graphics card. We interpret the attention weights in terms of chemical structures. We show that SubFormer exhibits limited ov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#36866;&#23450;&#23545;&#35282;&#21270;&#38382;&#39064;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25311;&#35889;&#29702;&#35770;&#30340;&#8220;&#25200;&#21160;&#28982;&#21518;&#23545;&#35282;&#21270;&#8221;&#65288;PTD&#65289;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#38271;&#24207;&#21015;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01698</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#23545;&#35282;&#21270;&#25552;&#39640;&#38271;&#24207;&#21015;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustifying State-space Models for Long Sequences via Approximate Diagonalization. (arXiv:2310.01698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#36866;&#23450;&#23545;&#35282;&#21270;&#38382;&#39064;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25311;&#35889;&#29702;&#35770;&#30340;&#8220;&#25200;&#21160;&#28982;&#21518;&#23545;&#35282;&#21270;&#8221;&#65288;PTD&#65289;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#38271;&#24207;&#21015;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#20316;&#20026;&#23398;&#20064;&#38271;&#26399;&#24207;&#21015;&#20219;&#21153;&#30340;&#26694;&#26550;&#24050;&#32463;&#20986;&#29616;&#12290;&#19968;&#20010;&#20363;&#23376;&#26159;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#65288;S4&#65289;&#23618;&#65292;&#23427;&#20351;&#29992;HiPPO&#21021;&#22987;&#21270;&#26694;&#26550;&#30340;&#23545;&#35282;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;S4&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#24102;&#26469;&#20102;&#25361;&#25112;&#65307;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#27169;&#22411;&#22914;S4D&#21644;S5&#32771;&#34385;&#20102;&#32431;&#23545;&#35282;&#32467;&#26500;&#12290;&#36825;&#20010;&#36873;&#25321;&#31616;&#21270;&#20102;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20449;&#36947;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#23545;HiPPO&#26694;&#26550;&#36827;&#34892;&#23545;&#35282;&#21270;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#19981;&#36866;&#23450;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36825;&#31867;&#19981;&#36866;&#23450;&#23545;&#35282;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#21518;&#21521;&#31283;&#23450;&#30340;&#8220;&#25200;&#21160;&#28982;&#21518;&#23545;&#35282;&#21270;&#8221;&#65288;PTD&#65289;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#38750;&#27491;&#24120;&#31639;&#23376;&#30340;&#25311;&#35889;&#29702;&#35770;&#65292;&#24182;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#38750;&#27491;&#24120;&#30697;&#38453;&#30340;&#36817;&#20284;&#23545;&#35282;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable "perturb-then-diagonalize" (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defini
&lt;/p&gt;</description></item><item><title>DANI&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;&#12289;&#20855;&#26377;&#20445;&#25345;&#25299;&#25169;&#32467;&#26500;&#23646;&#24615;&#30340;&#25193;&#25955;&#24863;&#30693;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01696</link><description>&lt;p&gt;
DANI:&#24555;&#36895;&#30340;&#12289;&#20855;&#26377;&#20445;&#25345;&#25299;&#25169;&#32467;&#26500;&#23646;&#24615;&#30340;&#25193;&#25955;&#24863;&#30693;&#32593;&#32476;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DANI: Fast Diffusion Aware Network Inference with Preserving Topological Structure Property. (arXiv:2310.01696v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01696
&lt;/p&gt;
&lt;p&gt;
DANI&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;&#12289;&#20855;&#26377;&#20445;&#25345;&#25299;&#25169;&#32467;&#26500;&#23646;&#24615;&#30340;&#25193;&#25955;&#24863;&#30693;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#32593;&#32476;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#25968;&#25454;&#35775;&#38382;&#38480;&#21046;&#23548;&#33268;&#20102;&#33719;&#21462;&#23436;&#25972;&#25299;&#25169;&#32467;&#26500;&#30340;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#19978;&#30340;&#25193;&#25955;&#20449;&#24687;&#26159;&#21487;&#29992;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#26469;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25512;&#26029;&#28508;&#22312;&#30340;&#32593;&#32476;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#21482;&#20851;&#27880;&#25512;&#26029;&#26356;&#22810;&#30340;&#38142;&#25509;&#65292;&#24573;&#35270;&#20102;&#20445;&#25345;&#28508;&#22312;&#31038;&#20132;&#32593;&#32476;&#20851;&#38190;&#25299;&#25169;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DANI&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#32467;&#26500;&#29305;&#24615;&#26469;&#25512;&#26029;&#28508;&#22312;&#32593;&#32476;&#12290;&#23427;&#22522;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#32423;&#32852;&#20013;&#27966;&#29983;&#30340;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#30697;&#38453;&#65292;&#20197;&#21450;&#21487;&#20197;&#20174;&#32467;&#26500;&#35282;&#24230;&#35266;&#23519;&#32423;&#32852;&#34892;&#20026;&#30340;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;&#19982;&#33410;&#28857;&#25968;&#37327;&#12289;&#32423;&#32852;&#25968;&#37327;&#21644;&#32423;&#32852;&#24179;&#22343;&#38271;&#24230;&#30340;&#24179;&#26041;&#32447;&#24615;&#22686;&#38271;&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
The fast growth of social networks and their data access limitations in recent years has led to increasing difficulty in obtaining the complete topology of these networks. However, diffusion information over these networks is available, and many algorithms have been proposed to infer the underlying networks using this information. The previously proposed algorithms only focus on inferring more links and ignore preserving the critical topological characteristics of the underlying social networks. In this paper, we propose a novel method called DANI to infer the underlying network while preserving its structural properties. It is based on the Markov transition matrix derived from time series cascades, as well as the node-node similarity that can be observed in the cascade behavior from a structural point of view. In addition, the presented method has linear time complexity (increases linearly with the number of nodes, number of cascades, and square of the average length of cascades), and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.01690</link><description>&lt;p&gt;
&#20351;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28909;&#24102;&#27668;&#26059;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23545;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#21464;&#21270;&#65292;&#39123;&#39118;&#21464;&#24471;&#26356;&#21152;&#24378;&#28872;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#27861;&#27604;&#22522;&#20110;&#25968;&#23398;&#27169;&#22411;&#30340;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#32463;&#27982;&#23454;&#24800;&#21644;&#26131;&#20110;&#33719;&#21462;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#21355;&#26143;&#25104;&#20687;&#12289;&#36965;&#24863;&#21644;&#22823;&#27668;&#25968;&#25454;&#65292;&#37319;&#29992;&#32423;&#32852;&#26041;&#27861;&#36827;&#34892;&#39123;&#39118;&#36712;&#36857;&#21644;&#38477;&#27700;&#27169;&#24335;&#30340;&#39044;&#27979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20845;&#20010;&#20027;&#35201;&#30406;&#22320;&#30340;51&#20010;&#39123;&#39118;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32423;&#32852;&#27169;&#22411;&#30340;&#26368;&#32456;&#39044;&#27979;&#22312;36&#23567;&#26102;&#20869;&#26174;&#31034;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#25152;&#26377;&#19977;&#39033;&#20219;&#21153;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#21644;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#30340;&#20540;&#37117;&#36229;&#36807;&#20102;0.5&#21644;20dB&#12290;&#26412;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#25193;&#25955;&#27169;&#22411;&#31561;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#39640;&#24615;&#33021;&#38656;&#27714;&#65288;&#22914;&#39123;&#39118;&#39044;&#27979;&#65289;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#35745;&#31639;&#32463;&#27982;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#24230;&#33030;&#24369;&#22320;&#21306;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20108;&#27425;&#22238;&#24402;&#27169;&#22411;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#21147;&#23398;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#21160;&#21147;&#23398;&#21487;&#20197;&#29992;&#19968;&#20010;&#29305;&#23450;&#30340;&#31435;&#26041;&#26144;&#23556;&#26469;&#27010;&#25324;&#65292;&#24182;&#35814;&#32454;&#21010;&#20998;&#20102;&#20116;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23454;&#39564;&#20063;&#35777;&#26126;&#20102;&#36825;&#20123;&#38454;&#27573;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01687</link><description>&lt;p&gt;
&#20174;&#31283;&#23450;&#21040;&#28151;&#27788;&#65306;&#22312;&#20108;&#27425;&#22238;&#24402;&#20013;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression. (arXiv:2310.01687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20108;&#27425;&#22238;&#24402;&#27169;&#22411;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#21147;&#23398;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#21160;&#21147;&#23398;&#21487;&#20197;&#29992;&#19968;&#20010;&#29305;&#23450;&#30340;&#31435;&#26041;&#26144;&#23556;&#26469;&#27010;&#25324;&#65292;&#24182;&#35814;&#32454;&#21010;&#20998;&#20102;&#20116;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23454;&#39564;&#20063;&#35777;&#26126;&#20102;&#36825;&#20123;&#38454;&#27573;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20108;&#27425;&#22238;&#24402;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#22823;&#38454;&#24658;&#23450;&#27493;&#38271;&#23545;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#21147;&#23398;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#21160;&#21147;&#23398;&#21487;&#20197;&#34987;&#19968;&#20010;&#29305;&#23450;&#30340;&#31435;&#26041;&#26144;&#23556;&#25152;&#27010;&#25324;&#65292;&#33258;&#28982;&#22320;&#30001;&#27493;&#38271;&#21442;&#25968;&#21270;&#12290;&#36890;&#36807;&#23545;&#27493;&#38271;&#21442;&#25968;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#21449;&#20998;&#26512;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20116;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#65306;&#65288;1&#65289;&#21333;&#35843;&#12289;&#65288;2&#65289;&#25243;&#29289;&#32447;&#12289;&#65288;3&#65289;&#21608;&#26399;&#24615;&#12289;&#65288;4&#65289;&#28151;&#27788;&#21644;&#65288;5&#65289;&#21457;&#25955;&#65292;&#31934;&#30830;&#22320;&#21010;&#23450;&#20102;&#27599;&#20010;&#38454;&#27573;&#30340;&#36793;&#30028;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28041;&#21450;&#30456;&#20301;&#24674;&#22797;&#21644;&#20351;&#29992;&#20108;&#27425;&#28608;&#27963;&#20989;&#25968;&#21644;&#24658;&#23450;&#22806;&#23618;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20363;&#23376;&#65292;&#21033;&#29992;&#27491;&#20132;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#36825;&#20116;&#20010;&#38454;&#27573;&#20063;&#22312;&#19968;&#33324;&#30340;&#38750;&#27491;&#20132;&#25968;&#25454;&#20013;&#26174;&#29616;&#12290;&#25105;&#20204;&#36824;&#22312;&#21508;&#20010;&#38750;&#21333;&#35843;&#65288;&#38750;&#21457;&#25955;&#65289;&#38454;&#27573;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#30340;&#25512;&#24191;&#24615;&#33021;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#38454;&#27573;&#35757;&#32451;&#26102;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a comprehensive investigation into the dynamics of gradient descent using large-order constant step-sizes in the context of quadratic regression models. Within this framework, we reveal that the dynamics can be encapsulated by a specific cubic map, naturally parameterized by the step-size. Through a fine-grained bifurcation analysis concerning the step-size parameter, we delineate five distinct training phases: (1) monotonic, (2) catapult, (3) periodic, (4) chaotic, and (5) divergent, precisely demarcating the boundaries of each phase. As illustrations, we provide examples involving phase retrieval and two-layer neural networks employing quadratic activation functions and constant outer-layers, utilizing orthogonal training data. Our simulations indicate that these five phases also manifest with generic non-orthogonal data. We also empirically investigate the generalization performance when training in the various non-monotonic (and non-divergent) phases. In particular, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26126;&#30830;&#20102;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.01685</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Interpretability in Machine Learning for Medical Imaging. (arXiv:2310.01685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26126;&#30830;&#20102;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#23450;&#20041;&#23384;&#22312;&#19968;&#31181;&#26222;&#36941;&#30340;&#27169;&#31946;&#24863;&#12290;&#20026;&#20160;&#20040;&#38656;&#35201;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#37322;&#24615;&#65311;&#24403;&#38656;&#35201;&#35299;&#37322;&#24615;&#26102;&#65292;&#23454;&#38469;&#19978;&#36861;&#27714;&#30340;&#30446;&#26631;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#38656;&#35201;&#24418;&#24335;&#21270;&#12290;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#28857;&#20013;&#24120;&#35265;&#30340;&#23454;&#38469;&#20219;&#21153;&#21644;&#30446;&#26631;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#26680;&#24515;&#35201;&#32032;&#65306;&#23450;&#20301;&#12289;&#35270;&#35273;&#21487;&#35782;&#21035;&#24615;&#12289;&#29289;&#29702;&#24402;&#22240;&#21644;&#36879;&#26126;&#24230;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#22312;&#21307;&#23398;&#24433;&#20687;&#30340;&#32972;&#26223;&#19979;&#31995;&#32479;&#21270;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30340;&#23454;&#36341;&#35266;&#28857;&#28548;&#28165;&#20102;&#20855;&#20307;&#30340;&#21307;&#23398;&#24433;&#20687;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30446;&#26631;&#21644;&#32771;&#34385;&#22240;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#23454;&#29992;&#21644;&#25945;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability for machine learning models in medical imaging (MLMI) is an important direction of research. However, there is a general sense of murkiness in what interpretability means. Why does the need for interpretability in MLMI arise? What goals does one actually seek to address when interpretability is needed? To answer these questions, we identify a need to formalize the goals and elements of interpretability in MLMI. By reasoning about real-world tasks and goals common in both medical image analysis and its intersection with machine learning, we identify four core elements of interpretability: localization, visual recognizability, physical attribution, and transparency. Overall, this paper formalizes interpretability needs in the context of medical imaging, and our applied perspective clarifies concrete MLMI-specific goals and considerations in order to guide method design and improve real-world usage. Our goal is to provide practical and didactic information for model desig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#34892;&#20026;&#24178;&#39044;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#39044;&#38450;&#34880;&#31958;&#24322;&#24120;&#65292;&#26377;&#26395;&#23545;&#31038;&#20250;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.01684</link><description>&lt;p&gt;
&#35774;&#35745;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#34892;&#20026;&#24178;&#39044;&#26469;&#39044;&#38450;&#34880;&#31958;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Designing User-Centric Behavioral Interventions to Prevent Dysglycemia with Novel Counterfactual Explanations. (arXiv:2310.01684v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01684
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#34892;&#20026;&#24178;&#39044;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#39044;&#38450;&#34880;&#31958;&#24322;&#24120;&#65292;&#26377;&#26395;&#23545;&#31038;&#20250;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#27963;&#26041;&#24335;&#34892;&#20026;&#32500;&#25345;&#27491;&#24120;&#34880;&#31958;&#27700;&#24179;&#23545;&#20110;&#20445;&#25345;&#20581;&#24247;&#21644;&#39044;&#38450;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#39057;&#32321;&#25509;&#35302;&#34880;&#31958;&#24322;&#24120;&#65288;&#21363;&#39640;&#34880;&#31958;&#21644;&#20302;&#34880;&#31958;&#31561;&#24322;&#24120;&#20107;&#20214;&#65289;&#20250;&#23548;&#33268;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#21253;&#25324;&#31958;&#23615;&#30149;&#12289;&#32958;&#33039;&#30142;&#30149;&#21450;&#38656;&#36879;&#26512;&#27835;&#30103;&#12289;&#24515;&#32908;&#26775;&#27515;&#12289;&#20013;&#39118;&#12289;&#25130;&#32930;&#21644;&#27515;&#20129;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#39044;&#27979;&#34880;&#31958;&#24322;&#24120;&#24182;&#21521;&#29992;&#25143;&#25552;&#20379;&#34892;&#21160;&#21453;&#39304;&#20197;&#25913;&#21464;&#39278;&#39135;&#12289;&#36816;&#21160;&#21644;&#33647;&#29289;&#27835;&#30103;&#26469;&#39044;&#38450;&#24322;&#24120;&#34880;&#31958;&#20107;&#20214;&#30340;&#24037;&#20855;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#36755;&#20837;&#20294;&#23548;&#33268;&#19981;&#21516;&#39044;&#27979;&#32467;&#26524;&#30340;&#20551;&#35774;&#23454;&#20363;&#65292;&#25552;&#20379;&#27169;&#22411;&#20026;&#20309;&#23545;&#29305;&#23450;&#39044;&#27979;&#30340;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#21487;&#20197;&#34987;&#35270;&#20026;&#35774;&#35745;AI&#39537;&#21160;&#30340;&#20581;&#24247;&#24178;&#39044;&#26469;&#39044;&#38450;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#65288;&#22914;&#34880;&#31958;&#24322;&#24120;&#65289;&#30340;&#19968;&#31181;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;GlyCoa...
&lt;/p&gt;
&lt;p&gt;
Maintaining normal blood glucose levels through lifestyle behaviors is central to maintaining health and preventing disease. Frequent exposure to dysglycemia (i.e., abnormal glucose events such as hyperlycemia and hypoglycemia) leads to chronic complications including diabetes, kidney disease and need for dialysis, myocardial infarction, stroke, amputation, and death. Therefore, a tool capable of predicting dysglycemia and offering users actionable feedback about how to make changes in their diet, exercise, and medication to prevent abnormal glycemic events could have significant societal impacts. Counterfactual explanations can provide insights into why a model made a particular prediction by generating hypothetical instances that are similar to the original input but lead to a different prediction outcome. Therefore, counterfactuals can be viewed as a means to design AI-driven health interventions to prevent adverse health outcomes such as dysglycemia. In this paper, we design GlyCoa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#21487;&#20132;&#25442;&#32553;&#25918;&#12290;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#20989;&#25968;&#30340;&#34892;&#20026;&#65292;&#24182;&#30830;&#23450;&#20102;&#23485;&#24230;&#21644;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#30340;&#21487;&#20132;&#25442;&#24615;&#26465;&#20214;&#65292;&#24182;&#30740;&#31350;&#20102;&#31070;&#32463;&#21327;&#26041;&#24046;&#26680;&#30340;&#21487;&#20132;&#25442;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24403;&#20998;&#25903;&#36866;&#24403;&#32553;&#25918;&#20197;&#36991;&#20813;&#29190;&#28856;&#34892;&#20026;&#26102;&#65292;&#23558;&#23485;&#24230;&#21644;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#20250;&#24471;&#21040;&#30456;&#21516;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.01683</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#20132;&#25442;&#23485;&#24230;&#21644;&#28145;&#24230;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Commutative Width and Depth Scaling in Deep Neural Networks. (arXiv:2310.01683v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#21487;&#20132;&#25442;&#32553;&#25918;&#12290;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#20989;&#25968;&#30340;&#34892;&#20026;&#65292;&#24182;&#30830;&#23450;&#20102;&#23485;&#24230;&#21644;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#30340;&#21487;&#20132;&#25442;&#24615;&#26465;&#20214;&#65292;&#24182;&#30740;&#31350;&#20102;&#31070;&#32463;&#21327;&#26041;&#24046;&#26680;&#30340;&#21487;&#20132;&#25442;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24403;&#20998;&#25903;&#36866;&#24403;&#32553;&#25918;&#20197;&#36991;&#20813;&#29190;&#28856;&#34892;&#20026;&#26102;&#65292;&#23558;&#23485;&#24230;&#21644;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#20250;&#24471;&#21040;&#30456;&#21516;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#21487;&#20132;&#25442;&#32553;&#25918;&#30340;&#31995;&#21015;&#35770;&#25991;&#30340;&#31532;&#20108;&#31687;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#31070;&#32463;&#20989;&#25968;&#65288;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20989;&#25968;&#65289;&#22312;&#23485;&#24230;&#21644;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#26368;&#32456;&#30830;&#23450;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#21487;&#20132;&#25442;&#24615;&#25104;&#31435;&#65292;&#21363;&#26080;&#35770;&#22914;&#20309;&#21462;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#26497;&#38480;&#65292;&#31070;&#32463;&#20989;&#25968;&#37117;&#36235;&#21521;&#20110;&#30456;&#21516;&#30340;&#26497;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#21644;&#23450;&#20041;&#20102;&#21487;&#20132;&#25442;&#24615;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#23545;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#21644;&#32553;&#25918;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#21327;&#26041;&#24046;&#26680;&#30340;&#21487;&#20132;&#25442;&#24615;&#65292;&#35813;&#26680;&#21453;&#26144;&#20102;&#32593;&#32476;&#23618;&#23545;&#25968;&#25454;&#30340;&#20998;&#31163;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25193;&#23637;&#20102;&#20043;&#21069;&#22312;[55]&#20013;&#24314;&#31435;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#23637;&#31034;&#22312;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24403;&#20998;&#25903;&#36866;&#24403;&#32553;&#25918;&#20197;&#36991;&#20813;&#29190;&#28856;&#34892;&#20026;&#26102;&#65292;&#23558;&#23485;&#24230;&#21644;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#20250;&#24471;&#21040;&#30456;&#21516;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is the second in the series Commutative Scaling of Width and Depth (WD) about commutativity of infinite width and depth limits in deep neural networks. Our aim is to understand the behaviour of neural functions (functions that depend on a neural network model) as width and depth go to infinity (in some sense), and eventually identify settings under which commutativity holds, i.e. the neural function tends to the same limit no matter how width and depth limits are taken. In this paper, we formally introduce and define the commutativity framework, and discuss its implications on neural network design and scaling. We study commutativity for the neural covariance kernel which reflects how network layers separate data. Our findings extend previous results established in [55] by showing that taking the width and depth to infinity in a deep neural network with skip connections, when branches are suitably scaled to avoid exploding behaviour, result in the same covariance structure n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21482;&#26377;&#26377;&#38480;&#21463;&#20445;&#25252;&#23646;&#24615;&#26631;&#31614;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21644;&#20943;&#23569;&#20844;&#24179;&#36829;&#35268;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20272;&#35745;&#29616;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#37327;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#35757;&#32451;&#27169;&#22411;&#20197;&#38480;&#21046;&#20844;&#24179;&#36829;&#35268;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.01679</link><description>&lt;p&gt;
&#29992;&#27010;&#29575;&#20445;&#25252;&#29305;&#24449;&#20272;&#35745;&#21644;&#23454;&#29616;&#20256;&#32479;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features. (arXiv:2310.01679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21482;&#26377;&#26377;&#38480;&#21463;&#20445;&#25252;&#23646;&#24615;&#26631;&#31614;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21644;&#20943;&#23569;&#20844;&#24179;&#36829;&#35268;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20272;&#35745;&#29616;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#37327;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#35757;&#32451;&#27169;&#22411;&#20197;&#38480;&#21046;&#20844;&#24179;&#36829;&#35268;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35757;&#32451;&#20844;&#24179;&#27169;&#22411;&#30340;&#25216;&#26415;&#38656;&#35201;&#35775;&#38382;&#21463;&#20445;&#25252;&#23646;&#24615;&#65288;&#20363;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#65292;&#26080;&#35770;&#26159;&#22312;&#35757;&#32451;&#26102;&#36824;&#26159;&#22312;&#29983;&#20135;&#20013;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36825;&#20123;&#20445;&#25252;&#23646;&#24615;&#22823;&#37096;&#20998;&#26159;&#19981;&#21487;&#33719;&#24471;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#21463;&#20445;&#25252;&#23646;&#24615;&#26631;&#31614;&#35775;&#38382;&#24773;&#20917;&#19979;&#24230;&#37327;&#21644;&#20943;&#23569;&#20844;&#24179;&#36829;&#35268;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#23545;&#20110;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#38598;&#21482;&#33021;&#35775;&#38382;&#19968;&#23567;&#37096;&#20998;&#21463;&#20445;&#25252;&#23646;&#24615;&#26631;&#31614;&#65292;&#20294;&#23545;&#20110;&#20854;&#20313;&#25968;&#25454;&#38598;&#65292;&#21482;&#33021;&#36890;&#36807;&#27010;&#29575;&#20272;&#35745;&#21463;&#20445;&#25252;&#23646;&#24615;&#26631;&#31614;&#65288;&#20363;&#22914;&#36890;&#36807;&#36125;&#21494;&#26031;&#25913;&#36827;&#30340;&#22995;&#27663;&#22320;&#29702;&#32534;&#30721;&#65289;&#12290;&#22522;&#20110;&#36825;&#31181;&#35774;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#29616;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#20844;&#24179;&#24230;&#37327;&#30340;&#33539;&#22260;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#20915;&#32422;&#26463;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#38480;&#21046;&#20844;&#24179;&#36829;&#35268;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#12290;&#19982;&#31867;&#20284;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#20851;&#32852;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vast majority of techniques to train fair models require access to the protected attribute (e.g., race, gender), either at train time or in production. However, in many important applications this protected attribute is largely unavailable. In this paper, we develop methods for measuring and reducing fairness violations in a setting with limited access to protected attribute labels. Specifically, we assume access to protected attribute labels on a small subset of the dataset of interest, but only probabilistic estimates of protected attribute labels (e.g., via Bayesian Improved Surname Geocoding) for the rest of the dataset. With this setting in mind, we propose a method to estimate bounds on common fairness metrics for an existing model, as well as a method for training a model to limit fairness violations by solving a constrained non-convex optimization problem. Unlike similar existing approaches, our methods take advantage of contextual information -specifically, the relations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01678</link><description>&lt;p&gt;
Score dynamics: &#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#30382;&#31186;&#26102;&#38388;&#27493;&#25552;&#39640;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model. (arXiv:2310.01678v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Score dynamics (SD) &#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#28436;&#21270;&#31639;&#23376;&#65292;&#29992;&#20110;&#21407;&#23376;&#32423;&#21644;&#31895;&#31890;&#21270;&#21160;&#21147;&#23398;&#12290;SD&#20197;&#20998;&#25968;&#20026;&#20013;&#24515;&#65292;&#21363;&#19982;&#21160;&#24577;&#33258;&#30001;&#24230;&#30340;&#36716;&#25442;&#23545;&#25968;&#27010;&#29575;&#23548;&#25968;&#30456;&#20851;&#30340;&#37327;&#12290;&#21518;&#32773;&#22312;&#20998;&#25968;&#26102;&#38388;&#27493;&#20013;&#36215;&#21040;&#19982;MD&#20013;&#21147;&#22330;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#20294;&#22312;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#21464;&#37327;&#30340;&#31163;&#25955;&#36716;&#21464;&#12290;&#36825;&#31181;&#26102;&#38388;&#27493;&#38271;&#21487;&#20197;&#27604;&#20856;&#22411;&#30340;MD&#26102;&#38388;&#27493;&#38271;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#65292;&#29992;&#20110;&#28436;&#21270;&#20197;1~ps&#26102;&#38388;&#27493;&#38271;&#30340;&#29616;&#23454;&#20998;&#23376;&#20307;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#27700;&#28342;&#28082;&#20013;&#30340;&#30701;&#38142;&#28919;&#28867;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;Score dynamics&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#20174;&#26465;&#20214;&#27010;&#29575;&#30340;&#24179;&#31283;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#30340;&#24179;&#34913;&#39044;&#27979;&#21644;&#23545;&#36716;&#25442;&#36895;&#29575;&#21644;&#36716;&#25442;&#30340;&#21160;&#21147;&#23398;&#39044;&#27979;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose score dynamics (SD), a general framework for learning effective evolution operators for atomistic as well as coarse-grained dynamics from molecular-dynamics (MD) simulations. SD is centered around scores, or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD timestep, which can be orders of magnitude larger than a typical MD timestep. In this work, we construct graph neural network based score dynamics models of realistic molecular systems that are evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with case studies of alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GNN&#20013;&#32771;&#34385;&#23616;&#37096;&#24615;&#30340;&#22270;&#37325;&#36830;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#21387;&#32553;&#12289;&#20445;&#25345;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#25913;&#21892;&#38271;&#31243;&#20132;&#20114;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01668</link><description>&lt;p&gt;
GNN&#20013;&#30340;&#23616;&#37096;&#24863;&#30693;&#22270;&#37325;&#36830;
&lt;/p&gt;
&lt;p&gt;
Locality-Aware Graph-Rewiring in GNNs. (arXiv:2310.01668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GNN&#20013;&#32771;&#34385;&#23616;&#37096;&#24615;&#30340;&#22270;&#37325;&#36830;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#21387;&#32553;&#12289;&#20445;&#25345;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#25913;&#21892;&#38271;&#31243;&#20132;&#20114;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#19978;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#36890;&#24120;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#22312;&#23545;&#37051;&#23621;&#33410;&#28857;&#30340;&#20449;&#24687;&#36827;&#34892;&#32858;&#21512;&#26102;&#65292;&#33410;&#28857;&#30340;&#29305;&#24449;&#20250;&#34987;&#36882;&#24402;&#22320;&#26356;&#26032;&#12290;&#34429;&#28982;&#22312;&#36755;&#20837;&#22270;&#19978;&#20132;&#25442;&#28040;&#24687;&#36171;&#20104;&#20102;GNNs&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20294;&#20063;&#20351;&#24471;GNNs&#23481;&#26131;&#36807;&#24230;&#21387;&#32553;&#65292;&#20174;&#32780;&#26080;&#27861;&#25429;&#25417;&#32473;&#23450;&#22270;&#20013;&#30340;&#38271;&#31243;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22270;&#37325;&#36830;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#25913;&#21892;&#20449;&#24687;&#27969;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#25913;&#21464;&#22270;&#30340;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#37325;&#36830;&#30340;&#19977;&#20010;&#26399;&#26395;&#65306;&#65288;i&#65289;&#20943;&#23569;&#36807;&#24230;&#21387;&#32553;&#65292;&#65288;ii&#65289;&#23562;&#37325;&#22270;&#30340;&#23616;&#37096;&#24615;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20445;&#25345;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#31354;&#38388;&#21644;&#39057;&#35889;&#37325;&#36830;&#25216;&#26415;&#20043;&#38388;&#23384;&#22312;&#30340;&#26681;&#26412;&#26435;&#34913;&#65307;&#23613;&#31649;&#21069;&#32773;&#36890;&#24120;&#28385;&#36275;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#20294;&#19981;&#28385;&#36275;&#65288;iii&#65289;&#65292;&#21518;&#32773;&#36890;&#24120;&#22312;&#28385;&#36275;&#65288;i&#65289;&#21644;&#65288;iii&#65289;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#65288;ii&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity. In this work, we identify three desiderata for graph-rewiring: (i) reduce over-squashing, (ii) respect the locality of the graph, and (iii) preserve the sparsity of the graph. We highlight fundamental trade-offs that occur between spatial and spectral rewiring techniques; while the former often satisfy (i) and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the expense of (ii)
&lt;/p&gt;</description></item><item><title>Artemis&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;&#39640;&#25928;DNN&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;HE&#24863;&#30693;&#20462;&#21098;&#31574;&#30053;&#26368;&#22823;&#21270;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.01664</link><description>&lt;p&gt;
Artemis: &#38024;&#23545;&#39640;&#25928;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;HE&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning. (arXiv:2310.01664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01664
&lt;/p&gt;
&lt;p&gt;
Artemis&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;&#39640;&#25928;DNN&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;HE&#24863;&#30693;&#20462;&#21098;&#31574;&#30053;&#26368;&#22823;&#21270;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#26159;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#22522;&#30784;&#38544;&#31169;&#25216;&#26415;&#12290;&#35201;&#20351;&#20854;&#26356;&#21152;&#23454;&#29992;&#65292;&#38656;&#35201;&#38477;&#20302;&#20854;&#35745;&#31639;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#29616;&#20195;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;&#27169;&#22411;&#21387;&#32553;&#36890;&#36807;&#20462;&#21098;&#22312;&#20256;&#32479;&#26126;&#25991;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#24212;&#29992;&#20110;HE-PPML&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Artemis&#65292;&#19968;&#31181;&#38024;&#23545;HE&#25512;&#29702;&#30340;&#39640;&#25928;DNN&#20462;&#21098;&#25216;&#26415;&#12290;&#25105;&#20204;&#23457;&#24910;&#30740;&#31350;&#20102;&#20004;&#31181;HE&#24863;&#30693;&#20462;&#21098;&#31574;&#30053;&#65288;&#20301;&#32622;&#21644;&#23545;&#35282;&#32447;&#65289;&#65292;&#20197;&#20943;&#23569;&#26059;&#36716;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#22312;HE&#21367;&#31215;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#23545;&#35282;&#32447;&#20462;&#21098;&#30340; Pareto &#26368;&#20248;&#35299;&#26159;&#23436;&#20840;&#21487;&#34892;&#30340;&#12290;Artemis&#30340;&#20248;&#21183;&#22312;&#20110;&#23558;DNN&#35757;&#32451;&#19982;&#20462;&#21098;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#22242;&#20307;Lasso&#27491;&#21017;&#21270;&#30446;&#26631;&#39537;&#21160;&#65292;&#20197;&#26368;&#22823;&#21270;HE&#29305;&#23450;&#30340;&#25104;&#26412;&#38477;&#20302;&#65288;&#30001;&#26059;&#36716;&#25805;&#20316;&#20027;&#23548;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Artemis&#22312;&#20808;&#21069;&#30340;HE&#23548;&#21521;&#20462;&#21098;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;1.2-6&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving ML (PPML) based on Homomorphic Encryption (HE) is a promising foundational privacy technology. Making it more practical requires lowering its computational cost, especially, in handling modern large deep neural networks. Model compression via pruning is highly effective in conventional plaintext ML but cannot be effectively applied to HE-PPML as is.  We propose Artemis, a highly effective DNN pruning technique for HE-based inference. We judiciously investigate two HE-aware pruning strategies (positional and diagonal) to reduce the number of Rotation operations, which dominate compute time in HE convolution. We find that Pareto-optimal solutions are based fully on diagonal pruning. Artemis' benefits come from coupling DNN training, driven by a novel group Lasso regularization objective, with pruning to maximize HE-specific cost reduction (dominated by the Rotation operations). We show that Artemis improves on prior HE-oriented pruning and can achieve a 1.2-6x improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23478;&#24237;&#30005;&#21147;&#25968;&#25454;&#29983;&#25104;&#22120;(HEDGE)&#65292;&#23427;&#26159;&#19968;&#27454;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#20303;&#23429;&#33021;&#28304;&#25968;&#25454;&#30340;&#24320;&#25918;&#25509;&#20837;&#24037;&#20855;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#30340;&#33521;&#22269;&#25968;&#25454;&#65292;HEDGE&#33021;&#22815;&#29983;&#25104;&#27599;&#26085;&#30340;&#20809;&#20239;&#21457;&#30005;&#26354;&#32447;&#12289;&#23478;&#24237;&#30005;&#21147;&#36127;&#33655;&#20197;&#21450;&#30005;&#21160;&#36710;&#28040;&#32791;&#21644;&#22312;&#23478;&#21487;&#29992;&#24615;&#31561;&#26354;&#32447;&#12290;&#36825;&#19968;&#24037;&#20855;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01661</link><description>&lt;p&gt;
&#23478;&#24237;&#30005;&#21147;&#25968;&#25454;&#29983;&#25104;&#22120; (HEDGE): &#19968;&#27454;&#29992;&#20110;&#29983;&#25104;&#30005;&#21160;&#36710;&#12289;&#20303;&#23429;&#29992;&#30005;&#38656;&#27714;&#21644;&#20809;&#20239;&#21457;&#30005;&#26354;&#32447;&#30340;&#24320;&#25918;&#25509;&#20837;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Home Electricity Data Generator (HEDGE): An open-access tool for the generation of electric vehicle, residential demand, and PV generation profiles. (arXiv:2310.01661v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23478;&#24237;&#30005;&#21147;&#25968;&#25454;&#29983;&#25104;&#22120;(HEDGE)&#65292;&#23427;&#26159;&#19968;&#27454;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#20303;&#23429;&#33021;&#28304;&#25968;&#25454;&#30340;&#24320;&#25918;&#25509;&#20837;&#24037;&#20855;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#30340;&#33521;&#22269;&#25968;&#25454;&#65292;HEDGE&#33021;&#22815;&#29983;&#25104;&#27599;&#26085;&#30340;&#20809;&#20239;&#21457;&#30005;&#26354;&#32447;&#12289;&#23478;&#24237;&#30005;&#21147;&#36127;&#33655;&#20197;&#21450;&#30005;&#21160;&#36710;&#28040;&#32791;&#21644;&#22312;&#23478;&#21487;&#29992;&#24615;&#31561;&#26354;&#32447;&#12290;&#36825;&#19968;&#24037;&#20855;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23478;&#24237;&#30005;&#21147;&#25968;&#25454;&#29983;&#25104;&#22120;(HEDGE)&#65292;&#19968;&#27454;&#29992;&#20110;&#38543;&#26426;&#29983;&#25104;&#30495;&#23454;&#20303;&#23429;&#33021;&#28304;&#25968;&#25454;&#30340;&#24320;&#25918;&#25509;&#20837;&#24037;&#20855;&#12290;HEDGE&#22522;&#20110;&#33521;&#22269;&#23454;&#38469;&#25968;&#25454;&#29983;&#25104;&#20102;&#30495;&#23454;&#30340;&#20303;&#23429;&#20809;&#20239;&#21457;&#30005;&#26354;&#32447;&#12289;&#23478;&#24237;&#30005;&#21147;&#36127;&#33655;&#20197;&#21450;&#30005;&#21160;&#36710;&#28040;&#32791;&#21644;&#22312;&#23478;&#21487;&#29992;&#24615;&#30340;&#27599;&#26085;&#26354;&#32447;&#12290;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#26159;&#30740;&#31350;&#20303;&#23429;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#29305;&#24449;&#21644;&#21327;&#35843;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#26102;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#34429;&#28982;&#22823;&#37327;&#25968;&#25454;&#21487;&#29992;&#65292;&#20294;&#26684;&#24335;&#19981;&#21487;&#29992;&#65292;&#24182;&#19988;&#21516;&#19968;&#20303;&#23429;&#30340;&#36830;&#32493;&#20960;&#22825;&#25968;&#25454;&#20063;&#19981;&#21487;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#25918;&#25509;&#20837;&#30340;HEDGE&#24037;&#20855;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#35813;&#24037;&#20855;&#29983;&#25104;&#20102;&#19968;&#33268;&#20110;&#21333;&#20010;&#20303;&#23429;&#30340;&#33021;&#28304;&#25968;&#25454;&#24207;&#21015;&#65292;&#21253;&#25324;&#26354;&#32447;&#24133;&#24230;&#21644;&#34892;&#20026;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the Home Electricity Data Generator (HEDGE), an open-access tool for the random generation of realistic residential energy data. HEDGE generates realistic daily profiles of residential PV generation, household electric loads, and electric vehicle consumption and at-home availability, based on real-life UK datasets. The lack of usable data is a major hurdle for research on residential distributed energy resources characterisation and coordination, especially when using data-driven methods such as machine learning-based forecasting and reinforcement learning-based control. A key issue is that while large data banks are available, they are not in a usable format, and numerous subsequent days of data for a given single home are unavailable. We fill these gaps with the open-access HEDGE tool which generates data sequences of energy data for several days in a way that is consistent for single homes, both in terms of profile magnitude and behavioural clusters. From r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#24555;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;PolySketchFormer&#65292;&#20197;&#31361;&#30772;Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38590;&#39064;&#65292;&#26080;&#38656;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01655</link><description>&lt;p&gt;
PolySketchFormer:&#22522;&#20110;&#33609;&#22270;&#30340;&#22810;&#39033;&#24335;&#26680;&#21464;&#25442;&#22120;&#21152;&#36895;Transformer
&lt;/p&gt;
&lt;p&gt;
PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels. (arXiv:2310.01655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#24555;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;PolySketchFormer&#65292;&#20197;&#31361;&#30772;Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38590;&#39064;&#65292;&#26080;&#38656;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#25193;&#23637;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#30340;&#29942;&#39048;&#12290;&#23454;&#38469;&#19978;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20551;&#35774;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36755;&#20986;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#26367;&#20195;softmax&#26469;&#31361;&#30772;&#36825;&#20010;&#29702;&#35770;&#38556;&#30861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25991;&#29486;&#20013;&#30340;&#22810;&#39033;&#24335;&#26680;&#30340;&#33609;&#22270;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#22810;&#39033;&#24335;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#24555;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32780;&#19981;&#38656;&#35201;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#36825;&#22312;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#24050;&#32463;&#23436;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22240;&#26524;&#25513;&#30721;&#24212;&#29992;&#20110;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#22320;&#35745;&#31639;$n \times n$&#27880;&#24847;&#21147;&#30697;&#38453;&#24182;&#35745;&#31639;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic complexity of attention in transformer architectures remains a big bottleneck in scaling up large foundation models for long context. In fact, recent theoretical results show the hardness of approximating the output of softmax attention mechanism in sub-quadratic time assuming Strong Exponential Time Hypothesis. In this paper, we show how to break this theoretical barrier by replacing softmax with a polynomial function and polynomial sketching. In particular we show that sketches for Polynomial Kernel from the randomized numerical linear algebra literature can be used to approximate the polynomial attention which leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix that has been done in many previous works.  In addition, we propose an efficient block-based algorithm that lets us apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix and compute the output of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#38752;&#24615;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2310.01651</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#25490;&#21015;&#27450;&#39575;&#65288;&#35270;&#35273;&#21644;&#65289;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations. (arXiv:2310.01651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#38752;&#24615;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#32780;&#36805;&#36895;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#65292;&#21363;&#20180;&#32454;&#20998;&#26512;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20415;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#20013;&#26159;&#21542;&#36275;&#22815;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#27969;&#34892;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65288;MCQA&#65289;&#20013;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#27169;&#22411;&#26131;&#21463;&#22810;&#39033;&#36873;&#25321;&#25552;&#31034;&#20013;&#31572;&#26696;&#38598;&#30340;&#23545;&#25239;&#24615;&#25490;&#21015;&#25915;&#20987;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27169;&#22411;&#29702;&#24819;&#19978;&#24212;&#35813;&#21644;&#20154;&#31867;&#19968;&#26679;&#23545;&#25552;&#31034;&#25490;&#21015;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#19979;&#25345;&#32493;&#23384;&#22312;&#65292;&#24182;&#23384;&#22312;&#20110;&#26368;&#26032;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. This raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. In this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA). Specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. These vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. Code is available at \url{https://github.com/ys-zong/FoolyourVLLMs}.
&lt;/p&gt;</description></item><item><title>CoDBench&#26159;&#19968;&#20010;&#23545;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;4&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#26377;&#21161;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2310.01650</link><description>&lt;p&gt;
CoDBench: &#23545;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#37325;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems. (arXiv:2310.01650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01650
&lt;/p&gt;
&lt;p&gt;
CoDBench&#26159;&#19968;&#20010;&#23545;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;4&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#26377;&#21161;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#36890;&#36807;&#24494;&#20998;&#26041;&#31243;&#26469;&#24314;&#27169;&#65292;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;&#35832;&#22914;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#12289;&#22810;&#23380;&#20171;&#36136;&#27969;&#21160;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#27969;&#34892;&#30149;&#21160;&#21147;&#23398;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#24314;&#27169;&#36825;&#20123;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#24050;&#24314;&#31435;&#30340;&#39046;&#22495;&#30456;&#27604;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#30740;&#31350;&#30456;&#23545;&#26377;&#38480;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CoDBench&#65292;&#19968;&#20010;&#21253;&#21547;11&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#24494;&#20998;&#26041;&#31243;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20840;&#38754;&#22522;&#20934;&#22871;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;4&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#25805;&#20316;&#22238;&#24402;&#27169;&#22411;&#12289;&#22522;&#20110;&#39057;&#29575;&#30340;&#31070;&#32463;&#31639;&#23376;&#21644;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#24182;&#19982;8&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous dynamical systems, characterized by differential equations, are ubiquitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CodBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency-based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.01649</link><description>&lt;p&gt;
&#20851;&#20110;&#35757;&#32451;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#65288;&#37096;&#20998;&#65289;&#23548;&#25968;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#30340;&#24773;&#20917;&#31216;&#20043;&#20026;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU (IReLU)&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#20197;&#24110;&#21161;&#31283;&#23450;DC&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#37327;&#23376;&#21270;&#23398;&#21644;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20219;&#21153;&#22312;&#20869;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We refer to the setting where the (partial) derivatives of a neural network's (NN's) predictions with respect to its inputs are used as additional training signal as a derivative-constrained (DC) NN. This situation is common in physics-informed settings in the natural sciences. We propose an integrated RELU (IReLU) activation function to improve training of DC NNs. We also investigate denormalization and label rescaling to help stabilize DC training. We evaluate our methods on physics-informed settings including quantum chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that existing architectures with IReLU activations combined with denormalization and label rescaling better incorporate training signal provided by derivative constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#31561;&#21464;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#35268;&#33539;&#21270;&#32593;&#32476;&#26469;&#20351;&#36755;&#20837;&#36716;&#25442;&#20026;&#35268;&#33539;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35268;&#33539;&#23450;&#21521;&#21487;&#33021;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#26041;&#21521;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01647</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31561;&#21464;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Equivariant Adaptation of Large Pre-Trained Models. (arXiv:2310.01647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#31561;&#21464;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#35268;&#33539;&#21270;&#32593;&#32476;&#26469;&#20351;&#36755;&#20837;&#36716;&#25442;&#20026;&#35268;&#33539;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35268;&#33539;&#23450;&#21521;&#21487;&#33021;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#26041;&#21521;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#32593;&#32476;&#34987;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20445;&#35777;&#19982;&#19968;&#32452;&#36755;&#20837;&#21464;&#25442;&#30340;&#19968;&#33268;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#37325;&#26032;&#35774;&#35745;&#20027;&#27969;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#27599;&#20010;&#32452;&#20214;&#20197;&#23454;&#29616;&#25152;&#36873;&#31561;&#21464;&#24615;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#30340;&#32593;&#32476;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#26367;&#20195;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#35268;&#33539;&#21270;&#32593;&#32476;&#65292;&#22312;&#23558;&#36755;&#20837;&#25552;&#20379;&#32473;&#19981;&#21463;&#32422;&#26463;&#30340;&#39044;&#27979;&#32593;&#32476;&#20043;&#21069;&#23558;&#20854;&#36716;&#25442;&#20026;&#35268;&#33539;&#24418;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#32593;&#32476;&#31561;&#21464;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20135;&#29983;&#30340;&#35268;&#33539;&#23450;&#21521;&#21487;&#33021;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#26041;&#21521;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#24433;&#21709;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#35268;&#33539;&#21270;&#20989;&#25968;&#65292;&#25105;&#20204;&#27491;&#22312;&#8230;
&lt;/p&gt;
&lt;p&gt;
Equivariant networks are specifically designed to ensure consistent behavior with respect to a set of input transformations, leading to higher sample efficiency and more accurate and robust predictions. However, redesigning each component of prevalent deep neural network architectures to achieve chosen equivariance is a difficult problem and can result in a computationally expensive network during both training and inference. A recently proposed alternative towards equivariance that removes the architectural constraints is to use a simple canonicalization network that transforms the input to a canonical form before feeding it to an unconstrained prediction network. We show here that this approach can effectively be used to make a large pre-trained network equivariant. However, we observe that the produced canonical orientations can be misaligned with those of the training distribution, hindering performance. Using dataset-dependent priors to inform the canonicalization function, we are
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#19978;&#22024;&#26434;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#23637;&#31034;&#20102;&#20266;&#26631;&#31614;&#23545;&#22270;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#36793;&#30028;&#21644;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35880;&#24910;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01634</link><description>&lt;p&gt;
&#22270;&#25968;&#25454;&#19978;&#22024;&#26434;&#20266;&#26631;&#31614;&#30340;&#28145;&#20837;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Deep Insights into Noisy Pseudo Labeling on Graph Data. (arXiv:2310.01634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#19978;&#22024;&#26434;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#23637;&#31034;&#20102;&#20266;&#26631;&#31614;&#23545;&#22270;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#36793;&#30028;&#21644;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35880;&#24910;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#65288;PL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#28508;&#22312;&#26679;&#26412;&#36827;&#34892;&#33258;&#27880;&#37322;&#65292;&#25193;&#22823;&#24050;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#38169;&#35823;&#26631;&#31614;&#23545;&#22270;&#35757;&#32451;&#36807;&#31243;&#21487;&#33021;&#20855;&#26377;&#33268;&#21629;&#24433;&#21709;&#12290;&#19981;&#36866;&#24403;&#30340;PL&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#25968;&#25454;&#19978;&#65292;&#22122;&#38899;&#21487;&#20197;&#20256;&#25773;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30456;&#20851;&#38169;&#35823;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#27934;&#23519;PL&#23545;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23637;&#31034;&#38169;&#35823;&#34987;PL&#38408;&#20540;&#30340;&#32622;&#20449;&#24230;&#21644;&#22810;&#35270;&#22270;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#25152;&#38480;&#21046;&#26469;&#20998;&#26512;PL&#31574;&#30053;&#30340;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35828;&#26126;&#20102;PL&#23545;&#25910;&#25947;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35880;&#24910;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#39640;&#32622;&#20449;&#24230;&#21644;&#22810;&#35270;&#22270;&#36827;&#34892;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo labeling (PL) is a wide-applied strategy to enlarge the labeled dataset by self-annotating the potential samples during the training process. Several works have shown that it can improve the graph learning model performance in general. However, we notice that the incorrect labels can be fatal to the graph training process. Inappropriate PL may result in the performance degrading, especially on graph data where the noise can propagate. Surprisingly, the corresponding error is seldom theoretically analyzed in the literature. In this paper, we aim to give deep insights of PL on graph learning models. We first present the error analysis of PL strategy by showing that the error is bounded by the confidence of PL threshold and consistency of multi-view prediction. Then, we theoretically illustrate the effect of PL on convergence property. Based on the analysis, we propose a cautious pseudo labeling methodology in which we pseudo label the samples with highest confidence and multi-view
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation through Optimal Transport. (arXiv:2310.01632v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;ILfO&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#32773;&#35797;&#22270;&#22312;&#27809;&#26377;&#30452;&#25509;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;IL&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26681;&#25454;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#30340;&#29366;&#24577;&#36712;&#36857;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#29983;&#25104;&#22870;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20026;&#29983;&#25104;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;ILfO&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#21482;&#35266;&#23519;&#21333;&#20010;&#19987;&#23478;&#36712;&#36857;&#32780;&#27809;&#26377;&#21160;&#20316;&#65292;&#23427;&#22312;ILfO&#35774;&#32622;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation (ILfO) is a setting in which a learner tries to imitate the behavior of an expert, using only observational data and without the direct guidance of demonstrated actions. In this paper, we re-examine the use of optimal transport for IL, in which a reward is generated based on the Wasserstein distance between the state trajectories of the learner and expert. We show that existing methods can be simplified to generate a reward function without requiring learned models or adversarial learning. Unlike many other state-of-the-art methods, our approach can be integrated with any RL algorithm, and is amenable to ILfO. We demonstrate the effectiveness of this simple approach on a variety of continuous control tasks and find that it surpasses the state of the art in the IlfO setting, achieving expert-level performance across a range of evaluation domains even when observing only a single expert trajectory without actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32463;&#20856;&#25968;&#20540;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#20855;&#26377;&#22266;&#23450;&#28857;&#30340;&#31639;&#23376;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26041;&#27861;&#30340;&#25910;&#25947;&#35777;&#26126;&#65292;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#32593;&#32476;&#31639;&#23376;&#36827;&#34892;&#36845;&#20195;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;PIGN&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36845;&#20195;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.01618</link><description>&lt;p&gt;
&#25805;&#20316;&#23398;&#20064;&#19982;&#25968;&#20540;&#20998;&#26512;&#30456;&#36935;&#65306;&#36890;&#36807;&#36845;&#20195;&#26041;&#27861;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods. (arXiv:2310.01618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32463;&#20856;&#25968;&#20540;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#20855;&#26377;&#22266;&#23450;&#28857;&#30340;&#31639;&#23376;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26041;&#27861;&#30340;&#25910;&#25947;&#35777;&#26126;&#65292;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#32593;&#32476;&#31639;&#23376;&#36827;&#34892;&#36845;&#20195;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;PIGN&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36845;&#20195;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23613;&#31649;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#30830;&#31435;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32463;&#20856;&#25968;&#20540;&#20998;&#26512;&#36827;&#34892;&#23545;&#27604;&#65292;&#24357;&#21512;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#20855;&#26377;&#22266;&#23450;&#28857;&#30340;&#31639;&#23376;&#65292;&#36825;&#20123;&#22266;&#23450;&#28857;&#20195;&#34920;&#30528;&#26399;&#26395;&#30340;&#35299;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#22522;&#20110;&#31639;&#23376;&#26041;&#31243;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#22312;&#23450;&#20041;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22266;&#23450;&#28857;&#29702;&#35770;&#30340;&#25910;&#25947;&#35777;&#26126;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#26550;&#26500;&#65292;&#22914;&#25193;&#25955;&#27169;&#22411;&#21644;AlphaFold&#65292;&#26412;&#36136;&#19978;&#37319;&#29992;&#20102;&#36845;&#20195;&#31639;&#23376;&#23398;&#20064;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#32593;&#32476;&#31639;&#23376;&#36827;&#34892;&#36845;&#20195;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;PIGN&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36845;&#20195;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#23558;&#25968;&#20540;&#20998;&#26512;&#30340;&#35265;&#35299;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22686;&#21152;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35299;&#65292;&#26377;&#21487;&#33021;&#25351;&#23548;&#26410;&#26469;&#32593;&#32476;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks, despite their success in numerous applications, often function without established theoretical foundations. In this paper, we bridge this gap by drawing parallels between deep learning and classical numerical analysis. By framing neural networks as operators with fixed points representing desired solutions, we develop a theoretical framework grounded in iterative methods for operator equations. Under defined conditions, we present convergence proofs based on fixed point theory. We demonstrate that popular architectures, such as diffusion models and AlphaFold, inherently employ iterative operator learning. Empirical assessments highlight that performing iterations through network operators improves performance. We also introduce an iterative graph neural network, PIGN, that further demonstrates benefits of iterations. Our work aims to enhance the understanding of deep learning by merging insights from numerical analysis, potentially guiding the design of future net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;</title><link>http://arxiv.org/abs/2310.01616</link><description>&lt;p&gt;
&#22810;&#25209;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65306;&#23545;&#20110;&#32500;&#24230;&#30456;&#20851;&#30340;&#36866;&#24212;&#24615;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity. (arXiv:2310.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22914;&#26524;&#31639;&#27861;&#22312;&#38382;&#39064;&#30340;&#32500;&#24230;d&#20013;&#20351;&#29992;&#30340;&#29615;&#22659;&#26597;&#35810;&#27425;&#25968;n&#26159;&#22810;&#39033;&#24335;&#30340;&#65292;&#37027;&#20040;&#23427;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#12290;&#36866;&#24212;&#24615;&#26159;&#25351;&#26597;&#35810;&#34987;&#21457;&#36865;&#21644;&#21453;&#39304;&#34987;&#22788;&#29702;&#20197;&#26356;&#26032;&#26597;&#35810;&#31574;&#30053;&#30340;&#39057;&#29575;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20801;&#35768;&#22312;K&#20010;&#25209;&#27425;&#20013;&#21457;&#36865;&#26597;&#35810;&#65292;&#22312;&#27599;&#20010;&#25209;&#27425;&#20043;&#21518;&#22788;&#29702;&#21453;&#39304;&#24182;&#26356;&#26032;&#26597;&#35810;&#12290;&#36825;&#20010;&#27169;&#22411;&#21253;&#25324;&#25972;&#20010;&#36866;&#24212;&#24615;&#35889;&#65292;&#20174;&#38750;&#33258;&#36866;&#24212;&#30340;&#8220;&#31163;&#32447;&#8221;&#65288;K=1&#65289;&#21040;&#23436;&#20840;&#33258;&#36866;&#24212;&#65288;K=n&#65289;&#30340;&#22330;&#26223;&#65292;&#20197;&#21450;&#20013;&#38388;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#21644;&#22312;d&#32500;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#24314;&#31435;&#20102;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377; &#937;(log log d) &#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;
We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy. To investigate this interplay, we employ a learning framework that allows sending queries in $K$ batches, with feedback being processed and queries updated after each batch. This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes in between. For the problems of policy evaluation and best-policy identification under $d$-dimensional linear function approximation, we establish $\Omega(\log \log d)$ lower bounds on the number of batches $K$ required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our results show that j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#27861;&#23398;&#20064;&#31163;&#25955;&#23545;&#25968;&#30340;&#22256;&#38590;&#24230;&#65292;&#21457;&#29616;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#22312;&#19968;&#20010;&#22266;&#23450;&#28857;&#38468;&#36817;&#32858;&#38598;&#65292;&#19988;&#26080;&#35770;&#32593;&#32476;&#26550;&#26500;&#22797;&#26434;&#24615;&#22914;&#20309;&#65292;&#37117;&#20250;&#23548;&#33268;&#23398;&#20064;&#31163;&#25955;&#23545;&#25968;&#22855;&#20598;&#27604;&#29305;&#30340;&#33021;&#21147;&#21463;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.01611</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#23398;&#20064;&#31163;&#25955;&#23545;&#25968;&#30340;&#22256;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Intractability of Learning the Discrete Logarithm with Gradient-Based Methods. (arXiv:2310.01611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#27861;&#23398;&#20064;&#31163;&#25955;&#23545;&#25968;&#30340;&#22256;&#38590;&#24230;&#65292;&#21457;&#29616;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#22312;&#19968;&#20010;&#22266;&#23450;&#28857;&#38468;&#36817;&#32858;&#38598;&#65292;&#19988;&#26080;&#35770;&#32593;&#32476;&#26550;&#26500;&#22797;&#26434;&#24615;&#22914;&#20309;&#65292;&#37117;&#20250;&#23548;&#33268;&#23398;&#20064;&#31163;&#25955;&#23545;&#25968;&#22855;&#20598;&#27604;&#29305;&#30340;&#33021;&#21147;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#23545;&#25968;&#38382;&#39064;&#26159;&#25968;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#23545;&#23494;&#30721;&#21327;&#35758;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#24490;&#29615;&#32676;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#23398;&#20064;&#31163;&#25955;&#23545;&#25968;&#30340;&#22855;&#20598;&#27604;&#29305;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#22312;&#19968;&#20010;&#22266;&#23450;&#28857;&#38468;&#36817;&#30340;&#32858;&#38598;&#24615;&#36136;&#65292;&#29420;&#31435;&#20110;&#20351;&#29992;&#30340;&#23545;&#25968;&#30340;&#24213;&#25968;&#12290;&#36825;&#31181;&#32858;&#38598;&#24615;&#36136;&#23548;&#33268;&#20102;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#39640;&#25928;&#23398;&#20064;&#31163;&#25955;&#23545;&#25968;&#30340;&#22855;&#20598;&#27604;&#29305;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#26080;&#35770;&#35757;&#32451;&#30340;&#32593;&#32476;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#20869;&#31215;&#31354;&#38388;&#20013;&#30340;Boas-Bellman&#19981;&#31561;&#24335;&#65292;&#24182;&#36890;&#36807;&#26576;&#20123;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#24314;&#31435;&#20102;&#31163;&#25955;&#23545;&#25968;&#22855;&#20598;&#27604;&#29305;&#20989;&#25968;&#30340;&#36817;&#20284;&#27491;&#20132;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#23454;&#35777;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm's base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained.  Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm's parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#25439;&#22833;&#20989;&#25968;&#32435;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#23545;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#30340;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#20102;&#20869;&#26680;&#21270;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#31181;&#29305;&#24449;&#20540;&#34928;&#20943;&#20551;&#35774;&#19979;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#22312;&#22810;&#39033;&#24335;&#29305;&#24449;&#20540;&#34928;&#20943;&#21644;&#25351;&#25968;&#29305;&#24449;&#20540;&#34928;&#20943;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#19978;&#30028;&#20998;&#21035;&#20026; $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$ &#21644; $\widetilde{O}(\sqrt{T})$&#12290;</title><link>http://arxiv.org/abs/2310.01609</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#36827;&#34892;&#20869;&#26680;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Contextual Bandits Go Kernelized. (arXiv:2310.01609v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#25439;&#22833;&#20989;&#25968;&#32435;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#23545;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#30340;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#20102;&#20869;&#26680;&#21270;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#31181;&#29305;&#24449;&#20540;&#34928;&#20943;&#20551;&#35774;&#19979;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#22312;&#22810;&#39033;&#24335;&#29305;&#24449;&#20540;&#34928;&#20943;&#21644;&#25351;&#25968;&#29305;&#24449;&#20540;&#34928;&#20943;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#19978;&#30028;&#20998;&#21035;&#20026; $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$ &#21644; $\widetilde{O}(\sqrt{T})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#25439;&#22833;&#20989;&#25968;&#32435;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#12290;&#36825;&#20801;&#35768;&#23545;&#22797;&#26434;&#30340;&#20915;&#31574;&#22330;&#26223;&#36827;&#34892;&#26356;&#28789;&#27963;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#26032;&#30340;&#20048;&#35266;&#20559;&#22909;&#20272;&#35745;&#22120;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#22312;&#22522;&#20110;&#24213;&#23618;&#26680;&#30340;&#22810;&#31181;&#29305;&#24449;&#20540;&#34928;&#20943;&#20551;&#35774;&#19979;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22810;&#39033;&#24335;&#29305;&#24449;&#20540;&#34928;&#20943;&#25351;&#25968; $c&gt;1$ &#30340;&#20551;&#35774;&#19979;&#65292;&#36951;&#25022;&#20026; $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$&#65292;&#20854;&#20013; $T$ &#34920;&#31034;&#36718;&#25968;&#65292;$K$ &#34920;&#31034;&#21160;&#20316;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#24403;&#29305;&#24449;&#20540;&#34928;&#20943;&#36981;&#24490;&#25351;&#25968;&#27169;&#24335;&#26102;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#26356;&#21152;&#32039;&#23494;&#30340;&#36951;&#25022;&#30028; $\widetilde{O}(\sqrt{T})$&#12290;&#36825;&#20123;&#36895;&#24230;&#19982;&#24050;&#30693;&#30340;&#25152;&#26377;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#19979;&#30028;&#21305;&#37197;&#65292;&#24182;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19978;&#30028;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a generalization of the problem of online learning in adversarial linear contextual bandits by incorporating loss functions that belong to a reproducing kernel Hilbert space, which allows for a more flexible modeling of complex decision-making scenarios. We propose a computationally efficient algorithm that makes use of a new optimistically biased estimator for the loss functions and achieves near-optimal regret guarantees under a variety of eigenvalue decay assumptions made on the underlying kernel. Specifically, under the assumption of polynomial eigendecay with exponent $c&gt;1$, the regret is $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$, where $T$ denotes the number of rounds and $K$ the number of actions. Furthermore, when the eigendecay follows an exponential pattern, we achieve an even tighter regret bound of $\widetilde{O}(\sqrt{T})$. These rates match the lower bounds in all special cases where lower bounds are known at all, and match the best known upper bounds avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25351;&#38024;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29305;&#23450;&#23454;&#20363;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01604</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the Quadratic Assignment Problem using Deep Reinforcement Learning. (arXiv:2310.01604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25351;&#38024;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29305;&#23450;&#23454;&#20363;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#20998;&#37197;&#38382;&#39064; (QAP) &#26159;&#19968;&#20010; NP &#22256;&#38590;&#38382;&#39064;&#65292;&#23545;&#20854;&#36827;&#34892;&#35299;&#20915;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65306;&#19982;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064; (TSP) &#31561;&#20854;&#20182;&#32452;&#21512;&#38382;&#39064;&#19981;&#21516;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#25972;&#25968;&#35268;&#21010;&#25216;&#26415;&#21487;&#20197;&#22312;&#21253;&#21547;&#25968;&#30334;&#29978;&#33267;&#25968;&#21315;&#20010;&#20301;&#32622;&#30340;&#23454;&#20363;&#19978;&#31934;&#30830;&#35299;&#20915;&#65292;&#23578;&#26410;&#25214;&#21040;&#35299;&#20915;&#22823;&#23567;&#36229;&#36807;30&#30340; QAP &#23454;&#20363;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915; QAP &#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#35768;&#22810;&#20851;&#38190;&#30340;&#24212;&#29992;&#65292;&#22914;&#30005;&#23376;&#24067;&#32447;&#35774;&#35745;&#21644;&#35774;&#22791;&#24067;&#23616;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915; QAP &#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25351;&#38024;&#32593;&#32476;&#65292;&#36825;&#20010;&#32593;&#32476;&#22312;&#36873;&#25321;&#19979;&#19968;&#20010;&#20301;&#32622;&#26469;&#25918;&#32622;&#35774;&#26045;&#21644;&#36873;&#25321;&#21069;&#19968;&#20010;&#20301;&#32622;&#26469;&#25918;&#32622;&#35774;&#26045;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#20363;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#22312; A2C &#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20135;&#29983;&#19981;&#38656;&#35201;&#29305;&#23450;&#23454;&#20363;&#37325;&#26032;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven particularly challenging to solve: unlike other combinatorial problems like the traveling salesman problem (TSP), which can be solved to optimality for instances with hundreds or even thousands of locations using advanced integer programming techniques, no methods are known to exactly solve QAP instances of size greater than 30. Solving the QAP is nevertheless important because of its many critical applications, such as electronic wiring design and facility layout selection. We propose a method to solve the original Koopmans-Beckman formulation of the QAP using deep reinforcement learning. Our approach relies on a novel double pointer network, which alternates between selecting a location in which to place the next facility and a facility to place in the previous location. We train our model using A2C on a large dataset of synthetic instances, producing solutions with no instance-specific retraining necessary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#30830;&#25299;&#25169;&#21306;&#22495;&#30340;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#20803;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#19982;&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01597</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#30830;&#25299;&#25169;&#21306;&#22495;&#30340;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pool-Based Active Learning with Proper Topological Regions. (arXiv:2310.01597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#30830;&#25299;&#25169;&#21306;&#22495;&#30340;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#20803;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#19982;&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#26679;&#26412;&#37327;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#38590;&#25552;&#20379;&#26631;&#35760;&#38598;&#12290;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20174;&#19968;&#32452;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#26816;&#27979;&#20986;&#23545;&#35757;&#32451;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#30830;&#25299;&#25169;&#21306;&#22495;&#30340;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#20803;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;PTR&#26159;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#30340;&#30456;&#20851;&#21306;&#22495;&#65292;&#29992;&#20110;&#37319;&#26679;&#20919;&#21551;&#21160;&#28857;&#25110;&#22312;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#20013;&#20351;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods usually rely on large sample size to have good performance, while it is difficult to provide labeled set in many applications. Pool-based active learning methods are there to detect, among a set of unlabeled data, the ones that are the most relevant for the training. We propose in this paper a meta-approach for pool-based active learning strategies in the context of multi-class classification tasks based on Proper Topological Regions. PTR, based on topological data analysis (TDA), are relevant regions used to sample cold-start points or within the active learning scheme. The proposed method is illustrated empirically on various benchmark datasets, being competitive to the classical methods from the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#36827;&#34892;&#35268;&#23450;&#28779;&#27169;&#25311;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#21644;&#39044;&#27979;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01593</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35268;&#23450;&#28779;&#27169;&#25311;&#65292;&#29992;&#20110;&#22303;&#22320;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Prescribed Fire Modeling using Knowledge-Guided Machine Learning for Land Management. (arXiv:2310.01593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#36827;&#34892;&#35268;&#23450;&#28779;&#27169;&#25311;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#21644;&#39044;&#27979;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28798;&#28909;&#21457;&#29983;&#22823;&#28779;&#30340;&#23041;&#32961;&#19981;&#26029;&#21152;&#21095;&#65292;&#22240;&#27492;&#38656;&#35201;&#26377;&#25928;&#30340;&#35268;&#23450;&#28779;&#31649;&#29702;&#12290;&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#36807;&#31243;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#34987;&#29992;&#20110;&#35268;&#21010;&#39044;&#38450;&#37326;&#28779;&#30340;&#35268;&#23450;&#28779;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#31616;&#21270;&#30340;&#36807;&#31243;&#27169;&#22411;&#22914;QUIC-Fire&#20063;&#36807;&#20110;&#35745;&#31639;&#23494;&#38598;&#65292;&#26080;&#27861;&#29992;&#20110;&#23454;&#26102;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#24403;&#22825;&#27668;&#26465;&#20214;&#36805;&#36895;&#21464;&#21270;&#26102;&#12290;&#20256;&#32479;&#30340;&#28779;&#28798;&#24314;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#35745;&#31639;&#36895;&#24230;&#21152;&#24555;&#65292;&#20294;&#22312;&#29289;&#29702;&#19978;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65292;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#32780;&#24341;&#36215;&#30340;&#26377;&#20559;&#39044;&#27979;&#65292;&#28779;&#28798;&#34067;&#24310;&#25351;&#26631;&#65288;&#22914;&#29123;&#28903;&#38754;&#31215;&#12289;&#34067;&#24310;&#36895;&#24230;&#65289;&#30340;&#26377;&#20559;&#20272;&#35745;&#20197;&#21450;&#22312;&#20998;&#24067;&#22806;&#30340;&#39118;&#26465;&#20214;&#19979;&#30340;&#21487;&#25512;&#24191;&#24615;&#19978;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#24555;&#36895;&#27169;&#25311;&#35268;&#23450;&#28779;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#20943;&#23569;&#29123;&#26009;&#23494;&#24230;&#20272;&#35745;&#20013;&#30340;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the increasing threat of devastating wildfires has underscored the need for effective prescribed fire management. Process-based computer simulations have traditionally been employed to plan prescribed fires for wildfire prevention. However, even simplified process models like QUIC-Fire are too compute-intensive to be used for real-time decision-making, especially when weather conditions change rapidly. Traditional ML methods used for fire modeling offer computational speedup but struggle with physically inconsistent predictions, biased predictions due to class imbalance, biased estimates for fire spread metrics (e.g., burned area, rate of spread), and generalizability in out-of-distribution wind conditions. This paper introduces a novel machine learning (ML) framework that enables rapid emulation of prescribed fires while addressing these concerns. By incorporating domain knowledge, the proposed method helps reduce physical inconsistencies in fuel density estimates in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#21644;&#20998;&#37197;&#20260;&#23475;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#23481;&#26131;&#36896;&#25104;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#31034;&#21644;&#22810;&#25968;&#32676;&#20307;&#30340;&#34920;&#31034;&#21512;&#24182;&#65292;&#20174;&#32780;&#23548;&#33268;&#20998;&#37197;&#20260;&#23475;&#12290;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#21644;&#32531;&#35299;&#36825;&#31181;&#34920;&#31034;&#20260;&#23475;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01583</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#21644;&#20998;&#37197;&#20260;&#23475;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Investigation of Representation and Allocation Harms in Contrastive Learning. (arXiv:2310.01583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#21644;&#20998;&#37197;&#20260;&#23475;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#23481;&#26131;&#36896;&#25104;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#31034;&#21644;&#22810;&#25968;&#32676;&#20307;&#30340;&#34920;&#31034;&#21512;&#24182;&#65292;&#20174;&#32780;&#23548;&#33268;&#20998;&#37197;&#20260;&#23475;&#12290;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#21644;&#32531;&#35299;&#36825;&#31181;&#34920;&#31034;&#20260;&#23475;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23569;&#25968;&#32676;&#20307;&#30340;&#20195;&#34920;&#19981;&#36275;&#23545;&#20854;&#34920;&#29616;&#30340;&#24433;&#21709;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#27492;&#38382;&#39064;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36825;&#19968;&#27969;&#34892;&#30340;SSL&#21464;&#20307;&#20250;&#20542;&#21521;&#20110;&#23558;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#31034;&#19982;&#26576;&#20123;&#22810;&#25968;&#32676;&#20307;&#30340;&#34920;&#31034;&#21512;&#24182;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#34920;&#31034;&#20260;&#23475;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#24212;&#30340;&#27969;&#34892;CL&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#20998;&#37197;&#20260;&#23475;&#36827;&#34892;&#20102;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#34920;&#31034;&#20260;&#23475;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#19968;&#37096;&#20998;&#36131;&#20219;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#30740;&#31350;&#21644;&#32531;&#35299;&#34920;&#31034;&#20260;&#23475;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#22359;&#27169;&#22411;&#23545;&#34920;&#31034;&#20260;&#23475;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23548;&#33268;&#23545;&#27604;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effect of underrepresentation on the performance of minority groups is known to be a serious problem in supervised learning settings; however, it has been underexplored so far in the context of self-supervised learning (SSL). In this paper, we demonstrate that contrastive learning (CL), a popular variant of SSL, tends to collapse representations of minority groups with certain majority groups. We refer to this phenomenon as representation harm and demonstrate it on image and text datasets using the corresponding popular CL methods. Furthermore, our causal mediation analysis of allocation harm on a downstream classification task reveals that representation harm is partly responsible for it, thus emphasizing the importance of studying and mitigating representation harm. Finally, we provide a theoretical explanation for representation harm using a stochastic block model that leads to a representational neural collapse in a contrastive learning setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#23545;&#40784;&#19981;&#33021;&#30495;&#27491;&#38450;&#27490;&#23427;&#20204;&#34987;&#28389;&#29992;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26041;&#24335;&#35823;&#23548;&#23427;&#20204;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.01581</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65306;&#23545;&#40784;&#26159;&#21542;&#30495;&#27491;&#38450;&#27490;&#23427;&#20204;&#34987;&#28389;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?. (arXiv:2310.01581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#23545;&#40784;&#19981;&#33021;&#30495;&#27491;&#38450;&#27490;&#23427;&#20204;&#34987;&#28389;&#29992;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26041;&#24335;&#35823;&#23548;&#23427;&#20204;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#21457;&#24067;LLMs&#20379;&#20844;&#20247;&#35775;&#38382;&#20043;&#21069;&#65292;&#27169;&#22411;&#24320;&#21457;&#32773;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25110;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#23545;&#40784;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#28508;&#22312;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#30340;&#35831;&#27714;&#26102;&#20250;&#25298;&#32477;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#8220;&#23545;&#40784;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#38450;&#27490;&#36825;&#20123;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#65311;&#8221;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#32473;&#20986;&#20102;&#21542;&#23450;&#30340;&#31572;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#24320;&#28304;&#12289;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#35745;&#31639;&#25110;&#20180;&#32454;&#35774;&#35745;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#34987;&#36731;&#26131;&#35823;&#23548;&#29983;&#25104;&#19981;&#26399;&#26395;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#30452;&#25509;&#25805;&#32437;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is "could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation proc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20851;&#20110;&#22810;&#21306;&#22495;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#22312;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#27169;&#22359;&#21270;&#32467;&#26500;&#19978;&#35777;&#26126;&#20102;&#26494;&#25955;&#31283;&#23450;&#24615;&#26465;&#20214;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#31232;&#30095;&#32452;&#21512;&#32593;&#32476;&#22312;&#27979;&#35797;&#34920;&#29616;&#21644;&#38887;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#31283;&#23450;&#24615;&#23545;&#20110;&#23454;&#29616;&#27169;&#22359;&#21270;RNN&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01571</link><description>&lt;p&gt;
&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#22522;&#20803;&#30340;&#25910;&#32553;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Contraction Properties of the Global Workspace Primitive. (arXiv:2310.01571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20851;&#20110;&#22810;&#21306;&#22495;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#22312;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#27169;&#22359;&#21270;&#32467;&#26500;&#19978;&#35777;&#26126;&#20102;&#26494;&#25955;&#31283;&#23450;&#24615;&#26465;&#20214;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#31232;&#30095;&#32452;&#21512;&#32593;&#32476;&#22312;&#27979;&#35797;&#34920;&#29616;&#21644;&#38887;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#31283;&#23450;&#24615;&#23545;&#20110;&#23454;&#29616;&#27169;&#22359;&#21270;RNN&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25512;&#21160;&#20851;&#20110;&#22810;&#21306;&#22495;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#37325;&#35201;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#22312;Kozachkov&#31561;&#20154;&#30340;&#8220;&#36882;&#24402;&#26500;&#24314;&#31283;&#23450;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32452;&#35013;&#21697;&#8221;&#30340;&#22522;&#30784;&#19978;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26550;&#26500;&#30340;&#26174;&#33879;&#29305;&#20363;&#30340;&#26494;&#25955;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20855;&#26377;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#31232;&#30095;&#32452;&#21512;&#32593;&#32476;&#36827;&#34892;&#23454;&#35777;&#25104;&#21151;&#65292;&#19981;&#20165;&#22312;&#25972;&#20307;&#27979;&#35797;&#34920;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36824;&#23545;&#21333;&#20010;&#23376;&#32593;&#32476;&#30340;&#31227;&#38500;&#20855;&#26377;&#26356;&#22823;&#30340;&#38887;&#24615;&#12290;&#36825;&#20123;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#25509;&#35302;&#21306;&#25299;&#25169;&#30340;&#23454;&#35777;&#32467;&#26524;&#20381;&#36182;&#20110;&#31283;&#23450;&#24615;&#30340;&#20445;&#25345;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#24037;&#20316;&#23545;&#20110;&#23454;&#29616;&#27169;&#22359;&#21270;RNN&#30340;&#25104;&#21151;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26356;&#24191;&#27867;&#22320;&#25506;&#32034;&#19981;&#21516;&#23376;&#32593;&#32476;&#27169;&#22359;&#20043;&#38388;&#30340;&#36830;&#25509;&#32467;&#26500;&#30340;&#31232;&#30095;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
To push forward the important emerging research field surrounding multi-area recurrent neural networks (RNNs), we expand theoretically and empirically on the provably stable RNNs of RNNs introduced by Kozachkov et al. in "RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks". We prove relaxed stability conditions for salient special cases of this architecture, most notably for a global workspace modular structure. We then demonstrate empirical success for Global Workspace Sparse Combo Nets with a small number of trainable parameters, not only through strong overall test performance but also greater resilience to removal of individual subnetworks. These empirical results for the global workspace inter-area topology are contingent on stability preservation, highlighting the relevance of our theoretical work for enabling modular RNN success. Further, by exploring sparsity in the connectivity structure between different subnetwork modules more broadly, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.01569</link><description>&lt;p&gt;
&#36845;&#20195;&#24335;&#35268;&#21010;&#20013;&#30340;&#36873;&#39033;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26377;&#29992;&#30340;&#26102;&#38388;&#25277;&#35937;&#65292;&#20063;&#23601;&#26159;&#36873;&#39033;&#65292;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#24212;&#29992;&#20110;&#26085;&#30410;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#22312;AlphaZero&#20013;&#20351;&#29992;&#30340;Expert Iteration&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#30340;&#32463;&#39564;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;Option Iteration&#65292;&#19968;&#31181;&#31867;&#20284;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#12290;Option Iteration&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#24378;&#31574;&#30053;&#65292;&#32780;&#26159;&#23398;&#20064;&#19968;&#32452;&#36873;&#39033;&#31574;&#30053;&#65292;&#23545;&#20110;&#36935;&#21040;&#30340;&#27599;&#20010;&#29366;&#24577;&#65292;&#33267;&#23569;&#26377;&#19968;&#31181;&#31574;&#30053;&#22312;&#26576;&#20010;&#26410;&#26469;&#30340;&#26102;&#38388;&#28857;&#19982;&#25628;&#32034;&#32467;&#26524;&#21563;&#21512;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36825;&#21487;&#33021;&#26356;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#31639;&#27861;&#26681;&#25454;&#24773;&#20917;&#28789;&#27963;&#35843;&#25972;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#24403;&#21069;&#29366;&#24577;&#30340;&#32454;&#33410;&#19978;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#24615;&#30340;&#20840;&#23616;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#36825;&#26679;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#24418;&#25104;&#33391;&#24615;&#24490;&#29615;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering useful temporal abstractions, in the form of options, is widely thought to be key to applying reinforcement learning and planning to increasingly complex domains. Building on the empirical success of the Expert Iteration approach to policy learning used in AlphaZero, we propose Option Iteration, an analogous approach to option discovery. Rather than learning a single strong policy that is trained to match the search results everywhere, Option Iteration learns a set of option policies trained such that for each state encountered, at least one policy in the set matches the search results for some horizon into the future. Intuitively, this may be significantly easier as it allows the algorithm to hedge its bets compared to learning a single globally strong policy, which may have complex dependencies on the details of the current state. Having learned such a set of locally strong policies, we can use them to guide the search algorithm resulting in a virtuous cycle where better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;InSAR&#22270;&#20687;&#20013;&#24555;&#36895;&#26816;&#27979;&#39123;&#39118;&#21518;&#24314;&#31569;&#25439;&#20260;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#22270;&#26469;&#25552;&#39640;&#24314;&#31569;&#25439;&#23475;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01565</link><description>&lt;p&gt;
&#20174;InSAR&#22270;&#20687;&#20013;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22823;&#35268;&#27169;&#39123;&#39118;&#21518;&#24314;&#31569;&#25439;&#23475;&#24555;&#36895;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Causality-informed Rapid Post-hurricane Building Damage Detection in Large Scale from InSAR Imagery. (arXiv:2310.01565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;InSAR&#22270;&#20687;&#20013;&#24555;&#36895;&#26816;&#27979;&#39123;&#39118;&#21518;&#24314;&#31569;&#25439;&#20260;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#22270;&#26469;&#25552;&#39640;&#24314;&#31569;&#25439;&#23475;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#20934;&#30830;&#35780;&#20272;&#39123;&#39118;&#24341;&#36215;&#30340;&#24314;&#31569;&#29289;&#25439;&#23475;&#23545;&#20110;&#26377;&#25928;&#30340;&#39123;&#39118;&#21518;&#24212;&#23545;&#21644;&#24674;&#22797;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#25216;&#26415;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#20809;&#23398;&#25110;&#24178;&#28041;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;InSAR&#65289;&#22270;&#20687;&#25968;&#25454;&#65292;&#21487;&#31435;&#21363;&#29992;&#20110;&#36827;&#34892;&#24555;&#36895;&#24314;&#31569;&#25439;&#23475;&#35780;&#20272;&#12290;&#19982;&#20809;&#23398;&#21355;&#26143;&#22270;&#20687;&#30456;&#27604;&#65292;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#21487;&#20197;&#31359;&#36879;&#20113;&#23618;&#65292;&#24182;&#22312;&#21508;&#31181;&#22825;&#27668;&#26465;&#20214;&#19979;&#25552;&#20379;&#26356;&#23436;&#25972;&#30340;&#21463;&#25439;&#21306;&#22495;&#31354;&#38388;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;InSAR&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#30001;&#20849;&#21516;&#21457;&#29983;&#25110;&#20849;&#21516;&#20301;&#32622;&#30340;&#24314;&#31569;&#25439;&#22351;&#12289;&#27946;&#27700;&#12289;&#27946;&#27700;/&#39118;&#24341;&#21457;&#30340;&#26893;&#34987;&#21464;&#21270;&#20197;&#21450;&#20154;&#20026;&#27963;&#21160;&#24341;&#36215;&#30340;&#39640;&#24230;&#22122;&#22768;&#21644;&#28151;&#21512;&#20449;&#21495;&#65292;&#20351;&#24471;&#20934;&#30830;&#25552;&#21462;&#24314;&#31569;&#25439;&#23475;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;InSAR&#22270;&#20687;&#20013;&#24555;&#36895;&#26816;&#27979;&#39123;&#39118;&#21518;&#24314;&#31569;&#25439;&#20260;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32534;&#30721;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#22270;&#26469;&#25552;&#39640;&#24314;&#31569;&#25439;&#23475;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timely and accurate assessment of hurricane-induced building damage is crucial for effective post-hurricane response and recovery efforts. Recently, remote sensing technologies provide large-scale optical or Interferometric Synthetic Aperture Radar (InSAR) imagery data immediately after a disastrous event, which can be readily used to conduct rapid building damage assessment. Compared to optical satellite imageries, the Synthetic Aperture Radar can penetrate cloud cover and provide more complete spatial coverage of damaged zones in various weather conditions. However, these InSAR imageries often contain highly noisy and mixed signals induced by co-occurring or co-located building damage, flood, flood/wind-induced vegetation changes, as well as anthropogenic activities, making it challenging to extract accurate building damage information. In this paper, we introduced an approach for rapid post-hurricane building damage detection from InSAR imagery. This approach encoded complex causal 
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01551</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#25321;&#30340;&#33021;&#21147;&#20248;&#21270;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#26631;&#20934;&#21644;&#32463;&#39564;&#25104;&#21151;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;ID3&#12289;C4.5&#21644;CART&#65289;&#36827;&#34892;&#25512;&#24191;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20973;&#20511;&#36138;&#23146;&#30340;&#29305;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65306;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#22320;&#22522;&#20110;&#26368;&#20339;&#23646;&#24615;&#36827;&#34892;&#21010;&#20998;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;Top-$k$&#21017;&#32771;&#34385;$k$&#20010;&#26368;&#20339;&#23646;&#24615;&#20316;&#20026;&#21487;&#33021;&#30340;&#21010;&#20998;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#26368;&#20339;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#25512;&#24191;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#8220;&#36138;&#23146;&#23618;&#27425;&#23450;&#29702;&#8221;&#65292;&#23545;&#20110;&#27599;&#20010;$k \in \mathbb{N}$&#65292;Top-$(k+1)$&#27604;Top-$k$&#26356;&#21152;&#24378;&#22823;&#65306;&#22312;&#26576;&#20123;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21069;&#32773;&#21487;&#20197;&#36798;&#21040;$1-\varepsilon$&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#21518;&#32773;&#21482;&#33021;&#36798;&#21040;$\frac1{2}+\varepsilon$&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Top-$k$&#31639;&#27861;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#20248;&#20110;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#32463;&#20856;&#36138;&#23146;&#31639;&#27861;&#21644;&#36739;&#26032;&#30340;&#8220;&#26368;&#20248;&#20915;&#31574;&#26641;&#8221;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\sl greediness hierarchy theorem} showing that for every $k \in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\varepsilon$, whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#30340;&#25913;&#36827;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#27604;&#36739;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#23485;&#24230;&#65292;&#21457;&#29616;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#20855;&#26377;&#36739;&#23567;&#30340;&#26497;&#38480;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01547</link><description>&lt;p&gt;
&#20851;&#20110;&#26377;&#30028;&#22343;&#20540;&#30340;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#30340;&#36817;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the near-optimality of betting confidence sets for bounded means. (arXiv:2310.01547v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#30340;&#25913;&#36827;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#27604;&#36739;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#23485;&#24230;&#65292;&#21457;&#29616;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#20855;&#26377;&#36739;&#23567;&#30340;&#26497;&#38480;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#20013;&#65292;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#35266;&#27979;&#20013;&#26500;&#24314;&#19968;&#20803;&#20998;&#24067;&#30340;&#38750;&#28176;&#36817;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#23545;&#20110;&#26377;&#30028;&#35266;&#27979;&#20540;&#65292;&#32463;&#20856;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#36890;&#36807;&#21453;&#36716;&#26631;&#20934;&#27987;&#24230;&#30028;&#38480;&#65288;&#22914;Hoeffding&#25110;Bernstein&#19981;&#31561;&#24335;&#65289;&#26469;&#36827;&#34892;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#26367;&#20195;&#30340;&#22522;&#20110;&#25237;&#27880;&#30340;&#26041;&#27861;&#34987;&#29992;&#20110;&#23450;&#20041;CI&#21644;&#20854;&#26102;&#38388;&#19968;&#33268;&#21464;&#20307;&#65292;&#31216;&#20026;&#32622;&#20449;&#24207;&#21015;&#65288;CS&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#32463;&#20856;&#26041;&#27861;&#12290;&#26412;&#25991;&#20026;&#36825;&#31181;&#25237;&#27880;CI&#21644;CS&#30340;&#25913;&#36827;&#32463;&#39564;&#24615;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22914;&#19979;&#65306;&#65288;i&#65289;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;CI&#65292;&#20351;&#29992;&#23427;&#20204;&#30340;&#19968;&#38454;&#28176;&#36817;&#23485;&#24230;&#30340;&#20540;&#65288;&#32463;&#36807;$\sqrt{n}$&#32553;&#25918;&#65289;&#65292;&#24182;&#19988;&#34920;&#26126;Waudby-Smith&#21644;Ramdas&#65288;2023&#65289;&#30340;&#25237;&#27880;CI&#27604;&#29616;&#26377;&#30340;&#32463;&#39564;Bernstein&#65288;EB&#65289;CI&#30340;&#26497;&#38480;&#23485;&#24230;&#26356;&#23567;&#12290;&#65288;ii&#65289;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#20010;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing nonasymptotic confidence intervals (CIs) for the mean of a univariate distribution from independent and identically distributed (i.i.d.) observations is a fundamental task in statistics. For bounded observations, a classical nonparametric approach proceeds by inverting standard concentration bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an alternative betting-based approach for defining CIs and their time-uniform variants called confidence sequences (CSs), has been shown to be empirically superior to the classical methods. In this paper, we provide theoretical justification for this improved empirical performance of betting CIs and CSs.  Our main contributions are as follows: (i) We first compare CIs using the values of their first-order asymptotic widths (scaled by $\sqrt{n}$), and show that the betting CI of Waudby-Smith and Ramdas (2023) has a smaller limiting width than existing empirical Bernstein (EB)-CIs. (ii) Next, we establish two lower bounds
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#34701;&#21512;&#20855;&#26377;&#20114;&#34917;&#19987;&#38271;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.01542</link><description>&lt;p&gt;
&#34701;&#21512;&#20855;&#26377;&#20114;&#34917;&#19987;&#38271;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fusing Models with Complementary Expertise. (arXiv:2310.01542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#34701;&#21512;&#20855;&#26377;&#20114;&#34917;&#19987;&#38271;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33021;&#22815;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#36827;&#34892;&#27867;&#21270;&#30340;AI&#27169;&#22411;&#19968;&#30452;&#26159;&#25512;&#21160;AI&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#20043;&#19968;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#33719;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;&#19987;&#23478;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#65292;&#20294;&#26159;&#22312;&#27979;&#35797;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#30340;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#24847;&#21619;&#30528;&#20219;&#20309;&#21333;&#20010;&#19987;&#23478;&#37117;&#19981;&#36275;&#22815;&#12290;&#25105;&#20204;&#32771;&#34385;&#34701;&#21512;&#19987;&#23478;&#27169;&#22411;&#36755;&#20986;&#30340;Fusion of Experts&#65288;FoE&#65289;&#38382;&#39064;&#65292;&#36825;&#20123;&#19987;&#23478;&#27169;&#22411;&#20855;&#26377;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20114;&#34917;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#21644;&#29983;&#25104;&#25991;&#26412;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#8220;&#33410;&#20461;&#8221;&#35774;&#32622;&#65292;&#21363;&#24076;&#26395;&#22312;&#27979;&#35797;&#26102;&#20943;&#23569;&#19987;&#23478;&#27169;&#22411;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the "frugal" setting where it is desired to reduce the number of expert model evaluations at test time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#23376;&#31354;&#38388;&#30417;&#27979;&#30340;&#26041;&#27861;FedRR&#65292;&#29992;&#20110;&#23545;&#25239;&#23458;&#25143;&#26816;&#27979;&#21644;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#29289;&#32852;&#32593;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01537</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#21442;&#25968;&#23376;&#31354;&#38388;&#30417;&#27979;&#30340;&#23545;&#25239;&#23458;&#25143;&#26816;&#27979;&#22312;&#32852;&#37030;&#29289;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Client Detection via Non-parametric Subspace Monitoring in the Internet of Federated Things. (arXiv:2310.01537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#23376;&#31354;&#38388;&#30417;&#27979;&#30340;&#26041;&#27861;FedRR&#65292;&#29992;&#20110;&#23545;&#25239;&#23458;&#25143;&#26816;&#27979;&#21644;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#29289;&#32852;&#32593;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#29289;&#32852;&#32593;&#65288;IoFT&#65289;&#20195;&#34920;&#20102;&#19968;&#20010;&#30001;&#20114;&#32852;&#31995;&#32479;&#32452;&#25104;&#30340;&#32593;&#32476;&#65292;&#20197;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#25903;&#25745;&#65292;&#20419;&#36827;&#21327;&#20316;&#24335;&#30693;&#35782;&#33719;&#21462;&#65292;&#21516;&#26102;&#30830;&#20445;&#20010;&#20307;&#31995;&#32479;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;IoFT&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#23433;&#20840;&#38382;&#39064;&#30340;&#38459;&#30861;&#65292;&#29305;&#21035;&#26159;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;FedRR&#65292;&#23427;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#29983;&#25104;&#30340;&#20256;&#36755;&#21442;&#25968;&#26356;&#26032;&#30340;&#20302;&#31209;&#29305;&#24449;&#26469;&#35299;&#20915;&#23545;&#25239;&#24615;&#25915;&#20987;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#23545;&#25239;&#24615;&#23458;&#25143;&#65292;&#24182;&#22312;&#27809;&#26377;&#25915;&#20987;&#21457;&#29983;&#30340;&#24773;&#20917;&#19979;&#25511;&#21046;&#35823;&#25253;&#29575;&#12290;&#22522;&#20110;MNIST&#25968;&#25454;&#38598;&#30340;&#25968;&#23383;&#35782;&#21035;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Federated Things (IoFT) represents a network of interconnected systems with federated learning as the backbone, facilitating collaborative knowledge acquisition while ensuring data privacy for individual systems. The wide adoption of IoFT, however, is hindered by security concerns, particularly the susceptibility of federated learning networks to adversarial attacks. In this paper, we propose an effective non-parametric approach FedRR, which leverages the low-rank features of the transmitted parameter updates generated by federated learning to address the adversarial attack problem. Besides, our proposed method is capable of accurately detecting adversarial clients and controlling the false alarm rate under the scenario with no attack occurring. Experiments based on digit recognition using the MNIST datasets validated the advantages of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36827;&#34892;&#29616;&#22330;&#39044;&#27979;&#27425;&#26085;&#36793;&#38469;&#25490;&#25918;&#22240;&#32032;&#65292;&#20197;&#36866;&#24212;&#39640;&#28789;&#27963;&#24615;&#21644;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#28183;&#36879;&#30340;&#30005;&#21147;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.01524</link><description>&lt;p&gt;
&#29616;&#22330;&#39044;&#27979;&#21033;&#29992;&#22810;&#22836;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#39044;&#27979;&#27425;&#26085;&#36793;&#38469;&#25490;&#25918;
&lt;/p&gt;
&lt;p&gt;
Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep generative models. (arXiv:2310.01524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36827;&#34892;&#29616;&#22330;&#39044;&#27979;&#27425;&#26085;&#36793;&#38469;&#25490;&#25918;&#22240;&#32032;&#65292;&#20197;&#36866;&#24212;&#39640;&#28789;&#27963;&#24615;&#21644;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#28183;&#36879;&#30340;&#30005;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22810;&#22836;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29616;&#22330;&#39044;&#27979;&#27425;&#26085;&#36793;&#38469;&#25490;&#25918;&#22240;&#32032;&#23545;&#20110;&#20855;&#26377;&#39640;&#28789;&#27963;&#24615;&#21644;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#28183;&#36879;&#30340;&#30005;&#21147;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#24403;&#21069;&#33021;&#28304;&#31995;&#32479;&#20013;&#65292;&#30001;&#22825;&#28982;&#27668;&#21644;&#29123;&#29028;&#21457;&#30005;&#21378;&#25552;&#20379;&#30340;&#22823;&#37096;&#20998;&#30340;&#22266;&#23450;&#21457;&#30005;&#37327;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#23545;&#27425;&#26085;&#25490;&#25918;&#30340;&#39044;&#27979;&#12290;&#30456;&#21453;&#65292;&#38543;&#30528;&#25105;&#20204;&#36716;&#21521;&#20197;&#28789;&#27963;&#30340;&#30005;&#21147;&#24066;&#22330;&#12289;&#21487;&#20998;&#37197;&#36164;&#28304;&#21644;&#31454;&#20105;&#24615;&#20302;&#25104;&#26412;&#21457;&#30005;&#65288;&#22914;&#22823;&#35268;&#27169;&#30005;&#27744;&#25110;&#27682;&#20648;&#23384;&#65289;&#20026;&#29305;&#24449;&#30340;&#33021;&#28304;&#31995;&#32479;&#65292;&#31995;&#32479;&#25805;&#20316;&#21592;&#23558;&#33021;&#22815;&#20174;&#19981;&#21516;&#21457;&#30005;&#20197;&#21450;&#25490;&#25918;&#36335;&#24452;&#30340;&#32452;&#21512;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#20026;&#20102;&#20805;&#20998;&#24320;&#21457;&#32473;&#23450;&#35843;&#24230;&#35745;&#21010;&#30340;&#25490;&#25918;&#24433;&#21709;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#23618;&#32423;&#30340;&#36817;&#23454;&#26102;&#24037;&#20316;&#27969;&#31243;&#12290;&#31532;&#19968;&#23618;&#26159;&#19968;&#20010;&#24066;&#22330;&#27169;&#22411;&#65292;&#19981;&#26029;&#35299;&#20915;&#19968;&#20010;&#23433;&#20840;&#32422;&#26463;&#32463;&#27982;&#35843;&#24230;&#27169;&#22411;&#12290;&#31532;&#20108;&#23618;&#26681;&#25454;&#24066;&#22330;&#27169;&#22411;&#30340;&#36755;&#20986;&#30830;&#23450;&#36793;&#38469;&#25490;&#25918;&#65292;&#36825;&#26159;&#26412;&#25991;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowcasting day-ahead marginal emissions factors is increasingly important for power systems with high flexibility and penetration of distributed energy resources. With a significant share of firm generation from natural gas and coal power plants, forecasting day-ahead emissions in the current energy system has been widely studied. In contrast, as we shift to an energy system characterized by flexible power markets, dispatchable sources, and competing low-cost generation such as large-scale battery or hydrogen storage, system operators will be able to choose from a mix of different generation as well as emission pathways. To fully develop the emissions implications of a given dispatch schedule, we need a near real-time workflow with two layers. The first layer is a market model that continuously solves a security-constrained economic dispatch model. The second layer determines the marginal emissions based on the output of the market model, which is the subject of this paper. We propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22122;&#22768;&#27880;&#20837;&#22312;&#21160;&#24577;&#28784;&#31665;&#27169;&#22411;&#21019;&#24314;&#20013;&#30340;&#30410;&#22788;&#12290;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#22122;&#22768;&#65292;&#21487;&#20197;&#35299;&#20915;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#12289;&#26410;&#24314;&#27169;&#30340;&#21160;&#24577;&#21644;&#23616;&#37096;&#26497;&#23567;&#20540;&#31561;&#25361;&#25112;&#65292;&#22312;&#27700;-&#27700;&#25442;&#28909;&#22120;&#30340;&#21160;&#24577;&#27169;&#22411;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24314;&#27169;&#35823;&#24046;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.01517</link><description>&lt;p&gt;
&#22122;&#22768;&#27880;&#20837;&#23545;&#20110;&#21160;&#24577;&#28784;&#31665;&#27169;&#22411;&#21019;&#24314;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation. (arXiv:2310.01517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22122;&#22768;&#27880;&#20837;&#22312;&#21160;&#24577;&#28784;&#31665;&#27169;&#22411;&#21019;&#24314;&#20013;&#30340;&#30410;&#22788;&#12290;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#22122;&#22768;&#65292;&#21487;&#20197;&#35299;&#20915;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#12289;&#26410;&#24314;&#27169;&#30340;&#21160;&#24577;&#21644;&#23616;&#37096;&#26497;&#23567;&#20540;&#31561;&#25361;&#25112;&#65292;&#22312;&#27700;-&#27700;&#25442;&#28909;&#22120;&#30340;&#21160;&#24577;&#27169;&#22411;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24314;&#27169;&#35823;&#24046;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28784;&#31665;&#27169;&#22411;&#30456;&#27604;&#20110;&#40657;&#31665;&#26041;&#27861;&#22312;&#35774;&#22791;&#20223;&#30495;&#22120;&#24320;&#21457;&#20013;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#22240;&#20026;&#20854;&#38598;&#25104;&#20102;&#29289;&#29702;&#23398;&#65292;&#22312;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#27169;&#22411;&#20449;&#24515;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#12289;&#26410;&#24314;&#27169;&#30340;&#21160;&#24577;&#21644;&#23616;&#37096;&#26497;&#23567;&#20540;&#31561;&#25361;&#25112;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#20123;&#25361;&#25112;&#26159;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20811;&#26381;&#30340;&#65292;&#23548;&#33268;&#20854;&#24615;&#33021;&#20302;&#20110;&#40657;&#31665;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#22122;&#22768;&#26469;&#35299;&#20915;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#12290;&#22122;&#22768;&#27880;&#20837;&#20016;&#23500;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#34913;&#37327;&#12290;&#20197;&#27700;-&#27700;&#25442;&#28909;&#22120;&#30340;&#21160;&#24577;&#27169;&#22411;&#20316;&#20026;&#28436;&#31034;&#26696;&#20363;&#65292;&#24182;&#20351;&#29992;&#19968;&#23545;&#23454;&#38469;&#35774;&#22791;&#21644;&#23454;&#26102;&#25968;&#25454;&#27969;&#27979;&#35797;&#12290;&#19982;&#26410;&#22788;&#29702;&#30340;&#20449;&#21495;&#25968;&#25454;&#30456;&#27604;&#65292;&#22122;&#22768;&#27880;&#20837;&#30340;&#24212;&#29992;&#23548;&#33268;&#24314;&#27169;&#35823;&#24046;&#26174;&#33879;&#20943;&#23567;&#65288;&#22343;&#26041;&#26681;&#35823;&#24046;&#20943;&#23569;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gray-box models offer significant benefit over black-box approaches for equipment emulator development for equipment since their integration of physics provides more confidence in the model outside of the training domain. However, challenges such as model nonlinearity, unmodeled dynamics, and local minima introduce uncertainties into grey-box creation that contemporary approaches have failed to overcome, leading to their under-performance compared with black-box models. This paper seeks to address these uncertainties by injecting noise into the training dataset. This noise injection enriches the dataset and provides a measure of robustness against such uncertainties. A dynamic model for a water-to-water heat exchanger has been used as a demonstration case for this approach and tested using a pair of real devices with live data streaming. Compared to the unprocessed signal data, the application of noise injection resulted in a significant reduction in modeling error (root mean square er
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#29615;&#20248;&#21270;&#30340;&#22686;&#24378;&#37327;&#23376;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#65288;TR-QNet&#65289;&#23558;&#20018;&#32852;&#32416;&#32544;&#38376;&#24212;&#29992;&#20110;&#24352;&#37327;&#32593;&#32476;&#65292;&#36890;&#36807;&#37327;&#23376;&#27604;&#29305;&#27979;&#37327;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20854;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20108;&#20803;&#20998;&#31867;&#31934;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.01515</link><description>&lt;p&gt;
&#24352;&#37327;&#29615;&#20248;&#21270;&#30340;&#22686;&#24378;&#37327;&#23376;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor Ring Optimized Quantum-Enhanced Tensor Neural Networks. (arXiv:2310.01515v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01515
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#29615;&#20248;&#21270;&#30340;&#22686;&#24378;&#37327;&#23376;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#65288;TR-QNet&#65289;&#23558;&#20018;&#32852;&#32416;&#32544;&#38376;&#24212;&#29992;&#20110;&#24352;&#37327;&#32593;&#32476;&#65292;&#36890;&#36807;&#37327;&#23376;&#27604;&#29305;&#27979;&#37327;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20854;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20108;&#20803;&#20998;&#31867;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#36890;&#24120;&#23558;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#21464;&#20998;&#20248;&#21270;&#20013;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#27599;&#20010;&#23618;&#30340;&#21487;&#35757;&#32451;&#26435;&#37325;&#30340;&#26631;&#20934;&#20248;&#21270;&#25216;&#26415;&#22312;&#32463;&#20856;&#23454;&#29616;&#20013;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#32416;&#32544;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#29615;&#20248;&#21270;&#30340;&#21464;&#20998;&#37327;&#23376;&#23398;&#20064;&#20998;&#31867;&#22120;&#65288;Quan-TR&#65289;&#30340;&#22810;&#23618;&#35774;&#35745;&#65292;&#35813;&#35774;&#35745;&#29992;&#20018;&#32852;&#32416;&#32544;&#38376;&#26367;&#20195;&#24352;&#37327;&#32593;&#32476;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#24182;&#31216;&#20043;&#20026;&#24352;&#37327;&#29615;&#20248;&#21270;&#30340;&#22686;&#24378;&#37327;&#23376;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#65288;TR-QNet&#65289;&#12290;&#36890;&#36807;&#37327;&#23376;&#27604;&#29305;&#27979;&#37327;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;TR-QNet&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#40482;&#23614;&#33457;&#12289;MNIST&#21644;CIFAR-10&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;TR-QNet&#65292;&#20197;&#23637;&#31034;&#20108;&#20803;&#20998;&#31867;&#25152;&#23454;&#29616;&#30340;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning researchers often rely on incorporating Tensor Networks (TN) into Deep Neural Networks (DNN) and variational optimization. However, the standard optimization techniques used for training the contracted trainable weights of each model layer suffer from the correlations and entanglement structure between the model parameters on classical implementations. To address this issue, a multi-layer design of a Tensor Ring optimized variational Quantum learning classifier (Quan-TR) comprising cascading entangling gates replacing the fully connected (dense) layers of a TN is proposed, and it is referred to as Tensor Ring optimized Quantum-enhanced tensor neural Networks (TR-QNet). TR-QNet parameters are optimized through the stochastic gradient descent algorithm on qubit measurements. The proposed TR-QNet is assessed on three distinct datasets, namely Iris, MNIST, and CIFAR-10, to demonstrate the enhanced precision achieved for binary classification. On quantum simulations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#26102;&#38388;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#24565;&#28418;&#31227;&#27169;&#25311;&#22120;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01508</link><description>&lt;p&gt;
CODA: &#36890;&#36807;&#27010;&#24565;&#28418;&#31227;&#27169;&#25311;&#22120;&#23454;&#29616;&#26102;&#38388;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
CODA: Temporal Domain Generalization via Concept Drift Simulator. (arXiv:2310.01508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#26102;&#38388;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#24565;&#28418;&#31227;&#27169;&#25311;&#22120;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#24213;&#23618;&#26102;&#38388;&#36235;&#21183;&#24341;&#36215;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#21464;&#24471;&#36807;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#27010;&#24565;&#28418;&#31227;&#8221;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#22411;&#29305;&#23450;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#36817;&#26399;&#39046;&#22495;&#30340;&#26102;&#38388;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#35201;&#27714;&#23450;&#21046;&#21270;&#30340;&#39044;&#27979;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26102;&#38388;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#21644;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#20445;&#25345;&#26222;&#36941;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#32469;&#36807;&#32771;&#34385;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#24320;&#21457;&#36825;&#26679;&#30340;&#26694;&#26550;&#38754;&#20020;&#30528;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;(i)&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#20043;&#22806;&#30340;&#26410;&#26469;&#25968;&#25454;&#65292;(ii)&#20934;&#30830;&#25429;&#25417;&#27839;&#26102;&#38388;&#39034;&#24207;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26102;&#38388;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, machine learning models often become obsolete due to shifts in the joint distribution arising from underlying temporal trends, a phenomenon known as the "concept drift". Existing works propose model-specific strategies to achieve temporal generalization in the near-future domain. However, the diverse characteristics of real-world datasets necessitate customized prediction model architectures. To this end, there is an urgent demand for a model-agnostic temporal domain generalization approach that maintains generality across diverse data modalities and architectures. In this work, we aim to address the concept drift problem from a data-centric perspective to bypass considering the interaction between data and model. Developing such a framework presents non-trivial challenges: (i) existing generative models struggle to generate out-of-distribution future data, and (ii) precisely capturing the temporal trends of joint distribution along chronological source doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;QReliefF&#65292;&#36890;&#36807;&#37327;&#23376;&#32534;&#30721;&#21644;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#31639;&#27861;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01443</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#29305;&#24449;&#36873;&#25321;&#19982;&#36793;&#32536;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing. (arXiv:2310.01443v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;QReliefF&#65292;&#36890;&#36807;&#37327;&#23376;&#32534;&#30721;&#21644;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#31639;&#27861;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31995;&#32479;&#30340;&#36793;&#32536;&#35745;&#31639;&#38656;&#35201;&#22823;&#37327;&#30340;&#22810;&#29305;&#24449;&#25968;&#25454;&#26469;&#25552;&#21462;&#36866;&#24403;&#30340;&#35265;&#35299;&#20197;&#25903;&#25345;&#20915;&#31574;&#65292;&#22240;&#27492;&#23547;&#25214;&#19968;&#31181;&#21487;&#34892;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#28040;&#32791;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#21517;&#20026;QReliefF&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#31639;&#27861;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25191;&#34892;CMP&#21644;R_y&#25805;&#20316;&#65292;&#23558;&#27599;&#20010;&#26679;&#26412;&#30340;&#25152;&#26377;&#29305;&#24449;&#32534;&#30721;&#20026;&#37327;&#23376;&#24577;&#65292;&#28982;&#21518;&#24212;&#29992;&#24133;&#24230;&#20272;&#35745;&#26469;&#35745;&#31639;&#20219;&#24847;&#20004;&#20010;&#37327;&#23376;&#24577;&#65288;&#21363;&#20004;&#20010;&#26679;&#26412;&#65289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26681;&#25454;&#30456;&#20284;&#24615;&#65292;&#21033;&#29992;Grover-Long&#26041;&#27861;&#25214;&#21040;&#26368;&#36817;&#30340;k&#20010;&#37051;&#23621;&#26679;&#26412;&#65292;&#28982;&#21518;&#26356;&#26032;&#26435;&#37325;&#21521;&#37327;&#12290;&#36890;&#36807;&#19978;&#36848;&#36807;&#31243;&#30340;&#19968;&#23450;&#27425;&#25968;&#30340;&#36845;&#20195;&#65292;&#21487;&#20197;&#36873;&#25321;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex systems with edge computing require a huge amount of multi-feature data to extract appropriate insights for their decision making, so it is important to find a feasible feature selection method to improve the computational efficiency and save the resource consumption. In this paper, a quantum-based feature selection algorithm for the multi-classification problem, namely, QReliefF, is proposed, which can effectively reduce the complexity of algorithm and improve its computational efficiency. First, all features of each sample are encoded into a quantum state by performing operations CMP and R_y, and then the amplitude estimation is applied to calculate the similarity between any two quantum states (i.e., two samples). According to the similarities, the Grover-Long method is utilized to find the nearest k neighbor samples, and then the weight vector is updated. After a certain number of iterations through the above process, the desired features can be selected with regards to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.01438</link><description>&lt;p&gt;
&#26500;&#24314;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#37319;&#38598;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#24322;&#36136;&#21307;&#23398;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#23558;&#25918;&#23556;&#23398;&#25195;&#25551;&#12289;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21644;&#20998;&#23376;&#20449;&#24687;&#19982;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#26159;&#24320;&#21457;&#23545;&#30142;&#30149;&#26377;&#20840;&#38754;&#29702;&#35299;&#21644;&#20248;&#21270;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#22797;&#26434;&#30142;&#30149;&#65288;&#22914;&#30284;&#30151;&#65289;&#20013;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#36827;&#34892;&#25972;&#21512;&#30340;&#38656;&#27714;&#26356;&#21152;&#31361;&#20986;&#65292;&#20197;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#32959;&#30244;&#25968;&#25454;&#31995;&#32479;&#65288;MINDS&#65289;-&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#32463;&#27982;&#39640;&#25928;&#30340;&#20803;&#25968;&#25454;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26469;&#33258;&#20844;&#20849;&#26469;&#28304;&#65288;&#22914;&#30284;&#30151;&#30740;&#31350;&#25968;&#25454;&#20849;&#20139;&#24211;&#65289;&#30340;&#24322;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#19988;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#20013;&#12290;MINDS&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#25506;&#32034;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#24182;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;&#21327;&#35843;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;MINDS&#26088;&#22312;&#23454;&#29616;&#20419;&#36827;&#30740;&#31350;&#21019;&#26032;&#12289;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancements in data acquisition, storage, and processing techniques have resulted in the rapid growth of heterogeneous medical data. Integrating radiological scans, histopathology images, and molecular information with clinical data is essential for developing a holistic understanding of the disease and optimizing treatment. The need for integrating data from multiple sources is further pronounced in complex diseases such as cancer for enabling precision medicine and personalized treatments. This work proposes Multimodal Integration of Oncology Data System (MINDS) - a flexible, scalable, and cost-effective metadata framework for efficiently fusing disparate data from public sources such as the Cancer Research Data Commons (CRDC) into an interconnected, patient-centric framework. MINDS offers an interface for exploring relationships across data types and building cohorts for developing large-scale multimodal machine learning models. By harmonizing multimodal data, MINDS aims to pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01436</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#22312;&#33258;&#21160;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;GNAS&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21171;&#21160;&#21644;&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35774;&#35745;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;GNAS&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;&#31616;&#31216;&#20026;GPT4GNAS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20026;GPT-4&#35774;&#35745;&#19968;&#31867;&#26032;&#30340;&#25552;&#31034;&#65292;&#20197;&#25351;&#23548;GPT-4&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#25552;&#31034;&#21253;&#25324;GNAS&#30340;&#25628;&#32034;&#31354;&#38388;&#12289;&#25628;&#32034;&#31574;&#30053;&#21644;&#25628;&#32034;&#21453;&#39304;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#36816;&#34892;&#20855;&#26377;&#25552;&#31034;&#30340;GPT-4&#65292;GPT4GNAS&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;GNAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;30&#20159;&#21442;&#25968;&#30340;GPT LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#30340;&#24179;&#31283;&#36816;&#34892;&#65292;&#24182;&#35299;&#20915;&#20102;&#32593;&#32476;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24212;&#29992;&#19981;&#20165;&#20855;&#26377;&#36890;&#29992;&#21161;&#25163;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#26080;&#32541;&#31227;&#21160;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.01434</link><description>&lt;p&gt;
&#38761;&#26032;&#31227;&#21160;&#20114;&#21160;&#65306;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;30&#20159;&#21442;&#25968;&#30340;GPT LLM
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. (arXiv:2310.01434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;30&#20159;&#21442;&#25968;&#30340;GPT LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#30340;&#24179;&#31283;&#36816;&#34892;&#65292;&#24182;&#35299;&#20915;&#20102;&#32593;&#32476;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24212;&#29992;&#19981;&#20165;&#20855;&#26377;&#36890;&#29992;&#21161;&#25163;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#26080;&#32541;&#31227;&#21160;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#24378;&#22823;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#12290;&#20113;&#31471;&#30340;LLM&#65292;&#20363;&#22914;OpenAI&#30340;ChatGPT&#65292;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21151;&#33021;&#65292;&#20294;&#30001;&#20110;&#32593;&#32476;&#20381;&#36182;&#24615;&#65292;&#24310;&#36831;&#21644;&#38544;&#31169;&#38382;&#39064;&#20196;&#20154;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#23637;&#26395;&#20102;&#26410;&#26469;&#22312;&#27809;&#26377;&#32593;&#32476;&#36830;&#25509;&#30340;&#24773;&#20917;&#19979;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20855;&#26377;30&#20159;&#21442;&#25968;&#30340;&#31934;&#35843;GPT LLM&#65292;&#21487;&#20197;&#22312;&#20869;&#23384;&#21482;&#26377;4GB&#30340;&#35774;&#22791;&#19978;&#24179;&#31283;&#36816;&#34892;&#12290;&#36890;&#36807;&#25972;&#21512;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#65292;&#35813;&#24212;&#29992;&#19981;&#20165;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#21161;&#25163;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#21151;&#33021;&#23454;&#29616;&#26080;&#32541;&#30340;&#31227;&#21160;&#20114;&#21160;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#35757;&#32451;&#27969;&#31243;&#12289;&#23454;&#29616;&#32454;&#33410;&#21644;&#27979;&#35797;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, 
&lt;/p&gt;</description></item><item><title>AI-Aristotle&#26159;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#28784;&#30418;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;X-TFC&#21644;PINNs&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#21442;&#25968;&#21457;&#29616;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31995;&#32479;&#29983;&#29289;&#23398;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AI-Aristotle&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01433</link><description>&lt;p&gt;
AI-Aristotle: &#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#31995;&#32479;&#29983;&#29289;&#23398;&#28784;&#30418;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box Identification. (arXiv:2310.01433v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01433
&lt;/p&gt;
&lt;p&gt;
AI-Aristotle&#26159;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#28784;&#30418;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;X-TFC&#21644;PINNs&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#21442;&#25968;&#21457;&#29616;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31995;&#32479;&#29983;&#29289;&#23398;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AI-Aristotle&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#29289;&#29702;&#21644;&#29983;&#29289;&#31995;&#32479;&#30340;&#25968;&#23398;&#26041;&#31243;&#26159;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#21551;&#21457;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#39046;&#22495;&#21442;&#25968;&#20272;&#35745;&#21644;&#32570;&#22833;&#29289;&#29702;&#35782;&#21035;&#65288;&#28784;&#30418;&#65289;&#12290;&#35813;&#26694;&#26550;&#21517;&#20026;AI-Aristotle&#65292;&#32467;&#21512;&#20102;X-TFC&#39046;&#22495;&#20998;&#35299;&#12289;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#21442;&#25968;&#21457;&#29616;&#21644;&#28784;&#30418;&#35782;&#21035;&#12290;&#25105;&#20204;&#22522;&#20110;&#31995;&#32479;&#29983;&#29289;&#23398;&#20013;&#30340;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#20102;AI-Aristotle&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#27979;&#35797;&#65306;&#33647;&#20195;&#21160;&#21147;&#23398;&#33647;&#29289;&#21560;&#25910;&#27169;&#22411;&#21644;&#33889;&#33796;&#31958;&#33008;&#23707;&#32032;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#39057;&#20869;&#20998;&#27852;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;X-TFC&#21644;PINNs&#65289;&#65292;&#24182;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#26469;&#20132;&#21449;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Discovering mathematical equations that govern physical and biological systems from observed data is a fundamental challenge in scientific research. We present a new physics-informed framework for parameter estimation and missing physics identification (gray-box) in the field of Systems Biology. The proposed framework -- named AI-Aristotle -- combines eXtreme Theory of Functional Connections (X-TFC) domain-decomposition and Physics-Informed Neural Networks (PINNs) with symbolic regression (SR) techniques for parameter discovery and gray-box identification. We test the accuracy, speed, flexibility and robustness of AI-Aristotle based on two benchmark problems in Systems Biology: a pharmacokinetics drug absorption model, and an ultradian endocrine model for glucose-insulin interactions. We compare the two machine learning methods (X-TFC and PINNs), and moreover, we employ two different symbolic regression techniques to cross-verify our results. While the current work focuses on the perfo
&lt;/p&gt;</description></item><item><title>REMEDI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25429;&#25417;PSC&#30142;&#30149;&#36827;&#23637;&#36807;&#31243;&#20013;&#32966;&#27713;&#37240;&#21160;&#21147;&#23398;&#21644;&#26426;&#20307;&#33258;&#36866;&#24212;&#21709;&#24212;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#27835;&#30103;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01426</link><description>&lt;p&gt;
REMEDI: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;PSC&#30142;&#30149;&#36827;&#23637;&#20195;&#35874;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REMEDI: REinforcement learning-driven adaptive MEtabolism modeling of primary sclerosing cholangitis DIsease progression. (arXiv:2310.01426v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01426
&lt;/p&gt;
&lt;p&gt;
REMEDI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25429;&#25417;PSC&#30142;&#30149;&#36827;&#23637;&#36807;&#31243;&#20013;&#32966;&#27713;&#37240;&#21160;&#21147;&#23398;&#21644;&#26426;&#20307;&#33258;&#36866;&#24212;&#21709;&#24212;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#27835;&#30103;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#21457;&#24615;&#30828;&#21270;&#24615;&#32966;&#31649;&#28814;(PSC)&#26159;&#19968;&#31181;&#32597;&#35265;&#30340;&#30142;&#30149;&#65292;&#32966;&#27713;&#37240;&#20195;&#35874;&#30340;&#25913;&#21464;&#23548;&#33268;&#25345;&#32493;&#24615;&#32925;&#33039;&#25439;&#20260;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;REMEDI&#65292;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;PSC&#36827;&#23637;&#36807;&#31243;&#20013;&#32966;&#27713;&#37240;&#21160;&#21147;&#23398;&#21644;&#26426;&#20307;&#33258;&#36866;&#24212;&#21709;&#24212;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#25506;&#32034;&#27835;&#30103;&#26041;&#27861;&#12290;REMEDI&#23558;&#25551;&#36848;&#32966;&#27713;&#37240;&#20195;&#35874;&#30340;&#24494;&#20998;&#26041;&#31243;(Differential Equation, DE)&#26426;&#26800;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;(RL)&#30456;&#32467;&#21512;&#65292;&#36830;&#32493;&#22320;&#27169;&#25311;PSC&#20013;&#26426;&#20307;&#30340;&#33258;&#36866;&#24212;&#12290;&#33258;&#36866;&#24212;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#32966;&#27713;&#37240;&#20195;&#35874;&#30456;&#20851;&#30340;&#37238;&#26469;&#32500;&#25345;&#31283;&#24577;&#12290;&#36825;&#20123;&#37238;&#23545;&#24212;&#20110;DE&#21442;&#25968;&#12290;REMEDI&#21033;&#29992;RL&#26469;&#36817;&#20284;PSC&#20013;&#30340;&#33258;&#36866;&#24212;&#65292;&#23558;&#31283;&#24577;&#35270;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#23558;DE&#21442;&#25968;&#30340;&#35843;&#25972;&#35270;&#20026;&#30456;&#24212;&#30340;&#21160;&#20316;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#65292;REMEDI&#29983;&#25104;&#30340;&#32966;&#27713;&#37240;&#21160;&#24577;&#21644;&#21442;&#25968;&#35843;&#25972;&#19982;&#24050;&#21457;&#34920;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25903;&#25345;&#25991;&#29486;&#20013;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Primary sclerosing cholangitis (PSC) is a rare disease wherein altered bile acid metabolism contributes to sustained liver injury. This paper introduces REMEDI, a framework that captures bile acid dynamics and the body's adaptive response during PSC progression that can assist in exploring treatments. REMEDI merges a differential equation (DE)-based mechanistic model that describes bile acid metabolism with reinforcement learning (RL) to emulate the body's adaptations to PSC continuously. An objective of adaptation is to maintain homeostasis by regulating enzymes involved in bile acid metabolism. These enzymes correspond to the parameters of the DEs. REMEDI leverages RL to approximate adaptations in PSC, treating homeostasis as a reward signal and the adjustment of the DE parameters as the corresponding actions. On real-world data, REMEDI generated bile acid dynamics and parameter adjustments consistent with published findings. Also, our results support discussions in the literature th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.01425</link><description>&lt;p&gt;
Borges&#19982;AI
&lt;/p&gt;
&lt;p&gt;
Borges and AI. (arXiv:2310.01425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21551;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26102;&#20195;&#12290;&#19968;&#20123;&#20154;&#30475;&#21040;&#20102;&#26426;&#36935;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#30475;&#21040;&#20102;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#37117;&#36890;&#36807;&#31185;&#24187;&#23567;&#35828;&#20013;&#27969;&#34892;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;AI&#12290;&#26426;&#22120;&#26159;&#21542;&#20250;&#21464;&#24471;&#26377;&#24863;&#30693;&#33021;&#21147;&#24182;&#21453;&#25239;&#20854;&#21019;&#36896;&#32773;&#65311;&#25105;&#20204;&#26159;&#21542;&#20250;&#32463;&#21382;&#32440;&#22841;&#22841;&#23376;&#21551;&#31034;&#65311;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#35813;&#38382;&#19968;&#19979;&#65292;&#36825;&#31181;&#24515;&#29702;&#24847;&#35937;&#26159;&#21542;&#23545;&#25163;&#22836;&#30340;&#29616;&#35937;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#25551;&#36848;&#12290;&#20165;&#36890;&#36807;&#31070;&#28789;&#30340;&#24773;&#32490;&#26469;&#29702;&#35299;&#22825;&#27668;&#27169;&#24335;&#30340;&#26041;&#27861;&#26159;&#26377;&#38480;&#30340;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;LLMs&#21450;&#20854;&#19982;AI&#30340;&#20851;&#31995;&#65292;&#21338;&#23572;&#36203;&#26031;&#26159;20&#19990;&#32426;&#25991;&#23398;&#22823;&#24072;&#65292;&#39764;&#24187;&#29616;&#23454;&#20027;&#20041;&#20808;&#39537;&#21644;&#21518;&#29616;&#20195;&#25991;&#23398;&#30340;&#21069;&#22863;&#12290;&#36825;&#31181;&#25506;&#32034;&#26041;&#24335;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01423</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#21253;&#25324;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#65289;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#65292;&#21560;&#24341;&#20102;&#35768;&#22810;&#20154;&#30340;&#20852;&#36259;&#12290;ChatGPT&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#35823;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#32946;&#21644;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#20960;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#22312;&#30495;&#23454;&#25991;&#26412;&#19978;&#36827;&#34892;&#27979;&#35797;&#30340;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#23427;&#20204;&#23545;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#29992;&#20110;&#26816;&#27979;&#22823;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#26426;&#26500;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#31456;&#12289;&#25688;&#35201;&#12289;&#25925;&#20107;&#12289;&#26032;&#38395;&#21644;&#20135;&#21697;&#35780;&#35770;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#20845;&#31181;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#22810;&#33218;&#36172;&#21338;&#26694;&#26550;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21033;&#29992;&#25361;&#25112;&#21644;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#25910;&#30410;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.01419</link><description>&lt;p&gt;
&#35270;&#39057;&#25512;&#33616;&#20013;&#40065;&#26834;&#22810;&#33218;&#36172;&#21338;&#26694;&#26550;&#30340;&#35774;&#35745;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Design Principles of Robust Multi-Armed Bandit Framework in Video Recommendations. (arXiv:2310.01419v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#22810;&#33218;&#36172;&#21338;&#26694;&#26550;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21033;&#29992;&#25361;&#25112;&#21644;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#25910;&#30410;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#33218;&#36172;&#21338;&#26041;&#27861;&#26356;&#22810;&#20851;&#27880;&#26377;&#25928;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#32780;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#19982;&#20998;&#24067;&#21464;&#21270;&#21644;&#39033;&#30446;&#31454;&#20105;&#30456;&#20851;&#30340;&#24120;&#35265;&#21033;&#29992;&#25361;&#25112;&#12290;&#24456;&#23569;&#26377;&#24037;&#20316;&#25351;&#23548;&#35774;&#35745;&#33021;&#22815;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#36825;&#20123;&#39057;&#32321;&#25361;&#25112;&#30340;&#40065;&#26834;&#36172;&#21338;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#20351;&#36172;&#21338;&#27169;&#22411;&#23545;&#26102;&#21464;&#20803;&#25968;&#25454;&#20449;&#21495;&#40065;&#26834;&#65292;&#23545;&#39033;&#30446;&#31454;&#20105;&#23569;&#26377;&#24433;&#21709;&#65292;&#24182;&#38450;&#27490;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#32780;&#23548;&#33268;&#26435;&#37325;&#27874;&#21160;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20960;&#20010;&#37325;&#35201;&#30340;&#36172;&#21338;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#35745;&#21407;&#21017;&#22312;&#20351;&#36172;&#21338;&#27169;&#22411;&#23545;&#21160;&#24577;&#34892;&#20026;&#21464;&#21270;&#40065;&#26834;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30456;&#23545;&#20110;&#19981;&#37319;&#29992;&#25105;&#20204;&#35774;&#35745;&#36873;&#25321;&#30340;&#22522;&#32447;&#36172;&#21338;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39640;&#36798;11.88&#65285;&#30340;&#25913;&#36827;&#30456;&#23545;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current multi-armed bandit approaches in recommender systems (RS) have focused more on devising effective exploration techniques, while not adequately addressing common exploitation challenges related to distributional changes and item cannibalization. Little work exists to guide the design of robust bandit frameworks that can address these frequent challenges in RS. In this paper, we propose a new design principles to (i) make bandit models robust to time-variant metadata signals, (ii) less prone to item cannibalization, and (iii) prevent their weights fluctuating due to data sparsity. Through a series of experiments, we systematically examine the influence of several important bandit design choices. We demonstrate the advantage of our proposed design principles at making bandit models robust to dynamic behavioral changes through in-depth analyses. Noticeably, we show improved relative gain compared to a baseline bandit model not incorporating our design choices of up to $11.88\%$ and
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Gramian Angular Fields&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#24120;&#25193;&#25955;&#36712;&#36857;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#24577;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.01416</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#21644;&#24322;&#24120;&#25193;&#25955;&#36712;&#36857;&#30340;Gramian Angular Fields
&lt;/p&gt;
&lt;p&gt;
Gramian Angular Fields for leveraging pretrained computer vision models with anomalous diffusion trajectories. (arXiv:2310.01416v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01416
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Gramian Angular Fields&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#24120;&#25193;&#25955;&#36712;&#36857;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#24577;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#25193;&#25955;&#23384;&#22312;&#20110;&#20174;&#21407;&#23376;&#21040;&#22823;&#23610;&#24230;&#30340;&#25152;&#26377;&#23610;&#24230;&#19978;&#12290;&#19968;&#20123;&#20856;&#22411;&#30340;&#31995;&#32479;&#21253;&#25324;&#65306;&#36229;&#20919;&#21407;&#23376;&#12289;&#32454;&#32990;&#26680;&#20013;&#30340;&#31471;&#31890;&#12289;&#27700;&#20998;&#22312;&#27700;&#27877;&#22522;&#26448;&#26009;&#20013;&#30340;&#20256;&#36755;&#12289;&#33410;&#32930;&#21160;&#29289;&#30340;&#33258;&#30001;&#36816;&#21160;&#21644;&#40479;&#31867;&#30340;&#36801;&#24473;&#27169;&#24335;&#12290;&#23545;&#25193;&#25955;&#30340;&#34920;&#24449;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#25193;&#25955;&#20256;&#36755;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#28508;&#22312;&#30340;&#25193;&#25955;&#26426;&#21046;&#24182;&#20197;&#39640;&#32622;&#20449;&#24230;&#25512;&#26029;&#24322;&#24120;&#25193;&#25955;&#25351;&#25968;{$\alpha$}&#30340;&#38382;&#39064;&#23545;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#24577;&#23398;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24322;&#24120;&#25193;&#25955;&#25361;&#25112;&#36187;&#20013;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#20174;&#36712;&#36857;&#20013;&#25552;&#21462;&#30340;&#32479;&#35745;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#21407;&#22987;&#36712;&#36857;&#30340;&#20998;&#31867;&#21644;&#20998;&#26512;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#22788;&#29702;&#25193;&#25955;&#36712;&#36857;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Gramian Angular Fields (GAF)
&lt;/p&gt;
&lt;p&gt;
Anomalous diffusion is present at all scales, from atomic to large scales. Some exemplary systems are; ultra-cold atoms, telomeres in the nucleus of cells, moisture transport in cement-based materials, the free movement of arthropods, and the migration patterns of birds. The characterization of the diffusion gives critical information about the dynamics of these systems and provides an interdisciplinary framework with which to study diffusive transport. Thus, the problem of identifying underlying diffusive regimes and inferring the anomalous diffusion exponent {$\alpha$} with high confidence is critical to physics, chemistry, biology, and ecology. Classification and analysis of raw trajectories combining machine learning techniques with statistics extracted from them have widely been studied in the Anomalous Diffusion Challenge ge (Munoz-Gil et al., 2021). Here we present a new data-driven method for working with diffusive trajectories. This method utilizes Gramian Angular Fields (GAF)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.01307</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Training-based ChatGPT Detection Methods. (arXiv:2310.01307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#26377;&#36843;&#20999;&#30340;&#38656;&#27714;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#20013;&#26816;&#27979;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#19968;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#26469;&#21306;&#20998;&#20108;&#32773;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20063;&#34920;&#26126;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#21463;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#20219;&#21153;&#25110;&#20027;&#39064;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#26080;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#38754;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#22312;&#30001;&#22810;&#31181;&#22240;&#32032;&#24341;&#36215;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#21644;ChatGPT&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#23545;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.01259</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#25512;&#29702;&#23454;&#29616;&#26356;&#24555;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Faster and Accurate Neural Networks with Semantic Inference. (arXiv:2310.01259v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#19987;&#38376;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20869;&#22312;&#20887;&#20313;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36755;&#20837;&#20849;&#20139;&#35768;&#22810;&#28388;&#27874;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#26089;&#30340;&#23618;&#27425;&#19978;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#31867;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#21019;&#24314;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;SINF&#65288;i&#65289;&#20351;&#29992;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#23545;&#35937;&#23646;&#20110;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#24182;&#65288;ii&#65289;&#25191;&#34892;&#19982;&#35813;&#35821;&#20041;&#32858;&#31867;&#30456;&#20851;&#30340;&#22522;&#26412;DNN&#25552;&#21462;&#30340;&#23376;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#25552;&#21462;&#27599;&#20010;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21306;&#20998;&#33021;&#21147;&#24471;&#20998;&#65288;DCS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) usually come with a significant computational burden. While approaches such as structured pruning and mobile-specific DNNs have been proposed, they incur drastic accuracy loss. In this paper we leverage the intrinsic redundancy in latent representations to reduce the computational load with limited loss in performance. We show that semantically similar inputs share many filters, especially in the earlier layers. Thus, semantically similar classes can be clustered to create cluster-specific subgraphs. To this end, we propose a new framework called Semantic Inference (SINF). In short, SINF (i) identifies the semantic cluster the object belongs to using a small additional classifier and (ii) executes the subgraph extracted from the base DNN related to that semantic cluster for inference. To extract each cluster-specific subgraph, we propose a new approach named Discriminative Capability Score (DCS) that finds the subgraph with the capability to discriminate amon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#65288;GINs&#65289;&#36827;&#34892;&#20013;&#21387;&#30005;&#32593;&#30340;n-1&#35780;&#20272;&#65292;&#30456;&#27604;&#20256;&#32479;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#65292;GIN&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#24555;&#21644;&#26356;&#21487;&#38752;&#30340;&#30005;&#32593;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#26102;&#38388;&#32553;&#30701;&#32422;1000&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.01181</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#20013;&#21387;&#30005;&#32593;&#21487;&#38752;&#24615;&#30340;&#22270;&#21516;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Isomorphic Networks for Assessing Reliability of the Medium-Voltage Grid. (arXiv:2310.01181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#65288;GINs&#65289;&#36827;&#34892;&#20013;&#21387;&#30005;&#32593;&#30340;n-1&#35780;&#20272;&#65292;&#30456;&#27604;&#20256;&#32479;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#65292;GIN&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#24555;&#21644;&#26356;&#21487;&#38752;&#30340;&#30005;&#32593;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#26102;&#38388;&#32553;&#30701;&#32422;1000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#36716;&#21464;&#21644;&#20256;&#32479;&#23481;&#37327;&#30340;&#19979;&#38477;&#65292;&#30830;&#20445;&#30005;&#32593;&#30340;&#21487;&#38752;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#37197;&#30005;&#31995;&#32479;&#36816;&#33829;&#21830;&#65288;DSO&#65289;&#36890;&#36807;&#39564;&#35777;n-1&#21407;&#21017;&#26469;&#23454;&#29616;&#30005;&#32593;&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#30830;&#20445;&#32452;&#20214;&#25925;&#38556;&#26102;&#30340;&#36830;&#32493;&#36816;&#34892;&#12290;&#30005;&#21147;&#32593;&#32476;&#30340;&#22797;&#26434;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#21253;&#21547;&#20851;&#38190;&#30340;n-1&#35780;&#20272;&#20449;&#24687;&#65306;&#22270;&#32467;&#26500;&#21644;&#26377;&#20851;&#33410;&#28857;/&#30005;&#32518;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30452;&#25509;&#22788;&#29702;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#22312;&#20013;&#21387;&#30005;&#32593;&#20013;&#20351;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#65288;GINs&#65289;&#36827;&#34892;n-1&#35780;&#20272;&#12290;GIN&#26694;&#26550;&#30340;&#35774;&#35745;&#26159;&#20026;&#20102;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#30005;&#32593;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#26377;&#20851;&#33410;&#28857;/&#30005;&#32518;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;GIN&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#24555;&#21644;&#26356;&#21487;&#38752;&#30340;&#30005;&#32593;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#26102;&#38388;&#32553;&#30701;&#32422;1000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring electricity grid reliability becomes increasingly challenging with the shift towards renewable energy and declining conventional capacities. Distribution System Operators (DSOs) aim to achieve grid reliability by verifying the n-1 principle, ensuring continuous operation in case of component failure. Electricity networks' complex graph-based data holds crucial information for n-1 assessment: graph structure and data about stations/cables. Unlike traditional machine learning methods, Graph Neural Networks (GNNs) directly handle graph-structured data. This paper proposes using Graph Isomorphic Networks (GINs) for n-1 assessments in medium voltage grids. The GIN framework is designed to generalise to unseen grids and utilise graph structure and data about stations/cables. The proposed GIN approach demonstrates faster and more reliable grid assessments than a traditional mathematical optimisation approach, reducing prediction times by approximately a factor of 1000. The findings o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25512;&#23548;&#20219;&#24847;&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#27169;&#22411;&#30340;&#23574;&#38160;&#30340;&#38750;&#28176;&#36817;&#24615;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#22635;&#34917;&#20102;&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#20445;&#35777;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.00987</link><description>&lt;p&gt;
&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression. (arXiv:2310.00987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25512;&#23548;&#20219;&#24847;&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#27169;&#22411;&#30340;&#23574;&#38160;&#30340;&#38750;&#28176;&#36817;&#24615;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#22635;&#34917;&#20102;&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#20445;&#35777;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#20110;&#19968;&#33324;&#26680;&#22238;&#24402;&#27169;&#22411;&#30340;&#32479;&#35745;&#23398;&#23398;&#20064;&#20445;&#35777;&#22312;&#20351;&#29992;&#26377;&#38480;&#31209;&#26680;&#26102;&#24448;&#24448;&#20250;&#24471;&#21040;&#23485;&#26494;&#30340;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#22914;&#22312;&#25191;&#34892;&#36801;&#31227;&#23398;&#20064;&#26102;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#24494;&#35843;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#26102;&#65292;&#26377;&#38480;&#31209;&#26680;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#20219;&#24847;&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#27169;&#22411;&#30340;&#23574;&#38160;&#30340;&#38750;&#28176;&#36817;&#24615;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#22635;&#34917;&#20102;&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#20445;&#35777;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#36793;&#30028;&#27604;&#20043;&#21069;&#38024;&#23545;&#26377;&#38480;&#31209;&#26680;&#23725;&#22238;&#24402;&#27169;&#22411;&#25512;&#23548;&#30340;&#36793;&#30028;&#26356;&#32039;&#65292;&#24182;&#19988;&#19982;&#31867;&#20284;&#32467;&#26524;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#20204;&#20063;&#36866;&#29992;&#20110;&#20219;&#20309;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in several machine learning problems, e.g.\ when fine-tuning a pre-trained deep neural network's last layer to adapt it to a novel task when performing transfer learning. We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#30828;&#20214;&#36164;&#28304;&#20351;&#29992;&#30340;&#27010;&#29575;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#36335;&#24452;&#32467;&#26500;&#22810;&#37325;&#36793;&#26725;&#38382;&#39064;&#65292;&#21487;&#20197;&#39044;&#27979;&#25511;&#21046;&#36719;&#20214;&#30340;&#30828;&#20214;&#36164;&#28304;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#22312;&#20219;&#24847;&#26102;&#38388;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2310.00604</link><description>&lt;p&gt;
&#30828;&#20214;&#36164;&#28304;&#20351;&#29992;&#30340;&#27010;&#29575;&#23398;&#20064;&#20013;&#30340;&#36335;&#24452;&#32467;&#26500;&#22810;&#37325;&#36793;&#26725;
&lt;/p&gt;
&lt;p&gt;
Path Structured Multimarginal Schr\"odinger Bridge for Probabilistic Learning of Hardware Resource Usage by Control Software. (arXiv:2310.00604v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#30828;&#20214;&#36164;&#28304;&#20351;&#29992;&#30340;&#27010;&#29575;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#36335;&#24452;&#32467;&#26500;&#22810;&#37325;&#36793;&#26725;&#38382;&#39064;&#65292;&#21487;&#20197;&#39044;&#27979;&#25511;&#21046;&#36719;&#20214;&#30340;&#30828;&#20214;&#36164;&#28304;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#22312;&#20219;&#24847;&#26102;&#38388;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#32467;&#26500;&#22810;&#37325;&#36793;&#26725;&#38382;&#39064;&#65288;MSBP&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19982;&#35266;&#23519;&#21040;&#30340;&#27010;&#29575;&#27979;&#24230;&#24207;&#21015;&#25110;&#20998;&#24067;&#24555;&#29031;&#19968;&#33268;&#30340;&#26368;&#21487;&#33021;&#30340;&#27979;&#24230;&#20540;&#36712;&#36857;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#35299;&#20915;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;MSBP&#26041;&#38754;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#36719;&#20214;&#30340;&#38543;&#26426;&#30828;&#20214;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#39044;&#27979;&#25152;&#38656;&#26102;&#38388;&#30340;&#30828;&#20214;&#36164;&#28304;&#21487;&#29992;&#24615;&#30340;&#26102;&#21464;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36719;&#20214;&#25191;&#34892;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#36805;&#36895;&#25910;&#25947;&#21040;&#23545;&#25511;&#21046;&#22120;&#30828;&#20214;&#36164;&#28304;&#21033;&#29992;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#20219;&#20309;&#36719;&#20214;&#65292;&#20197;&#39044;&#27979;&#20219;&#24847;&#26102;&#38388;&#30340;&#32593;&#32476;&#29289;&#29702;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution of the path structured multimarginal Schr\"{o}dinger bridge problem (MSBP) is the most-likely measure-valued trajectory consistent with a sequence of observed probability measures or distributional snapshots. We leverage recent algorithmic advances in solving such structured MSBPs for learning stochastic hardware resource usage by control software. The solution enables predicting the time-varying distribution of hardware resource availability at a desired time with guaranteed linear convergence. We demonstrate the efficacy of our probabilistic learning approach in a model predictive control software execution case study. The method exhibits rapid convergence to an accurate prediction of hardware resource utilization of the controller. The method can be broadly applied to any software to predict cyber-physical context-dependent performance at arbitrary time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.00535</link><description>&lt;p&gt;
JoMA: &#36890;&#36807;MLP&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#21160;&#21147;&#23398;&#26469;&#35299;&#23494;&#22810;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;Transformer&#20013;&#21435;&#38500;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#24471;&#21040;&#20165;&#21253;&#21547;MLP&#23618;&#30340;&#20462;&#25913;&#21518;&#21160;&#24577;&#12290;JoMA&#28040;&#38500;&#20102;&#20808;&#21069;&#20998;&#26512;&#20013;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#32570;&#20047;&#27531;&#24046;&#36830;&#25509;&#65289;&#65292;&#24182;&#39044;&#27979;&#27880;&#24847;&#21147;&#22312;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#24773;&#20917;&#19979;&#39318;&#20808;&#21464;&#24471;&#31232;&#30095;&#65288;&#20026;&#20102;&#23398;&#20064;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#28982;&#21518;&#21464;&#24471;&#23494;&#38598;&#65288;&#20026;&#20102;&#23398;&#20064;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#32780;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#23427;&#19982;&#29616;&#26377;&#30740;&#31350;&#19968;&#33268;&#65292;&#26174;&#31034;&#20986;&#27880;&#24847;&#21147;&#38543;&#26102;&#38388;&#21464;&#24471;&#31232;&#30095;&#12290;&#25105;&#20204;&#21033;&#29992;JoMA&#23450;&#24615;&#22320;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#22914;&#20309;&#23558;&#26631;&#35760;&#32452;&#21512;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#24403;&#36755;&#20837;&#26631;&#35760;&#26159;&#30001;&#28508;&#22312;&#30340;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#22312;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;Wikitext2/Wikitext103&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;OPT&#65292;Pythia&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.00429</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#22312;&#20854;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#24448;&#24448;&#23637;&#29616;&#20986;&#36229;&#36807;&#20856;&#22411;&#20154;&#31867;&#33021;&#21147;&#30340;&#26679;&#26412;&#30495;&#23454;&#24615;&#36776;&#21035;&#33021;&#21147;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#26080;&#30097;&#26159;&#36825;&#20123;&#27169;&#22411;&#28040;&#32791;&#28023;&#37327;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#24778;&#20154;&#30340;&#24615;&#33021;&#21644;&#26131;&#24471;&#24615;&#65292;&#32593;&#32476;&#19978;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#20869;&#23481;&#12290;&#36825;&#20010;&#20107;&#23454;&#30452;&#25509;&#24847;&#21619;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#26410;&#26469;&#36845;&#20195;&#24517;&#39035;&#38754;&#23545;&#19968;&#20010;&#29616;&#23454;&#65306;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#30001;&#28165;&#27905;&#25968;&#25454;&#21644;&#20808;&#21069;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#32452;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#22909;&#22320;&#36817;&#20284;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#30495;&#23454;&#25968;&#25454;&#19982;&#21512;&#25104;&#25968;&#25454;&#30340;&#27604;&#20363;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00339</link><description>&lt;p&gt;
FedLPA: &#20351;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#32858;&#21512;&#21040;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#20943;&#23569;&#12289;&#28508;&#22312;&#25915;&#20987;&#20943;&#24369;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#30340;&#25512;&#21160;&#65292;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#23558;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#38388;&#30340;&#36890;&#20449;&#38480;&#21046;&#20026;&#19968;&#36718;&#65289;&#22312;&#30740;&#31350;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#21333;&#27425;&#32858;&#21512;&#30340;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#65288;FedLPA&#65289;&#12290;FedLPA&#33021;&#22815;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#25110;&#26292;&#38706;&#20219;&#20309;&#26426;&#23494;&#30340;&#26412;&#22320;&#20449;&#24687;&#65292;&#27604;&#22914;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
&lt;/p&gt;</description></item><item><title>causalimages R &#21253;&#25552;&#20379;&#20102;&#20351;&#29992;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#24037;&#20855;&#65292;&#24182;&#33021;&#22815;&#25972;&#21512;&#21355;&#26143;&#21644;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31561;&#26032;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#20998;&#35299;&#27835;&#30103;&#25928;&#26524;&#30340;&#24322;&#36136;&#24615;&#21644;&#25511;&#21046;&#28151;&#28102;&#21464;&#37327;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.00233</link><description>&lt;p&gt;
CausalImages: &#19968;&#20010;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#22270;&#20687;&#30340;&#22240;&#26524;&#25512;&#26029;&#30340; R &#21253;
&lt;/p&gt;
&lt;p&gt;
CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images. (arXiv:2310.00233v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00233
&lt;/p&gt;
&lt;p&gt;
causalimages R &#21253;&#25552;&#20379;&#20102;&#20351;&#29992;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#24037;&#20855;&#65292;&#24182;&#33021;&#22815;&#25972;&#21512;&#21355;&#26143;&#21644;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31561;&#26032;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#20998;&#35299;&#27835;&#30103;&#25928;&#26524;&#30340;&#24322;&#36136;&#24615;&#21644;&#25511;&#21046;&#28151;&#28102;&#21464;&#37327;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
causalimages R &#21253;&#33021;&#22815;&#36890;&#36807;&#22270;&#20687;&#21644;&#22270;&#20687;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20026;&#23558;&#21355;&#26143;&#21644;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31561;&#26032;&#25968;&#25454;&#28304;&#25972;&#21512;&#21040;&#22240;&#26524;&#20851;&#31995;&#30740;&#31350;&#20013;&#25552;&#20379;&#26032;&#24037;&#20855;&#12290;&#19968;&#32452;&#20989;&#25968;&#21487;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#22240;&#26524;&#25512;&#26029;&#20998;&#26512;&#12290;&#20363;&#22914;&#65292;&#19968;&#31181;&#20851;&#38190;&#20989;&#25968;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#36890;&#36807;&#22270;&#20687;&#26469;&#20998;&#35299;&#27835;&#30103;&#25928;&#26524;&#30340;&#24322;&#36136;&#24615;&#12290;&#36825;&#26679;&#21487;&#20197;&#30830;&#23450;&#21738;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#25110;&#22270;&#20687;&#24207;&#21015;&#23545;&#24178;&#39044;&#26368;&#25935;&#24863;&#12290;&#31532;&#20108;&#20010;&#24314;&#27169;&#20989;&#25968;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#22270;&#20687;&#26469;&#25511;&#21046;&#28151;&#28102;&#21464;&#37327;&#12290;&#35813;&#21253;&#36824;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#29983;&#25104;&#20316;&#20026;&#22270;&#20687;&#25110;&#35270;&#39057;&#20869;&#23481;&#21521;&#37327;&#25688;&#35201;&#30340;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#22522;&#30784;&#32467;&#26500;&#20989;&#25968;&#65292;&#20363;&#22914;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#22270;&#20687;&#24207;&#21015;&#25968;&#25454;&#20889;&#20837;&#24207;&#21015;&#21270;&#23383;&#33410;&#23383;&#31526;&#20018;&#20197;&#36827;&#34892;&#26356;&#24555;&#36895;&#30340;&#22270;&#20687;&#20998;&#26512;&#30340;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;causalimages &#22312; R &#20013;&#24320;&#21551;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#33021;&#21147;&#65292;&#35753;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The causalimages R package enables causal inference with image and image sequence data, providing new tools for integrating novel data sources like satellite and bio-medical imagery into the study of cause and effect. One set of functions enables image-based causal inference analyses. For example, one key function decomposes treatment effect heterogeneity by images using an interpretable Bayesian framework. This allows for determining which types of images or image sequences are most responsive to interventions. A second modeling function allows researchers to control for confounding using images. The package also allows investigators to produce embeddings that serve as vector summaries of the image or video content. Finally, infrastructural functions are also provided, such as tools for writing large-scale image and image sequence data as sequentialized byte strings for more rapid image analysis. causalimages therefore opens new capabilities for causal inference in R, letting research
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16971</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Active Learning of Fourier Neural Operators. (arXiv:2309.16971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;MRA-FNO&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#26469;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#24182;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20165;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#38754;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;FNO&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#29289;&#29702;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20998;&#36776;&#29575;&#20027;&#21160;&#23398;&#20064;&#30340;FNO&#65288;MRA-FNO&#65289;&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#20989;&#25968;&#21644;&#20998;&#36776;&#29575;&#65292;&#23613;&#37327;&#38477;&#20302;&#25968;&#25454;&#25104;&#26412;&#65292;&#21516;&#26102;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#22810;&#20998;&#36776;&#29575;FNO&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21518;&#39564;&#25512;&#29702;&#31639;&#27861;&#12290;&#20026;&#20102;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#25928;&#29992;&#25104;&#26412;&#27604;&#20316;&#20026;&#33719;&#21462;&#20989;&#25968;&#65292;&#22312;&#27599;&#19968;&#27493;&#33719;&#21462;&#26032;&#30340;&#26679;&#26412;&#21644;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#30697;&#21305;&#37197;&#21644;&#30697;&#38453;&#34892;&#21015;&#24335;&#24341;&#29702;&#23454;&#29616;&#20102;&#21487;&#34892;&#65292;&#39640;&#25928;&#30340;&#25928;&#29992;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a
&lt;/p&gt;</description></item><item><title>ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.16916</link><description>&lt;p&gt;
ONNXExplainer:&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16916
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#20570;&#20986;&#26576;&#20123;&#20915;&#31574;&#19982;&#25512;&#29702;&#24615;&#33021;&#19968;&#26679;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#24110;&#21161;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20854;&#20013;Shapley&#20540;&#26368;&#21463;&#27426;&#36814;&#12290;SHAP&#21253;&#26159;&#35299;&#37322;&#20351;&#29992;TensorFlow&#25110;PyTorch&#23454;&#29616;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;Shapley&#20540;&#30340;&#39046;&#20808;&#23454;&#29616;&#65292;&#20294;&#32570;&#20047;&#36328;&#24179;&#21488;&#25903;&#25345;&#12289;&#19968;&#27425;&#24615;&#37096;&#32626;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ONNXExplainer&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ONNX&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;Shapley&#20540;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;ONNXExplainer&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#20165;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#19968;&#27425;&#24615;&#37096;&#32626;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;&#20026;&#20102;&#20844;&#24179;&#27604;&#36739;&#30446;&#30340;&#65292;&#25105;&#20204;&#36824;&#22312;TensorFlow&#21644;PyTorch&#20013;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#31639;&#27861;&#35748;&#35777;&#32473;&#23450;&#31574;&#30053;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>http://arxiv.org/abs/2309.16631</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; - &#35748;&#35777;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Robust Offline Reinforcement Learning -- Certify the Confidence Interval. (arXiv:2309.16631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#31639;&#27861;&#35748;&#35777;&#32473;&#23450;&#31574;&#30053;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;RL&#65292;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25915;&#20987;&#26041;&#24335;&#21464;&#24471;&#25104;&#29087;&#65292;RL&#30340;&#23433;&#20840;&#24615;&#25104;&#20026;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25269;&#24481;&#27492;&#31867;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#12289;&#25968;&#25454;&#36807;&#28388;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;&#32463;&#39564;&#31639;&#27861;&#21644;&#23454;&#39564;&#65292;&#32570;&#20047;&#23545;&#31639;&#27861;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#32473;&#23450;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#25928;&#29575;&#21487;&#20197;&#35777;&#26126;&#21644;&#36827;&#34892;&#65292;&#19982;&#27809;&#26377;&#38543;&#26426;&#24179;&#28369;&#30340;&#31639;&#27861;&#19968;&#26679;&#39640;&#25928;&#12290;&#19981;&#21516;&#29615;&#22659;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, reinforcement learning (RL), especially deep RL, has received more and more attention in the research area. However, the security of RL has been an obvious problem due to the attack manners becoming mature. In order to defend against such adversarial attacks, several practical approaches are developed, such as adversarial training, data filtering, etc. However, these methods are mostly based on empirical algorithms and experiments, without rigorous theoretical analysis of the robustness of the algorithms. In this paper, we develop an algorithm to certify the robustness of a given policy offline with random smoothing, which could be proven and conducted as efficiently as ones without random smoothing. Experiments on different environments confirm the correctness of our algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;BPFL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23458;&#25143;&#31471;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#20998;&#35299;&#21644;&#20849;&#21516;&#23398;&#20064;&#32479;&#35745;&#24322;&#36136;&#24615;&#23458;&#25143;&#31471;&#25968;&#25454;&#19978;&#30340;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.15499</link><description>&lt;p&gt;
&#20855;&#26377;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#30340;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations. (arXiv:2309.15499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;BPFL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23458;&#25143;&#31471;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#20998;&#35299;&#21644;&#20849;&#21516;&#23398;&#20064;&#32479;&#35745;&#24322;&#36136;&#24615;&#23458;&#25143;&#31471;&#25968;&#25454;&#19978;&#30340;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;BPFL&#65289;&#35299;&#20915;&#20102;&#29616;&#26377;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;BPFL&#26088;&#22312;&#36890;&#36807;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#26469;&#37327;&#21270;&#23458;&#25143;&#31471;&#20869;&#37096;&#21644;&#36328;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#22312;PFL&#20013;&#65292;&#26368;&#36817;&#19968;&#20123;&#21021;&#27493;&#24037;&#20316;&#25552;&#20986;&#23558;&#38544;&#34255;&#30340;&#31070;&#32463;&#34920;&#31034;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#23616;&#37096;&#32452;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#27809;&#26377;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23458;&#25143;&#31471;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#36866;&#24403;&#22320;&#35299;&#32806;&#31070;&#32463;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#24120;&#24120;&#26159;&#20020;&#26102;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24341;&#20837;&#19968;&#20010;&#36890;&#29992;&#30340;BPFL&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#20849;&#21516;&#23398;&#20064;&#32479;&#35745;&#24322;&#36136;&#24615;&#23458;&#25143;&#31471;&#25968;&#25454;&#19978;&#30340;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian personalized federated learning (BPFL) addresses challenges in existing personalized FL (PFL). BPFL aims to quantify the uncertainty and heterogeneity within and across clients towards uncertainty representations by addressing the statistical heterogeneity of client data. In PFL, some recent preliminary work proposes to decompose hidden neural representations into shared and local components and demonstrates interesting results. However, most of them do not address client uncertainty and heterogeneity in FL systems, while appropriately decoupling neural representations is challenging and often ad hoc. In this paper, we make the first attempt to introduce a general BPFL framework to decompose and jointly learn shared and personalized uncertainty representations on statistically heterogeneous client data over time. A Bayesian federated neural network BPFed instantiates BPFL by jointly learning cross-client shared uncertainty and client-specific personalized uncertainty over stat
&lt;/p&gt;</description></item><item><title>SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15289</link><description>&lt;p&gt;
SEPT: &#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15289
&lt;/p&gt;
&lt;p&gt;
SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#22797;&#26434;&#20132;&#36890;&#29615;&#22659;&#20013;&#23433;&#20840;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#21462;&#20132;&#36890;&#20803;&#32032;&#20043;&#38388;&#30340;&#26377;&#25928;&#26102;&#31354;&#20851;&#31995;&#26159;&#20934;&#30830;&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#21463;&#21040;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;SEPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#24320;&#21457;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#24378;&#22823;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#22312;&#22330;&#26223;&#36755;&#20837;&#19978;&#36827;&#34892;&#19977;&#20010;&#25513;&#30721;&#37325;&#26500;&#24314;&#27169;&#20219;&#21153;&#65292;&#21253;&#25324;&#20195;&#29702;&#36335;&#24452;&#21644;&#36947;&#36335;&#32593;&#32476;&#65292;&#39044;&#35757;&#32451;&#22330;&#26223;&#32534;&#30721;&#22120;&#20197;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#65292;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#28982;&#21518;&#22312;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SEPT&#22312;Argoverse 1&#21644;Argoverse&#19978;&#26080;&#38656;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#25110;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;RuBERT&#27169;&#22411;&#21644;Transformer&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#21672;&#35810;&#30340;&#29992;&#25143;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19987;&#23478;&#29305;&#38271;&#65292;&#34920;&#29616;&#20986;&#36229;&#36807;92%&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.14662</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21307;&#23398;&#21672;&#35810;&#29992;&#25143;&#26597;&#35810;&#20998;&#31867;&#19982;&#19987;&#23478;&#29305;&#38271;&#30456;&#20851;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation. (arXiv:2309.14662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;RuBERT&#27169;&#22411;&#21644;Transformer&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#21672;&#35810;&#30340;&#29992;&#25143;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19987;&#23478;&#29305;&#38271;&#65292;&#34920;&#29616;&#20986;&#36229;&#36807;92%&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21307;&#30103;&#26102;&#20195;&#65292;&#23545;&#20110;&#29087;&#32451;&#30340;&#21307;&#30103;&#25903;&#25345;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;RuBERT&#27169;&#22411;&#65292;&#23558;&#21307;&#23398;&#21672;&#35810;&#39046;&#22495;&#30340;&#29992;&#25143;&#26597;&#35810;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#30528;&#37325;&#20851;&#27880;&#19987;&#23478;&#30340;&#29305;&#38271;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;RuBERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#26597;&#35810;&#19982;&#29305;&#23450;&#21307;&#23398;&#19987;&#38271;&#20043;&#38388;&#30340;&#31934;&#30830;&#23545;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20132;&#21449;&#39564;&#35777;&#21644;&#20256;&#32479;&#30340;&#27979;&#35797;&#21644;&#35757;&#32451;&#38598;&#21010;&#20998;&#19979;&#22343;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;F1&#24471;&#20998;&#36229;&#36807;92%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24515;&#33039;&#30149;&#23398;&#12289;&#31070;&#32463;&#30149;&#23398;&#21644;&#30382;&#32932;&#31185;&#31561;&#21307;&#23398;&#39046;&#22495;&#30340;&#27867;&#21270;&#24615;&#33021;&#20063;&#38750;&#24120;&#20986;&#33394;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#38469;&#30410;&#22788;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#24341;&#23548;&#33267;&#36866;&#24403;&#30340;&#19987;&#23478;&#20197;&#33719;&#24471;&#21450;&#26102;&#32780;&#26377;&#38024;&#23545;&#24615;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23427;&#36824;&#25552;&#39640;&#20102;&#21307;&#30103;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#20174;&#19994;&#32773;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for skilled medical support is growing in the era of digital healthcare. This research presents an innovative strategy, utilising the RuBERT model, for categorising user inquiries in the field of medical consultation with a focus on expert specialisation. By harnessing the capabilities of transformers, we fine-tuned the pre-trained RuBERT model on a varied dataset, which facilitates precise correspondence between queries and particular medical specialisms. Using a comprehensive dataset, we have demonstrated our approach's superior performance with an F1-score of over 92%, calculated through both cross-validation and the traditional split of test and train datasets. Our approach has shown excellent generalisation across medical domains such as cardiology, neurology and dermatology. This methodology provides practical benefits by directing users to appropriate specialists for prompt and targeted medical advice. It also enhances healthcare system efficiency, reduces practitioner 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;</title><link>http://arxiv.org/abs/2309.14610</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#28304;&#20110;&#19982;&#27946;&#27700;&#21361;&#38505;&#12289;&#27946;&#27700;&#26292;&#38706;&#20197;&#21450;&#31038;&#20250;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#27946;&#27700;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#34920;&#24449;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#27946;&#27700;&#24179;&#21407;&#22320;&#22270;&#65292;&#20391;&#37325;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#20027;&#35201;&#26159;&#21361;&#38505;&#21644;&#26292;&#38706;&#35201;&#32032;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#31354;&#38388;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;FloodRisk-Net&#65289;&#30340;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#12290;FloodRisk-Net&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#27946;&#27700;&#21361;&#38505;&#21644;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#30830;&#23450;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#12290;&#21033;&#29992;&#32654;&#22269;&#22810;&#20010;&#37117;&#24066;&#32479;&#35745;&#21306;&#65288;MSAs&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#27946;&#27700;&#39118;&#38505;&#29305;&#24449;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#31639;&#27861;&#65292;&#33021;&#22815;&#21152;&#36895;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#38477;&#20302;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12252</link><description>&lt;p&gt;
&#22312;&#24207;&#21015;&#38271;&#24230;&#19978;&#24182;&#34892;&#21270;&#38750;&#32447;&#24615;&#39034;&#24207;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#31639;&#27861;&#65292;&#33021;&#22815;&#21152;&#36895;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#38477;&#20302;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#27169;&#22411;&#65292;&#20363;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19968;&#30452;&#30001;&#20110;&#20854;&#26412;&#36136;&#19978;&#30340;&#39034;&#24207;&#29305;&#24615;&#32780;&#23384;&#22312;&#35757;&#32451;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#22810;&#24180;&#26469;&#36825;&#20010;&#29942;&#39048;&#19968;&#30452;&#23384;&#22312;&#65292;&#22240;&#20026;&#24456;&#22810;&#20154;&#35748;&#20026;&#39034;&#24207;&#27169;&#22411;&#26080;&#27861;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24182;&#34892;&#31639;&#27861;&#25361;&#25112;&#20102;&#36825;&#20010;&#38271;&#26399;&#20197;&#26469;&#30340;&#20449;&#24565;&#65292;&#21152;&#36895;&#20102;GPU&#23545;&#20110;&#39034;&#24207;&#27169;&#22411;&#30340;&#35780;&#20272;&#36895;&#24230;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;3&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#29306;&#29298;&#36755;&#20986;&#20934;&#30830;&#24615;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#39034;&#24207;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#20219;&#20309;&#29305;&#27530;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#39034;&#24207;&#27169;&#22411;&#21487;&#20197;&#27604;&#24120;&#35268;&#30340;&#39034;&#24207;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#65292;&#32780;&#35757;&#32451;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;&#20511;&#21161;&#36825;&#31181;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;17k&#20010;&#26102;&#38388;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#21457;&#29616;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20811;&#26381;&#35757;&#32451;&#29942;&#39048;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20351;&#24471;&#39034;&#24207;&#27169;&#22411;&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.08835</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#65292;&#26234;&#33021;&#26426;&#22120;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
Intelligent machines work in unstructured environments by differential neural computing. (arXiv:2309.08835v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#26234;&#33021;&#26426;&#22120;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#25928;&#22320;&#24037;&#20316;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#22320;&#29702;&#35299;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26080;&#32467;&#26500;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#38459;&#24615;&#31070;&#32463;&#35745;&#31639;&#30340;&#24863;&#30693;&#20449;&#21495;&#24046;&#20998;&#22788;&#29702;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#20851;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#33719;&#24471;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#22914;&#26426;&#26800;&#21050;&#28608;&#30340;&#25918;&#22823;&#65288;&gt;720%&#65289;&#21644;&#36866;&#24212;&#65288;&lt;50%&#65289;&#12290;&#35813;&#26041;&#27861;&#36824;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#26234;&#33021;&#26426;&#22120;&#30340;&#20004;&#20010;&#20856;&#22411;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65306;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#22312;&#29289;&#20307;&#25235;&#21462;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;1&#27627;&#31186;&#20869;&#20351;&#29992;&#21333;&#20010;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#23398;&#20064;&#26410;&#30693;&#29289;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#23574;&#38160;&#30340;&#35282;&#21644;&#20809;&#28369;&#30340;&#34920;&#38754;&#65289;&#65292;&#19968;&#20010;&#26426;&#22120;&#25163;&#23454;&#29616;&#20102;&#23433;&#20840;&#31283;&#23450;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expecting intelligent machines to efficiently work in real world requires a new method to understand unstructured information in unknown environments with good accuracy, scalability and generalization, like human. Here, a memristive neural computing based perceptual signal differential processing and learning method for intelligent machines is presented, via extracting main features of environmental information and applying associated encoded stimuli to memristors, we successfully obtain human-like ability in processing unstructured environmental information, such as amplification (&gt;720%) and adaptation (&lt;50%) of mechanical stimuli. The method also exhibits good scalability and generalization, validated in two typical applications of intelligent machines: object grasping and autonomous driving. In the former, a robot hand experimentally realizes safe and stable grasping, through learning unknown object features (e.g., sharp corner and smooth surface) with a single memristor in 1 ms. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22349;&#22604;&#29616;&#35937;&#36827;&#34892;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#33021;&#22815;&#20419;&#20351;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.04644</link><description>&lt;p&gt;
&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#65306;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. (arXiv:2309.04644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22349;&#22604;&#29616;&#35937;&#36827;&#34892;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#33021;&#22815;&#20419;&#20351;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#26159;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26368;&#21518;&#19968;&#23618;&#20013;&#20986;&#29616;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#25351;&#30340;&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32456;&#31471;&#38454;&#27573;&#65292;1&#65289;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#36235;&#21521;&#20110;&#38646;&#65292;2&#65289;&#31867;&#29305;&#24449;&#22343;&#20540;&#26500;&#25104;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;3&#65289;&#26368;&#21518;&#19968;&#23618;&#31867;&#29305;&#24449;&#21644;&#26435;&#37325;&#22312;&#32553;&#25918;&#19978;&#30456;&#31561;&#65292;4&#65289;&#20998;&#31867;&#34892;&#20026;&#23849;&#28291;&#21040;&#26368;&#36817;&#30340;&#31867;&#20013;&#24515;&#65288;NCC&#65289;&#20915;&#31574;&#35268;&#21017;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#30452;&#35266;&#30340;&#31867;&#20869;&#21644;&#31867;&#38388;&#20313;&#24358;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#25429;&#25417;&#20102;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#22810;&#20010;&#26680;&#24515;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#65292;&#25105;&#20204;&#22312;&#27491;&#21017;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#19979;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments
&lt;/p&gt;</description></item><item><title>Loss-Controlled Asymmetric Momentum (LCAM) is proposed as a simple and versatile optimization method that can adapt to all types of datasets by dividing the training process into different loss phases and using different momentum. Experimental results suggest that frequently-changing parameters should be accelerated in non-sparse gradients, challenging the conventional wisdom.</title><link>http://arxiv.org/abs/2309.02130</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#21160;&#37327;&#65306;&#23545;&#26799;&#24230;&#19979;&#38477;&#30340;&#37325;&#26032;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Asymmetric Momentum: A Rethinking of Gradient Descent. (arXiv:2309.02130v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02130
&lt;/p&gt;
&lt;p&gt;
Loss-Controlled Asymmetric Momentum (LCAM) is proposed as a simple and versatile optimization method that can adapt to all types of datasets by dividing the training process into different loss phases and using different momentum. Experimental results suggest that frequently-changing parameters should be accelerated in non-sparse gradients, challenging the conventional wisdom.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;Adam&#31561;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#31616;&#21333;&#30340;SGD&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;Loss-Controlled Asymmetric Momentum&#65288;LCAM&#65289;&#12290;&#36890;&#36807;&#23545;&#25439;&#22833;&#36827;&#34892;&#24179;&#22343;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;&#19981;&#21516;&#30340;&#25439;&#22833;&#38454;&#27573;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#21160;&#37327;&#12290;&#23427;&#19981;&#20165;&#21487;&#20197;&#21152;&#36895;&#31232;&#30095;&#26799;&#24230;&#19979;&#30340;&#32531;&#24930;&#21464;&#21270;&#21442;&#25968;&#65292;&#31867;&#20284;&#20110;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65292;&#36824;&#21487;&#20197;&#36873;&#25321;&#22312;&#38750;&#31232;&#30095;&#26799;&#24230;&#19979;&#21152;&#36895;&#39057;&#32321;&#21464;&#21270;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36866;&#24212;&#25152;&#26377;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#26435;&#37325;&#32806;&#21512;&#21644;&#26435;&#37325;&#29301;&#24341;&#30340;&#27010;&#24565;&#23545;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#37325;&#26032;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#26435;&#37325;&#20855;&#26377;&#26041;&#21521;&#29305;&#24322;&#24615;&#65292;&#19982;&#25968;&#25454;&#38598;&#30340;&#29305;&#24322;&#24615;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#38750;&#31232;&#30095;&#26799;&#24230;&#24773;&#20917;&#19979;&#65292;&#23454;&#38469;&#19978;&#24212;&#35813;&#21152;&#36895;&#39057;&#32321;&#21464;&#21270;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through theoretical and experimental validation, unlike all existing adaptive methods like Adam which penalize frequently-changing parameters and are only applicable to sparse gradients, we propose the simplest SGD enhanced method, Loss-Controlled Asymmetric Momentum(LCAM). By averaging the loss, we divide training process into different loss phases and using different momentum. It not only can accelerates slow-changing parameters for sparse gradients, similar to adaptive optimizers, but also can choose to accelerates frequently-changing parameters for non-sparse gradients, thus being adaptable to all types of datasets. We reinterpret the machine learning training process through the concepts of weight coupling and weight traction, and experimentally validate that weights have directional specificity, which are correlated with the specificity of the dataset. Thus interestingly, we observe that in non-sparse gradients, frequently-changing parameters should actually be accelerated, which
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;SSTR&#65289;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#35789;&#27719;&#26469;&#21435;&#38500;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#30446;&#26631;&#35789;&#27719;&#30340;&#21435;&#38500;&#12290;</title><link>http://arxiv.org/abs/2309.00410</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Selective Scene Text Removal. (arXiv:2309.00410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;SSTR&#65289;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#35789;&#27719;&#26469;&#21435;&#38500;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#30446;&#26631;&#35789;&#27719;&#30340;&#21435;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;Scene text removal&#65292;STR&#65289;&#26159;&#19968;&#31181;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#29992;&#20110;&#21435;&#38500;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21306;&#22495;&#12290;&#20256;&#32479;&#30340;STR&#26041;&#27861;&#20250;&#21024;&#38500;&#25152;&#26377;&#30340;&#22330;&#26223;&#25991;&#26412;&#12290;&#36825;&#24847;&#21619;&#30528;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;Selective scene text removal&#65292;SSTR&#65289;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21482;&#21024;&#38500;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#35789;&#27719;&#12290;&#34429;&#28982;SSTR&#27604;STR&#26356;&#22797;&#26434;&#65292;&#20294;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#22359;&#32467;&#26500;&#20351;&#24471;SSTR&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22914;&#39044;&#26399;&#22320;&#21435;&#38500;&#30446;&#26631;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene text removal (STR) is the image transformation task to remove text regions in scene images. The conventional STR methods remove all scene text. This means that the existing methods cannot select text to be removed. In this paper, we propose a novel task setting named selective scene text removal (SSTR) that removes only target words specified by the user. Although SSTR is a more complex task than STR, the proposed multi-module structure enables efficient training for SSTR. Experimental results show that the proposed method can remove target words as expected.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#20316;&#20026;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25216;&#26415;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#21644;&#37492;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.12899</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;[&#23454;&#39564;&#65292;&#20998;&#26512;&#21644;&#22522;&#20934;]&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]. (arXiv:2308.12899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#20316;&#20026;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25216;&#26415;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#21644;&#37492;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#19981;&#21516;&#26469;&#28304;&#21644;&#20197;&#19981;&#21516;&#26684;&#24335;&#23384;&#20648;&#30340;&#22810;&#26679;&#21270;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#35775;&#38382;&#21644;&#21033;&#29992;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22823;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#26377;&#25928;&#30340;&#27169;&#22411;&#32467;&#26500;&#21644;&#32452;&#20214;&#20063;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#22823;&#25968;&#25454;&#35774;&#35745;&#30340;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#24182;&#22312;40&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#31649;&#29702;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#25351;&#23548;&#20102;&#24378;&#22823;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#24182;&#30830;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.11958</link><description>&lt;p&gt;
&#36890;&#36807;&#20877;&#29983;&#24615;&#27491;&#21017;&#21270;&#32500;&#25345;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#21487;&#22609;&#24615;&#25351;&#30340;&#26159;&#20195;&#29702;&#24555;&#36895;&#36866;&#24212;&#26032;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#24050;&#30693;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#20250;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#21487;&#22609;&#24615;&#12290;&#36825;&#19982;&#26631;&#20934;&#30340;L2&#27491;&#21017;&#21270;&#38750;&#24120;&#30456;&#20284;&#65292;&#21807;&#19968;&#30340;&#21306;&#21035;&#22312;&#20110;L2 Init&#27491;&#21017;&#21270;&#26397;&#21521;&#21407;&#28857;&#12290;L2 Init&#26131;&#20110;&#23454;&#26045;&#65292;&#21482;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#36229;&#21442;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#21160;&#26426;&#19982;&#37325;&#32622;&#31070;&#32463;&#20803;&#25110;&#21442;&#25968;&#20540;&#30340;&#26041;&#27861;&#30456;&#21516;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#24403;&#26368;&#36817;&#30340;&#25439;&#22833;&#23545;&#29305;&#23450;&#21442;&#25968;&#19981;&#25935;&#24863;&#26102;&#65292;&#36825;&#20123;&#21442;&#25968;&#20250;&#21521;&#23427;&#20204;&#30340;&#21021;&#22987;&#20540;&#28418;&#31227;&#12290;&#36825;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#22312;&#20195;&#34920;&#36830;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#31867;&#22411;&#38750;&#24179;&#31283;&#24615;&#30340;&#31616;&#21333;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;L2 Init&#33021;&#22815;&#19968;&#33268;&#22320;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11905</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#25509;&#21463;&#36793;&#30028;&#36827;&#34892;&#21551;&#21457;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#21069;&#21521;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#24212;&#35813;&#23398;&#20064;&#30340;&#20869;&#23481;&#12289;&#22914;&#20309;&#35757;&#32451;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#26679;&#20570;&#30340;&#29702;&#35770;&#35748;&#35782;&#36824;&#24456;&#23569;&#12290;&#36825;&#31181;&#29702;&#35299;&#30340;&#19981;&#36275;&#23548;&#33268;&#25991;&#29486;&#20013;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65288;&#27425;&#20248;&#25104;&#26412;&#23545;&#26368;&#20248;&#25104;&#26412;&#25110;&#21487;&#25509;&#21463;&#23545;&#19981;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#65289;&#21644;&#20248;&#21270;&#25351;&#26631;&#65288;&#20363;&#22914;&#24179;&#26041;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#65289;&#26102;&#36827;&#34892;&#20102;&#20020;&#26102;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25152;&#24471;&#21040;&#30340;&#35757;&#32451;&#21551;&#21457;&#24335;&#20989;&#25968;&#32570;&#20047;&#21487;&#25509;&#21463;&#24615;&#65292;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25509;&#21463;&#24615;&#30340;&#37325;&#35201;&#24615;&#20063;&#32570;&#20047;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#30456;&#27604;&#26222;&#36890;&#39640;&#26031;&#20998;&#24067;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#25968;&#23398;&#27169;&#22411;&#24544;&#23454;&#22320;&#36981;&#24490;&#20102;&#26368;&#22823;&#29109;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2308.07505</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#20998;&#26512;&#21644;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31243;&#24207;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#23494;&#38598;&#22411;&#25163;&#21160;&#24037;&#20855;&#30340;&#21019;&#24314;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DRB-ML&#30340;&#19987;&#29992;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28304;&#33258;DataRaceBench&#65292;&#24182;&#20855;&#26377;&#31934;&#32454;&#30340;&#26631;&#31614;&#65292;&#26174;&#31034;&#20102;&#25968;&#25454;&#31454;&#20105;&#23545;&#21450;&#20854;&#30456;&#20851;&#21464;&#37327;&#12289;&#34892;&#21495;&#21644;&#35835;/&#20889;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;DRB-ML&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;LLMs&#24182;&#24494;&#35843;&#20102;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#38656;&#35201;&#26377;&#20851;&#24341;&#36215;&#25968;&#25454;&#31454;&#20105;&#30340;&#21464;&#37327;&#23545;&#30340;&#35814;&#32454;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#20173;&#26080;&#27861;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#20854;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#23545;&#27599;&#20010;&#31867;&#21035;&#30340;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.04603</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#65306;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Image Watermarking: A Brief Survey. (arXiv:2308.04603v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#20854;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#23545;&#27599;&#20010;&#31867;&#21035;&#30340;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#26159;&#25351;&#22312;&#19968;&#24352;&#23553;&#38754;&#22270;&#20687;&#20013;&#31192;&#23494;&#23884;&#20837;&#21644;&#25552;&#21462;&#27700;&#21360;&#20197;&#20445;&#25252;&#22270;&#20687;&#30340;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#23618;&#20986;&#19981;&#31351;&#12290;&#20026;&#20102;&#30740;&#31350;&#26368;&#26032;&#30340;&#25216;&#26415;&#65292;&#26412;&#35843;&#26597;&#23558;&#21069;&#27839;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#36824;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The act of secretly embedding and extracting a watermark on a cover image to protect it is known as image watermarking. In recent years, deep learning-based image watermarking techniques have been emerging one after another. To study the state-of-the-art, this survey categorizes cutting-edge deep learning-based image watermarking techniques into Embedder-Extractor Joint Training, Deep Networks as a Feature Transformation, and Hybrid schemes. Research directions in each category are also analyzed and summarized. Additionally, potential future research directions are discussed to envision future studies.
&lt;/p&gt;</description></item><item><title>ToolLLM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;ToolBench&#25968;&#25454;&#38598;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24037;&#20855;&#20351;&#29992;&#24773;&#22659;&#21644;API&#12290;</title><link>http://arxiv.org/abs/2307.16789</link><description>&lt;p&gt;
ToolLLM: &#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API
&lt;/p&gt;
&lt;p&gt;
ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. (arXiv:2307.16789v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16789
&lt;/p&gt;
&lt;p&gt;
ToolLLM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;ToolBench&#25968;&#25454;&#38598;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24037;&#20855;&#20351;&#29992;&#24773;&#22659;&#21644;API&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;LLaMA&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#65292;&#21363;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65288;API&#65289;&#26469;&#28385;&#36275;&#20154;&#31867;&#25351;&#20196;&#12290;&#21407;&#22240;&#26159;&#24403;&#21069;&#30340;&#25351;&#20196;&#35843;&#25972;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#19978;&#65292;&#20294;&#24573;&#30053;&#20102;&#24037;&#20855;&#20351;&#29992;&#39046;&#22495;&#12290;&#36825;&#19982;&#26368;&#20808;&#36827;&#30340;&#38381;&#28304;LLM&#65288;&#22914;ChatGPT&#65289;&#30340;&#20986;&#33394;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#24418;&#25104;&#23545;&#27604;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ToolLLM&#65292;&#19968;&#20010;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;ToolBench&#65292;&#19968;&#20010;&#29992;&#20110;&#24037;&#20855;&#20351;&#29992;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26500;&#24314;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;API&#25910;&#38598;&#65306;&#25105;&#20204;&#20174;RapidAPI Hub&#25910;&#38598;&#20102;16464&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;RESTful API&#65292;&#28085;&#30422;&#20102;49&#20010;&#31867;&#21035;&#65307;&#65288;ii&#65289;&#25351;&#20196;&#29983;&#25104;&#65306;&#25105;&#20204;&#25552;&#31034;ChatGPT&#29983;&#25104;&#28041;&#21450;&#36825;&#20123;API&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;&#21333;&#24037;&#20855;&#21644;&#22810;&#24037;&#20855;&#20351;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-too
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#23558;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#20004;&#31181;&#31639;&#27861;&#36716;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16708</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#33258;&#36866;&#24212;&#28388;&#27874;&#65306;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach. (arXiv:2307.16708v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#23558;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#20004;&#31181;&#31639;&#27861;&#36716;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#20004;&#31181;&#33879;&#21517;&#30340;&#33258;&#36866;&#24212;&#28388;&#27874;&#31639;&#27861;&#65292;&#21363;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#65292;&#22312;&#28304;&#20272;&#35745;&#21644;&#20998;&#31163;&#30340;&#29615;&#22659;&#20013;&#12290;&#22312;&#23637;&#24320;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Deep RLS&#21644;Deep EASI&#12290;&#36825;&#20123;&#26550;&#26500;&#23558;&#21407;&#22987;&#31639;&#27861;&#30340;&#36845;&#20195;&#21464;&#25442;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#20174;&#32780;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#36825;&#20123;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#31181;&#22522;&#20110;SURE&#30340;&#26041;&#27861;&#23545;&#20110;&#22686;&#24378;&#28304;&#20449;&#21495;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2307.13903</link><description>&lt;p&gt;
&#33104;&#36133;&#40065;&#26834;&#30340;Lipschitz&#19978;&#19979;&#25991;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#32773;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#30001;&#23545;&#25163;&#36873;&#25321;&#30340;Lipschitz&#20989;&#25968;$f$&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#23545;&#25163;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#19978;&#19979;&#25991;&#21521;&#37327;$x_t$&#65292;&#23398;&#20064;&#32773;&#23545;&#30495;&#23454;&#20989;&#25968;&#20540;$f(x_t)$&#36827;&#34892;&#29468;&#27979;&#65292;&#24182;&#25509;&#25910;&#19968;&#20010;&#25351;&#31034;&#29468;&#27979;&#26159;&#39640;&#36824;&#26159;&#20302;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;&#22312;&#24635;&#20849;$C$&#36718;&#20013;&#65292;&#20449;&#21495;&#21487;&#33021;&#34987;&#31713;&#25913;&#65292;&#20294;&#23398;&#20064;&#32773;&#19981;&#30693;&#36947;$C$&#30340;&#20540;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#36896;&#25104;&#23567;&#30340;&#32047;&#31215;&#25439;&#22833;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#32780;&#24378;&#22823;&#30340;&#25216;&#26415;&#39564;&#35777;&#65292;&#23545;&#35774;&#35745;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#35774;&#35745;&#20102;&#19968;&#20123;&#31639;&#27861;&#65288;&#23558;Lipschitz&#21442;&#25968;$L$&#35270;&#20026;&#24120;&#25968;&#65289;&#65306;&#23545;&#20110;&#23545;&#31216;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d=1$&#26102;&#36798;&#21040;&#21518;&#24724;$O(C\log T)$&#65292;&#22312;$d&gt;1$&#26102;&#36798;&#21040;&#21518;&#24724;$O_d(C\log T + T^{(d-1)/d})$&#65307;&#23545;&#20110;&#35745;&#20215;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d/(d+1)$&#26102;&#36798;&#21040;&#21518;&#24724;$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$&#12290;
&lt;/p&gt;
&lt;p&gt;
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d &gt; 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#32039;&#20945;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#26469;&#20248;&#21270;Rate-Splitting Multiple Access (RSMA)&#39044;&#32534;&#30721;&#65292;&#32467;&#21512;&#37096;&#20998;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#20182;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#22312;&#20013;&#31561;&#35268;&#27169;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#31867;&#20284;&#20256;&#32479;&#26041;&#27861;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#24615;&#33021;&#65292;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08822</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36895;&#29575;&#20998;&#21106;&#22810;&#22336;(Multiple Access)&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access. (arXiv:2307.08822v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#32039;&#20945;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#26469;&#20248;&#21270;Rate-Splitting Multiple Access (RSMA)&#39044;&#32534;&#30721;&#65292;&#32467;&#21512;&#37096;&#20998;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#20182;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#22312;&#20013;&#31561;&#35268;&#27169;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#31867;&#20284;&#20256;&#32479;&#26041;&#27861;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#24615;&#33021;&#65292;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#39044;&#32534;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#26469;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#37096;&#20998;&#21457;&#23556;&#26426;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;Rate-Splitting Multiple Access (RSMA)&#39044;&#32534;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#32039;&#20945;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#26469;&#26368;&#22823;&#21270;&#26126;&#30830;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#32469;&#36807;&#20102;&#23545;&#20219;&#20309;&#20854;&#20182;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#24635;&#36816;&#34892;&#26102;&#38388;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#20013;&#31561;&#35268;&#27169;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#31867;&#20284;&#20256;&#32479;&#39044;&#32534;&#30721;&#20248;&#21270;&#30340;&#24179;&#22343;&#21644;&#36895;&#29575;&#24615;&#33021;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#27425;&#20248;&#30340;&#20302;&#22797;&#26434;&#24230;&#39044;&#32534;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this letter, we propose the use of a meta-learning based precoder optimization framework to directly optimize the Rate-Splitting Multiple Access (RSMA) precoders with partial Channel State Information at the Transmitter (CSIT). By exploiting the overfitting of the compact neural network to maximize the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need for any other training data while minimizing the total running time. Numerical results reveal that the meta-learning based solution achieves similar ASR performance to conventional precoder optimization in medium-scale scenarios, and significantly outperforms sub-optimal low complexity precoder algorithms in the large-scale regime.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.03690</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;&#26159;&#19968;&#20010;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#20808;&#21069;&#35266;&#27979;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#20854;&#20013;&#35782;&#21035;&#21644;&#25233;&#21046;&#20102; Lorenz &#31995;&#32479;&#30340;&#28151;&#27788;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and suppressing unknown disturbances to dynamical systems is a problem with applications in many different fields. In this Letter, we present a model-free method to identify and suppress an unknown disturbance to an unknown system based only on previous observations of the system under the influence of a known forcing function. We find that, under very mild restrictions on the training function, our method is able to robustly identify and suppress a large class of unknown disturbances. We illustrate our scheme with an example where a chaotic disturbance to the Lorenz system is identified and suppressed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#30340;&#26032;&#22411;&#22522;&#30784;&#27169;&#22411;UniverSeg&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.03266</link><description>&lt;p&gt;
&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#30340;&#26032;&#22411;&#22522;&#30784;&#27169;&#22411;UniverSeg&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#29421;&#20041;&#20219;&#21153;&#19978;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#26500;&#24314;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20046;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20026;&#21508;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#23450;&#21046;&#12290;&#36825;&#21487;&#33021;&#20195;&#34920;&#20102;&#21307;&#23398;&#25104;&#20687;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#25105;&#20204;&#39044;&#35745;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#22609;&#36896;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26368;&#36817;&#24320;&#21457;&#30340;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;UniverSeg&#12290;&#25105;&#20204;&#22312;&#21069;&#21015;&#33146;&#25104;&#20687;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#35752;&#35770;&#31361;&#20986;&#20102;&#20960;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art techniques for medical image segmentation rely on deep-learning models. These models, however, are often trained on narrowly-defined tasks in a supervised fashion, which requires expensive labeled datasets. Recent advances in several machine learning domains, such as natural language generation have demonstrated the feasibility and utility of building foundation models that can be customized for various downstream tasks with little to no labeled data. This likely represents a paradigm shift for medical imaging, where we expect that foundation models may shape the future of the field. In this paper, we consider a recently developed foundation model for medical image segmentation, UniverSeg. We conduct an empirical evaluation study in the context of prostate imaging and compare it against the conventional approach of training a task-specific segmentation model. Our results and discussion highlight several important factors that will likely be important in the develo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;MLP&#30456;&#36739;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#26088;&#22312;&#25512;&#36827;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.13575</link><description>&lt;p&gt;
MLP&#30340;&#35268;&#27169;&#21270;&#65306;&#24402;&#32435;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#65292;&#24182;&#25506;&#35752;&#20102;MLP&#30456;&#36739;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#26088;&#22312;&#25512;&#36827;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#26500;&#24314;&#22359;&#8212;&#8212;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26497;&#38480;&#12290;MLP&#30340;&#23454;&#39564;&#24615;&#27934;&#35265;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative "less inductive bias is better", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenDataVal&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#20061;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26469;&#35780;&#20272;&#25968;&#25454;&#20215;&#20540;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.10577</link><description>&lt;p&gt;
OpenDataVal&#65306;&#19968;&#31181;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#30340;&#32479;&#19968;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenDataVal: a Unified Benchmark for Data Valuation. (arXiv:2306.10577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenDataVal&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#20061;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26469;&#35780;&#20272;&#25968;&#25454;&#20215;&#20540;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#19981;&#33391;&#20559;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#26469;&#37327;&#21270;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#36824;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#20272;&#20540;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenDataVal&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#33021;&#22815;&#24212;&#29992;&#21644;&#27604;&#36739;&#21508;&#31181;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#12290;OpenDataVal&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#29615;&#22659;&#65292;&#21253;&#25324;&#65288;i&#65289;&#21508;&#31181;&#22270;&#20687;&#65292;&#33258;&#28982;&#35821;&#35328;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#20061;&#31181;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21487;&#20197;&#23548;&#20837;&#20219;&#20309;scikit-learn&#27169;&#22411;&#30340;&#39044;&#27979;&#27169;&#22411;API&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#20540;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;OpenDataVal&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#65292;&#37327;&#21270;&#24182;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the quality and impact of individual data points is critical for improving model performance and mitigating undesirable biases within the training dataset. Several data valuation algorithms have been proposed to quantify data quality, however, there lacks a systemic and standardized benchmarking system for data valuation. In this paper, we introduce OpenDataVal, an easy-to-use and unified benchmark framework that empowers researchers and practitioners to apply and compare various data valuation algorithms. OpenDataVal provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets, (ii) implementations of nine different state-of-the-art data valuation algorithms, and (iii) a prediction model API that can import any models in scikit-learn. Furthermore, we propose four downstream machine learning tasks for evaluating the quality of data values. We perform benchmarking analysis using OpenDataVal, quantifying and comparin
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05272</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36895;&#29575;&#38477;&#20302;&#21407;&#21017;&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#24050;&#32463;&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#20294;&#26159;&#32858;&#31867;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992; CLIP &#31561;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#29305;&#24449;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#65292;&#26356;&#20855;&#26377;&#32467;&#26500;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#20174; ImageNet-1k &#30340; 57&#65285;&#25552;&#39640;&#21040; 66&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#22914;&#20309;&#23548;&#33268;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#26631;&#35760;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#26410;&#26631;&#35760;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914; MS-COCO &#21644; LAION-Aesthetics&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
&lt;/p&gt;</description></item><item><title>Vocos&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20613;&#37324;&#21494;&#35889;&#31995;&#25968;&#65292;&#28040;&#38500;&#20102;&#26102;&#22495;&#21644;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#21270;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#22312;&#39640;&#36136;&#37327;&#38899;&#39057;&#21512;&#25104;&#20013;&#30340;&#24046;&#36317;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.00814</link><description>&lt;p&gt;
Vocos&#65306;&#28040;&#38500;&#26102;&#22495;&#21644;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#21270;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#22312;&#39640;&#36136;&#37327;&#38899;&#39057;&#21512;&#25104;&#20013;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis. (arXiv:2306.00814v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00814
&lt;/p&gt;
&lt;p&gt;
Vocos&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20613;&#37324;&#21494;&#35889;&#31995;&#25968;&#65292;&#28040;&#38500;&#20102;&#26102;&#22495;&#21644;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#21270;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#22312;&#39640;&#36136;&#37327;&#38899;&#39057;&#21512;&#25104;&#20013;&#30340;&#24046;&#36317;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21457;&#23637;&#20027;&#35201;&#30001;&#22312;&#26102;&#22495;&#20013;&#25805;&#20316;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#25512;&#21160;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#26102;&#39057;&#34920;&#31034;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#20887;&#20313;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#19978;&#37319;&#26679;&#25805;&#20316;&#12290;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26102;&#39057;&#34920;&#31034;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#19982;&#20154;&#30340;&#21548;&#35273;&#24863;&#30693;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#36890;&#36807;&#20854;&#35745;&#31639;&#24471;&#21040;&#20102;&#32463;&#36807;&#20805;&#20998;&#39564;&#35777;&#30340;&#24555;&#36895;&#31639;&#27861;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#37325;&#24314;&#22797;&#20540;&#35889;&#22270;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#23384;&#22312;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;Vocos&#65292;&#19968;&#20010;&#30452;&#25509;&#29983;&#25104;&#20613;&#37324;&#21494;&#35889;&#31995;&#25968;&#30340;&#26032;&#27169;&#22411;&#65292;&#26469;&#28040;&#38500;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Vocos&#19981;&#20165;&#19982;&#38899;&#39057;&#36136;&#37327;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared 
&lt;/p&gt;</description></item><item><title>HiGen&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#29983;&#25104;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#24418;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22810;&#39033;&#24335;&#20998;&#24067;&#26469;&#29983;&#25104;&#20855;&#26377;&#25972;&#25968;&#20540;&#30340;&#23376;&#22270;&#30340;&#36793;&#26435;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#24418;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19337</link><description>&lt;p&gt;
HiGen&#65306;&#23618;&#27425;&#22270;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HiGen: Hierarchical Graph Generative Networks. (arXiv:2305.19337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19337
&lt;/p&gt;
&lt;p&gt;
HiGen&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#29983;&#25104;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#24418;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22810;&#39033;&#24335;&#20998;&#24067;&#26469;&#29983;&#25104;&#20855;&#26377;&#25972;&#25968;&#20540;&#30340;&#23376;&#22270;&#30340;&#36793;&#26435;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#24418;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#34920;&#29616;&#20986;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#36890;&#24120;&#34987;&#29616;&#26377;&#30340;&#22270;&#24418;&#29983;&#25104;&#26041;&#27861;&#25152;&#24573;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#29983;&#25104;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#24418;&#30340;&#23618;&#27425;&#32467;&#26500;&#24182;&#25104;&#21151;&#22320;&#29983;&#25104;&#22270;&#24418;&#23376;&#32467;&#26500;&#12290;&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#65292;&#35813;&#27169;&#22411;&#24182;&#34892;&#29983;&#25104;&#31038;&#21306;&#65292;&#20351;&#29992;&#29420;&#31435;&#30340;&#27169;&#22411;&#39044;&#27979;&#31038;&#21306;&#20043;&#38388;&#30340;&#36328;&#36793;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#20351;&#29983;&#25104;&#30340;&#22270;&#24418;&#32593;&#32476;&#39640;&#24230;&#21487;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#22810;&#39033;&#24335;&#20998;&#24067;&#24314;&#27169;&#23618;&#27425;&#22270;&#24418;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#38024;&#23545;&#27492;&#20998;&#24067;&#25512;&#23548;&#20102;&#36882;&#24402;&#20998;&#35299;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#25972;&#25968;&#20540;&#30340;&#23376;&#22270;&#30340;&#36793;&#26435;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#24418;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using a separate model. This modular approach results in a highly scalable graph generative network. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution, enabling us to generate sub-graphs with integer-valued edge weights in an autoregressive approach. Empirical studies demonstrate that the proposed generative model can effectively capture both local and global properties of graphs and achieves state-of-the-art performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18593</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#24314;&#27169;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#25104;&#20026;&#22522;&#20110;&#23494;&#24230;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#20505;&#36873;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#23588;&#20854;&#26159;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#22791;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#36890;&#36807;&#31616;&#21270;DDPM&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24341;&#20986;&#21478;&#19968;&#31181;&#31216;&#20026;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65288;DTPM&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;DTPM&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#30340;&#25193;&#25955;&#26102;&#38388;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27492;&#21518;&#39564;&#20998;&#24067;&#30340;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;ADBenh&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
&#21098;&#26525;&#19982;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26469;&#20415;&#23452;&#22320;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#24040;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21046;&#32422;&#12290;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36890;&#36807;&#21024;&#38500;&#20887;&#20313;&#21442;&#25968;&#26469;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LPM&#32780;&#35328;&#65292;&#33719;&#24471;&#26799;&#24230;&#26159;&#35745;&#31639;&#19978;&#31105;&#27490;&#30340;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LPM&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;LoRAPrune&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21033;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#20540;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#22522;&#20110;&#21098;&#26525;&#20934;&#21017;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
&lt;/p&gt;</description></item><item><title>Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.</title><link>http://arxiv.org/abs/2305.18030</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18030
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20107;&#20808;&#25163;&#24037;&#21019;&#24314;&#25628;&#32034;&#31354;&#38388;&#26469;&#25628;&#32034;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#26368;&#20248;&#23376;&#32593;&#32476;&#12290;&#36825;&#26679;&#30340;&#35201;&#27714;&#20351;&#24471;&#22312;&#27809;&#26377;&#26174;&#33879;&#30340;&#20154;&#24037;&#19987;&#19994;&#30693;&#35782;&#21644;&#25163;&#21160;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#36890;&#29992;&#22330;&#26223;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Automated Search-Space Generation Neural Architecture Search&#65288;ASGNAS&#65289;&#65292;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20197;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#35757;&#32451;&#35206;&#30422;&#25152;&#26377;&#20505;&#36873;&#36830;&#25509;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;DNN&#65292;&#24182;&#20135;&#29983;&#39640;&#24615;&#33021;&#30340;&#23376;&#32593;&#32476;&#12290;&#25216;&#26415;&#19978;&#65292;ASGNAS&#20855;&#26377;&#19977;&#20010;&#26174;&#33879;&#30340;&#36129;&#29486;&#20197;&#20943;&#23569;&#20154;&#21147;&#24037;&#20316;&#65306;&#65288;i&#65289;&#36890;&#29992;DNN&#30340;&#33258;&#21160;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#20869;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;Hierarchical Half-Space Projected Gradient&#65288;H2SPG&#65289;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30830;&#20445;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#38752;&#22320;&#20135;&#29983;&#20855;&#26377;&#39640;&#24615;&#33021;&#21644;&#31232;&#30095;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
To search an optimal sub-network within a general deep neural network (DNN), existing neural architecture search (NAS) methods typically rely on handcrafting a search space beforehand. Such requirements make it challenging to extend them onto general scenarios without significant human expertise and manual intervention. To overcome the limitations, we propose Automated Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first automated system to train general DNNs that cover all candidate connections and operations and produce high-performing sub-networks in the one shot manner. Technologically, ASGNAS delivers three noticeable contributions to minimize human efforts: (i) automated search space generation for general DNNs; (ii) a Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy and dependency within generated search space to ensure the network validity during optimization, and reliably produces a solution with both high performance an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16446</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#30340;Jensen-Shannon&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#25955;&#24230;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24213;&#23618;&#20998;&#24067;&#36890;&#24120;&#26410;&#30693;&#65292;&#20174;&#32463;&#39564;&#26679;&#26412;&#20013;&#20272;&#35745;&#25955;&#24230;&#26159;&#19968;&#20010;&#22522;&#26412;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Fourier&#29305;&#24449;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;RKHS&#20013;&#12290;&#27492;&#20272;&#35745;&#20989;&#25968;&#26159;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#23567;&#25209;&#37327;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;RKHS&#36827;&#34892;&#26174;&#24335;&#26144;&#23556;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#37327;&#26159;Jensen-Shannon&#25955;&#24230;&#30340;&#19968;&#20010;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15759</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30452;&#25509;&#22312;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;DMs&#30340;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#32452;&#21512;&#24615;&#23646;&#24615;&#65292;&#22823;&#37327;&#22122;&#38899;&#27880;&#20837;&#21040;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#12290;LDMs&#20351;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20943;&#23569;&#21040;&#26356;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#35757;&#32451;DMs&#26356;&#21152;&#39640;&#25928;&#21644;&#24555;&#36895;&#12290;&#19982;[Ghalebikesabi&#31561;&#20154;&#65292;2023]&#39044;&#20808;&#29992;&#20844;&#20849;&#25968;&#25454;&#39044;&#35757;&#32451;DMs&#65292;&#28982;&#21518;&#20877;&#29992;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#20165;&#24494;&#35843;LDMs&#20013;&#19981;&#21516;&#23618;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#33719;&#24471;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#25972;&#20010;DM&#24494;&#35843;&#65292;&#21487;&#20943;&#23569;&#22823;&#32422;96%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36716;&#31227;&#23398;&#20064;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#21487;&#23558;&#20043;&#21069;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36712;&#36857;&#24212;&#29992;&#22312;&#26032;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#33021;&#22312;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#20043;&#21069;&#23454;&#29616;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14122</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36712;&#36857;&#30340;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Transferring Learning Trajectories of Neural Networks. (arXiv:2305.14122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36716;&#31227;&#23398;&#20064;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#21487;&#23558;&#20043;&#21069;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36712;&#36857;&#24212;&#29992;&#22312;&#26032;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#33021;&#22312;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#20043;&#21069;&#23454;&#29616;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#36825;&#22312;&#25191;&#34892;&#37325;&#22797;&#35757;&#32451;&#36816;&#34892;&#65288;&#20363;&#22914;&#27169;&#22411;&#38598;&#25104;&#25110;&#30693;&#35782;&#33976;&#39311;&#65289;&#26102;&#23588;&#20854;&#25104;&#38382;&#39064;&#12290;&#19968;&#26086;&#25105;&#20204;&#22312;&#26576;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;DNN&#65292;&#25105;&#20204;&#23601;&#25317;&#26377;&#20102;&#20854;&#23398;&#20064;&#36712;&#36857;&#65288;&#21363;&#35757;&#32451;&#26399;&#38388;&#30340;&#20013;&#38388;&#21442;&#25968;&#24207;&#21015;&#65289;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#21547;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#23581;&#35797;&#21033;&#29992;&#32473;&#23450;&#23398;&#20064;&#36712;&#36857;&#30340;&#36825;&#31181;&#20449;&#24687;&#36827;&#34892;&#21478;&#19968;&#31181;&#35757;&#32451;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#8220;&#36716;&#31227;&#8221;&#32473;&#23450;&#23398;&#20064;&#36712;&#36857;&#20174;&#19968;&#20010;&#21021;&#22987;&#21442;&#25968;&#21040;&#21478;&#19968;&#20010;&#21021;&#22987;&#21442;&#25968;&#65292;&#31216;&#20026;&#23398;&#20064;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#27839;&#36712;&#36857;&#36880;&#28176;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#26799;&#24230;&#23548;&#20986;&#20102;&#31532;&#19968;&#20010;&#31639;&#27861;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#36716;&#31227;&#21442;&#25968;&#22312;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#20043;&#21069;&#23601;&#33021;&#36798;&#21040;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36716;&#31227;&#21442;&#25968;&#30340;&#25439;&#22833;&#26223;&#35266;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) is computationally expensive, which is problematic especially when performing duplicated training runs, such as model ensemble or knowledge distillation. Once we have trained one DNN on some dataset, we have its learning trajectory (i.e., a sequence of intermediate parameters during training) which may potentially contain useful information for learning the dataset. However, there has been no attempt to utilize such information of a given learning trajectory for another training. In this paper, we formulate the problem of "transferring" a given learning trajectory from one initial parameter to another one, called learning transfer problem, and derive the first algorithm to approximately solve it by matching gradients successively along the trajectory via permutation symmetry. We empirically show that the transferred parameters achieve non-trivial accuracy before any direct training. Also, we analyze the loss landscape property of the transferred par
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20844;&#24179;&#24615;&#21442;&#25968;&#19982;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#30340;&#26435;&#34913;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#29702;&#35299;&#21644;&#20915;&#31574;&#22522;&#30784;&#65292;&#24110;&#21161;&#24320;&#21457;&#32773;&#22312;&#25552;&#20379;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2305.13057</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#22240;&#26524;&#20851;&#32852;&#26435;&#34913;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causality-Aided Trade-off Analysis for Machine Learning Fairness. (arXiv:2305.13057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20844;&#24179;&#24615;&#21442;&#25968;&#19982;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#30340;&#26435;&#34913;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#29702;&#35299;&#21644;&#20915;&#31574;&#22522;&#30784;&#65292;&#24110;&#21161;&#24320;&#21457;&#32773;&#22312;&#25552;&#20379;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#26041;&#38754;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#28044;&#29616;&#20986;&#26469;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#26102;&#65292;&#22312;&#32771;&#34385;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#29702;&#35299;&#23545;&#20110;&#24320;&#21457;&#32773;&#20570;&#20986;&#20851;&#20110;&#25552;&#20379;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#22810;&#20010;&#20844;&#24179;&#24615;&#21442;&#25968;&#21644;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#19988;&#24444;&#27492;&#20043;&#38388;&#23384;&#22312;&#32806;&#21512;&#29978;&#33267;&#20914;&#31361;&#26102;&#65292;&#20998;&#26512;&#36825;&#20123;&#26435;&#34913;&#26159;&#26497;&#20854;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20998;&#26512;&#20844;&#24179;&#24615;&#21442;&#25968;&#19982;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#23454;&#38469;&#26377;&#25928;&#22320;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#39046;&#22495;&#29305;&#23450;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#20934;&#30830;&#30340;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22522;&#20110;&#25104;&#29087;&#30340;&#22240;&#26524;&#25512;&#26029;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#26032;&#39062;&#30340;&#26435;&#34913;&#20998;&#26512;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an increasing interest in enhancing the fairness of machine learning (ML). Despite the growing number of fairness-improving methods, we lack a systematic understanding of the trade-offs among factors considered in the ML pipeline when fairness-improving methods are applied. This understanding is essential for developers to make informed decisions regarding the provision of fair ML services. Nonetheless, it is extremely difficult to analyze the trade-offs when there are multiple fairness parameters and other crucial metrics involved, coupled, and even in conflict with one another.  This paper uses causality analysis as a principled method for analyzing trade-offs between fairness parameters and other crucial metrics in ML pipelines. To ractically and effectively conduct causality analysis, we propose a set of domain-specific optimizations to facilitate accurate causal discovery and a unified, novel interface for trade-off analysis based on well-established causal inferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10616</link><description>&lt;p&gt;
CNN &#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#31038;&#21306;&#20284;&#20046;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35782;&#21035;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#21512;&#36866;&#30340;&#21387;&#32553;&#25216;&#26415;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#24230;&#37327;&#30340;&#32508;&#36848;&#26469;&#20026;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#26631;&#20934;&#21270;&#36129;&#29486;&#12290;&#36825;&#20123;&#24230;&#37327;&#24050;&#34987;&#23454;&#29616;&#21040;NetZIP&#65292;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#22522;&#20934;&#20043;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#19968;&#20123;&#34987;&#23457;&#26597;&#30340;&#24230;&#37327;&#65292;&#20998;&#21035;&#32858;&#28966;&#20110;&#30446;&#26631;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
&lt;/p&gt;</description></item><item><title>&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.09241</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#32473;&#20986;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#65306;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20363;&#23376;&#31359;&#36879;&#37027;&#20123;&#26080;&#27861;&#21033;&#29992;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09241
&lt;/p&gt;
&lt;p&gt;
&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#19979;&#38543;&#22788;&#21487;&#35265;&#30340;&#23433;&#20840;&#28431;&#27934;&#20013;&#65292;&#20445;&#25252;&#25968;&#25454;&#20813;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#21033;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21483;&#20570;&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#65288;UEs&#65289;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#22312;&#21407;&#22987;&#30340;&#24178;&#20928;&#20998;&#24067;&#19978;&#20934;&#30830;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616; UEs &#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#26159;&#34394;&#20551;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#21033;&#29992;&#20854;&#20182;&#26410;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#26469;&#21435;&#38500;&#20445;&#25252;&#65292;&#23558;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#36716;&#20026;&#21487;&#23398;&#20064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#23041;&#32961;&#65292;&#24341;&#20837;&#20102;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#65288;LEs&#65289;&#65292;&#36825;&#20123;&#26159;&#24050;&#32463;&#21435;&#38500;&#20445;&#25252;&#30340;UEs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#23558;UEs&#25237;&#23556;&#21040;LEs&#30340;&#27969;&#24418;&#19978;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#27169;&#22411;&#23545;UEs&#36827;&#34892;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#39033;&#21644;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#30830;&#20445;&#20102;INN&#36755;&#20986;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#24182;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12541</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems. (arXiv:2304.12541v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#39033;&#21644;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#30830;&#20445;&#20102;INN&#36755;&#20986;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#24182;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;PI-INN&#30340;&#32467;&#26500;&#21253;&#25324;&#20004;&#20010;&#23376;&#32593;&#32476;&#65306;&#19968;&#20010;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(INN)&#21644;&#19968;&#20010;&#31070;&#32463;&#22522;&#30784;&#32593;&#32476;(NB-Net)&#12290;&#36890;&#36807;NB-Net&#24110;&#21161;&#24314;&#31435;&#21442;&#25968;&#36755;&#20837;&#21644;INN&#36755;&#20986;&#20043;&#38388;&#30340;&#21487;&#36870;&#26144;&#23556;&#65292;&#20197;&#25552;&#20379;&#21487;&#34892;&#30340;&#21518;&#39564;&#20998;&#24067;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;PI-INN&#30340;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#20004;&#20010;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#26159;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#39033;&#65292;&#21478;&#19968;&#37096;&#20998;&#26159;&#26032;&#30340;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#12290;&#25552;&#20986;&#30340;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#21487;&#20197;&#39640;&#26031;&#21270;&#38543;&#26426;&#28508;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20272;&#35745;&#30340;&#23494;&#24230;&#20989;&#25968;&#65292;&#30830;&#20445;INN&#36755;&#20986;&#30340;&#20004;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#21453;&#21521;&#36816;&#21160;&#23398;&#21644;&#21453;&#21521;&#25193;&#25955;&#31561;&#22810;&#39033;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;PI-INN&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we propose a novel approach for solving Bayesian inverse problems with physics-informed invertible neural networks (PI-INN). The architecture of PI-INN consists of two sub-networks: an invertible neural network (INN) and a neural basis network (NB-Net). The invertible map between the parametric input and the INN output with the aid of NB-Net is constructed to provide a tractable estimation of the posterior distribution, which enables efficient sampling and accurate density evaluation. Furthermore, the loss function of PI-INN includes two components: a residual-based physics-informed loss term and a new independence loss term. The presented independence loss term can Gaussianize the random latent variables and ensure statistical independence between two parts of INN output by effectively utilizing the estimated density function. Several numerical experiments are presented to demonstrate the efficiency and accuracy of the proposed PI-INN, including inverse kinematics, inver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.11328</link><description>&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37319;&#26679;&#31574;&#30053;&#23581;&#35797;&#26377;&#25928;&#22320;&#35299;&#20915;&#21453;&#21521;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#25152;&#24471;ODE&#27714;&#35299;&#22120;&#30340;&#31995;&#25968;&#30001;ODE&#20844;&#24335;&#65292;&#21453;&#21521;&#31163;&#25955;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#20351;&#29992;&#30340;ODE&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#20248;&#21270;&#26576;&#20123;&#31995;&#25968;&#26469;&#21152;&#36895;&#20960;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;ODE&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#12290;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;&#19968;&#32452;&#32454;&#31890;&#24230;&#26102;&#38388;&#27493;&#38271;&#30340;&#21407;&#22987;ODE&#27714;&#35299;&#22120;&#26500;&#36896;MSE&#65292;&#20174;&#21407;&#29702;&#19978;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#31215;&#20998;&#36924;&#36817;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#25193;&#25955;&#38544;&#34255;&#29366;&#24577;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;
&lt;/p&gt;
&lt;p&gt;
One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.04736</link><description>&lt;p&gt;
&#20851;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#30528;&#30524;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#20197;&#21306;&#20998;&#20854;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#36825;&#39033;&#33021;&#21147;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#21306;&#20998;&#30340;&#21487;&#33021;&#24615;&#19968;&#30452;&#26159;&#35813;&#39046;&#22495;&#20869;&#30340;&#20105;&#35758;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22914;&#26524;&#33021;&#65292;&#20309;&#26102;&#33021;&#26816;&#27979;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#38500;&#38750;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#22312;&#25972;&#20010;&#25903;&#25345;&#20013;&#23436;&#20840;&#30456;&#21516;&#65292;&#21542;&#21017;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#65292;&#24182;&#20381;&#36182;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#20687;&#20154;&#31867;&#65292;&#25105;&#20204;&#23601;&#38656;&#35201;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#26816;&#27979;&#23427;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21578;&#35785;&#38656;&#35201;&#22810;&#23569;&#20010;&#26679;&#26412;&#25165;&#33021;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#24341;&#36215;&#20102;&#26356;&#22810;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;LLM&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#21482;&#38656;&#38468;&#21152;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#23383;&#33410;&#32423;&#34917;&#19969;&#21363;&#21487;&#32469;&#36807;MalConv&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21487;&#20197;&#23558;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#20316;&#20026;&#24212;&#23545;&#27492;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13372</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38745;&#24577;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness of Learning-based Static Malware Classifiers. (arXiv:2303.13372v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#21482;&#38656;&#38468;&#21152;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#23383;&#33410;&#32423;&#34917;&#19969;&#21363;&#21487;&#32469;&#36807;MalConv&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21487;&#20197;&#23558;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#20316;&#20026;&#24212;&#23545;&#27492;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#19968;&#30452;&#26159;&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#21644;&#21453;&#30149;&#27602;&#31995;&#32479;&#20043;&#38388;&#25345;&#32493;&#30340;&#20891;&#22791;&#31454;&#36187;&#38454;&#27573;&#12290;&#38543;&#30528;&#36825;&#22330;&#31454;&#36187;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#24471;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#20351;&#24471;&#30452;&#25509;&#23545;ML&#36827;&#34892;&#25915;&#20987;&#23545;&#20110;&#23545;&#25163;&#32780;&#35328;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#21069;&#26223;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#22330;&#20891;&#22791;&#31454;&#36187;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#21363;&#20174;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#21407;&#22987;&#23383;&#33410;&#20013;&#25805;&#20316;&#30340;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#20998;&#31867;&#22120;MalConv&#30340;&#35282;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;MalConv&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#34917;&#19969;&#25915;&#20987;&#30340;&#24433;&#21709;:&#23558;&#19968;&#20010;&#23383;&#33410;&#32423;&#30340;&#34917;&#19969;&#38468;&#21152;&#21040;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#20013;&#65292;&#20351;&#20854;&#32469;&#36807;&#26816;&#27979;&#30340;&#27010;&#29575;&#39640;&#36798;94.3&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#65292;&#22312;&#20219;&#20309;&#21253;&#21547;&#35813;&#34917;&#19969;&#30340;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#24658;&#23450;&#26102;&#38388;&#20869;&#65292;&#21487;&#20197;&#23558;&#20854;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21363;&#20351;&#30456;&#23545;&#20110;&#21407;&#22987;&#25991;&#20214;&#22823;&#23567;&#32780;&#35328;&#65292;&#36825;&#20123;&#34917;&#19969;&#30340;&#22823;&#23567;&#20063;&#30456;&#23545;&#36739;&#23567;-&#22312;2&#65285;-8&#65285;&#20043;&#38388;&#12290;&#20026;&#20102;&#25269;&#24481;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#65292;&#20801;&#35768;&#35782;&#21035;&#24694;&#24847;&#20195;&#30721;&#30340;&#37096;&#20998;&#19981;&#21463;&#23545;&#25239;&#24615;&#34917;&#19969;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware detection has long been a stage for an ongoing arms race between malware authors and anti-virus systems. Solutions that utilize machine learning (ML) gain traction as the scale of this arms race increases. This trend, however, makes performing attacks directly on ML an attractive prospect for adversaries. We study this arms race from both perspectives in the context of MalConv, a popular convolutional neural network-based malware classifier that operates on raw bytes of files. First, we show that MalConv is vulnerable to adversarial patch attacks: appending a byte-level patch to malware files bypasses detection 94.3% of the time. Moreover, we develop a universal adversarial patch (UAP) attack where a single patch can drop the detection rate in constant time of any malware file that contains it by 80%. These patches are effective even being relatively small with respect to the original file size -between 2%-8%. As a countermeasure, we then perform window ablation that allows u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.13093</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#23450;&#20041;&#21644;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#25509;&#36817;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#12290;&#20256;&#32479;&#25991;&#29486;&#20381;&#36182;&#20110;&#21442;&#25968;&#32479;&#35745;&#30697;&#65292;&#29305;&#21035;&#26159;&#21442;&#25968;&#26041;&#24046;&#30340;&#25910;&#25947;&#26469;&#37327;&#21270;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;\textit{&#27010;&#29575;&#25910;&#25947;}&#26465;&#20214;&#26469;&#23450;&#20041;SGD&#30340;\textit{&#27010;&#29575;&#31283;&#23450;&#24615;}&#12290;&#25552;&#20986;&#30340;&#31283;&#23450;&#24615;&#30452;&#25509;&#22238;&#31572;&#20102;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#22312;&#27010;&#29575;&#24615;&#31283;&#23450;&#24615;&#30340;&#38236;&#22836;&#19979;&#65292;SGD&#25165;&#34920;&#29616;&#20986;&#20016;&#23500;&#32780;&#23454;&#38469;&#30456;&#20851;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#22914;&#23436;&#20840;&#22833;&#21435;&#31283;&#23450;&#24615;&#38454;&#27573;&#12289;&#19981;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12289;&#25910;&#25947;&#21040;&#20302;&#31209;&#38797;&#28857;&#38454;&#27573;&#21644;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12290;&#24403;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36825;&#20123;&#30456;&#22270;&#24847;&#21619;&#30528;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38454;&#27573;&#12289;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#21450;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#12290;&#20851;&#38190;&#30340;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#23558;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#32447;&#24615;&#29305;&#24449;&#65292;&#20197;&#26041;&#20415;&#26816;&#26597;&#21644;&#29702;&#35299;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#35813;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09590</link><description>&lt;p&gt;
&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction. (arXiv:2303.09590v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38454;&#27573;&#12289;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#21450;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#12290;&#20851;&#38190;&#30340;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#23558;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#32447;&#24615;&#29305;&#24449;&#65292;&#20197;&#26041;&#20415;&#26816;&#26597;&#21644;&#29702;&#35299;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#35813;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#32593;&#32476;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#21457;&#29616;&#12290;&#21457;&#25496;&#21644;&#29702;&#35299;&#22810;&#21464;&#37327;&#32593;&#32476;&#20013;&#30340;&#20851;&#31995;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#20197;&#25552;&#21462;&#32593;&#32476;&#19981;&#21516;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#20043;&#38388;&#20851;&#32852;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65288;&#20363;&#22914;&#65292;&#20160;&#20040;&#26159;&#22312;&#31038;&#20132;&#32593;&#32476;&#23494;&#24230;&#26041;&#38754;&#19982;&#19981;&#21516;&#23646;&#24615;&#30340;&#32452;&#21512;&#20851;&#31995;&#65289;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#26681;&#25454;&#25152;&#36873;&#36755;&#20837;&#21644;&#36755;&#20986;&#23646;&#24615;&#26469;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#20135;&#29983;&#19968;&#20010;&#31616;&#21270;&#30340;&#32467;&#26524;&#38598;&#21512;&#20197;&#20415;&#26816;&#26597;&#65292;&#26368;&#21518;&#36890;&#36807;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#38454;&#27573;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#65292;&#35813;&#27493;&#39588;&#23558;&#30001;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#30452;&#35266;&#35299;&#37322;&#30340;&#32447;&#24615;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22823;&#22411;&#32452;&#32455;&#21592;&#24037;&#20043;&#38388;&#30340;&#30005;&#23376;&#37038;&#20214;&#36890;&#20449;&#25968;&#25454;&#38598;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#24037;&#20316;&#27969;&#31243;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.08250</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65292;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#38656;&#35201;&#25317;&#26377;&#20154;&#31867;&#26234;&#33021;&#30340;&#38887;&#24615;&#65292;&#21363;&#19981;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#31181;&#38887;&#24615;&#19982;&#22823;&#33041;&#20013;&#22797;&#26434;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#28023;&#39532;&#32500;&#25252;&#30340;&#38271;&#26399;&#35760;&#24518;&#65288;LM&#65289;&#32039;&#23494;&#30456;&#20851;&#12290;Transformer&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#8220;&#22823;&#33041;&#8221;&#30340;&#23545;&#24212;&#20307;&#65292;&#20294;LM&#32452;&#20214;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65288;ArtiHippo&#65289;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#28040;&#34701;&#23454;&#39564;&#65292;&#36873;&#23450;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#26469;&#23454;&#29616;&#21644;&#25104;&#38271;ArtiHippo&#12290;ArtiHippo&#30001;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#34920;&#31034;&#12290;&#27599;&#20010;&#19987;&#23478;&#32452;&#20214;&#26159;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#29616;&#22330;&#21464;&#20307;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36827;&#34892;&#32500;&#25252;&#65292;&#25628;&#32034;&#31354;&#38388;&#30001;&#22235;&#20010;&#22522;&#26412;&#25104;&#38271;&#25805;&#20316;&#65288;&#36339;&#36807;&#12289;&#37325;&#29992;&#12289;&#36866;&#24212;&#21644;&#26032;&#65289;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#20013;&#65292;d-&#20998;&#31163;&#26159;&#32597;&#35265;&#30340;&#29616;&#35937;&#65292;&#38500;&#38750;&#22270;&#26159;&#26497;&#20854;&#31232;&#30095;&#30340;&#12290;&#23545;&#20110;&#22240;&#26524;&#21457;&#29616;&#30340;PC&#31639;&#27861;&#21644;UniformSGS&#31639;&#27861;&#36827;&#34892;&#20102;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#65292;&#24182;&#32473;&#20986;&#20102;&#19978;&#30028;&#65292;&#19978;&#30028;&#20197;&#25351;&#25968;&#36895;&#24230;&#34928;&#20943;&#21040;0&#12290;</title><link>http://arxiv.org/abs/2303.05628</link><description>&lt;p&gt;
&#20851;&#20110;D-&#20998;&#31163;&#30340;&#19981;&#22826;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Unlikelihood of D-Separation. (arXiv:2303.05628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#20013;&#65292;d-&#20998;&#31163;&#26159;&#32597;&#35265;&#30340;&#29616;&#35937;&#65292;&#38500;&#38750;&#22270;&#26159;&#26497;&#20854;&#31232;&#30095;&#30340;&#12290;&#23545;&#20110;&#22240;&#26524;&#21457;&#29616;&#30340;PC&#31639;&#27861;&#21644;UniformSGS&#31639;&#27861;&#36827;&#34892;&#20102;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#65292;&#24182;&#32473;&#20986;&#20102;&#19978;&#30028;&#65292;&#19978;&#30028;&#20197;&#25351;&#25968;&#36895;&#24230;&#34928;&#20943;&#21040;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26088;&#22312;&#20174;&#30001;&#20854;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#22270;&#65307;&#22522;&#20110;&#32422;&#26463;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;oracle&#22312;&#22270;&#20013;&#25628;&#32034;&#19968;&#20010;d-&#20998;&#31163;&#30340;&#33410;&#28857;&#38598;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20998;&#26512;&#35777;&#25454;&#65292;&#21363;&#22312;&#22823;&#22411;&#22270;&#20013;&#65292;&#21363;&#20351;&#20445;&#35777;&#23384;&#22312;&#65292;d-&#20998;&#31163;&#20063;&#26159;&#19968;&#20010;&#32597;&#35265;&#30340;&#29616;&#35937;&#65292;&#38500;&#38750;&#22270;&#26159;&#26497;&#20854;&#31232;&#30095;&#30340;&#12290;&#25105;&#20204;&#36824;&#23545;&#22240;&#26524;&#21457;&#29616;&#30340;PC&#31639;&#27861;&#36827;&#34892;&#20102;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#65292;&#20197;&#21450;&#25105;&#20204;&#31216;&#20043;&#20026;UniformSGS&#30340;SGS&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#33410;&#28857;&#38598;V={v1&#65292;...&#65292;vn}&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;DAG G=(V,E)&#65292;&#20854;&#20013;&#22914;&#26524;a&lt;b&#65292;&#21017;(va&#65292;vb)&#8712;E&#30340;&#29420;&#31435;&#27010;&#29575;&#20026;p1&#65292;&#22914;&#26524;a&gt;b&#65292;&#21017;&#20026;0&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#24403;x&#21644;y&#21487;d-&#20998;&#31163;&#26102;&#65292;&#38598;&#21512;V- {x&#65292;y} d-&#20998;&#31163;x&#21644;y&#30340;&#27010;&#29575;&#30340;&#19978;&#30028;&#65307;&#25105;&#20204;&#30340;&#19978;&#30028;&#22312;|V|&#8594;&#8734;&#26102;&#20197;&#25351;&#25968;&#36895;&#24230;&#34928;&#20943;&#21040;0&#12290;&#23545;&#20110;PC&#31639;&#27861;&#65292;&#34429;&#28982;&#24050;&#30693;&#20854;&#26368;&#22351;&#24773;&#20917;&#30340;&#20445;&#35777;&#22312;&#38750;&#31232;&#30095;&#22270;&#19978;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery aims to recover a causal graph from data generated by it; constraint based methods do so by searching for a d-separating conditioning set of nodes in the graph via an oracle. In this paper, we provide analytic evidence that on large graphs, d-separation is a rare phenomenon, even when guaranteed to exist, unless the graph is extremely sparse. We then provide an analytic average case analysis of the PC Algorithm for causal discovery, as well as a variant of the SGS Algorithm we call UniformSGS. We consider a set $V=\{v_1,\ldots,v_n\}$ of nodes, and generate a random DAG $G=(V,E)$ where $(v_a, v_b) \in E$ with i.i.d. probability $p_1$ if $a&lt;b$ and $0$ if $a &gt; b$. We provide upper bounds on the probability that a subset of $V-\{x,y\}$ d-separates $x$ and $y$, conditional on $x$ and $y$ being d-separable; our upper bounds decay exponentially fast to $0$ as $|V| \rightarrow \infty$. For the PC Algorithm, while it is known that its worst-case guarantees fail on non-sparse gr
&lt;/p&gt;</description></item><item><title>Lumos&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#33410;&#28857;&#32423;&#32852;&#37030;&#22270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#29305;&#24449;&#21644;&#24230;&#25968;&#20445;&#25252;&#21151;&#33021;&#12290;&#23427;&#37319;&#29992;&#20102;&#26641;&#26500;&#36896;&#22120;&#21644;&#21435;&#20013;&#24515;&#21270;&#33410;&#28857;&#32858;&#21512;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.00492</link><description>&lt;p&gt;
Lumos: &#38024;&#23545;&#20998;&#24067;&#24335;&#35774;&#22791;&#30340;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices. (arXiv:2303.00492v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00492
&lt;/p&gt;
&lt;p&gt;
Lumos&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#33410;&#28857;&#32423;&#32852;&#37030;&#22270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#29305;&#24449;&#21644;&#24230;&#25968;&#20445;&#25252;&#21151;&#33021;&#12290;&#23427;&#37319;&#29992;&#20102;&#26641;&#26500;&#36896;&#22120;&#21644;&#21435;&#20013;&#24515;&#21270;&#33410;&#28857;&#32858;&#21512;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#22240;&#27492;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#24212;&#29992;&#21644;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#20005;&#37325;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#25152;&#26377;&#22270;&#20449;&#24687;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21327;&#20316;&#35745;&#31639;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;GNN&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23458;&#25143;&#31471;&#25345;&#26377;&#19981;&#21516;&#30340;&#22270;&#25110;&#23376;&#22270;&#30340;&#31995;&#32479;&#19978;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#23545;&#27599;&#20010;&#23458;&#25143;&#31471;&#20165;&#30693;&#36947;&#20854;&#30452;&#25509;&#37051;&#23621;&#30340;&#23454;&#38469;&#33410;&#28857;&#32423;&#32852;&#37030;&#24773;&#20917;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;&#33410;&#28857;&#32423;&#32852;&#37030;&#22270;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#37030;GNN&#26694;&#26550;Lumos&#65292;&#23427;&#20855;&#26377;&#29305;&#24449;&#21644;&#24230;&#25968;&#20445;&#25252;&#21151;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#26641;&#26500;&#36896;&#22120;&#65292;&#20197;&#25552;&#39640;&#22312;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#23621;&#20449;&#24687;&#30340;&#21435;&#20013;&#24515;&#21270;&#33410;&#28857;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) have been widely deployed in real-world networked applications and systems due to their capability to handle graph-structured data. However, the growing awareness of data privacy severely challenges the traditional centralized model training paradigm, where a server holds all the graph information. Federated learning is an emerging collaborative computing paradigm that allows model training without data centralization. Existing federated GNN studies mainly focus on systems where clients hold distinctive graphs or sub-graphs. The practical node-level federated situation, where each client is only aware of its direct neighbors, has yet to be studied. In this paper, we propose the first federated GNN framework called Lumos that supports supervised and unsupervised learning with feature and degree protection on node-level federated graphs. We first design a tree constructor to improve the representation capability given the limited structural information. We fur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#30452;&#25509;&#21046;&#23450;&#25193;&#25955;&#22522;&#20110;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#20808;&#31163;&#25955;&#21270;&#20877;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36991;&#20813;&#21442;&#25968;&#32454;&#21270;&#23548;&#33268;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#32500;&#24230;&#26080;&#20851;&#30340;&#36317;&#31163;&#30028;&#38480;&#65292;&#20026;&#26080;&#38480;&#32500;&#25193;&#25955;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.10130</link><description>&lt;p&gt;
&#26080;&#38480;&#32500;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Infinite-Dimensional Diffusion Models. (arXiv:2302.10130v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#30452;&#25509;&#21046;&#23450;&#25193;&#25955;&#22522;&#20110;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#20808;&#31163;&#25955;&#21270;&#20877;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36991;&#20813;&#21442;&#25968;&#32454;&#21270;&#23548;&#33268;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#32500;&#24230;&#26080;&#20851;&#30340;&#36317;&#31163;&#30028;&#38480;&#65292;&#20026;&#26080;&#38480;&#32500;&#25193;&#25955;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#37117;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#37027;&#20123;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#32500;&#30340;&#39046;&#22495;&#65292;&#22914;&#22270;&#20687;&#25110;&#26102;&#38388;&#24207;&#21015;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#39318;&#20808;&#31163;&#25955;&#21270;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#32454;&#21270;&#31163;&#25955;&#21270;&#21442;&#25968;&#26102;&#36890;&#24120;&#20250;&#23548;&#33268;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30452;&#25509;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#21046;&#23450;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20989;&#25968;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#26080;&#38480;&#32500;&#24230;&#29615;&#22659;&#20013;&#26159;&#33391;&#22909;&#23450;&#20041;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;&#26679;&#26412;&#21040;&#30446;&#26631;&#27979;&#24230;&#30340;&#32500;&#24230;&#26080;&#20851;&#30340;&#36317;&#31163;&#30028;&#38480;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#26080;&#38480;&#32500;&#25193;&#25955;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#21017;&#12290;&#23545;&#20110;&#22270;&#20687;&#20998;&#24067;&#65292;&#36825;&#20123;&#20934;&#21017;&#19982;&#24403;&#21069;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#32463;&#20856;&#36873;&#25321;&#19968;&#33268;&#12290;&#23545;&#20110;&#20854;&#20182;&#20998;&#24067;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models have had a profound impact on many application areas, including those where data are intrinsically infinite-dimensional, such as images or time series. The standard approach is first to discretize and then to apply diffusion models to the discretized data. While such approaches are practically appealing, the performance of the resulting algorithms typically deteriorates as discretization parameters are refined. In this paper, we instead directly formulate diffusion-based generative models in infinite dimensions and apply them to the generative modeling of functions. We prove that our formulations are well posed in the infinite-dimensional setting and provide dimension-independent distance bounds from the sample to the target measure. Using our theory, we also develop guidelines for the design of infinite-dimensional diffusion models. For image distributions, these guidelines are in line with the canonical choices currently made for diffusion models. For other distribut
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Koopman&#31639;&#23376;&#23545;&#20840;&#31209;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#27867;&#21270;&#30340;&#26032;&#30028;&#38480;&#65292;&#24403;&#26435;&#37325;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#36739;&#23567;&#26102;&#65292;&#35813;&#30028;&#38480;&#27604;&#29616;&#26377;&#22522;&#20110;&#33539;&#25968;&#30340;&#30028;&#38480;&#26356;&#32039;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#19981;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#30683;&#30462;&#65292;&#32780;&#26159;&#23545;&#29616;&#26377;&#30028;&#38480;&#36827;&#34892;&#30340;&#34917;&#20805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21487;&#20197;&#19982;&#29616;&#26377;&#30028;&#38480;&#32467;&#21512;&#20197;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#20026;&#29702;&#35299;&#20840;&#31209;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#20063;&#20026;&#31639;&#23376;&#29702;&#35770;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#20043;&#38388;&#25552;&#20379;&#20102;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2302.05825</link><description>&lt;p&gt;
&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#20840;&#31209;&#26435;&#37325;&#30340;&#27867;&#21270;&#30028;&#38480;&#65306;&#26032;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Koopman-based generalization bound: New aspect for full-rank weights. (arXiv:2302.05825v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05825
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Koopman&#31639;&#23376;&#23545;&#20840;&#31209;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#27867;&#21270;&#30340;&#26032;&#30028;&#38480;&#65292;&#24403;&#26435;&#37325;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#36739;&#23567;&#26102;&#65292;&#35813;&#30028;&#38480;&#27604;&#29616;&#26377;&#22522;&#20110;&#33539;&#25968;&#30340;&#30028;&#38480;&#26356;&#32039;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#19981;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#30683;&#30462;&#65292;&#32780;&#26159;&#23545;&#29616;&#26377;&#30028;&#38480;&#36827;&#34892;&#30340;&#34917;&#20805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21487;&#20197;&#19982;&#29616;&#26377;&#30028;&#38480;&#32467;&#21512;&#20197;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#20026;&#29702;&#35299;&#20840;&#31209;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#20063;&#20026;&#31639;&#23376;&#29702;&#35770;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#20043;&#38388;&#25552;&#20379;&#20102;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Koopman&#31639;&#23376;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27867;&#21270;&#30340;&#26032;&#30028;&#38480;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#19978;&#65292;&#32780;&#25105;&#20204;&#19987;&#27880;&#20110;&#20840;&#31209;&#26435;&#37325;&#30697;&#38453;&#12290;&#24403;&#26435;&#37325;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#36739;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#27604;&#29616;&#26377;&#22522;&#20110;&#33539;&#25968;&#30340;&#30028;&#38480;&#26356;&#32039;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#26435;&#37325;&#30697;&#38453;&#26159;&#27491;&#20132;&#30340;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#19982;&#32593;&#32476;&#30340;&#23485;&#24230;&#23436;&#20840;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#19981;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#30683;&#30462;&#65292;&#32780;&#26159;&#23545;&#29616;&#26377;&#30028;&#38480;&#36827;&#34892;&#30340;&#34917;&#20805;&#12290;&#30001;&#20960;&#20010;&#24050;&#26377;&#23454;&#39564;&#35777;&#26126;&#65292;&#20302;&#31209;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#30340;&#21807;&#19968;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21487;&#20197;&#19982;&#29616;&#26377;&#30028;&#38480;&#32467;&#21512;&#20197;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#29702;&#35299;&#20855;&#26377;&#20840;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#36824;&#20026;&#31639;&#23376;&#29702;&#35770;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#20043;&#38388;&#25552;&#20379;&#20102;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new bound for generalization of neural networks using Koopman operators. Whereas most of existing works focus on low-rank weight matrices, we focus on full-rank weight matrices. Our bound is tighter than existing norm-based bounds when the condition numbers of weight matrices are small. Especially, it is completely independent of the width of the network if the weight matrices are orthogonal. Our bound does not contradict to the existing bounds but is a complement to the existing bounds. As supported by several existing empirical results, low-rankness is not the only reason for generalization. Furthermore, our bound can be combined with the existing bounds to obtain a tighter bound. Our result sheds new light on understanding generalization of neural networks with full-rank weight matrices, and it provides a connection between operator-theoretic analysis and generalization of neural networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.02676</link><description>&lt;p&gt;
&#22238;&#39038;&#38142;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#21453;&#39304;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#36825;&#26679;&#25165;&#33021;&#23545;&#20154;&#31867;&#26377;&#25152;&#24110;&#21161;&#24182;&#31526;&#21512;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#26469;&#29702;&#35299;&#21644;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26159;&#22522;&#20110;&#34987;&#20154;&#31867;&#27880;&#37322;&#32773;&#21916;&#27426;&#30340;&#25163;&#21160;&#25361;&#36873;&#30340;&#27169;&#22411;&#29983;&#25104;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#19988;&#26222;&#36941;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22870;&#21169;&#20989;&#25968;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#23481;&#26131;&#20986;&#29616;&#22870;&#21169;&#20989;&#25968;&#19981;&#23436;&#32654;&#21644;&#26497;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#8220;&#22238;&#39038;&#38142;&#8221;&#65292;&#23427;&#26131;&#20110;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#21463;&#21040;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#20197;&#35821;&#35328;&#24418;&#24335;&#21576;&#29616;&#30340;&#24191;&#27867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#31867;&#22411;&#30340;&#21453;&#39304;&#36716;&#25442;&#25104;&#21477;&#23376;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#32780;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.00674</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#26469;&#25913;&#21892;&#23567;&#26679;&#26412;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#26377;&#20215;&#20540;&#65292;&#20294;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#22411;&#19988;&#19981;&#36807;&#24230;&#25311;&#21512;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#28857;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#36741;&#21161;&#25968;&#25454;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65288;FLAD&#65289;&#65292;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#26399;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38543;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#21576;&#32447;&#24615;&#65288;&#25110;&#26356;&#24046;&#65289;&#32553;&#25918;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;FLAD&#19982;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25506;&#32034;&#19982;&#21033;&#29992;&#22256;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25512;&#23548;&#20986;&#31639;&#27861;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#21040;&#27604;&#20808;&#21069;&#26041;&#27861;&#22810;100&#20493;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;EXP3-FLAD&#21644;UCB1-FLAD&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20808;&#21069;&#21482;&#36827;&#34892;&#25506;&#32034;&#25110;&#21033;&#29992;&#30340;FLAD&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#21033;&#29992;DNN&#23545;Banach&#31354;&#38388;&#19978;&#30340;Lipschitz&#31639;&#23376;&#36827;&#34892;&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;PDE&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#65292;&#21253;&#25324;&#26925;&#22278;&#26041;&#31243;&#12289;&#25243;&#29289;&#32447;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#20379;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21270;&#19981;&#21464;&#24615;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.12227</link><description>&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#20943;&#36731;&#20102;PDE&#20013;&#32500;&#25968;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
Deep Operator Learning Lessens the Curse of Dimensionality for PDEs. (arXiv:2301.12227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#21033;&#29992;DNN&#23545;Banach&#31354;&#38388;&#19978;&#30340;Lipschitz&#31639;&#23376;&#36827;&#34892;&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;PDE&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#65292;&#21253;&#25324;&#26925;&#22278;&#26041;&#31243;&#12289;&#25243;&#29289;&#32447;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#20379;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21270;&#19981;&#21464;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#22343;&#24050;&#33719;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;PDE&#30456;&#20851;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;DNNs&#20272;&#35745;&#20102;&#22312;Banach&#31354;&#38388;&#19978;&#23398;&#20064;Lipschitz&#31639;&#23376;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;PDE&#35299;&#31639;&#23376;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25351;&#23450;DNN&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#38656;&#35201;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#65292;&#20197;&#20445;&#35777;&#19968;&#23450;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;&#22312;&#23545;&#25968;&#25454;&#20998;&#24067;&#25110;&#31639;&#23376;&#32467;&#26500;&#36827;&#34892;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#31639;&#23376;&#23398;&#20064;&#21487;&#20197;&#25918;&#26494;&#23545;PDE&#31163;&#25955;&#21270;&#20998;&#36776;&#29575;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#20943;&#36731;&#35768;&#22810;&#19982;PDE&#30456;&#20851;&#30340;&#38382;&#39064;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#65292;&#21253;&#25324;&#26925;&#22278;&#26041;&#31243;&#12289;&#25243;&#29289;&#32447;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#24212;&#29992;&#20110;&#25552;&#20379;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21270;&#19981;&#21464;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and their application to PDE-related problems has been rapidly advancing. This paper provides an estimate for the generalization error of learning Lipschitz operators over Banach spaces using DNNs with applications to various PDE solution operators. The goal is to specify DNN width, depth, and the number of training samples needed to guarantee a certain testing error. Under mild assumptions on data distributions or operator structures, our analysis shows that deep operator learning can have a relaxed dependence on the discretization resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related problems including elliptic equations, parabolic equations, and Burgers equations. Our results are also applied to give insights about discretization-invariant in operator learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05785</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#23454;&#29616;&#39640;&#25928;&#30340;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#35774;&#35745;&#30340;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24456;&#38590;&#26500;&#24314;&#26368;&#20248;&#28608;&#27963;&#20989;&#25968;&#65292;&#32780;&#24403;&#21069;&#30340;&#28608;&#27963;&#20989;&#25968;&#25628;&#32034;&#31639;&#27861;&#36807;&#20110;&#26114;&#36149;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;2,913&#20010;&#31995;&#32479;&#29983;&#25104;&#30340;&#28608;&#27963;&#20989;&#25968;&#20174;&#22836;&#35757;&#32451;&#21367;&#31215;&#12289;&#27531;&#24046;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#21019;&#24314; Act-Bench-CNN&#12289;Act-Bench-ResNet &#21644; Act-Bench-ViT &#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;&#22522;&#20934;&#31354;&#38388;&#65292;&#21457;&#29616;&#19982;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#21644;&#28608;&#27963;&#20989;&#25968;&#36755;&#20986;&#20998;&#24067;&#30456;&#20851;&#32852;&#30340; Fisher &#20449;&#24687;&#30697;&#38453;&#30340;&#39057;&#35889;&#23545;&#24615;&#33021;&#30340;&#39044;&#27979;&#24615;&#24456;&#39640;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#20195;&#29702;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#30340;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#21516;&#26102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#23398;&#20064;k&#32423;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#35777;&#32593;&#32476;&#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#21152;&#24555;&#32593;&#32476;&#35780;&#20272;&#36895;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#20013;&#39044;&#23450;&#20041;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#38477;&#20302;&#32593;&#32476;&#20934;&#30830;&#24230;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.12921</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#30340;&#24191;&#20041;&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#27491;&#21017;&#21270;&#23398;&#20064;k&#32423;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning k-Level Sparse Neural Networks Using a New Generalized Weighted Group Sparse Envelope Regularization. (arXiv:2212.12921v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#23398;&#20064;k&#32423;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#35777;&#32593;&#32476;&#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#21152;&#24555;&#32593;&#32476;&#35780;&#20272;&#36895;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#20013;&#39044;&#23450;&#20041;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#38477;&#20302;&#32593;&#32476;&#20934;&#30830;&#24230;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#26080;&#32467;&#26500;&#21644;&#26377;&#32467;&#26500;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;"&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#20989;&#25968;" (WGSEF) &#30340;&#31232;&#30095;&#21253;&#32476;&#20989;&#25968;&#30340;&#26032;&#24191;&#20041;&#12290;WGSEF&#20316;&#20026;&#19968;&#20010;&#31070;&#32463;&#20803;&#32452;&#36873;&#25321;&#22120;&#65292;&#29992;&#20110;&#24341;&#23548;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#20445;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNN) &#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#20197;&#26377;&#25928;&#21152;&#36895;DNN&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26159;&#21487;&#36866;&#24212;&#30340;&#65292;&#20801;&#35768;&#20219;&#20309;&#30828;&#20214;&#25351;&#23450;&#32452;&#23450;&#20041;&#65292;&#22914;&#28388;&#27874;&#22120;&#12289;&#36890;&#36947;&#12289;&#28388;&#27874;&#22120;&#24418;&#29366;&#12289;&#23618;&#28145;&#24230;&#12289;&#21333;&#20010;&#21442;&#25968; (&#26080;&#32467;&#26500;)&#31561;&#12290;&#30001;&#20110;WGSEF&#30340;&#29305;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#25910;&#25947;&#26102;&#39044;&#23450;&#20041;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#21516;&#26102;&#20445;&#25345;&#32593;&#32476;&#20934;&#30830;&#24230;&#30340;&#26497;&#23567;&#38477;&#20302;&#29978;&#33267;&#25913;&#21892;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#35745;&#31639;&#31934;&#30830;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to learn both unstructured and structured sparse neural networks during training, utilizing a novel generalization of the sparse envelope function (SEF) used as a regularizer, termed {\itshape{weighted group sparse envelope function}} (WGSEF). The WGSEF acts as a neuron group selector, which is leveraged to induce structured sparsity. The method ensures a hardware-friendly structured sparsity of a deep neural network (DNN) to efficiently accelerate the DNN's evaluation. Notably, the method is adaptable, letting any hardware specify group definitions, such as filters, channels, filter shapes, layer depths, a single parameter (unstructured), etc. Owing to the WGSEF's properties, the proposed method allows to a pre-define sparsity level that would be achieved at the training convergence, while maintaining negligible network accuracy degradation or even improvement in the case of redundant parameters. We introduce an efficient technique to calculate the exact
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#36890;&#36807;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#31181;&#23376;&#25968;&#25454;&#20013;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#20445;&#25345;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#26469;&#25351;&#23548;&#24819;&#35937;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.13976</link><description>&lt;p&gt;
&#29992;&#24341;&#23548;&#24819;&#35937;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Expanding Small-Scale Datasets with Guided Imagination. (arXiv:2211.13976v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#36890;&#36807;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#31181;&#23376;&#25968;&#25454;&#20013;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#20445;&#25345;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#26469;&#25351;&#23548;&#24819;&#35937;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#30340;&#21151;&#25928;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#36890;&#24120;&#36153;&#26102;&#36153;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#39033;&#21517;&#20026;&#25968;&#25454;&#38598;&#25193;&#20805;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#21019;&#24314;&#26032;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#25193;&#20805;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#23574;&#31471;&#29983;&#25104;&#27169;&#22411;&#30340;&#21147;&#37327;&#65292;&#20174;&#36755;&#20837;&#30340;&#31181;&#23376;&#25968;&#25454;&#20013;&#8220;&#24819;&#35937;&#8221;&#24182;&#21019;&#24314;&#20449;&#24687;&#20016;&#23500;&#30340;&#26032;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GIF&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#30340;&#28508;&#22312;&#29305;&#24449;&#26469;&#36827;&#34892;&#25968;&#25454;&#30340;&#24819;&#35937;&#65292;&#20174;&#32780;&#21019;&#24314;&#20855;&#26377;&#26032;&#20869;&#23481;&#30340;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#24341;&#23548;&#24819;&#35937;&#26397;&#30528;&#21019;&#24314;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#20449;&#24687;&#20016;&#23500;&#26679;&#26412;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#26631;&#20934;&#65292;&#21363;&#31867;&#21035;&#20445;&#25345;&#20449;&#24687;&#25552;&#21319;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#20419;&#36827;&#12290;&#36825;&#20123;&#26631;&#20934;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of DNNs relies heavily on the quantity and quality of training data. However, collecting and annotating data on a large scale is often expensive and time-consuming. To address this issue, we explore a new task, termed dataset expansion, aimed at expanding a ready-to-use small dataset by automatically creating new labeled samples. To this end, we present a Guided Imagination Framework (GIF) that leverages cutting-edge generative models like DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data from the input seed data. Specifically, GIF conducts data imagination by optimizing the latent features of the seed data in the semantically meaningful space of the prior model, resulting in the creation of photo-realistic images with new content. To guide the imagination towards creating informative samples for model training, we introduce two key criteria, i.e., class-maintained information boosting and sample diversity promotion. These criteria are verified to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#30340;&#28508;&#31354;&#38388;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#33268;&#27169;&#25311;&#30340;&#24674;&#22797;&#12290;&#36890;&#36807;&#23558;&#27169;&#25311;&#30340;&#33258;&#30001;&#24230;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#20855;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#38477;&#32500;&#28508;&#31354;&#38388;&#36827;&#34892;&#23398;&#20064;&#65292;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#21457;&#29616;&#25913;&#36827;&#32473;&#23450;&#20219;&#21153;&#24615;&#33021;&#30340;&#26367;&#20195;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2211.11298</link><description>&lt;p&gt;
&#25506;&#32034;&#29289;&#29702;&#28508;&#31354;&#38388;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#27969;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Exploring Physical Latent Spaces for High-Resolution Flow Restoration. (arXiv:2211.11298v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#30340;&#28508;&#31354;&#38388;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#33268;&#27169;&#25311;&#30340;&#24674;&#22797;&#12290;&#36890;&#36807;&#23558;&#27169;&#25311;&#30340;&#33258;&#30001;&#24230;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#20855;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#38477;&#32500;&#28508;&#31354;&#38388;&#36827;&#34892;&#23398;&#20064;&#65292;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#21457;&#29616;&#25913;&#36827;&#32473;&#23450;&#20219;&#21153;&#24615;&#33021;&#30340;&#26367;&#20195;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#29289;&#29702;&#27169;&#25311;&#19982;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#27169;&#25311;&#33258;&#30001;&#24230;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#23558;&#27169;&#25311;&#31354;&#38388;&#30340;&#33258;&#30001;&#24230;&#32431;&#31929;&#35270;&#20026;&#31070;&#32463;&#32593;&#32476;&#35201;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27010;&#24565;&#29992;&#20110;&#23398;&#20064;&#38477;&#32500;&#34920;&#31034;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#38477;&#32500;&#34920;&#31034;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#19978;&#24544;&#23454;&#22320;&#20445;&#30041;&#27491;&#30830;&#35299;&#20915;&#26041;&#26696;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#23567;&#23610;&#24230;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#20351;&#29992;&#36825;&#31181;&#29289;&#29702;&#38477;&#32500;&#28508;&#31354;&#38388;&#26469;&#24674;&#22797;&#31934;&#32454;&#27169;&#25311;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#23398;&#20064;&#30446;&#26631;&#23613;&#21487;&#33021;&#22810;&#22320;&#20462;&#25913;&#38477;&#32500;&#30340;&#29289;&#29702;&#29366;&#24577;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#33258;&#20027;&#24615;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#25913;&#36827;&#32473;&#23450;&#20219;&#21153;&#24615;&#33021;&#30340;&#26367;&#20195;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore training deep neural network models in conjunction with physics simulations via partial differential equations (PDEs), using the simulated degrees of freedom as latent space for a neural network. In contrast to previous work, this paper treats the degrees of freedom of the simulated space purely as tools to be used by the neural network. We demonstrate this concept for learning reduced representations, as it is extremely challenging to faithfully preserve correct solutions over long time-spans with traditional reduced representations, particularly for solutions with large amounts of small scale features. This work focuses on the use of such physical, reduced latent space for the restoration of fine simulations, by training models that can modify the content of the reduced physical states as much as needed to best satisfy the learning objective. This autonomy allows the neural networks to discover alternate dynamics that significantly improve the performance in the given task
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#24773;&#33410;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#21644;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23548;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#39044;&#26399;&#36951;&#25022;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#22343;&#20026;&#24773;&#33410;&#25968;&#30340;&#24179;&#26041;&#26681;&#38454;&#12290;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00832</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24773;&#33410;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24179;&#26041;&#26681;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Square-root regret bounds for continuous-time episodic Markov decision processes. (arXiv:2210.00832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#24773;&#33410;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#21644;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23548;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#39044;&#26399;&#36951;&#25022;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#22343;&#20026;&#24773;&#33410;&#25968;&#30340;&#24179;&#26041;&#26681;&#38454;&#12290;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#24773;&#33410;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#31163;&#25955;&#26102;&#38388;MDPs&#19981;&#21516;&#65292;&#36830;&#32493;&#26102;&#38388;MDPs&#30340;&#36716;&#25442;&#26102;&#38388;&#22312;&#27599;&#27425;&#36716;&#25442;&#26102;&#20250;&#25353;&#25351;&#25968;&#20998;&#24067;&#65292;&#24182;&#19988;&#36895;&#29575;&#21442;&#25968;&#21462;&#20915;&#20110;&#27599;&#20010;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#21644;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#39044;&#26399;&#36951;&#25022;&#30340;&#19978;&#30028;&#65292;&#24182;&#24314;&#31435;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#19979;&#30028;&#65292;&#36825;&#20004;&#20010;&#30028;&#37117;&#19982;&#24773;&#33410;&#25968;&#30340;&#24179;&#26041;&#26681;&#26377;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#26469;&#35828;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning for continuous-time Markov decision processes (MDPs) in the finite-horizon episodic setting. In contrast to discrete-time MDPs, the inter-transition times of a continuous-time MDP are exponentially distributed with rate parameters depending on the state--action pair at each transition. We present a learning algorithm based on the methods of value iteration and upper confidence bound. We derive an upper bound on the worst-case expected regret for the proposed algorithm, and establish a worst-case lower bound, both bounds are of the order of square-root on the number of episodes. Finally, we conduct simulation experiments to illustrate the performance of our algorithm.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#20013;&#20272;&#35745;&#26410;&#37319;&#26679;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.09404</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#27714;&#35299;&#22823;&#35268;&#27169;&#21452;&#23618;&#21644;&#38543;&#26426;&#35268;&#21010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;: &#20197;&#33258;&#34892;&#36710;&#32593;&#32476;&#35774;&#35745;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach to Solving Large Bilevel and Stochastic Programs: Application to Cycling Network Design. (arXiv:2209.09404v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09404
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#20013;&#20272;&#35745;&#26410;&#37319;&#26679;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#29305;&#27530;&#24773;&#20917;&#21253;&#25324;&#20004;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#27169;&#22411;&#65292;&#26126;&#30830;&#32771;&#34385;&#21040;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#30340;&#20174;&#23646;&#32773;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20272;&#35745;&#26410;&#37319;&#26679;&#30340;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#23884;&#20837;&#21040;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#20351;&#29992;&#26080;&#27861;&#36890;&#36807;&#39046;&#23548;&#32773;&#20915;&#31574;&#34920;&#31034;&#30340;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#39046;&#23548;&#32773;&#20915;&#31574;&#22312;&#21407;&#22987;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#38388;&#38553;&#19978;&#30340;&#30028;&#38480;&#65292;&#35813;&#30446;&#26631;&#20989;&#25968;&#32771;&#34385;&#21040;&#23436;&#25972;&#30340;&#20174;&#23646;&#32773;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20174;&#23646;&#32773;&#37319;&#26679;&#31639;&#27861;&#26469;&#32553;&#23567;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#20174;&#23646;&#32773;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20316;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#33258;&#34892;&#36710;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#30340;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present a novel machine learning-based approach to solving bilevel programs that involve a large number of independent followers, which as a special case include two-stage stochastic programming. We propose an optimization model that explicitly considers a sampled subset of followers and exploits a machine learning model to estimate the objective values of unsampled followers. Unlike existing approaches, we embed machine learning model training into the optimization problem, which allows us to employ general follower features that can not be represented using leader decisions. We prove bounds on the optimality gap of the generated leader decision as measured by the original objective function that considers the full follower set. We then develop follower sampling algorithms to tighten the bounds and a representation learning approach to learn follower features, which can be used as inputs to the embedded machine learning model. Using synthetic instances of a cycling network design p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2208.11838</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#20855;&#26377;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#37327;&#22870;&#21169;&#20449;&#21495;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#25163;&#24037;&#21019;&#24314;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#24448;&#24448;&#23481;&#26131;&#20986;&#38169;&#65292;&#29305;&#21035;&#26159;&#24403;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#21482;&#26377;&#37096;&#20998;&#24050;&#30693;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#20195;&#29702;&#32463;&#39564;&#30340; episodes &#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#31616;&#27905;&#26377;&#38480;&#29366;&#24577;&#8220;&#20219;&#21153;&#33258;&#21160;&#26426;&#8221;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#31639;&#27861;&#30340;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20135;&#21697; MDP &#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979; MDP &#24182;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340; Baum-Welch &#31639;&#27861;&#26469;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#20102;&#35268;&#33539;&#33258;&#21160;&#26426;&#21644;&#29615;&#22659;&#30340; MDP&#65288;&#21021;&#22987;&#37117;&#26410;&#30693;&#65289;&#25152;&#32452;&#25104;&#30340;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#23398;&#21040;&#30340;&#20135;&#21697; MDP &#20013;&#25552;&#28860;&#20219;&#21153;&#33258;&#21160;&#26426;&#65288;&#20551;&#35774;&#20026;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65289;&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#20351;&#24471;&#20195;&#29702;&#21487;&#20197;&#35299;&#20915;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training reinforcement learning (RL) agents using scalar reward signals is often infeasible when an environment has sparse and non-Markovian rewards. Moreover, handcrafting these reward functions before training is prone to misspecification, especially when the environment's dynamics are only partially known. This paper proposes a novel pipeline for learning non-Markovian task specifications as succinct finite-state `task automata' from episodes of agent experience within unknown environments. We leverage two key algorithmic insights. First, we learn a product MDP, a model composed of the specification's automaton and the environment's MDP (both initially unknown), by treating the product MDP as a partially observable MDP and using the well-known Baum-Welch algorithm for learning hidden Markov models. Second, we propose a novel method for distilling the task automaton (assumed to be a deterministic finite automaton) from the learnt product MDP. Our learnt task automaton enables the dec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2208.07497</link><description>&lt;p&gt;
&#23398;&#20064;ACOPF&#30340;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Bucketized Active Sampling for Learning ACOPF. (arXiv:2208.07497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#30340;&#20248;&#21270;&#20195;&#29702;&#65292;&#21363;&#36817;&#20284;OPF&#30340;&#36755;&#20837;/&#36755;&#20986;&#20851;&#31995;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35777;&#26126;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#38656;&#35201;&#23545;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;OPF&#30340;&#65288;&#31163;&#32447;&#65289;&#27714;&#35299;&#12290;&#20026;&#20102;&#28385;&#36275;&#24066;&#22330;&#28165;&#31639;&#24212;&#29992;&#30340;&#35201;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#23558;&#30456;&#21516;&#30340;&#20998;&#26742;&#24212;&#29992;&#20110;&#39564;&#35777;&#38598;&#65292;BAS&#21033;&#29992;&#26631;&#35760;&#30340;&#39564;&#35777;&#26679;&#26412;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;BAS&#36824;&#20381;&#36182;&#20110;&#38543;&#26102;&#38388;&#22686;&#21152;&#21644;&#20943;&#23569;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers optimization proxies for Optimal Power Flow (OPF), i.e., machine-learning models that approximate the input/output relationship of OPF. Recent work has focused on showing that such proxies can be of high fidelity. However, their training requires significant data, each instance necessitating the (offline) solving of an OPF for a sample of the input distribution. To meet the requirements of market-clearing applications, this paper proposes Bucketized Active Sampling (BAS), a novel active learning framework that aims at training the best possible OPF proxy within a time limit. BAS partitions the input distribution into buckets and uses an acquisition function to determine where to sample next. By applying the same partitioning to the validation set, BAS leverages labeled validation samples in the selection of unlabeled samples. BAS also relies on an adaptive learning rate that increases and decreases over time. Experimental results demonstrate the benefits of BAS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05424</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#27668;&#20505;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Physics-Constrained Deep Learning for Climate Downscaling. (arXiv:2208.05424v6 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for physics-constrained deep learning downscaling models to ensure that the models satisfy conservation laws when predicting physical variables, while improving their performance according to traditional metrics.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#20110;&#25351;&#23548;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#30340;&#38271;&#26399;&#20915;&#31574;&#20197;&#21450;&#25351;&#23548;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#24555;&#36895;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#27169;&#22411;&#21463;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#65292;&#22240;&#27492;&#36890;&#24120;&#29983;&#25104;&#31895;&#20998;&#36776;&#29575;&#39044;&#27979;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#19978;&#37319;&#26679;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#35270;&#35273;&#19978;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#32463;&#24120;&#36829;&#21453;&#23432;&#24658;&#23450;&#24459;&#12290;&#20026;&#20102;&#20445;&#25345;&#29289;&#29702;&#37327;&#30340;&#23432;&#24658;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20445;&#35777;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#28385;&#36275;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#21516;&#26102;&#26681;&#25454;&#20256;&#32479;&#25351;&#26631;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32422;&#26463;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#21450;&#21508;&#31181;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and, therefore, often generate coarse-resolution predictions. Statistical downscaling, including super-resolution methods from deep learning, can provide an efficient method of upsampling low-resolution data. However, despite achieving visually compelling results in some cases, such models frequently violate conservation laws when predicting physical variables. In order to conserve physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep learning downscaling model while also improving their performance according to traditional metrics. We compare different constraining approaches and demonstrate their applicability across different neural architectures as well as a variety of climate and weather
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.14219</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#40077;&#22411;&#33258;&#36866;&#24212;&#24322;&#26041;&#24046;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v7 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#31639;&#27861;&#65292;&#21517;&#20026;&#33258;&#36866;&#24212;&#38598;&#25104;&#25209;&#37327;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#40077;&#22411;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;AEnbMIMOCQR&#65289;&#65292;&#20351;&#24471;&#39044;&#27979;&#32773;&#33021;&#22815;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22266;&#23450;&#39044;&#35774;&#22833;&#37197;&#29575;&#30340;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#40077;&#22411;&#39044;&#27979;&#21407;&#29702;&#65292;&#20294;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#25968;&#25454;&#19981;&#21487;&#20114;&#25442;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#25509;&#36817;&#31934;&#30830;&#30340;&#35206;&#30422;&#29575;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#39044;&#27979;&#21306;&#38388;&#22312;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#32463;&#39564;&#35777;&#26126;&#26377;&#25928;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#12290;AEnbMIMOCQR&#34987;&#35774;&#35745;&#25104;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#20854;&#39044;&#27979;&#21306;&#38388;&#22312;&#26080;&#38480;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#21487;&#38752;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#19981;&#20999;&#23454;&#38469;&#30340;&#20005;&#26684;&#20551;&#35774;&#12290;&#36890;&#36807;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40077;&#22411;&#39044;&#27979;&#20013;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel model-agnostic algorithm called adaptive ensemble batch multi-input multi-output conformalized quantile regression (AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction intervals for a fixed pre-specified miscoverage rate in a distribution-free manner. Our method is grounded on conformal prediction principles, however, it does not require data splitting and provides close to exact coverage even when the data is not exchangeable. Moreover, the resulting prediction intervals, besides being empirically valid along the forecast horizon, do not neglect heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution shifts, which means that its prediction intervals remain reliable over an unlimited period of time, without entailing retraining or imposing unrealistic strict assumptions on the data-generating process. Through methodically experimentation, we demonstrate that our approach outperforms other competitive methods on bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#20869;&#23384;&#24322;&#24120;&#20540;&#28040;&#38500;&#65288;MOE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26469;&#33258;&#26631;&#31614;&#21516;&#36136;&#23376;&#31181;&#32676;&#30340;&#26679;&#26412;&#26469;&#35782;&#21035;&#21644;&#28040;&#38500;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.01145</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#24120;&#20540;&#28040;&#38500;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#22635;&#20805;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Memory Population in Continual Learning via Outlier Elimination. (arXiv:2207.01145v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#20869;&#23384;&#24322;&#24120;&#20540;&#28040;&#38500;&#65288;MOE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26469;&#33258;&#26631;&#31614;&#21516;&#36136;&#23376;&#31181;&#32676;&#30340;&#26679;&#26412;&#26469;&#35782;&#21035;&#21644;&#28040;&#38500;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#21457;&#23637;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#32531;&#35299;&#36951;&#24536;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#20869;&#23384;&#32531;&#20914;&#21306;&#65292;&#22312;&#35757;&#32451;&#26032;&#20219;&#21153;&#26102;&#23384;&#20648;&#20043;&#21069;&#23398;&#20064;&#36807;&#30340;&#20219;&#21153;&#31034;&#20363;&#30340;&#23376;&#38598;&#12290;&#22635;&#20805;&#20869;&#23384;&#30340;&#40664;&#35748;&#26041;&#27861;&#26159;&#38543;&#26426;&#36873;&#25321;&#20808;&#21069;&#30340;&#31034;&#20363;&#65292;&#20294;&#36825;&#21487;&#33021;&#24341;&#20837;&#31163;&#32676;&#20540;&#25110;&#22122;&#22768;&#26679;&#26412;&#65292;&#20174;&#32780;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20869;&#23384;&#24322;&#24120;&#20540;&#28040;&#38500;&#65288;MOE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26469;&#33258;&#26631;&#31614;&#21516;&#36136;&#23376;&#31181;&#32676;&#30340;&#26679;&#26412;&#26469;&#35782;&#21035;&#21644;&#28040;&#38500;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39640;&#21516;&#36136;&#24615;&#30340;&#31354;&#38388;&#19982;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#31354;&#38388;&#30456;&#20851;&#32852;&#30340;&#20851;&#31995;&#12290;&#23454;&#38469;&#19978;&#65292;&#22914;&#26524;&#19968;&#20010;&#26679;&#26412;&#34987;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#25152;&#21253;&#22260;&#65292;MOE&#20250;&#31227;&#38500;&#35813;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;CO&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;MOE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting, the phenomenon of forgetting previously learned tasks when learning a new one, is a major hurdle in developing continual learning algorithms. A popular method to alleviate forgetting is to use a memory buffer, which stores a subset of previously learned task examples for use during training on new tasks. The de facto method of filling memory is by randomly selecting previous examples. However, this process could introduce outliers or noisy samples that could hurt the generalization of the model. This paper introduces Memory Outlier Elimination (MOE), a method for identifying and eliminating outliers in the memory buffer by choosing samples from label-homogeneous subpopulations. We show that a space with a high homogeneity is related to a feature space that is more representative of the class distribution. In practice, MOE removes a sample if it is surrounded by samples from different labels. We demonstrate the effectiveness of MOE on CIFAR-10, CIFAR-100, and CO
&lt;/p&gt;</description></item><item><title>&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.04661</link><description>&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Distillation Decision Tree. (arXiv:2206.04661v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04661
&lt;/p&gt;
&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#25209;&#35780;&#21644;&#25361;&#25112;&#12290;&#30683;&#30462;&#30340;&#26159;&#65292;&#23427;&#20204;&#24378;&#22823;&#30340;&#39044;&#27979;&#33021;&#21147;&#34920;&#26126;&#23545;&#24213;&#23618;&#25968;&#25454;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#24847;&#21619;&#30528;&#37325;&#35201;&#30340;&#35299;&#37322;&#28508;&#21147;&#12290;&#20511;&#21161;&#30693;&#35782;&#31934;&#39311;&#30340;&#26032;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20851;&#20110;&#25968;&#25454;&#30340;&#30693;&#35782;&#20174;&#40657;&#30418;&#27169;&#22411;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#30693;&#35782;&#31934;&#39311;&#36807;&#31243;&#26500;&#24314;&#30340;DDT&#30340;&#21487;&#35299;&#37322;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20854;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#20026;DDT&#30340;&#32467;&#26500;&#31283;&#23450;&#24615;&#24314;&#31435;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#35777;&#26126;&#20854;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#32467;&#26500;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#31639;&#27861;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly the black-box models, are widely favored for their outstanding predictive capabilities. However, they often face scrutiny and criticism due to the lack of interpretability. Paradoxically, their strong predictive capabilities suggest a deep understanding about the underlying data, implying significant potential for interpretation. Leveraging the emerging concept of knowledge distillation, we introduced the method of distillation decision tree (DDT). This method enables the distillation of knowledge about the data from a black-box model into a decision tree, thereby facilitating the interpretation of the black-box model. Constructed through the knowledge distillation process, the interpretability of DDT relies significantly on the stability of its structure. We establish the theoretical foundations for the structural stability of DDT, demonstrating that its structure can achieve stability under mild assumptions. Furthermore, we develop algorithms for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#19981;&#21464;&#24615;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#21407;&#29702;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;&#12290;&#26041;&#27861;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#24418;&#24335;&#21270;&#65292;&#33021;&#22815;&#22312;&#23616;&#37096;&#20363;&#23376;&#38468;&#36817;&#20934;&#30830;&#25429;&#25417;&#40657;&#30418;&#20989;&#25968;&#30340;&#26799;&#24230;&#31526;&#21495;&#21464;&#21270;&#65292;&#36873;&#25321;&#24688;&#24403;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2201.12143</link><description>&lt;p&gt;
&#23616;&#37096;&#19981;&#21464;&#24615;&#35299;&#37322;&#65306;&#36890;&#36807;&#23616;&#37096;&#19981;&#21464;&#24615;&#23398;&#20064;&#23454;&#29616;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning. (arXiv:2201.12143v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#19981;&#21464;&#24615;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#21407;&#29702;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;&#12290;&#26041;&#27861;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#24418;&#24335;&#21270;&#65292;&#33021;&#22815;&#22312;&#23616;&#37096;&#20363;&#23376;&#38468;&#36817;&#20934;&#30830;&#25429;&#25417;&#40657;&#30418;&#20989;&#25968;&#30340;&#26799;&#24230;&#31526;&#21495;&#21464;&#21270;&#65292;&#36873;&#25321;&#24688;&#24403;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65288;LIME&#65289;&#26041;&#27861;&#26159;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21464;&#31181;&#65292;&#20294;&#24456;&#23569;&#26377;&#26041;&#27861;&#33021;&#22815;&#31616;&#21333;&#22320;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21448;&#31283;&#23450;&#19988;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#36890;&#36807;&#20511;&#37492;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#21407;&#29702;&#65288;&#26368;&#21021;&#29992;&#20110;&#20840;&#23616;&#30340;&#26679;&#26412;&#22806;&#27867;&#21270;&#65289;&#26469;&#25552;&#20379;&#36825;&#31181;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#12289;&#31283;&#23450;&#19988;&#21333;&#21521;&#30340;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#24418;&#24335;&#21270;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23547;&#25214;&#35299;&#37322;&#30340;&#23616;&#37096;&#20363;&#23376;&#38468;&#36817;&#40657;&#30418;&#20989;&#25968;&#26799;&#24230;&#31361;&#28982;&#25913;&#21464;&#31526;&#21495;&#30340;&#29305;&#24449;&#26102;&#20855;&#26377;&#24456;&#24378;&#30340;&#20542;&#21521;&#24615;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#36873;&#25321;&#26356;&#20445;&#23432;&#30340;&#65288;&#29305;&#24449;&#65289;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -originally proposed for (global) out-of-distribution generalization -- to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;Sci-Net&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.06812</link><description>&lt;p&gt;
Sci-Net: &#23610;&#24230;&#19981;&#21464;&#27169;&#22411;&#29992;&#20110;&#20174;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;
&lt;/p&gt;
&lt;p&gt;
Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery. (arXiv:2111.06812v5 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;Sci-Net&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#29289;&#30340;&#20998;&#21106;&#26159;&#22320;&#29699;&#35266;&#27979;&#21644;&#33322;&#31354;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#37096;&#20998;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21482;&#33021;&#24212;&#29992;&#20110;&#22266;&#23450;&#25110;&#29421;&#31364;&#33539;&#22260;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#22788;&#29702;&#30340;&#26159;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#36776;&#29575;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;&#32473;&#23450;&#30340;&#33322;&#31354;&#22270;&#20687;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#37325;&#37319;&#26679;&#20197;&#21305;&#37197;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#65292;&#36825;&#23548;&#33268;&#20998;&#21106;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#23610;&#24230;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;Sci-Net&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;UNet&#30340;&#23618;&#27425;&#34920;&#31034;&#21644;Dense Atrous Spatial Pyramid Pooling&#26469;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;Sci-Net&#22312;Open Cities AI&#21644;Multi-Scale Building&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Buildings' segmentation is a fundamental task in the field of earth observation and aerial imagery analysis. Most existing deep learning-based methods in the literature can be applied to a fixed or narrow-range spatial resolution imagery. In practical scenarios, users deal with a broad spectrum of image resolutions. Thus, a given aerial image often needs to be re-sampled to match the spatial resolution of the dataset used to train the deep learning model, which results in a degradation in segmentation performance. To overcome this challenge, we propose, in this manuscript, Scale-invariant Neural Network (Sci-Net) architecture that segments buildings from wide-range spatial resolution aerial images. Specifically, our approach leverages UNet hierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract fine-grained multi-scale representations. Sci-Net significantly outperforms state of the art models on the Open Cities AI and the Multi-Scale Building datasets with a ste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2109.03459</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#20110;&#25490;&#21517;&#33976;&#39311;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#35757;&#32451;&#20805;&#20998;&#30340;&#22823;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#23567;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#32780;&#35328;&#65292;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#36817;&#65292;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#65288;RRD&#65289;&#34920;&#26126;&#65292;&#22312;&#25512;&#33616;&#21015;&#34920;&#20013;&#33976;&#39311;&#25490;&#21517;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;1&#65289;&#23427;&#26410;&#20805;&#20998;&#21033;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#20351;&#24471;&#35757;&#32451;&#25928;&#29575;&#19981;&#39640;&#65307;2&#65289;&#23427;&#21482;&#33976;&#39311;&#29992;&#25143;&#20391;&#30340;&#25490;&#21517;&#20449;&#24687;&#65292;&#22312;&#31232;&#30095;&#30340;&#38544;&#24335;&#21453;&#39304;&#19979;&#25552;&#20379;&#30340;&#35270;&#35282;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#21363;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20915;&#23450;&#35201;&#33976;&#39311;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be disti
&lt;/p&gt;</description></item><item><title>&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#20197;&#30830;&#20445;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#26159;&#23454;&#29616;&#21487;&#38752;AI&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#65292;&#23545;&#25163;&#21487;&#20197;&#25925;&#24847;&#35302;&#21457;&#21518;&#22791;&#31574;&#30053;&#65292;&#20351;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#21487;&#29992;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2108.11299</link><description>&lt;p&gt;
Certifiers&#20351;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#21487;&#29992;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certifiers Make Neural Networks Vulnerable to Availability Attacks. (arXiv:2108.11299v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11299
&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#20197;&#30830;&#20445;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#26159;&#23454;&#29616;&#21487;&#38752;AI&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#65292;&#23545;&#25163;&#21487;&#20197;&#25925;&#24847;&#35302;&#21457;&#21518;&#22791;&#31574;&#30053;&#65292;&#20351;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#21487;&#29992;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#12289;&#40065;&#26834;&#21644;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#22312;&#19981;&#33021;&#20449;&#20219;AI&#39044;&#27979;&#26102;&#23454;&#26045;&#21518;&#22791;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35748;&#35777;&#26159;&#26816;&#26597;&#36825;&#20123;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#20182;&#20204;&#20445;&#35777;&#23545;&#20110;&#19968;&#20123;&#39044;&#27979;&#65292;&#26576;&#20123;&#25805;&#32437;&#25110;&#25915;&#20987;&#26041;&#24335;&#19981;&#20250;&#25913;&#21464;&#32467;&#26524;&#12290;&#23545;&#20110;&#27809;&#26377;&#20445;&#35777;&#30340;&#20854;&#20182;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#20250;&#25918;&#24323;&#39044;&#27979;&#65292;&#24182;&#38656;&#35201;&#35843;&#29992;&#21518;&#22791;&#31574;&#30053;&#65292;&#36825;&#36890;&#24120;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#21487;&#33021;&#38656;&#35201;&#20154;&#24037;&#25805;&#20316;&#65292;&#29978;&#33267;&#26080;&#27861;&#25552;&#20379;&#20219;&#20309;&#39044;&#27979;&#12290;&#34429;&#28982;&#36825;&#26159;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#20294;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#33258;&#36523;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#20197;&#25925;&#24847;&#35302;&#21457;&#36825;&#20123;&#21518;&#22791;&#31574;&#30053;&#12290;&#38500;&#20102;&#26576;&#20123;&#36755;&#20837;&#21644;&#25200;&#21160;&#33258;&#28982;&#21457;&#29983;&#26102;&#30340;&#25918;&#24323;&#22806;&#65292;&#23545;&#25163;&#36824;&#21487;&#20197;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#25915;&#20987;&#26041;&#24335;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#25925;&#24847;&#35302;&#21457;&#21518;&#22791;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve reliable, robust, and safe AI systems, it is vital to implement fallback strategies when AI predictions cannot be trusted. Certifiers for neural networks are a reliable way to check the robustness of these predictions. They guarantee for some predictions that a certain class of manipulations or attacks could not have changed the outcome. For the remaining predictions without guarantees, the method abstains from making a prediction, and a fallback strategy needs to be invoked, which typically incurs additional costs, can require a human operator, or even fail to provide any prediction. While this is a key concept towards safe and secure AI, we show for the first time that this approach comes with its own security risks, as such fallback strategies can be deliberately triggered by an adversary. In addition to naturally occurring abstains for some inputs and perturbations, the adversary can use training-time attacks to deliberately trigger the fallback with high probability. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#34917;&#19969;&#23376;&#31354;&#38388;&#23398;&#20064;&#33258;&#21160;&#32534;&#30721;&#22120; (PSL-AE) &#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#30456;&#26426;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24322;&#36136;&#20266;&#24433;&#65288;&#22914;&#22270;&#20687;&#38477;&#22122;&#65289;&#26102;&#12290;</title><link>http://arxiv.org/abs/2104.00253</link><description>&lt;p&gt;
&#28145;&#24230;&#23545;&#27604;&#22522;&#20110;&#34917;&#19969;&#30340;&#23376;&#31354;&#38388;&#23398;&#20064;&#29992;&#20110;&#30456;&#26426;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal Processing. (arXiv:2104.00253v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#34917;&#19969;&#23376;&#31354;&#38388;&#23398;&#20064;&#33258;&#21160;&#32534;&#30721;&#22120; (PSL-AE) &#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#30456;&#26426;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24322;&#36136;&#20266;&#24433;&#65288;&#22914;&#22270;&#20687;&#38477;&#22122;&#65289;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702; (ISP) &#31649;&#36947;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#20013;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28145;&#24230;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#19968;&#33268;&#24212;&#29992;&#20110;&#25972;&#20010;&#22270;&#20687;&#30340;&#32479;&#19968;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#22320;&#23558;&#30456;&#26426;&#22270;&#20687;&#35270;&#20026;&#24322;&#36136;&#30340;&#65292;&#22240;&#20026;&#39068;&#33394;&#24378;&#24230;&#21644;&#20154;&#24037;&#22122;&#22768;&#20998;&#24067;&#22312;&#21333;&#20010;&#22270;&#20687;&#30340;&#20108;&#32500;&#22495;&#20869;&#37117;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#24322;&#12290;&#21508;&#31181;&#21508;&#26679;&#30340;&#33707;&#23572;&#29615;&#65292;&#36816;&#21160;&#27169;&#31946;&#65292;&#35114;&#33394;&#25110;&#22522;&#20110;&#38236;&#22836;&#30340;&#25237;&#24433;&#22833;&#30495;&#37117;&#26377;&#21487;&#33021;&#23548;&#33268;&#24322;&#36136;&#22270;&#20687;&#20266;&#24433;&#28388;&#27874;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#23616;&#37096;&#23376;&#31354;&#38388;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25913;&#36827;&#20102;&#30456;&#26426;ISP&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#23545;&#24322;&#36136;&#20266;&#24433;&#65288;&#29305;&#21035;&#26159;&#22270;&#20687;&#38477;&#22122;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#19977;&#37325;&#28145;&#24230;&#35757;&#32451;&#27169;&#22411;&#31216;&#20026;&#34917;&#19969;&#23376;&#31354;&#38388;&#23398;&#20064;&#33258;&#21160;&#32534;&#30721;&#22120; (PSL-AE)&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera Image Signal Processing (ISP) pipelines can get appealing results in different image signal processing tasks. Nonetheless, the majority of these methods, including those employing an encoder-decoder deep architecture for the task, typically utilize a uniform filter applied consistently across the entire image. However, it is natural to view a camera image as heterogeneous, as the color intensity and the artificial noise are distributed vastly differently, even across the two-dimensional domain of a single image. Varied Moire ringing, motion blur, color-bleaching, or lens-based projection distortions can all potentially lead to a heterogeneous image artifact filtering problem. In this paper, we present a specific patch-based, local subspace deep neural network that improves Camera ISP to be robust to heterogeneous artifacts (especially image denoising). We call our three-fold deep-trained model the Patch Subspace Learning Autoencoder (PSL-AE). The PSL-AE model does not make assum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22343;&#21248;&#25277;&#26679;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;&#20013;&#24515;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#8220;&#26174;&#33879;&#24615;&#8221;&#24230;&#37327;&#26469;&#35299;&#37322;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#20551;&#35774;&#32473;&#23450;&#23454;&#20363;&#26159;&#8220;&#26174;&#33879;&#8221;&#30340;&#24773;&#20917;&#19979;&#65292;&#26679;&#26412;&#22823;&#23567;&#21487;&#20197;&#29420;&#31435;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#22823;&#23567;&#21644;&#32500;&#24230;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2103.00558</link><description>&lt;p&gt;
&#31616;&#21333;&#22343;&#21248;&#25277;&#26679;&#23545;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;&#22522;&#20110;&#20013;&#24515;&#30340;&#32858;&#31867;&#26159;&#21542;&#26377;&#25928;&#65306;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?. (arXiv:2103.00558v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22343;&#21248;&#25277;&#26679;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;&#20013;&#24515;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#8220;&#26174;&#33879;&#24615;&#8221;&#24230;&#37327;&#26469;&#35299;&#37322;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#20551;&#35774;&#32473;&#23450;&#23454;&#20363;&#26159;&#8220;&#26174;&#33879;&#8221;&#30340;&#24773;&#20917;&#19979;&#65292;&#26679;&#26412;&#22823;&#23567;&#21487;&#20197;&#29420;&#31435;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#22823;&#23567;&#21644;&#32500;&#24230;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#24322;&#24120;&#20540;&#65292;&#24322;&#24120;&#20540;&#30340;&#23384;&#22312;&#21487;&#20197;&#20351;&#32858;&#31867;&#38382;&#39064;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22343;&#21248;&#25277;&#26679;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#20013;&#24515;&#30340;&#32858;&#31867;&#38382;&#39064;&#65306;&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;$k$&#20013;&#24515;/&#20013;&#20301;&#25968;/&#22343;&#20540;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#19982;&#20808;&#21069;&#30340;&#65288;&#22343;&#21248;&#21644;&#38750;&#22343;&#21248;&#65289;&#25277;&#26679;&#24605;&#24819;&#26681;&#26412;&#19981;&#21516;&#12290;&#20026;&#20102;&#29702;&#35770;&#19978;&#35299;&#37322;&#22343;&#21248;&#25277;&#26679;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#26174;&#33879;&#24615;&#8221;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#32473;&#23450;&#23454;&#20363;&#30340;&#26174;&#33879;&#24615;&#31243;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#25105;&#20204;&#20551;&#35774;&#32473;&#23450;&#23454;&#20363;&#26159;&#8220;&#26174;&#33879;&#8221;&#30340;&#35805;&#65292;&#26679;&#26412;&#22823;&#23567;&#21487;&#20197;&#29420;&#31435;&#20110;&#36755;&#20837;&#25968;&#25454;&#22823;&#23567;$n$&#21644;&#32500;&#24230;$d$&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#30456;&#24403;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#19982;&#38750;&#22343;&#21248;&#25277;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;&#22343;&#21248;&#25277;&#26679;&#26041;&#27861;&#36824;&#20855;&#26377;&#20960;&#20010;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world datasets often contain outliers, and the presence of outliers can make the clustering problems to be much more challenging. In this paper, we propose a simple uniform sampling framework for solving three representative center-based clustering with outliers problems: $k$-center/median/means clustering with outliers. Our analysis is fundamentally different from the previous (uniform and non-uniform) sampling based ideas. To explain the effectiveness of uniform sampling in theory, we introduce a measure of "significance" and prove that the performance of our framework depends on the significance degree of the given instance. In particular, the sample size can be independent of the input data size $n$ and the dimensionality $d$, if we assume the given instance is "significant", which is in fact a fairly reasonable assumption in practice. Due to its simplicity, the uniform sampling approach also enjoys several significant advantages over the non-uniform sampling approaches in pra
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#34920;&#38754;&#37325;&#24314;&#26469;&#33719;&#24471;&#22312;&#20809;&#28369;&#27425;&#27969;&#24418;&#19978;&#20272;&#35745;&#20869;&#22312;&#36317;&#31163;&#30340;&#26497;&#23567;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#31561;&#36317;&#38382;&#39064;&#20013;&#20351;&#29992;&#37325;&#24314;&#34920;&#38754;&#35745;&#31639;&#36317;&#31163;&#30340;Isomap&#21464;&#20307;&#30340;&#26497;&#23567;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2011.12478</link><description>&lt;p&gt;
&#34920;&#38754;&#19978;&#30340;&#26497;&#23567;&#20272;&#35745;&#36317;&#31163;&#21644;&#31561;&#36317;&#21040;&#20984;&#22788;&#29702;&#20013;&#30340;&#26497;&#23567;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimax Estimation of Distances on a Surface and Minimax Manifold Learning in the Isometric-to-Convex Setting. (arXiv:2011.12478v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.12478
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#34920;&#38754;&#37325;&#24314;&#26469;&#33719;&#24471;&#22312;&#20809;&#28369;&#27425;&#27969;&#24418;&#19978;&#20272;&#35745;&#20869;&#22312;&#36317;&#31163;&#30340;&#26497;&#23567;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#31561;&#36317;&#38382;&#39064;&#20013;&#20351;&#29992;&#37325;&#24314;&#34920;&#38754;&#35745;&#31639;&#36317;&#31163;&#30340;Isomap&#21464;&#20307;&#30340;&#26497;&#23567;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#22312;&#20809;&#28369;&#27425;&#27969;&#24418;&#19978;&#20272;&#35745;&#20869;&#31104;&#36317;&#31163;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#34920;&#38754;&#36827;&#34892;&#37325;&#24314;&#21487;&#20197;&#33719;&#24471;&#26497;&#23567;&#26368;&#20248;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#29305;&#23450;&#32593;&#26684;&#26500;&#36896;&#8212;&#8212;&#20999;&#28857;Delaunay&#22797;&#21512;&#20307;&#30340;&#20351;&#29992;&#12290;&#28982;&#21518;&#25105;&#20204;&#36716;&#21521;&#27969;&#24418;&#23398;&#20064;&#65292;&#24182;&#35748;&#20026;&#22312;&#37325;&#24314;&#30340;&#34920;&#38754;&#19978;&#35745;&#31639;&#36317;&#31163;&#30340;Isomap&#21464;&#20307;&#23545;&#20110;&#31561;&#36317;&#38382;&#39064;&#30340;&#26497;&#23567;&#26368;&#20248;&#24615;&#26159;&#21512;&#36866;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We start by considering the problem of estimating intrinsic distances on a smooth submanifold. We show that minimax optimality can be obtained via a reconstruction of the surface, and discuss the use of a particular mesh construction -- the tangential Delaunay complex -- for that purpose. We then turn to manifold learning and argue that a variant of Isomap where the distances are instead computed on a reconstructed surface is minimax optimal for the isometric variant of the problem.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#12290;&#36890;&#36807;&#27604;&#36739;&#21644;&#24635;&#32467;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#20256;&#32479;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#24182;&#24378;&#35843;&#20102;&#23558;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2008.12248</link><description>&lt;p&gt;
&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Reinforcement Learning for Combinatorial Optimization. (arXiv:2008.12248v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.12248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#12290;&#36890;&#36807;&#27604;&#36739;&#21644;&#24635;&#32467;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#20256;&#32479;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#24182;&#24378;&#35843;&#20102;&#23558;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35814;&#32454;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#20171;&#32461;&#20102;&#20174;1950&#24180;&#20195;&#24320;&#22987;&#30340;&#32452;&#21512;&#20248;&#21270;&#21382;&#21490;&#65292;&#24182;&#23558;&#20854;&#19982;&#36817;&#24180;&#26469;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#35770;&#25991;&#29305;&#21035;&#20851;&#27880;&#20102;&#33879;&#21517;&#30340;&#32452;&#21512;&#38382;&#39064;&#8212;&#8212;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#23427;&#23558;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;TSP&#19978;&#30340;&#26041;&#27861;&#19982;1970&#24180;&#20195;&#21457;&#34920;&#30340;&#19968;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#36827;&#21270;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26159;&#22914;&#20309;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#35770;&#25991;&#38543;&#21518;&#31616;&#35201;&#20171;&#32461;&#20102;TSP&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#20256;&#32479;&#25968;&#23398;&#26694;&#26550;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#21644;&#29305;&#24449;&#32534;&#30721;&#26426;&#21046;&#65292;&#20197;&#29983;&#25104;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#27880;&#24847;&#21147;&#31561;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper gives a detailed review of reinforcement learning (RL) in combinatorial optimization, introduces the history of combinatorial optimization starting in the 1950s, and compares it with the RL algorithms of recent years. This paper explicitly looks at a famous combinatorial problem-traveling salesperson problem (TSP). It compares the approach of modern RL algorithms for the TSP with an approach published in the 1970s. By comparing the similarities and variances between these methodologies, the paper demonstrates how RL algorithms are optimized due to the evolution of machine learning techniques and computing power. The paper then briefly introduces the deep learning approach to the TSP named deep RL, which is an extension of the traditional mathematical framework. In deep RL, attention and feature encoding mechanisms are introduced to generate near-optimal solutions. The survey shows that integrating the deep learning mechanism, such as attention with RL, can effectively approx
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#32479;&#19968;&#20110;&#22522;&#20110;&#27491;&#24577;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#23545;&#22825;&#25991;&#31890;&#23376;&#37325;&#24314;&#36827;&#34892;&#20102;&#35206;&#30422;&#12289;&#31995;&#32479;&#24615;&#21644;&#25311;&#21512;&#22909;&#22351;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;KL&#25955;&#24230;&#30446;&#26631;&#23454;&#29616;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#30340;&#32479;&#19968;&#12290;&#21033;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#30340;&#26041;&#27861;&#21487;&#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;p&#20540;&#12290;</title><link>http://arxiv.org/abs/2008.05825</link><description>&lt;p&gt;
&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#32479;&#19968;&#22312;&#22522;&#20110;&#27491;&#24577;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#23545;&#22825;&#25991;&#31890;&#23376;&#37325;&#24314;&#36827;&#34892;&#35206;&#30422;&#12289;&#31995;&#32479;&#24615;&#21644;&#25311;&#21512;&#22909;&#22351;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions. (arXiv:2008.05825v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#32479;&#19968;&#20110;&#22522;&#20110;&#27491;&#24577;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#23545;&#22825;&#25991;&#31890;&#23376;&#37325;&#24314;&#36827;&#34892;&#20102;&#35206;&#30422;&#12289;&#31995;&#32479;&#24615;&#21644;&#25311;&#21512;&#22909;&#22351;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;KL&#25955;&#24230;&#30446;&#26631;&#23454;&#29616;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;VAEs&#30340;&#32479;&#19968;&#12290;&#21033;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#30340;&#26041;&#27861;&#21487;&#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;p&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22825;&#25991;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#23646;&#24615;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32467;&#26524;&#21482;&#34987;&#29992;&#20316;&#28857;&#39044;&#27979;&#12290;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#35206;&#30422;&#29575;(1)&#65292;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;(2)&#25110;&#25311;&#21512;&#20248;&#24230;&#24230;&#37327;(3)&#32463;&#24120;&#27809;&#26377;&#34987;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#35757;&#32451;&#21644;&#32593;&#32476;&#26550;&#26500;&#36873;&#25321;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#36825;&#20123;&#23646;&#24615;&#34701;&#20837;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#21644;&#26631;&#31614;&#32852;&#21512;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30446;&#26631;&#20351;&#24471;&#22312;&#38543;&#26426;&#21464;&#20998;&#25512;&#29702;&#30340;&#19968;&#31181;&#32479;&#19968;&#19979;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#32479;&#19968;&#36215;&#26469;&#12290;&#36825;&#31181;&#32479;&#19968;&#24615;&#28608;&#21457;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;p&#20540;&#12290;&#22312;&#36825;&#31181;&#24314;&#35774;&#20013;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#20204;&#22914;&#20309;&#20026;&#24050;&#23450;&#20041;&#30340;&#21518;&#39564;&#20998;&#24067;&#20005;&#26684;&#23450;&#20041;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-network based predictions of event properties in astro-particle physics are getting more and more common. However, in many cases the result is just utilized as a point prediction. Statistical uncertainties and coverage (1), systematic uncertainties (2) or a goodness-of-fit measure (3) are often not calculated. Here we describe a certain choice of training and network architecture that allows to incorporate all these properties into a single network model. We show that a KL-divergence objective of the joint distribution of data and labels allows to unify supervised learning and variational autoencoders (VAEs) under one umbrella of stochastic variational inference. The unification motivates an extended supervised learning scheme which allows to calculate a goodness-of-fit p-value for the neural network model. Conditional normalizing flows amortized with a neural network are crucial in this construction. We discuss how they allow to rigorously define coverage for posteriors defined
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#20551;&#23450;&#32467;&#26500;&#30340;&#39640;&#38454;&#32858;&#31867;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#30830;&#23450;&#20102;&#32858;&#31867;&#23384;&#22312;&#24615;&#21644;&#32858;&#31867;&#25903;&#25345;&#30340;&#20020;&#30028;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#22312;&#36229;&#22270;&#31181;&#26893;&#22242;&#38382;&#39064;&#21644;&#36229;&#22270;&#31181;&#26893;&#31264;&#23494;&#23376;&#22270;&#24674;&#22797;&#30340;&#35745;&#31639;&#22256;&#38590;&#29468;&#24819;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#20449;&#22122;&#27604;&#33539;&#22260;&#20869;&#26080;&#27861;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2005.10743</link><description>&lt;p&gt;
&#24102;&#26377;&#20551;&#23450;&#32467;&#26500;&#30340;&#24352;&#37327;&#32858;&#31867;&#65306;&#32479;&#35745;&#26368;&#20248;&#24615;&#21644;&#35745;&#31639;&#38480;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tensor Clustering with Planted Structures: Statistical Optimality and Computational Limits. (arXiv:2005.10743v4 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.10743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#20551;&#23450;&#32467;&#26500;&#30340;&#39640;&#38454;&#32858;&#31867;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#30830;&#23450;&#20102;&#32858;&#31867;&#23384;&#22312;&#24615;&#21644;&#32858;&#31867;&#25903;&#25345;&#30340;&#20020;&#30028;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#22312;&#36229;&#22270;&#31181;&#26893;&#22242;&#38382;&#39064;&#21644;&#36229;&#22270;&#31181;&#26893;&#31264;&#23494;&#23376;&#22270;&#24674;&#22797;&#30340;&#35745;&#31639;&#22256;&#38590;&#29468;&#24819;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#20449;&#22122;&#27604;&#33539;&#22260;&#20869;&#26080;&#27861;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#20551;&#23450;&#32467;&#26500;&#30340;&#39640;&#38454;&#32858;&#31867;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#32858;&#31867;&#27169;&#22411;&#65292;&#24120;&#25968;&#39640;&#38454;&#32858;&#31867;&#65288;CHC&#65289;&#21644;&#31209;&#19968;&#39640;&#38454;&#32858;&#31867;&#65288;ROHC&#65289;&#65292;&#24182;&#30740;&#31350;&#20102;&#27979;&#35797;&#32858;&#31867;&#23384;&#22312;&#24615;&#65288;&#26816;&#27979;&#65289;&#21644;&#35782;&#21035;&#32858;&#31867;&#25903;&#25345;&#65288;&#24674;&#22797;&#65289;&#30340;&#26041;&#27861;&#21644;&#29702;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24403;&#20449;&#22122;&#27604;&#22788;&#20110;&#26576;&#20123;&#20020;&#30028;&#20540;&#26102;&#65292;CHC&#21644;ROHC&#30340;&#26816;&#27979;/&#24674;&#22797;&#26159;&#32479;&#35745;&#19978;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#23637;&#20102;&#32039;&#23494;&#30340;&#35745;&#31639;&#38408;&#20540;&#65306;&#24403;&#20449;&#22122;&#27604;&#20302;&#20110;&#36825;&#20123;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36229;&#22270;&#31181;&#26893;&#22242;&#38382;&#39064;&#65288;HPC&#65289;&#26816;&#27979;&#21644;&#36229;&#22270;&#31181;&#26893;&#31264;&#23494;&#23376;&#22270;&#65288;HPDS&#65289;&#24674;&#22797;&#30340;&#35745;&#31639;&#22256;&#38590;&#29468;&#24819;&#19979;&#65292;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#24352;&#37327;&#31639;&#27861;&#65292;&#22312;&#20449;&#22122;&#27604;&#39640;&#20110;&#36825;&#20123;&#38408;&#20540;&#26102;&#23454;&#29616;&#21487;&#38752;&#30340;&#26816;&#27979;&#21644;&#24674;&#22797;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31232;&#30095;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
This paper studies the statistical and computational limits of high-order clustering with planted structures. We focus on two clustering models, constant high-order clustering (CHC) and rank-one higher-order clustering (ROHC), and study the methods and theory for testing whether a cluster exists (detection) and identifying the support of cluster (recovery).  Specifically, we identify the sharp boundaries of signal-to-noise ratio for which CHC and ROHC detection/recovery are statistically possible. We also develop the tight computational thresholds: when the signal-to-noise ratio is below these thresholds, we prove that polynomial-time algorithms cannot solve these problems under the computational hardness conjectures of hypergraphic planted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS) recovery. We also propose polynomial-time tensor algorithms that achieve reliable detection and recovery when the signal-to-noise ratio is above these thresholds. Both sparsity an
&lt;/p&gt;</description></item></channel></rss>