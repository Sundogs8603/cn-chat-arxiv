<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26469;&#25913;&#21892;&#24369;&#21040;&#24378;&#27867;&#21270;&#30340;&#21327;&#21516;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#25945;&#24072;&#20849;&#21516;&#30417;&#30563;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15505</link><description>&lt;p&gt;
Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26469;&#25913;&#21892;&#24369;&#21040;&#24378;&#27867;&#21270;&#30340;&#21327;&#21516;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#25945;&#24072;&#20849;&#21516;&#30417;&#30563;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#26377;&#21147;&#30340;&#27169;&#22411;&#32463;&#36807;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#21518;&#65292;&#30001;&#20110;&#32570;&#20047;&#32988;&#20219;&#30340;&#30417;&#30563;&#32773;&#65292;&#22312;&#24341;&#23548;&#20854;&#34892;&#20026;&#26102;&#21487;&#33021;&#20250;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#30417;&#30563;&#22122;&#22768;&#65292;&#19968;&#20010;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#21518;&#21487;&#33021;&#20250;&#36229;&#36234;&#20854;&#24369;&#25945;&#24072;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24040;&#22823;&#33021;&#21147;&#24046;&#36317;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#25945;&#24072;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30340;&#36890;&#25165;&#25945;&#24072;&#65292;&#20849;&#21516;&#30417;&#30563;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#38024;&#23545;&#21327;&#21516;&#30417;&#30563;&#30340;&#32452;&#20214;&#65306;(i)&#25105;&#20204;&#36880;&#27493;&#20132;&#26367;&#36827;&#34892;&#23398;&#29983;&#35757;&#32451;&#21644;&#25945;&#24072;&#20998;&#37197;&#65292;&#21033;&#29992;&#24378;&#22823;&#23398;&#29983;&#27169;&#22411;&#30340;&#22686;&#38271;&#26469;&#35782;&#21035;&#21487;&#33021;&#30340;&#30417;&#30563;&#26041;&#24335;&#65307;(ii)&#25105;&#20204;&#35880;&#24910;&#22320;&#24378;&#21270;&#25945;&#24072;-&#23398;&#29983;&#21644;&#23616;&#37096;-&#20840;&#23616;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15505v1 Announce Type: cross  Abstract: Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consist
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#37096;&#32626;&#21644;&#24536;&#35760;"&#26041;&#27861;&#65292;&#32467;&#21512;&#24265;&#20215;&#20256;&#24863;&#22120;&#21644;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#32467;&#26500;&#25439;&#20260;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20165;&#38656;&#23398;&#20064;3&#23567;&#26102;&#25968;&#25454;&#21363;&#21487;&#33258;&#20027;&#35782;&#21035;&#21644;&#23450;&#20301;&#19981;&#21516;&#31867;&#22411;&#30340;&#26410;&#30693;&#25439;&#20260;&#12290;</title><link>https://arxiv.org/abs/2402.15492</link><description>&lt;p&gt;
&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#24847;&#22806;&#30340;&#32467;&#26500;&#25439;&#20260;
&lt;/p&gt;
&lt;p&gt;
Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#37096;&#32626;&#21644;&#24536;&#35760;"&#26041;&#27861;&#65292;&#32467;&#21512;&#24265;&#20215;&#20256;&#24863;&#22120;&#21644;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#32467;&#26500;&#25439;&#20260;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20165;&#38656;&#23398;&#20064;3&#23567;&#26102;&#25968;&#25454;&#21363;&#21487;&#33258;&#20027;&#35782;&#21035;&#21644;&#23450;&#20301;&#19981;&#21516;&#31867;&#22411;&#30340;&#26410;&#30693;&#25439;&#20260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#23545;&#20110;&#30830;&#20445;&#24314;&#31569;&#29289;&#21644;&#26725;&#26753;&#31561;&#32467;&#26500;&#30340;&#23433;&#20840;&#24615;&#21644;&#38271;&#23551;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#39062;&#30340;&#8220;&#37096;&#32626;&#21644;&#24536;&#35760;&#8221;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#32467;&#26500;&#20013;&#30340;&#25439;&#20260;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#24265;&#20215;&#20256;&#24863;&#22120;&#30340;&#20840; pass &#23398;&#20064;&#21644;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#30340;&#21327;&#21516;&#32452;&#21512;&#65292;&#22312;&#20165;&#23398;&#20064;&#20102; 3 &#23567;&#26102;&#30340;&#25968;&#25454;&#21518;&#65292;&#23601;&#33021;&#33258;&#20027;&#26816;&#27979;&#21644;&#23450;&#20301;&#19981;&#21516;&#31867;&#22411;&#30340;&#24847;&#22806;&#25439;&#20260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15492v1 Announce Type: new  Abstract: Structural health monitoring (SHM) is vital for ensuring the safety and longevity of structures like buildings and bridges. As the volume and scale of structures and the impact of their failure continue to grow, there is a dire need for SHM techniques that are scalable, inexpensive, operate passively without human intervention, and customized for each mechanical structure without the need for complex baseline models. We present a novel "deploy-and-forget" approach for automated detection and localization of damages in structures. It is based on a synergistic combination of fully passive measurements from inexpensive sensors and a mechanics-informed autoencoder. Once deployed, our solution continuously learns and adapts a bespoke baseline model for each structure, learning from its undamaged state's response characteristics. After learning from just 3 hours of data, it can autonomously detect and localize different types of unforeseen dam
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21367;&#31215;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#35768;&#22810;&#31867;&#22411;&#30340;CNNs&#21487;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;CNNs&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#26032;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.15490</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#21367;&#31215;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15490
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21367;&#31215;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#35768;&#22810;&#31867;&#22411;&#30340;CNNs&#21487;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;CNNs&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#26032;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNNs) &#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#22270;&#20687;&#20998;&#21106;&#12290;&#26377;&#35768;&#22810;&#31867;&#22411;&#30340;CNNs&#26088;&#22312;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#21644;&#35201;&#27714;&#65292;&#21253;&#25324;1D&#12289;2D&#21644;3D CNNs&#65292;&#20197;&#21450;&#25193;&#24352;&#30340;&#12289;&#20998;&#32452;&#30340;&#12289;&#27880;&#24847;&#21147;&#30340;&#12289;&#28145;&#24230;&#21487;&#20998;&#30340;&#21367;&#31215;&#21644;NAS&#31561;&#12290;&#27599;&#31181;&#31867;&#22411;&#30340;CNN&#20855;&#26377;&#20854;&#29420;&#29305;&#30340;&#32467;&#26500;&#21644;&#29305;&#28857;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#28145;&#20837;&#20102;&#35299;&#24182;&#23545;&#36825;&#20123;&#19981;&#21516;&#31867;&#22411;&#30340;CNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#27599;&#31181;&#31867;&#22411;CNN&#30340;&#24615;&#33021;&#12289;&#38480;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#21487;&#20197;&#24110;&#21161;&#26410;&#26469;&#24320;&#21457;&#26032;&#30340;&#25913;&#36827;&#26550;&#26500;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#30740;&#31350;&#20154;&#21592;&#29992;&#20110;&#30740;&#31350;&#25110;&#24320;&#21457;&#30340;&#24179;&#21488;&#21644;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15490v1 Announce Type: new  Abstract: In today's digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It's crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or develop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;</title><link>https://arxiv.org/abs/2402.15487</link><description>&lt;p&gt;
RoboEXP: &#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;&#23454;&#29616;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#25506;&#32034;&#21608;&#22260;&#29615;&#22659;&#20197;&#36866;&#24212;&#24182;&#24212;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#24182;&#29983;&#25104;&#19968;&#20010;&#25429;&#25417;&#22522;&#30784;&#29615;&#22659;&#32467;&#26500;&#30340;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65288;ACSG&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15487v1 Announce Type: cross  Abstract: Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Explo
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;</title><link>https://arxiv.org/abs/2402.15478</link><description>&lt;p&gt;
Transformer&#26159;&#34920;&#29616;&#21147;&#24378;&#22823;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#26469;&#35828;&#34920;&#29616;&#21147;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformers are Expressive, But Are They Expressive Enough for Regression?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15478
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#25351;&#30340;&#26159;&#23427;&#33021;&#22815;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#12290;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26159;&#23436;&#20840;&#34920;&#29616;&#21147;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#20805;&#24403;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#19982;&#29616;&#26377;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#22312;&#21487;&#38752;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20381;&#36182;&#20110;&#20855;&#26377;&#21487;&#35266;&#21306;&#38388;&#30340;&#20998;&#27573;&#24120;&#25968;&#36924;&#36817;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#8220;Transformer&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#36890;&#36807;&#23454;&#39564;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#8230;&#8230;&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15478v1 Announce Type: new  Abstract: Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#30340;&#20559;&#35265;&#28040;&#38500;&#31574;&#30053;&#65292;&#22522;&#20110;&#20869;&#29983;&#24615;&#27010;&#24565;&#24182;&#37319;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#39044;&#27979;&#27169;&#22411;&#20570;&#20219;&#20309;&#20551;&#35774;</title><link>https://arxiv.org/abs/2402.15477</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#26469;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Debiasing Machine Learning Models by Using Weakly Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15477
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#30340;&#20559;&#35265;&#28040;&#38500;&#31574;&#30053;&#65292;&#22522;&#20110;&#20869;&#29983;&#24615;&#27010;&#24565;&#24182;&#37319;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#39044;&#27979;&#27169;&#22411;&#20570;&#20219;&#20309;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31639;&#27861;&#20915;&#31574;&#30340;&#20559;&#35265;&#28040;&#38500;&#38382;&#39064;&#20013;&#25506;&#35752;&#20102;&#19968;&#20010;&#24773;&#20917;&#65292;&#21363;&#31639;&#27861;&#30340;&#36755;&#20986;&#21644;&#25935;&#24863;&#21464;&#37327;&#22343;&#20026;&#36830;&#32493;&#30340;&#24773;&#24418;&#12290;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#22788;&#29702;&#30340;&#26159;&#31163;&#25955;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#36825;&#24847;&#21619;&#30528;&#20559;&#35265;&#26159;&#38024;&#23545;&#30001;&#26631;&#31614;&#23450;&#20041;&#30340;&#20154;&#32676;&#23376;&#38598;&#36827;&#34892;&#27979;&#37327;&#30340;&#65292;&#32780;&#24573;&#30053;&#20102;&#31639;&#27861;&#20559;&#35265;&#30340;&#37325;&#35201;&#24773;&#20917;&#65292;&#21363;&#25935;&#24863;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#12290;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#20851;&#20110;&#24180;&#40836;&#25110;&#36130;&#21153;&#29366;&#20917;&#32780;&#20570;&#20986;&#30340;&#19981;&#20844;&#24179;&#20915;&#31574;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#30340;&#20559;&#35265;&#28040;&#38500;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#35745;&#37327;&#32463;&#27982;&#23398;&#39046;&#22495;&#30340;&#20869;&#29983;&#24615;&#27010;&#24565;&#12290;&#38500;&#20102;&#35299;&#20915;&#36825;&#20010;&#26032;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20559;&#35265;&#28040;&#38500;&#31574;&#30053;&#36824;&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#35201;&#27714;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#20197;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#36827;&#34892;&#27979;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#19978;&#26159;&#26080;&#20851;&#30340;&#65292;&#21363;&#23427;&#19981;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15477v1 Announce Type: new  Abstract: We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous. Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous. Typical examples are unfair decisions made with respect to the age or the financial status. In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics. In addition to solve this new problem, our bias mitigation strategy is a weakly supervised learning method which requires that a small portion of the data can be measured in a fair manner. It is model agnostic, in the sense that it does not make any hypothesis on the prediction model. It also m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.15473</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#22312;RLHF&#20013;&#39640;&#25928;&#24314;&#27169;&#22870;&#21169;&#65306;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15473
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#25104;&#20026;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26397;&#21521;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#30340;&#20027;&#23548;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#19968;&#20010;&#33021;&#22815;&#21453;&#26144;&#19982;&#20154;&#31867;&#30456;&#20851;&#30340;&#28508;&#22312;&#22870;&#21169;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;{$\varphi$}&#65289;&#12290;&#34429;&#28982;&#36825;&#19968;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#19975;&#65289;&#26469;&#35757;&#32451;{$\varphi$}&#12290;&#22914;&#26524;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#34987;&#26222;&#36941;&#20351;&#29992;&#65292;&#36825;&#31181;&#22823;&#35268;&#27169;&#20559;&#22909;&#27880;&#37322;&#26159;&#21487;&#20197;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#24615;&#36136;&#12290;&#36825;&#23545;&#20110;&#25910;&#38598;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;{$\varphi$}&#20013;&#65292;&#20174;&#32780;&#20943;&#23569;&#25152;&#38656;&#27880;&#37322;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27425;&#27169;&#22359;&#30446;&#26631;&#20989;&#25968;&#36807;&#28388;&#22823;&#37327;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#31639;&#27861; FAIR&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20013;&#32570;&#20047;&#22823;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15472</link><description>&lt;p&gt;
FAIR&#65306;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
FAIR: Filtering of Automatically Induced Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27425;&#27169;&#22359;&#30446;&#26631;&#20989;&#25968;&#36807;&#28388;&#22823;&#37327;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#31639;&#27861; FAIR&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20013;&#32570;&#20047;&#22823;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#26159;&#25104;&#21151;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20851;&#38190;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#24403;&#24212;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#39046;&#22495;&#26102;&#12290;&#24369;&#30417;&#30563;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21152;&#36895;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#35268;&#21017;&#21019;&#24314;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#35201;&#27714;&#29992;&#25143;&#32534;&#20889;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#35268;&#21017;&#38598;&#65292;&#20197;&#23558;&#26631;&#31614;&#20998;&#37197;&#32473;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#33258;&#21160;&#35268;&#21017;&#35825;&#23548;&#65288;ARI&#65289;&#26041;&#27861;&#36890;&#36807;&#20174;&#23569;&#37327;&#24102;&#26631;&#31614;&#38598;&#19978;&#30340;&#29305;&#24449;&#33258;&#21160;&#21019;&#24314;&#35268;&#21017;&#24182;&#20174;&#20013;&#36807;&#28388;&#20986;&#26368;&#32456;&#19968;&#32452;&#35268;&#21017;&#26469;&#36991;&#24320;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;ARI&#26041;&#27861;&#20013;&#65292;&#20851;&#38190;&#27493;&#39588;&#26159;&#20174;&#22823;&#37327;&#33258;&#21160;&#21019;&#24314;&#30340;&#35268;&#21017;&#20013;&#36807;&#28388;&#20986;&#19968;&#32452;&#39640;&#36136;&#37327;&#26377;&#29992;&#30340;&#35268;&#21017;&#23376;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#65288;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#36807;&#28388;&#65289;&#65292;&#20351;&#29992;&#32771;&#34385;&#21040;&#27425;&#27169;&#22359;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#27861;&#20174;&#22823;&#37327;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#20013;&#36807;&#28388;&#20986;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15472v1 Announce Type: new  Abstract: The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the
&lt;/p&gt;</description></item><item><title>&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.15449</link><description>&lt;p&gt;
&#37325;&#22797;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Repetition Improves Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15449
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25913;&#36827;&#20174;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#25968;&#25454;&#12289;&#39592;&#24178;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25110;&#36890;&#36807;&#25351;&#20196;&#25913;&#36827;&#20219;&#21153;&#24046;&#24322;&#21270;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19968;&#20010;&#26550;&#26500;&#38480;&#21046;&#65306;&#20196;&#29260;&#23884;&#20837;&#19981;&#33021;&#21253;&#21547;&#26469;&#33258;&#36755;&#20837;&#20013;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#8220;&#22238;&#22768;&#23884;&#20837;&#8221;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#23558;&#36755;&#20837;&#37325;&#22797;&#20004;&#27425;&#65292;&#24182;&#20174;&#31532;&#20108;&#27425;&#20986;&#29616;&#20013;&#25552;&#21462;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#20196;&#29260;&#30340;&#22238;&#22768;&#23884;&#20837;&#21487;&#20197;&#32534;&#30721;&#20851;&#20110;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;LLMs&#36827;&#34892;&#23884;&#20837;&#12290;&#22312;MTEB&#25490;&#34892;&#27036;&#19978;&#65292;&#22238;&#22768;&#23884;&#20837;&#22312;&#38646;&#23556;&#20987;&#20013;&#27604;&#32463;&#20856;&#23884;&#20837;&#25552;&#39640;&#20102;&#36229;&#36807;9%&#65292;&#22312;&#24494;&#35843;&#26102;&#25552;&#39640;&#20102;&#32422;0.7%&#12290;&#20351;&#29992;Mistral-7B&#27169;&#22411;&#30340;&#22238;&#22768;&#23884;&#20837;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, "echo embeddings," in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#23384;&#22312;&#30340;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#30340;&#21147;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.15444</link><description>&lt;p&gt;
&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#23384;&#22312;&#30340;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MMKGC&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#23454;&#20307;&#30340;&#32467;&#26500;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#32435;&#20837;&#21028;&#21035;&#27169;&#22411;&#26469;&#39044;&#27979;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20013;&#32570;&#22833;&#30340;&#19977;&#20803;&#32452;&#12290;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#23558;&#20849;&#21516;&#24037;&#20316;&#20197;&#34913;&#37327;&#19977;&#20803;&#32452;&#30340;&#21487;&#33021;&#24615;&#12290;&#29616;&#26377;&#30340;MMKGC&#26041;&#27861;&#24573;&#35270;&#20102;&#23454;&#20307;&#20043;&#38388;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#27169;&#24577;&#34701;&#21512;&#19981;&#36275;&#20197;&#21450;&#23545;&#21407;&#22987;&#27169;&#24577;&#20449;&#24687;&#30340;&#20302;&#25928;&#21033;&#29992;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#65292;&#20197;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#22312;MMKGC&#20013;&#30340;&#21147;&#37327;&#12290;AdaMF-MAT&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#24577;&#26435;&#37325;&#23454;&#29616;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;MMKGC&#27169;&#22411;&#21644;&#35757;&#32451;&#30340;&#21327;&#21516;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15444v1 Announce Type: new  Abstract: Multi-modal knowledge graph completion (MMKGC) aims to predict the missing triples in the multi-modal knowledge graphs by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive Multi-modal Fusion and Modality Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive modality weights and further generates adversarial samples by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.15441</link><description>&lt;p&gt;
&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Active Few-Shot Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#26159;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#21644;&#36716;&#23548;&#20027;&#21160;&#23398;&#20064;&#30340;&#27867;&#21270;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#22522;&#20110;&#36716;&#23548;&#23398;&#20064;&#65288;ITL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#37319;&#26679;&#20197;&#26368;&#22823;&#21270;&#33719;&#24471;&#23545;&#25351;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;ITL&#22343;&#21248;&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#33719;&#21462;&#30340;&#26368;&#23567;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#25512;&#23548;&#20986;&#36825;&#31181;&#27867;&#21270;&#30028;&#38480;&#30340;&#20154;&#65292;&#36825;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#23558;ITL&#24212;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;ITL&#26126;&#26174;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#19979;&#30028;&#65292;&#36890;&#36807;Chernoff&#25955;&#24230;&#26469;&#34920;&#36798;&#65292;&#23558;&#20854;&#25299;&#23637;&#21040;&#20855;&#26377;&#27425;&#25351;&#25968;&#23614;&#37096;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#36845;&#20195;&#31639;&#27861;&#22312;&#36825;&#20123;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#35823;&#24046;&#29575;</title><link>https://arxiv.org/abs/2402.15432</link><description>&lt;p&gt;
&#22312;&#27425;&#25351;&#25968;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#26497;&#23567;&#21270;&#32858;&#31867;&#35823;&#24046;&#65306;&#36890;&#29992;&#19979;&#30028;&#21644;&#26368;&#20339;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#19979;&#30028;&#65292;&#36890;&#36807;Chernoff&#25955;&#24230;&#26469;&#34920;&#36798;&#65292;&#23558;&#20854;&#25299;&#23637;&#21040;&#20855;&#26377;&#27425;&#25351;&#25968;&#23614;&#37096;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#36845;&#20195;&#31639;&#27861;&#22312;&#36825;&#20123;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#35823;&#24046;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#24120;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#12290;&#22312;&#39640;&#26031;&#21644;&#27425;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#24674;&#22797;&#32858;&#31867;&#26631;&#31614;&#30340;&#26368;&#20339;&#35823;&#24046;&#29575;&#28041;&#21450;&#21040;&#29305;&#23450;&#30340;&#20449;&#22122;&#27604;&#12290;&#31616;&#21333;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#22914;Lloyd&#31639;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#36825;&#20010;&#26368;&#20339;&#35823;&#24046;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#20219;&#20309;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#35823;&#24046;&#29575;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#19979;&#30028;&#65292;&#36890;&#36807;Chernoff&#25955;&#24230;&#26469;&#34920;&#36798;&#65292;&#36825;&#26159;&#19968;&#20010;&#27604;&#20449;&#22122;&#27604;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#20449;&#24687;&#24230;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#36845;&#20195;&#31639;&#27861;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#19979;&#30028;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#35823;&#24046;&#30340;&#20301;&#32622;-&#23610;&#24230;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26356;&#36866;&#21512;&#30001;&#27850;&#26494;&#25110;&#36127;&#20108;&#39033;&#28151;&#21512;&#27169;&#22411;&#24314;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#20998;&#24067;&#23646;&#20110;&#25351;&#25968;&#26063;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15432v1 Announce Type: cross  Abstract: Clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models. The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as Lloyd's algorithm, attain this optimal error rate. In this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through a Chernoff divergence, a more versatile measure of model information than signal-to-noise ratios. We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors. Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family. In such m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#19981;&#21464;&#24615;&#26500;&#24314;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#19981;&#21464;&#24615;&#34920;&#31034;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21487;&#21306;&#20998;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15430</link><description>&lt;p&gt;
&#36739;&#22823;&#23610;&#24230;&#19979;&#29992;&#20110;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#35270;&#35273;&#20219;&#21153;&#30340;&#20998;&#23618;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#19981;&#21464;&#24615;&#26500;&#24314;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#19981;&#21464;&#24615;&#34920;&#31034;&#22312;&#36739;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21487;&#21306;&#20998;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#31283;&#20581;&#19988;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#31995;&#32479;&#26159;&#36808;&#21521;&#21487;&#38752;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#26159;&#32771;&#34385;&#22312;&#22522;&#26412;&#22270;&#20687;&#34920;&#31034;&#20013;&#23884;&#20837;&#20219;&#21153;&#25152;&#38656;&#30340;&#19981;&#21464;&#32467;&#26500;&#65292;&#20363;&#22914;&#20960;&#20309;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#30340;&#34920;&#31034;&#36890;&#24120;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#21487;&#38752;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#12289;&#23454;&#36341;&#21644;&#24212;&#29992;&#30340;&#35282;&#24230;&#23545;&#20998;&#23618;&#19981;&#21464;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#20307;&#31995;&#32467;&#26500;&#20294;&#20197;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26500;&#24314;&#36229;&#23436;&#22791;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36890;&#29992;&#34013;&#22270;&#12289;&#20855;&#20307;&#23450;&#20041;&#12289;&#19981;&#21464;&#29305;&#24615;&#21644;&#25968;&#20540;&#23454;&#29616;&#12290;&#22312;&#23454;&#36341;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23450;&#21046;&#19981;&#21464;&#24615;&#20197;&#36866;&#24212;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15430v1 Announce Type: cross  Abstract: Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15429</link><description>&lt;p&gt;
ProTIP&#65306;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#25239;&#38543;&#26426;&#25200;&#21160;&#30340;&#27010;&#29575;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#23637;&#29616;&#20102;&#22312;&#31616;&#21333;&#25991;&#26412;&#25551;&#36848;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#19968;&#26679;&#65292;DMs&#23384;&#22312;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;T2I DMs&#30340;&#40065;&#26834;&#24615;&#26102;&#65292;&#23384;&#22312;&#20197;&#20108;&#20803;&#25110;&#26368;&#22351;&#24773;&#20917;&#38382;&#39064;&#35299;&#26041;&#38754;&#30340;&#23581;&#35797;&#65292;&#20294;&#26080;&#27861;&#22238;&#31572;&#27169;&#22411;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AE&#65289;&#26102;&#30340;&#24635;&#20307;&#40065;&#26834;&#24615;&#22914;&#20309;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;T2I DMs&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65307;&#28982;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#35780;&#20272;&#12290;&#20027;&#35201;&#25361;&#25112;&#28304;&#33258;&#65306;i&#65289;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#21644;ii&#65289;&#30830;&#23450;&#25200;&#21160;&#36755;&#20837;&#26159;&#21542;&#20026;AE&#28041;&#21450;&#27604;&#36739;&#20004;&#20010;&#36755;&#20986;&#20998;&#24067;&#65292;&#36825;&#19982;&#20854;&#20182;DL&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#19981;&#21516;&#65292;&#20854;&#20013;AE&#26159;&#22312;&#26631;&#31614;&#38169;&#35823;&#39044;&#27979;&#26102;&#34987;&#35782;&#21035;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15429v1 Announce Type: cross  Abstract: Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.15422</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#19988;&#39640;&#36136;&#37327;&#30340;&#30149;&#20154;&#24635;&#32467;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#32463;&#24120;&#38754;&#20020;&#38590;&#20197;&#29702;&#35299;&#20854;&#20303;&#38498;&#24773;&#20917;&#30340;&#22256;&#38590;&#65292;&#32780;&#21307;&#25252;&#20154;&#21592;&#36164;&#28304;&#26377;&#38480;&#20197;&#25552;&#20379;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#29983;&#25104;&#24635;&#32467;&#30340;&#24544;&#23454;&#24615;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#29992;&#20110;&#24187;&#35273;&#65292;&#35753;&#20004;&#20301;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#20102;100&#20010;&#30495;&#23454;&#24635;&#32467;&#21644;100&#20010;&#29983;&#25104;&#30340;&#24635;&#32467;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;Llama 2&#27599;&#20010;&#24635;&#32467;&#30340;&#24187;&#35273;&#20174;2.60&#38477;&#20302;&#21040;1.55&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;&#34429;&#28982;&#25928;&#26524;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#24403;&#20351;&#29992;&#20116;&#20010;&#20363;&#23376;&#25552;&#31034;GPT-4&#26102;&#65292;&#35813;&#25928;&#26524;&#35201;&#23567;&#24471;&#22810;&#65288;0.70&#38477;&#33267;0.40&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;&#26080;&#24187;&#35273;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;&#21363;&#20351;&#22312;&#24187;&#35273;&#33258;&#30001;&#25968;&#25454;&#19979;&#65292;GPT-4&#20063;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15422v1 Announce Type: cross  Abstract: Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#25512;&#29702;&#26469;&#21010;&#20998;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#26597;&#35810;&#20449;&#24687;&#24182;&#37325;&#26032;&#23450;&#20041;&#22870;&#21169;&#23398;&#20064;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15420</link><description>&lt;p&gt;
PREDILECT&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#25512;&#29702;&#21010;&#20998;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#25512;&#29702;&#26469;&#21010;&#20998;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#26597;&#35810;&#20449;&#24687;&#24182;&#37325;&#26032;&#23450;&#20041;&#22870;&#21169;&#23398;&#20064;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#20154;&#31867;&#36890;&#36807;&#23545;&#19981;&#21516;&#29366;&#24577;-&#21160;&#20316;&#24207;&#21015;&#34920;&#36798;&#20559;&#22909;&#26469;&#22609;&#36896;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20026;&#26426;&#22120;&#20154;&#21046;&#23450;&#29616;&#23454;&#25919;&#31574;&#38656;&#35201;&#20154;&#31867;&#23545;&#22823;&#37327;&#26597;&#35810;&#30340;&#21709;&#24212;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#25193;&#23637;&#27599;&#20010;&#26597;&#35810;&#25910;&#38598;&#30340;&#20449;&#24687;&#65292;&#21253;&#21547;&#20559;&#22909;&#21644;&#21487;&#36873;&#25991;&#26412;&#25552;&#31034;&#65292;&#26469;&#35299;&#20915;&#26679;&#26412;&#25928;&#29575;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#20174;&#20154;&#31867;&#25552;&#20379;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#36866;&#24212;&#39069;&#22806;&#30340;&#26597;&#35810;&#20449;&#24687;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#22870;&#21169;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#21547;&#28789;&#27963;&#30340;&#37325;&#28857; &#8212;&#8212; &#21253;&#21547;&#30456;&#23545;&#39640;&#20449;&#24687;&#37327;&#19988;&#19982;&#38646;&#23556;&#26679;&#26412;&#20256;&#36882;&#30340;&#29305;&#24449;&#30456;&#20851;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22312;&#20223;&#30495;&#22330;&#26223;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15420v1 Announce Type: cross  Abstract: Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36716;&#25442;&#22120;&#25968;&#23398;&#26694;&#26550;&#25506;&#35752;&#20102;LoRA&#31639;&#27861;&#23545;Token&#32858;&#31867;&#32467;&#26500;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#21442;&#25968;&#19979;&#65292;&#20462;&#25913;&#21518;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#21160;&#24577;&#30340;&#32858;&#31867;&#34920;&#29616;&#20986;&#36739;&#38271;&#26102;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#20294;&#20173;&#22312;&#30701;&#26102;&#38388;&#20869;&#20445;&#25345;&#23494;&#20999;&#30456;&#20284;&#12290;</title><link>https://arxiv.org/abs/2402.15415</link><description>&lt;p&gt;
LoRA&#23545;&#36716;&#25442;&#22120;&#20013;&#32858;&#31867;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of LoRA on the Emergence of Clusters in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36716;&#25442;&#22120;&#25968;&#23398;&#26694;&#26550;&#25506;&#35752;&#20102;LoRA&#31639;&#27861;&#23545;Token&#32858;&#31867;&#32467;&#26500;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#21442;&#25968;&#19979;&#65292;&#20462;&#25913;&#21518;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#21160;&#24577;&#30340;&#32858;&#31867;&#34920;&#29616;&#20986;&#36739;&#38271;&#26102;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#20294;&#20173;&#22312;&#30701;&#26102;&#38388;&#20869;&#20445;&#25345;&#23494;&#20999;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}&#25552;&#20986;&#30340;&#36716;&#25442;&#22120;&#25968;&#23398;&#26694;&#26550;&#65292;&#25506;&#35752;&#27880;&#24847;&#21147;&#21442;&#25968;&#21644;&#21021;&#22987;&#26631;&#35760;&#20540;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#26631;&#35760;&#32858;&#31867;&#30340;&#32467;&#26500;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#20462;&#25913;&#21518;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#21160;&#24577;&#20013;&#30340;&#32858;&#31867;&#21487;&#33021;&#22312;&#36739;&#38271;&#26102;&#38388;&#20869;&#19982;&#21407;&#22987;&#32858;&#31867;&#24046;&#24322;&#26174;&#33879;&#65292;&#20294;&#22312;&#36739;&#30701;&#26102;&#38388;&#38388;&#38548;&#20869;&#65292;&#23427;&#20204;&#22312;&#21442;&#25968;&#24046;&#24322;&#30340;&#24433;&#21709;&#19979;&#20173;&#20445;&#25345;&#23494;&#20999;&#30456;&#20284;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;LoRA&#31639;&#27861;\cite{hu2021lora,peft}&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20026;&#24494;&#35843;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#22686;&#36827;&#20102;&#25105;&#20204;&#23545;LoRA&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15415v1 Announce Type: new  Abstract: In this paper, we employ the mathematical framework on Transformers developed by \citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters. Our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences. This work contributes to the fine-tuning field through practical applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our understanding of the behavior of LoRA-enhanced Transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;LoRA&#27169;&#22359;&#30340;&#21487;&#32452;&#21512;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#23569;&#26679;&#26412;&#24773;&#26223;&#19979;&#65292;&#26080;&#35770;&#26159;&#22343;&#21248;&#32452;&#25104;&#36824;&#26159;&#23398;&#20064;&#32452;&#25104;&#65292;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#37117;&#21487;&#20197;&#25552;&#39640;&#23545;&#26410;&#30693;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15414</link><description>&lt;p&gt;
&#32452;&#21512;&#39640;&#25928;&#21442;&#25968;&#27169;&#22359;&#26159;&#21542;&#25913;&#21892;&#23567;&#26679;&#26412;&#36801;&#31227;&#31934;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;LoRA&#27169;&#22359;&#30340;&#21487;&#32452;&#21512;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#23569;&#26679;&#26412;&#24773;&#26223;&#19979;&#65292;&#26080;&#35770;&#26159;&#22343;&#21248;&#32452;&#25104;&#36824;&#26159;&#23398;&#20064;&#32452;&#25104;&#65292;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#37117;&#21487;&#20197;&#25552;&#39640;&#23545;&#26410;&#30693;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#34987;&#35270;&#20026;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#30340;&#26631;&#20934;&#12290;&#29305;&#21035;&#26159;&#65292;&#20302;&#31209;&#21442;&#25968;&#36866;&#24212;&#24615;&#30340;&#39640;&#25928;&#24615;&#20419;&#36827;&#20102;&#25968;&#30334;&#20010;&#23450;&#21046;LoRA&#27169;&#22359;&#30340;&#21019;&#24314;&#21644;&#20849;&#20139;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;&#22312;&#21508;&#31181;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#32780;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;LoRA&#27169;&#22359;&#30340;&#21487;&#32452;&#21512;&#24615;&#65292;&#30740;&#31350;&#20102;&#21512;&#24182;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22359;&#26159;&#21542;&#22686;&#24378;&#20102;&#23545;&#26410;&#30693;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#35780;&#20272;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;a&#65289;&#22343;&#21248;&#32452;&#25104;&#65292;&#21253;&#25324;&#23558;&#19978;&#28216;LoRA&#27169;&#22359;&#22343;&#21248;&#21152;&#26435;&#24179;&#22343;&#65292;&#20197;&#21450;&#65288;b&#65289;&#23398;&#20064;&#32452;&#25104;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#19978;&#28216;&#27169;&#22359;&#19978;&#23398;&#20064;&#26435;&#37325;&#65292;&#24182;&#25191;&#34892;&#21152;&#26435;&#24179;&#22343;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#26223;&#20013;&#65292;&#21363;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#26080;&#35770;&#26159;&#22343;&#21248;&#32452;&#25104;&#36824;&#26159;&#23398;&#20064;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15414v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition
&lt;/p&gt;</description></item><item><title>G-RepsNet &#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31561;&#21464;&#32593;&#32476;&#65292;&#29992;&#20110;&#20219;&#24847;&#30697;&#38453;&#32676;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#20013;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;&#31616;&#21333;&#24265;&#20215;&#30340;&#24352;&#37327;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#36890;&#29992;&#31561;&#21464;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.15413</link><description>&lt;p&gt;
G-RepsNet&#65306;&#19968;&#31181;&#24555;&#36895;&#32780;&#36890;&#29992;&#30340;&#26500;&#24314;&#29992;&#20110;&#20219;&#24847;&#30697;&#38453;&#32676;&#30340;&#31561;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
G-RepsNet: A Fast and General Construction of Equivariant Networks for Arbitrary Matrix Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15413
&lt;/p&gt;
&lt;p&gt;
G-RepsNet &#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31561;&#21464;&#32593;&#32476;&#65292;&#29992;&#20110;&#20219;&#24847;&#30697;&#38453;&#32676;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#20013;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;&#31616;&#21333;&#24265;&#20215;&#30340;&#24352;&#37327;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#36890;&#29992;&#31561;&#21464;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#31561;&#21464;&#24615;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#19968;&#33324;&#32676;&#21644;&#22495;&#26500;&#24314;&#39640;&#25928;&#30340;&#31561;&#21464;&#32593;&#32476;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;Finzi&#31561;&#20154;&#65288;2021&#65289;&#30340;&#24037;&#20316;&#30452;&#25509;&#35299;&#20915;&#20102;&#20219;&#24847;&#30697;&#38453;&#32676;&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#31561;&#21464;MLP&#65288;EMLP&#65289;&#12290;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#25193;&#23637;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#25193;&#23637;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Group Representation Networks&#65288;G-RepsNets&#65289;&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31561;&#21464;&#32593;&#32476;&#65292;&#29992;&#20110;&#20219;&#24847;&#30697;&#38453;&#32676;&#65292;&#20854;&#20013;&#29305;&#24449;&#20351;&#29992;&#24352;&#37327;&#22810;&#39033;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#20851;&#38190;&#30452;&#35273;&#26159;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#20013;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#20197;&#21450;&#31616;&#21333;&#24265;&#20215;&#30340;&#24352;&#37327;&#25805;&#20316;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#36890;&#29992;&#31561;&#21464;&#32593;&#32476;&#12290;&#25105;&#20204;&#21457;&#29616;G-RepsNet&#22312;&#20855;&#26377;O(5)&#12289;O(1, 3)&#21644;O(3)&#23545;&#31216;&#24615;&#30340;&#20960;&#39033;&#20219;&#21153;&#20013;&#19982;EMLP&#31454;&#20105;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15413v1 Announce Type: new  Abstract: Group equivariance is a strong inductive bias useful in a wide range of deep learning tasks. However, constructing efficient equivariant networks for general groups and domains is difficult. Recent work by Finzi et al. (2021) directly solves the equivariance constraint for arbitrary matrix groups to obtain equivariant MLPs (EMLPs). But this method does not scale well and scaling is crucial in deep learning. Here, we introduce Group Representation Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix groups with features represented using tensor polynomials. The key intuition for our design is that using tensor representations in the hidden layers of a neural network along with simple inexpensive tensor operations can lead to expressive universal equivariant networks. We find G-RepsNet to be competitive to EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3) with scalars, vectors, and second-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;&#30340;&#31639;&#27861;&#27169;&#26495;, &#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#26368;&#22351;&#24773;&#24418;&#29702;&#35770;&#65292;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#65292;&#20294;&#26080;&#38656;&#36125;&#21494;&#26031;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.15411</link><description>&lt;p&gt;
&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Optimisic Information Directed Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;&#30340;&#31639;&#27861;&#27169;&#26495;, &#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#26368;&#22351;&#24773;&#24418;&#29702;&#35770;&#65292;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#65292;&#20294;&#26080;&#38656;&#36125;&#21494;&#26031;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#19978;&#19979;&#25991;&#24378;&#30423;&#38382;&#39064;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#25439;&#22833;&#20989;&#25968;&#34987;&#20551;&#23450;&#23646;&#20110;&#24050;&#30693;&#30340;&#21442;&#25968;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#23427;&#22312;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#22522;&#20110;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#30340;&#26368;&#22351;&#24773;&#24418;&#29702;&#35770;&#20043;&#38388;&#26550;&#36215;&#20102;&#26725;&#26753;&#12290;&#27762;&#21462;&#36825;&#20004;&#26041;&#38754;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;&#30340;&#31639;&#27861;&#27169;&#26495;&#65292;&#24182;&#23637;&#31034;&#23427;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#20110;&#32463;&#20856;&#36125;&#21494;&#26031;IDS&#26041;&#27861;&#21487;&#23454;&#29616;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#65292;&#20294;&#21364;&#26080;&#38656;&#20219;&#20309;&#36125;&#21494;&#26031;&#20551;&#35774;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#25216;&#26415;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#19968;&#20010;&#20048;&#35266;&#30340;&#26367;&#20195;&#27169;&#22411;&#29992;&#20110;&#36951;&#25022;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23450;&#20041;&#19968;&#20010;&#22522;&#20110;&#39057;&#29575;&#30340;&#22522;&#20110;Russo&#21644;Van Roy (2018)&#30340;&#20449;&#24687;&#27604;&#65292;&#20197;&#21450;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15411v1 Announce Type: new  Abstract: We study the problem of online learning in contextual bandit problems where the loss function is assumed to belong to a known parametric function class. We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the decision-estimation coefficient. Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions. The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a l
&lt;/p&gt;</description></item><item><title>&#22312;&#22788;&#29702;&#25317;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36890;&#36807;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#24322;&#36136;&#32553;&#25918;&#65292;Lasso&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#20272;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15409</link><description>&lt;p&gt;
&#25317;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;Lasso&#65306;&#39640;&#25928;&#20272;&#35745;&#12289;&#21327;&#21464;&#37327;&#37325;&#26032;&#32553;&#25918;&#21644;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15409
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#25317;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36890;&#36807;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#24322;&#36136;&#32553;&#25918;&#65292;Lasso&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#20272;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#24863;&#20852;&#36259;&#30340;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#26102;&#65292;Lasso&#30340;&#32479;&#35745;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#22791;&#36873;&#26041;&#26696;&#22914;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30456;&#27604;&#65292;Lasso&#30340;&#39044;&#27979;&#35823;&#24046;&#20250;&#21464;&#24471;&#20005;&#37325;&#20005;&#37325;&#12290;&#30001;&#20110;&#22312;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#23384;&#22312;&#19968;&#20010;&#34987;&#26222;&#36941;&#29468;&#27979;&#30340;&#35745;&#31639;&#32479;&#35745;&#26435;&#34913;&#65292;&#36890;&#24120;&#19981;&#21487;&#33021;&#19968;&#33324;&#24615;&#22320;&#20943;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#65292;&#20854;&#20013;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#26469;&#33258;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30001;&#24378;&#30456;&#20851;&#24615;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#30340;&#20462;&#22797;&#26041;&#27861;&#12290;&#34429;&#28982;&#26631;&#20934;&#21270;&#21327;&#21464;&#37327;&#30340;Lasso&#22833;&#36133;&#20102;&#65292;&#20294;&#26377;&#19968;&#31181;&#24322;&#36136;&#32553;&#25918;&#30340;&#21327;&#21464;&#37327;&#65292;Lasso&#23558;&#31361;&#28982;&#33719;&#24471;&#23545;&#20272;&#35745;&#30340;&#24378;&#26377;&#21147;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15409v1 Announce Type: cross  Abstract: It is well-known that the statistical performance of Lasso can suffer significantly when the covariates of interest have strong correlations. In particular, the prediction error of Lasso becomes much worse than computationally inefficient alternatives like Best Subset Selection. Due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general.   In this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables. In this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix. While Lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which Lasso will suddenly obtain strong provable guarantees for estimation. Moreover, we design a simple, efficient procedure fo
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;conformal prediction&#26694;&#26550;&#20026;Deep Operator Network&#20013;&#30340;&#22238;&#24402;&#38382;&#39064;&#25552;&#20379;&#20102;&#20855;&#26377;&#35206;&#30422;&#20445;&#35777;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#24341;&#20837;Quantile-DeepONet&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15406</link><description>&lt;p&gt;
Conformalized-DeepONet: &#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#20013;&#30340;&#26080;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty Quantification in Deep Operator Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15406
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;conformal prediction&#26694;&#26550;&#20026;Deep Operator Network&#20013;&#30340;&#22238;&#24402;&#38382;&#39064;&#25552;&#20379;&#20102;&#20855;&#26377;&#35206;&#30422;&#20445;&#35777;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#24341;&#20837;Quantile-DeepONet&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270; (UQ) &#26694;&#26550;&#8212;&#8212;conformal prediction&#65292;&#29992;&#20110;&#22312;Deep Operator Network (DeepONet) &#22238;&#24402;&#20013;&#33719;&#24471;&#20855;&#26377;&#35206;&#30422;&#20445;&#35777;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;split conformal prediction &#21152;&#24378;&#20102;&#20316;&#32773;&#20808;&#21069;&#25552;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550; (B-DeepONet &#21644; Prob-DeepONet)&#12290;&#36890;&#36807;&#23558;conformal prediction &#19982;&#25105;&#20204;&#30340;Prob- &#21644;B-DeepONets &#32467;&#21512;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#36890;&#36807;&#20026;DeepONet &#39044;&#27979;&#29983;&#25104;&#20005;&#26684;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Quantile-DeepONet&#65292;&#23427;&#20801;&#35768;&#26356;&#33258;&#28982;&#22320;&#20351;&#29992;split conformal prediction&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26080;&#20998;&#24067;&#26377;&#25928;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#31216;&#20026;split conformal Quantile-DeepONet &#22238;&#24402;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#26222;&#36890;&#30340;&#12289;&#20559;&#24494;&#20998;&#26041;&#31243;&#25968;&#20540;&#31034;&#20363;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15406v1 Announce Type: new  Abstract: In this paper, we adopt conformal prediction, a distribution-free uncertainty quantification (UQ) framework, to obtain confidence prediction intervals with coverage guarantees for Deep Operator Network (DeepONet) regression. Initially, we enhance the uncertainty quantification frameworks (B-DeepONet and Prob-DeepONet) previously proposed by the authors by using split conformal prediction. By combining conformal prediction with our Prob- and B-DeepONets, we effectively quantify uncertainty by generating rigorous confidence intervals for DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that allows for a more natural use of split conformal prediction. We refer to this distribution-free effective uncertainty quantification framework as split conformal Quantile-DeepONet regression. Finally, we demonstrate the effectiveness of the proposed methods using various ordinary, partial differential equation numerical examples, a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#26410;&#26631;&#35760;&#21644;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20302;&#25968;&#25454;&#24773;&#22659;&#19979;&#20248;&#20110;&#30417;&#30563;&#35757;&#32451;&#21644;&#20854;&#20182;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#39072;&#35206;&#20102;&#24120;&#35265;&#35748;&#30693;&#65292;&#26102;&#38388;&#24207;&#21015;&#21487;&#20197;&#20174;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;</title><link>https://arxiv.org/abs/2402.15404</link><description>&lt;p&gt;
&#32858;&#32780;&#39044;&#35757;&#32451;&#65292;&#20998;&#32780;&#19981;&#32988;&#65281;&#36890;&#36807;&#21516;&#26102;&#22312;75&#20010;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15404
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#26410;&#26631;&#35760;&#21644;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20302;&#25968;&#25454;&#24773;&#22659;&#19979;&#20248;&#20110;&#30417;&#30563;&#35757;&#32451;&#21644;&#20854;&#20182;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#39072;&#35206;&#20102;&#24120;&#35265;&#35748;&#30693;&#65292;&#26102;&#38388;&#24207;&#21015;&#21487;&#20197;&#20174;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#39046;&#22495;&#65292;&#39044;&#35757;&#32451;&#34987;&#29992;&#20110;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26469;&#28304;&#21644;&#30446;&#26631;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#21305;&#37197;&#65292;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#24182;&#19981;&#23481;&#26131;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#12290;&#20107;&#23454;&#19978;&#65292;&#26222;&#36941;&#35748;&#20026;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24182;&#19981;&#22863;&#25928;&#65281;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35768;&#22810;&#26410;&#26631;&#35760;&#21644;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#65292;&#28982;&#21518;&#21487;&#20197;&#22312;&#22810;&#20010;&#30446;&#26631;&#22495;&#65288;&#20363;&#22914;&#20998;&#31867;&#65289;&#20013;&#37325;&#22797;&#20351;&#29992;&#21333;&#19968;&#23398;&#21040;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XD-MixUp&#25554;&#20540;&#26041;&#27861;&#21644;Soft&#25554;&#20540;&#19978;&#19979;&#25991;&#23545;&#27604;&#65288;SICC&#65289;&#25439;&#22833;&#12290;&#32463;&#39564;&#35777;&#65292;&#36825;&#20248;&#20110;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#26102;&#30340;&#30417;&#30563;&#35757;&#32451;&#21644;&#20854;&#20182;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#19968;&#20107;&#23454;&#35777;&#26126;&#20102;&#24120;&#35265;&#30475;&#27861;&#30340;&#38169;&#35823;&#65306;&#25105;&#20204;&#23454;&#38469;&#19978;&#21487;&#20197;&#20174;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15404v1 Announce Type: new  Abstract: In natural language processing and vision, pretraining is utilized to learn effective representations. Unfortunately, the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target. Actually, common belief is that multi-dataset pretraining does not work for time series! Au contraire, we introduce a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned representation can then be reused in several target domains for, say, classification. Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss. Empirically, this outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes. This disproves the common belief: We can actually learn from multiple time series datasets
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15402</link><description>&lt;p&gt;
&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#65306;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#20154;&#24212;&#37325;&#26032;&#37197;&#32622;&#29289;&#20307;&#21040;&#30001;RGB-D&#22270;&#20687;&#25351;&#23450;&#30340;&#26399;&#26395;&#30446;&#26631;&#37197;&#32622;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#25506;&#32034;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24863;&#30693;&#35823;&#24046;&#25935;&#24863;&#65292;&#24182;&#19988;&#36739;&#23569;&#20851;&#27880;&#20219;&#21153;&#32423;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#37325;&#26032;&#25490;&#21015;&#26410;&#30693;&#29289;&#20307;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22122;&#22768;&#24863;&#30693;&#22914;&#20309;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#24433;&#21709;&#25235;&#21462;&#21644;&#25918;&#32622;&#65292;&#24182;&#23637;&#31034;&#36825;&#26679;&#30340;&#20998;&#31163;&#32467;&#26500;&#19981;&#23481;&#26131;&#25913;&#21892;&#20219;&#21153;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#31163;&#32467;&#26500;&#20316;&#20026;&#20808;&#39564;&#30340;GSP&#65292;&#19968;&#20010;&#21452;&#29615;&#31995;&#32479;&#12290;&#23545;&#20110;&#20869;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#20027;&#21160;&#35266;&#23519;&#31574;&#30053;&#20197;&#25552;&#39640;&#25918;&#32622;&#30340;&#24863;&#30693;&#12290;&#23545;&#20110;&#22806;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#25235;&#21462;&#31574;&#30053;&#65292;&#24847;&#35782;&#21040;&#29289;&#20307;&#21305;&#37197;&#21644;&#25235;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15402v1 Announce Type: cross  Abstract: We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability gu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#21487;&#20197;&#21435;&#38500;&#38750;&#32447;&#24615;&#21644;&#36991;&#20813;&#35823;&#24046;&#20256;&#25773;&#30340;$d$-&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#31163;&#31574;&#30053;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#22312;&#32447;DRMDP&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15399</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#65306;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#39640;&#25928;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15399
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#21487;&#20197;&#21435;&#38500;&#38750;&#32447;&#24615;&#21644;&#36991;&#20813;&#35823;&#24046;&#20256;&#25773;&#30340;$d$-&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#31163;&#31574;&#30053;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#22312;&#32447;DRMDP&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20854;&#20013;&#31574;&#30053;&#22312;&#28304;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37096;&#32626;&#21040;&#19981;&#21516;&#30340;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22312;&#32447;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DRMDPs&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#22312;&#19982;&#28304;&#22495;&#20132;&#20114;&#26102;&#65292;&#23547;&#27714;&#22312;&#28304;&#22495;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#30340;&#26368;&#24046;&#21160;&#24577;&#19979;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#31163;&#31574;&#30053;RL&#20013;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#22312;&#32447;DRMDPs&#12290;&#25105;&#20204;&#21457;&#29616;DRMDPs&#30340;&#23545;&#20598;&#24418;&#24335;&#21487;&#33021;&#20250;&#24341;&#20837;&#38750;&#32447;&#24615;&#65292;&#21363;&#20351;&#21517;&#20041;&#36716;&#31227;&#26680;&#26159;&#32447;&#24615;&#30340;&#65292;&#20063;&#20250;&#23548;&#33268;&#35823;&#24046;&#20256;&#25773;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;$d$-&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20351;&#29992;&#24635;&#21464;&#24046;&#36317;&#31163;&#65292;&#25105;&#20204;&#21435;&#38500;&#20102;&#36825;&#31181;&#39069;&#22806;&#30340;&#38750;&#32447;&#24615;&#65292;&#36991;&#20813;&#20102;&#35823;&#24046;&#20256;&#25773;&#12290;&#28982;&#21518;&#25105;&#20204;&#24341;&#20837;&#20102;DR-LSVI-UCB&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#31163;&#31574;&#30053;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#22312;&#32447;DRMDP&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15399v1 Announce Type: new  Abstract: We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain. We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel. We provide the first study on online DRMDPs with function approximation for off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation. By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation. We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics
&lt;/p&gt;</description></item><item><title>TransFlower&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#37319;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#26469;&#39044;&#27979;&#22478;&#24066;&#36890;&#21220;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15398</link><description>&lt;p&gt;
TransFlower: &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#36827;&#34892;&#36890;&#21220;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15398
&lt;/p&gt;
&lt;p&gt;
TransFlower&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#37319;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#26469;&#39044;&#27979;&#22478;&#24066;&#36890;&#21220;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22478;&#24066;&#35268;&#21010;&#21644;&#36890;&#21220;&#27969;&#20043;&#38388;&#30340;&#32852;&#31995;&#23545;&#25351;&#23548;&#22478;&#24066;&#21457;&#23637;&#21644;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#36328;&#36234;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#22478;&#24066;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#36825;&#20123;&#39046;&#22495;&#21450;&#20854;&#19981;&#21516;&#20851;&#27880;&#28857;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#22478;&#24066;&#30740;&#31350;&#26041;&#27861;&#65292;&#22914;&#24341;&#21147;&#21644;&#36752;&#23556;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#22810;&#20010;&#21464;&#37327;&#30340;&#22788;&#29702;&#26377;&#38480;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#36807;&#20110;&#31616;&#21270;&#19988;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#22914;&#31354;&#38388;&#21508;&#21521;&#21516;&#24615;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#20998;&#26512;&#36890;&#21220;&#27969;&#31561;&#22797;&#26434;&#31038;&#20250;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TransFlower&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#12289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#26469;&#39044;&#27979;&#22478;&#24066;&#36890;&#21220;&#27169;&#24335;&#12290;&#23427;&#20855;&#26377;&#19968;&#20010;&#24102;&#26377;&#21508;&#21521;&#24322;&#24615;&#24863;&#30693;&#30340;&#22320;&#29702;&#31354;&#38388;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15398v1 Announce Type: cross  Abstract: Understanding the link between urban planning and commuting flows is crucial for guiding urban development and policymaking. This research, bridging computer science and urban studies, addresses the challenge of integrating these fields with their distinct focuses. Traditional urban studies methods, like the gravity and radiation models, often underperform in complex scenarios due to their limited handling of multiple variables and reliance on overly simplistic and unrealistic assumptions, such as spatial isotropy. While deep learning models offer improved accuracy, their black-box nature poses a trade-off between performance and explainability -- both vital for analyzing complex societal phenomena like commuting flows. To address this, we introduce TransFlower, an explainable, transformer-based model employing flow-to-flow attention to predict urban commuting patterns. It features a geospatial encoder with an anisotropy-aware relative
&lt;/p&gt;</description></item><item><title>NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15393</link><description>&lt;p&gt;
NeuralThink: &#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#36827;&#34892;&#22806;&#25512;&#30340;&#31639;&#27861;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15393
&lt;/p&gt;
&lt;p&gt;
NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25797;&#38271;&#27169;&#24335;&#35782;&#21035;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#26041;&#24335;&#19978;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#24605;&#32500;&#26041;&#27861;&#23637;&#29616;&#20102;&#23398;&#20064;&#21487;&#20197;&#22806;&#25512;&#30340;&#31639;&#27861;&#30340;&#28508;&#21147;&#65306;&#22312;&#36739;&#23567;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#24182;&#22312;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#21040;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23616;&#38480;&#20110;&#23545;&#31216;&#20219;&#21153;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#30456;&#21516;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NeuralThink&#65292;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#20219;&#21153;&#22806;&#25512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NeuralThink &#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#19968;&#30452;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#34892;&#22870;&#21169;&#38598;&#27010;&#24565;&#65292;&#20197;&#24212;&#23545;&#31163;&#32447;&#35774;&#23450;&#30340;&#26426;&#20250;&#21644;&#38480;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15392</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#24378;&#21270;&#23398;&#20064;&#65306;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#21644;&#21487;&#35777;&#26126;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#34892;&#22870;&#21169;&#38598;&#27010;&#24565;&#65292;&#20197;&#24212;&#23545;&#31163;&#32447;&#35774;&#23450;&#30340;&#26426;&#20250;&#21644;&#38480;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26088;&#22312;&#20174;&#34892;&#20026;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#30446;&#21069;&#24050;&#32463;&#36880;&#28176;&#20197;&#20272;&#35745;&#21487;&#34892;&#22870;&#21169;&#38598;&#21512;&#20316;&#20026;IRL&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#36873;&#25321;&#21333;&#19968;&#22870;&#21169;&#25512;&#36831;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29616;&#26377;&#30340;&#21046;&#23450;&#21644;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38024;&#23545;&#22312;&#32447;&#35774;&#32622;&#25552;&#20986;&#65292;&#24182;&#36798;&#21040;&#20998;&#26512;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#26126;&#26174;&#19981;&#29616;&#23454;&#65292;&#22312;&#37027;&#37324;&#31163;&#32447;&#25968;&#25454;&#38598;&#26356;&#20026;&#26222;&#36941;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#25429;&#25417;&#31163;&#32447;&#29615;&#22659;&#26426;&#36935;&#21644;&#38480;&#21046;&#30340;&#21487;&#34892;&#22870;&#21169;&#38598;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15392v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations. For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward. However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will. This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario. In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation. This requires the i
&lt;/p&gt;</description></item><item><title>Genie&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#20132;&#20114;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21160;&#20316;&#21487;&#25511;&#30340;&#34394;&#25311;&#19990;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#28508;&#22312;&#34892;&#21160;&#31354;&#38388;&#20197;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#27169;&#20223;&#26410;&#35265;&#35270;&#39057;&#34892;&#20026;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15391</link><description>&lt;p&gt;
Genie&#65306;&#29983;&#25104;&#20132;&#20114;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Genie: Generative Interactive Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15391
&lt;/p&gt;
&lt;p&gt;
Genie&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#20132;&#20114;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21160;&#20316;&#21487;&#25511;&#30340;&#34394;&#25311;&#19990;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#28508;&#22312;&#34892;&#21160;&#31354;&#38388;&#20197;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#27169;&#20223;&#26410;&#35265;&#35270;&#39057;&#34892;&#20026;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Genie&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26080;&#30417;&#30563;&#26041;&#24335;&#20174;&#26410;&#26631;&#35760;&#30340;&#20114;&#32852;&#32593;&#35270;&#39057;&#20013;&#35757;&#32451;&#32780;&#25104;&#30340;&#29983;&#25104;&#24335;&#20132;&#20114;&#29615;&#22659;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#34987;&#25552;&#31034;&#29983;&#25104;&#36890;&#36807;&#25991;&#26412;&#12289;&#21512;&#25104;&#22270;&#20687;&#12289;&#29031;&#29255;&#29978;&#33267;&#32032;&#25551;&#25551;&#36848;&#30340;&#26080;&#38480;&#31181;&#31867;&#30340;&#21160;&#20316;&#21487;&#25511;&#34394;&#25311;&#19990;&#30028;&#12290;&#25317;&#26377;110&#20159;&#20010;&#21442;&#25968;&#30340;Genie&#21487;&#20197;&#34987;&#35270;&#20026;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#12290;&#20854;&#30001;&#26102;&#31354;&#35270;&#39057;&#26631;&#35760;&#22120;&#12289;&#33258;&#22238;&#24402;&#21160;&#21147;&#23398;&#27169;&#22411;&#20197;&#21450;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#28508;&#22312;&#34892;&#21160;&#27169;&#22411;&#32452;&#25104;&#12290;&#23613;&#31649;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#22320;&#38754;&#30495;&#20540;&#34892;&#21160;&#26631;&#31614;&#25110;&#20854;&#20182;&#36890;&#24120;&#22312;&#19990;&#30028;&#27169;&#22411;&#25991;&#29486;&#20013;&#25214;&#21040;&#30340;&#39046;&#22495;&#29305;&#23450;&#35201;&#27714;&#65292;&#20294;Genie&#20351;&#29992;&#25143;&#21487;&#20197;&#22522;&#20110;&#36880;&#24103;&#22522;&#30784;&#22312;&#29983;&#25104;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#12290;&#36827;&#19968;&#27493;&#65292;&#25152;&#24471;&#21040;&#30340;&#23398;&#20064;&#28508;&#22312;&#34892;&#21160;&#31354;&#38388;&#26377;&#21161;&#20110;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#27169;&#20223;&#26469;&#33258;&#26410;&#35265;&#35270;&#39057;&#30340;&#34892;&#20026;&#65292;&#20026;&#26410;&#26469;&#22521;&#35757;&#36890;&#29992;&#20195;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15391v1 Announce Type: cross  Abstract: We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.
&lt;/p&gt;</description></item><item><title>&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.15390</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#20462;&#22797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explorations of Self-Repair in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15390
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#29421;&#31364;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#21457;&#29616;&#20102;&#33258;&#20462;&#22797;&#29616;&#35937;&#65292;&#21363;&#22914;&#26524;&#21093;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#20214;&#65292;&#21518;&#32493;&#32452;&#20214;&#20250;&#25913;&#21464;&#20854;&#34892;&#20026;&#20197;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#36807;&#21435;&#30340;&#25991;&#29486;&#65292;&#23637;&#31034;&#20102;&#24403;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#21093;&#31163;&#21333;&#20010;&#27880;&#24847;&#21147;&#22836;&#26102;&#65292;&#33258;&#20462;&#22797;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#65292;&#33258;&#20462;&#22797;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#22240;&#20026;&#22836;&#37096;&#30340;&#21407;&#22987;&#30452;&#25509;&#25928;&#26524;&#24182;&#26410;&#23436;&#20840;&#24674;&#22797;&#65292;&#24182;&#19988;&#26159;&#22024;&#26434;&#30340;&#65292;&#22240;&#20026;&#33258;&#20462;&#22797;&#31243;&#24230;&#22312;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#26174;&#33879;&#21464;&#21270;&#65288;&#26377;&#26102;&#36229;&#36807;&#21407;&#22987;&#25928;&#26524;&#65289;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20419;&#25104;&#33258;&#20462;&#22797;&#30340;&#20004;&#31181;&#19981;&#21516;&#26426;&#21046;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#65288;&#21487;&#20462;&#22797;&#30452;&#25509;&#25928;&#26524;&#30340;30%&#65289;&#20197;&#21450;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.15374</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier detection by ensembling uncertainty with negative objectness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#30417;&#30563;&#24335;&#35270;&#35273;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#21151;&#33021;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#40723;&#21169;&#26631;&#20934;&#23553;&#38381;&#38598;&#27169;&#22411;&#22312;&#36127;&#35757;&#32451;&#25968;&#25454;&#20013;&#20135;&#29983;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#28151;&#28102;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#36127;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#65292;&#36825;&#20123;logits&#23545;&#24212;&#20110;K&#20010;&#22522;&#26412;&#30495;&#23454;&#31867;&#21035;&#21644;&#19968;&#20010;&#24322;&#24120;&#31867;&#21035;&#12290;&#36825;&#31181;&#35774;&#32622;&#20801;&#35768;&#25105;&#20204;&#21046;&#23450;&#19968;&#31181;&#26032;&#22855;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#20316;&#20026;&#20998;&#24067;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#31867;&#21035;&#30340;&#21518;&#39564;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36127;&#23545;&#35937;&#24615;&#12290;&#29616;&#22312;&#65292;&#24322;&#24120;&#20540;&#21487;&#20197;&#36890;&#36807;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#36127;&#25968;&#25454;&#30456;&#20284;&#20043;&#22788;&#29420;&#31435;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23884;&#20837;&#21040;&#19968;&#20010;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#65292;&#35813;&#32467;&#26500;&#20855;&#26377;K+2&#20010;&#31867;&#21035;&#30340;&#25513;&#30721;&#32423;&#21035;&#35782;&#21035;&#12290;&#35757;&#32451;&#36807;&#31243;&#40723;&#21169;&#26032;&#39062;&#30340;K+2-th&#31867;&#21035;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15374v1 Announce Type: cross  Abstract: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#32467;&#21512;&#20102;BERT&#36890;&#36947;&#21644;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#26469;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#20114;&#21160;&#21644;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.15370</link><description>&lt;p&gt;
&#21452;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#28508;&#21147;&#36827;&#34892;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#32467;&#21512;&#20102;BERT&#36890;&#36947;&#21644;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#26469;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#20114;&#21160;&#21644;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;ASTE&#65289;&#26159;&#31934;&#32454;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#24314;&#27169;&#19977;&#20803;&#32452;&#20803;&#32032;&#22266;&#26377;&#30340;&#21477;&#27861;-&#35821;&#20041;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;ASTE&#20219;&#21153;&#20013;&#21477;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;\emph{&#21452;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#28508;&#21147;}&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#21452;&#36890;&#36947;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;BERT&#36890;&#36947;&#26469;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#29992;&#20110;&#20840;&#38754;&#25429;&#25417;&#21477;&#27861;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65292;&#20197;&#25429;&#33719;&#20381;&#36182;&#21477;&#27861;&#19982;&#27880;&#24847;&#21147;&#35821;&#20041;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22359;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15370v1 Announce Type: cross  Abstract: Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these m
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30149;&#20363;-&#23545;&#29031;&#30740;&#31350;&#30340;&#36923;&#36753;&#22238;&#24402;&#21322;&#30417;&#30563;&#25512;&#26029;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#21487;&#20197;&#35782;&#21035;&#25130;&#36317;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#25130;&#36317;&#21442;&#25968;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#21487;&#36776;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15365</link><description>&lt;p&gt;
&#38024;&#23545;&#30149;&#20363;-&#23545;&#29031;&#30740;&#31350;&#30340;&#36923;&#36753;&#22238;&#24402;&#21322;&#30417;&#30563;&#25512;&#26029;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient semi-supervised inference for logistic regression under case-control studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15365
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30149;&#20363;-&#23545;&#29031;&#30740;&#31350;&#30340;&#36923;&#36753;&#22238;&#24402;&#21322;&#30417;&#30563;&#25512;&#26029;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#21487;&#20197;&#35782;&#21035;&#25130;&#36317;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#25130;&#36317;&#21442;&#25968;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#21487;&#36776;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#32467;&#26524;&#21644;&#21327;&#21464;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#20165;&#21253;&#21547;&#21327;&#21464;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#30340;&#25512;&#26029;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#32467;&#26524;&#26159;&#20108;&#36827;&#21046;&#30340;&#65292;&#32463;&#36807;&#30149;&#20363;-&#23545;&#29031;&#25277;&#26679;&#30340;&#26041;&#24335;&#25910;&#38598;&#26631;&#35760;&#25968;&#25454;&#12290;&#30149;&#20363;-&#23545;&#29031;&#25277;&#26679;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25277;&#26679;&#26041;&#26696;&#65292;&#21487;&#20943;&#36731;&#20108;&#36827;&#21046;&#25968;&#25454;&#20013;&#30340;&#19981;&#24179;&#34913;&#32467;&#26500;&#12290;&#22312;&#36923;&#36753;&#27169;&#22411;&#20551;&#35774;&#19979;&#65292;&#30149;&#20363;-&#23545;&#29031;&#25968;&#25454;&#20173;&#28982;&#21487;&#20197;&#20026;&#22238;&#24402;&#27169;&#22411;&#30340;&#26012;&#29575;&#21442;&#25968;&#25552;&#20379;&#19968;&#33268;&#30340;&#20272;&#35745;&#37327;&#12290;&#28982;&#32780;&#65292;&#25130;&#36317;&#21442;&#25968;&#26159;&#19981;&#21487;&#36776;&#35782;&#30340;&#12290;&#22240;&#27492;&#65292;&#19981;&#33021;&#20174;&#30149;&#20363;-&#23545;&#29031;&#25968;&#25454;&#20013;&#20272;&#35745;&#36793;&#38469;&#27604;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#20013;&#35782;&#21035;&#25130;&#36317;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15365v1 Announce Type: cross  Abstract: Semi-supervised learning has received increasingly attention in statistics and machine learning. In semi-supervised learning settings, a labeled data set with both outcomes and covariates and an unlabeled data set with covariates only are collected. We consider an inference problem in semi-supervised settings where the outcome in the labeled data is binary and the labeled data is collected by case-control sampling. Case-control sampling is an effective sampling scheme for alleviating imbalance structure in binary data. Under the logistic model assumption, case-control data can still provide consistent estimator for the slope parameter of the regression model. However, the intercept parameter is not identifiable. Consequently, the marginal case proportion cannot be estimated from case-control data. We find out that with the availability of the unlabeled data, the intercept parameter can be identified in semi-supervised learning setting.
&lt;/p&gt;</description></item><item><title>&#30452;&#25509;&#20272;&#35745;&#29983;&#29289;&#22768;&#23398;&#25968;&#25454;&#20013;&#30340;&#21628;&#21483;&#23494;&#24230;&#32780;&#19981;&#32771;&#34385;&#20998;&#31867;&#22120;&#20998;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#38408;&#20540;&#36873;&#25321;&#24341;&#36215;&#30340;&#20559;&#24046;&#35745;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.15360</link><description>&lt;p&gt;
&#31105;&#27490;&#20351;&#29992;&#25152;&#26377;&#38408;&#20540;&#65306;&#29983;&#29289;&#22768;&#23398;&#25968;&#25454;&#20013;&#21628;&#21483;&#23494;&#24230;&#30340;&#30452;&#25509;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15360
&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20272;&#35745;&#29983;&#29289;&#22768;&#23398;&#25968;&#25454;&#20013;&#30340;&#21628;&#21483;&#23494;&#24230;&#32780;&#19981;&#32771;&#34385;&#20998;&#31867;&#22120;&#20998;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#38408;&#20540;&#36873;&#25321;&#24341;&#36215;&#30340;&#20559;&#24046;&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#23398;&#25253;&#65306;2402.15360v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#34987;&#21160;&#22768;&#23398;&#30417;&#27979;(PAM)&#30740;&#31350;&#20135;&#29983;&#25104;&#21315;&#19978;&#19975;&#23567;&#26102;&#30340;&#38899;&#39057;&#65292;&#21487;&#20197;&#29992;&#20110;&#30417;&#27979;&#29305;&#23450;&#21160;&#29289;&#31181;&#32676;&#12289;&#36827;&#34892;&#24191;&#27867;&#30340;&#29983;&#29289;&#22810;&#26679;&#24615;&#35843;&#26597;&#12289;&#26816;&#27979;&#30423;&#29454;&#31561;&#23041;&#32961;&#65292;&#31561;&#31561;&#12290;&#29992;&#20110;&#29289;&#31181;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#22788;&#29702;&#29983;&#29289;&#22768;&#23398;&#35843;&#26597;&#29983;&#25104;&#30340;&#22823;&#37327;&#38899;&#39057;&#65292;&#21152;&#24555;&#20102;&#20998;&#26512;&#36895;&#24230;&#24182;&#22686;&#21152;&#20102;PAM&#20316;&#20026;&#31649;&#29702;&#24037;&#20855;&#30340;&#25928;&#29992;&#12290;&#22312;&#24120;&#35265;&#20570;&#27861;&#20013;&#65292;&#23545;&#20998;&#31867;&#22120;&#36755;&#20986;&#20998;&#25968;&#24212;&#29992;&#38408;&#20540;&#65292;&#24182;&#23558;&#36229;&#36807;&#38408;&#20540;&#30340;&#20998;&#25968;&#32858;&#21512;&#25104;&#26816;&#27979;&#35745;&#25968;&#12290;&#38408;&#20540;&#30340;&#36873;&#25321;&#20135;&#29983;&#20102;&#26377;&#20559;&#35265;&#30340;&#40483;&#21483;&#35745;&#25968;&#65292;&#36825;&#20123;&#40483;&#21483;&#35745;&#25968;&#21463;&#21040;&#22312;&#25968;&#25454;&#38598;&#23376;&#38598;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#35823;&#25253;/&#28431;&#25253;&#29575;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20513;&#23548;&#30452;&#25509;&#20272;&#35745;&#36890;&#35805;&#23494;&#24230;&#65306;&#21253;&#21547;&#30446;&#26631;&#40483;&#22768;&#30340;&#26816;&#27979;&#31383;&#21475;&#30340;&#27604;&#20363;&#65292;&#32780;&#19981;&#32771;&#34385;&#20998;&#31867;&#22120;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33268;&#21147;&#20110;&#23454;&#29616;&#19968;&#31181;&#29702;&#24819;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15360v1 Announce Type: cross  Abstract: Passive acoustic monitoring (PAM) studies generate thousands of hours of audio, which may be used to monitor specific animal populations, conduct broad biodiversity surveys, detect threats such as poachers, and more. Machine learning classifiers for species identification are increasingly being used to process the vast amount of audio generated by bioacoustic surveys, expediting analysis and increasing the utility of PAM as a management tool. In common practice, a threshold is applied to classifier output scores, and scores above the threshold are aggregated into a detection count. The choice of threshold produces biased counts of vocalizations, which are subject to false positive/negative rates that may vary across subsets of the dataset. In this work, we advocate for directly estimating call density: The proportion of detection windows containing the target vocalization, regardless of classifier score. Our approach targets a desirabl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#39640;&#26031;&#29380;&#21033;&#20811;&#38647;&#38543;&#26426;&#22330;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#20998;&#31867;&#35266;&#27979;&#25968;&#25454;&#65292;&#22312;&#31354;&#38388;&#39044;&#27979;&#20013;&#21462;&#24471;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.15359</link><description>&lt;p&gt;
&#27969;&#24335;&#39640;&#26031;&#29380;&#21033;&#20811;&#38647;&#38543;&#26426;&#22330;&#29992;&#20110;&#39640;&#32500;&#20998;&#31867;&#35266;&#27979;&#30340;&#31354;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#39640;&#26031;&#29380;&#21033;&#20811;&#38647;&#38543;&#26426;&#22330;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#20998;&#31867;&#35266;&#27979;&#25968;&#25454;&#65292;&#22312;&#31354;&#38388;&#39044;&#27979;&#20013;&#21462;&#24471;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#27969;&#24335;&#39640;&#26031;&#29380;&#21033;&#20811;&#38647;&#38543;&#26426;&#22330;&#65288;S-GDRF&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#27969;&#24335;&#31354;&#38388;&#20998;&#24067;&#31232;&#30095;&#12289;&#39640;&#32500;&#20998;&#31867;&#35266;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#39640;&#25928;&#22320;&#23398;&#20064;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#27169;&#24335;&#65292;&#20801;&#35768;&#24555;&#36895;&#25512;&#26029;&#21644;&#26597;&#35810;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#30340;&#39640;&#20998;&#36776;&#29575;&#28014;&#28216;&#29983;&#29289;&#22270;&#20687;&#25968;&#25454;&#31995;&#21015;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;VGP&#65289;&#33021;&#22815;&#20570;&#20986;&#26356;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#20174;&#27969;&#24335;&#20998;&#31867;&#25968;&#25454;&#20013;&#23398;&#20064;&#35266;&#27979;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;S-GDRF&#20026;&#26377;&#25928;&#22320;&#22312;&#39640;&#32500;&#20998;&#31867;&#35266;&#27979;&#19978;&#36827;&#34892;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#32780;&#36825;&#22312;&#27492;&#21069;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15359v1 Announce Type: cross  Abstract: We present the Streaming Gaussian Dirichlet Random Field (S-GDRF) model, a novel approach for modeling a stream of spatiotemporally distributed, sparse, high-dimensional categorical observations. The proposed approach efficiently learns global and local patterns in spatiotemporal data, allowing for fast inference and querying with a bounded time complexity. Using a high-resolution data series of plankton images classified with a neural network, we demonstrate the ability of the approach to make more accurate predictions compared to a Variational Gaussian Process (VGP), and to learn a predictive distribution of observations from streaming categorical data. S-GDRFs open the door to enabling efficient informative path planning over high-dimensional categorical observations, which until now has not been feasible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#20687;&#21435;&#22122;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#30528;&#37325;&#20851;&#27880;&#24402;&#19968;&#21270;&#31561;&#21464;&#24615;&#36136;&#65292;&#31361;&#20986;&#20102;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2402.15352</link><description>&lt;p&gt;
&#23545;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21435;&#22122;&#26041;&#27861;&#30340;&#24402;&#19968;&#21270;&#31561;&#21464;&#24615;&#36136;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#20687;&#21435;&#22122;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#30528;&#37325;&#20851;&#27880;&#24402;&#19968;&#21270;&#31561;&#21464;&#24615;&#36136;&#65292;&#31361;&#20986;&#20102;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21435;&#22122;&#21487;&#33021;&#26159;&#22270;&#20687;&#22788;&#29702;&#20013;&#26368;&#21476;&#32769;&#19988;&#20173;&#28982;&#26159;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#20043;&#19968;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#35770;&#27010;&#24565;&#65292;&#24182;&#38543;&#30528;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#22312;&#36817;&#24180;&#26469;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#22270;&#20687;&#21435;&#22122;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#65292;&#23545;&#27492;&#28436;&#21464;&#36807;&#31243;&#20013;&#38416;&#26126;&#30340;&#20027;&#35201;&#21407;&#21017;&#36827;&#34892;&#20998;&#31867;&#65292;&#29305;&#21035;&#20851;&#27880;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#23427;&#34987;&#26500;&#24819;&#20026;&#19968;&#20010;&#20197;&#24403;&#21069;&#26041;&#27861;&#20026;&#26694;&#26550;&#30340;&#25945;&#31243;&#65292;&#25552;&#20379;&#23545;&#25991;&#29486;&#20013;&#26368;&#26377;&#25928;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#65292;&#31361;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#20043;&#38388;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20102;&#24402;&#19968;&#21270;&#31561;&#21464;&#24615;&#36136;&#65292;&#36825;&#19968;&#24615;&#36136;&#20196;&#20154;&#24778;&#35766;&#22320;&#24182;&#27809;&#26377;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15352v1 Announce Type: cross  Abstract: Image denoising is probably the oldest and still one of the most active research topic in image processing. Many methodological concepts have been introduced in the past decades and have improved performances significantly in recent years, especially with the emergence of convolutional neural networks and supervised deep learning. In this paper, we propose a survey of guided tour of supervised and unsupervised learning methods for image denoising, classifying the main principles elaborated during this evolution, with a particular concern given to recent developments in supervised learning. It is conceived as a tutorial organizing in a comprehensive framework current approaches. We give insights on the rationales and limitations of the most performant methods in the literature, and we highlight the common features between many of them. Finally, we focus on on the normalization equivariance properties that is surprisingly not guaranteed 
&lt;/p&gt;</description></item><item><title>AutoMMLab&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#22686;&#24378;AutoML&#31995;&#32479;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#35821;&#35328;&#25351;&#20196;&#26469;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#25972;&#20010;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#38750;&#19987;&#23478;&#20010;&#20307;&#26356;&#23481;&#26131;&#26500;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15351</link><description>&lt;p&gt;
AutoMMLab&#65306;&#20174;&#35821;&#35328;&#25351;&#20196;&#33258;&#21160;&#29983;&#25104;&#21487;&#37096;&#32626;&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15351
&lt;/p&gt;
&lt;p&gt;
AutoMMLab&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#22686;&#24378;AutoML&#31995;&#32479;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#35821;&#35328;&#25351;&#20196;&#26469;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#25972;&#20010;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#38750;&#19987;&#23478;&#20010;&#20307;&#26356;&#23481;&#26131;&#26500;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15351v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26159;&#19968;&#32452;&#26088;&#22312;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#36807;&#31243;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;AutoML&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#22411;&#24320;&#21457;&#30340;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65288;&#20363;&#22914;&#36229;&#21442;&#25968;&#20248;&#21270;&#65289;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#21270;&#25972;&#20010;&#31471;&#21040;&#31471;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;AutoML&#31995;&#32479;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoMMLab&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#22686;&#24378;AutoML&#31995;&#32479;&#65292;&#25353;&#29031;&#29992;&#25143;&#30340;&#35821;&#35328;&#25351;&#20196;&#26469;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#25972;&#20010;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;AutoMMLab&#31995;&#32479;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#20316;&#20026;&#36830;&#25509;AutoML&#21644;OpenMMLab&#31038;&#21306;&#30340;&#26725;&#26753;&#65292;&#20351;&#38750;&#19987;&#23478;&#20010;&#20307;&#33021;&#22815;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#35821;&#35328;&#30028;&#38754;&#36731;&#26494;&#26500;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;RU-LLaMA&#26469;&#29702;&#35299;&#29992;&#25143;&#30340;&#35831;&#27714;&#24182;&#23433;&#25490;&#25972;&#20010;&#27969;&#27700;&#32447;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120; c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15351v1 Announce Type: new  Abstract: Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer c
&lt;/p&gt;</description></item><item><title>Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.15350</link><description>&lt;p&gt;
Farsight&#65306;&#22312;AI&#24212;&#29992;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#22521;&#20859;&#36127;&#36131;&#20219;&#30340;AI&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Farsight: Fostering Responsible AI Awareness During AI Application Prototyping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15350
&lt;/p&gt;
&lt;p&gt;
Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#39537;&#21160;&#30028;&#38754;&#20351;&#24471;&#21407;&#22411;&#35774;&#35745;&#21644;&#26500;&#24314;AI&#24212;&#29992;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#21487;&#33021;&#22312;AI&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;Farsight&#65292;&#24110;&#21161;&#20154;&#20204;&#35782;&#21035;&#20182;&#20204;&#27491;&#22312;&#35774;&#35745;&#21407;&#22411;&#30340;AI&#24212;&#29992;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;&#26681;&#25454;&#29992;&#25143;&#30340;&#25552;&#31034;&#65292;Farsight&#31361;&#20986;&#26174;&#31034;&#20102;&#19982;&#30456;&#20851;AI&#20107;&#20214;&#26377;&#20851;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25506;&#32034;&#21644;&#32534;&#36753;LLM&#29983;&#25104;&#30340;&#29992;&#20363;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19982;10&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#20849;&#21516;&#35774;&#35745;&#30740;&#31350;&#30340;&#35774;&#35745;&#35265;&#35299;&#65292;&#20197;&#21450;&#19982;42&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#12290;&#22312;&#20351;&#29992;Farsight&#21518;&#65292;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#20013;&#30340;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#24037;&#20855;&#27604;&#29616;&#26377;&#36164;&#28304;&#26356;&#26377;&#29992;&#19988;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15350v1 Announce Type: cross  Abstract: Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user's prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.15347</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Safe Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#35780;&#20272;&#36829;&#21453;&#20808;&#39564;&#26410;&#30693;&#65288;&#23433;&#20840;&#65289;&#32422;&#26463;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26410;&#30693;&#20989;&#25968;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#22312;&#26410;&#30693;&#20989;&#25968;&#19978;&#25918;&#32622;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#19988;&#20165;&#20801;&#35768;&#22312;&#39640;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#20869;&#36827;&#34892;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#22495;&#30340;&#31163;&#25955;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#30452;&#25509;&#25193;&#23637;&#21040;&#36830;&#32493;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21033;&#29992;&#32422;&#26463;&#30340;&#35268;&#21017;&#20551;&#35774;&#30340;&#26041;&#24335;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#30452;&#25509;&#21033;&#29992;GP&#21518;&#39564;&#26469;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#30340;&#23433;&#20840;&#21442;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#23558;&#36825;&#19968;&#25506;&#32034;&#20934;&#21017;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21463;&#38480;Fourier&#22522;&#30340;&#36731;&#37327;&#32423;&#12289;&#28789;&#27963;&#19988;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27010;&#29575;&#23494;&#24230;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#36924;&#36817;&#21508;&#31181;&#22810;&#27169;&#24577;1&#32500;&#23494;&#24230;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#22240;&#24335;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#23398;&#20064;&#21387;&#32553;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15345</link><description>&lt;p&gt;
Fourier&#22522;&#23494;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fourier Basis Density Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15345
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21463;&#38480;Fourier&#22522;&#30340;&#36731;&#37327;&#32423;&#12289;&#28789;&#27963;&#19988;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27010;&#29575;&#23494;&#24230;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#36924;&#36817;&#21508;&#31181;&#22810;&#27169;&#24577;1&#32500;&#23494;&#24230;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#22240;&#24335;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#23398;&#20064;&#21387;&#32553;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#28789;&#27963;&#19988;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27010;&#29575;&#23494;&#24230;&#27169;&#22411;&#65292;&#20854;&#30001;&#19968;&#20010;&#21463;&#38480;&#30340;Fourier&#22522;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#22312;&#36924;&#36817;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;1&#32500;&#23494;&#24230;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#23494;&#24230;&#36890;&#24120;&#24456;&#38590;&#25311;&#21512;&#12290;&#19982;[1]&#20013;&#24341;&#20837;&#30340;&#28145;&#24230;&#22240;&#24335;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#31867;&#20284;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#20132;&#21449;&#29109;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#29609;&#20855;&#21387;&#32553;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#23398;&#20064;&#21387;&#32553;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15345v1 Announce Type: new  Abstract: We introduce a lightweight, flexible and end-to-end trainable probability density model parameterized by a constrained Fourier basis. We assess its performance at approximating a range of multi-modal 1D densities, which are generally difficult to fit. In comparison to the deep factorized model introduced in [1], our model achieves a lower cross entropy at a similar computational budget. In addition, we also evaluate our method on a toy compression task, demonstrating its utility in learned compression.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#22266;&#23450;&#25110;&#36882;&#20943;&#23398;&#20064;&#29575;&#30340;SGD&#36827;&#34892;&#38750;&#20984;&#20248;&#21270;&#26102;&#65292;&#25209;&#37327;&#22823;&#23567;&#19982;&#36845;&#20195;&#21644;SFO&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20351;&#29992;&#20851;&#38190;&#25209;&#37327;&#22823;&#23567;&#30340;SGD&#21487;&#20197;&#26368;&#23567;&#21270;SFO&#22797;&#26434;&#24230;</title><link>https://arxiv.org/abs/2402.15344</link><description>&lt;p&gt;
&#20351;&#29992;&#22266;&#23450;&#21644;&#36882;&#20943;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#36845;&#20195;&#21644;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#32773;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15344
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#22266;&#23450;&#25110;&#36882;&#20943;&#23398;&#20064;&#29575;&#30340;SGD&#36827;&#34892;&#38750;&#20984;&#20248;&#21270;&#26102;&#65292;&#25209;&#37327;&#22823;&#23567;&#19982;&#36845;&#20195;&#21644;SFO&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20351;&#29992;&#20851;&#38190;&#25209;&#37327;&#22823;&#23567;&#30340;SGD&#21487;&#20197;&#26368;&#23567;&#21270;SFO&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#21644;&#25209;&#37327;&#22823;&#23567;&#65292;&#24433;&#21709;&#35757;&#32451;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#21644;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#32773;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#12290;&#20808;&#21069;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#20351;&#29992;&#22266;&#23450;&#23398;&#20064;&#29575;&#30340;SGD&#65292;&#38543;&#30528;&#25209;&#37327;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#35757;&#32451;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#20943;&#23569;&#65292;&#24182;&#19988;SFO&#22797;&#26434;&#24230;&#22312;&#20851;&#38190;&#25209;&#37327;&#22823;&#23567;&#26102;&#26368;&#23567;&#21270;&#65292;&#19968;&#26086;&#25209;&#37327;&#22823;&#23567;&#36229;&#36807;&#35813;&#22823;&#23567;&#21518;&#22686;&#21152;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#22266;&#23450;&#25110;&#36882;&#20943;&#23398;&#20064;&#29575;&#30340;SGD&#36827;&#34892;&#38750;&#20984;&#20248;&#21270;&#26102;&#65292;&#25209;&#37327;&#22823;&#23567;&#19982;&#25152;&#38656;&#36845;&#20195;&#21644;SFO&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#20851;&#38190;&#25209;&#37327;&#22823;&#23567;&#30340;SGD&#21487;&#20197;&#26368;&#23567;&#21270;SFO&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15344v1 Announce Type: cross  Abstract: The performance of stochastic gradient descent (SGD), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. They both affect the number of iterations and the stochastic first-order oracle (SFO) complexity needed for training. In particular, the previous numerical results indicated that, for SGD using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates and show that SGD using the critical batch size minimizes the SFO complexity. We also provide numerical compariso
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;NuNER&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.15343</link><description>&lt;p&gt;
NuNER: &#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15343
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;NuNER&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#25968;&#25454;&#26631;&#27880;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20026;&#35299;&#20915;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;NuNER&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#12290;NuNER&#21487;&#20197;&#34987;&#24494;&#35843;&#20197;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#19979;&#28216;&#30340;NER&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#23454;&#20307;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#26159;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#23558;NuNER&#35270;&#20026;&#26368;&#36817;&#34987;LLMs&#35299;&#38145;&#30340;&#26356;&#24191;&#27867;&#30340;&#29305;&#23450;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#30340;&#19968;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15343v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;LLMs&#25506;&#32034;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20854;&#22312;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#19978;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15337</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#27839;&#30528;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#23545;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#65306;&#24494;&#35843;&#31574;&#30053;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;LLMs&#25506;&#32034;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20854;&#22312;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#19978;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#31354;&#38388;&#20197;&#23454;&#20307;&#30340;&#21407;&#22987;&#35821;&#20041;&#29305;&#24449;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#23398;&#20064;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#24314;&#27169;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#26102;&#12290;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#28860;&#27010;&#24565;&#31354;&#38388;&#26368;&#36817;&#20986;&#29616;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#20351;&#29992;&#30456;&#23545;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#31574;&#30053;&#25506;&#26597;&#39044;&#35757;&#32451;&#30340;LLMs&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#26681;&#25454;&#32473;&#23450;&#30340;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#23545;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#30340;&#20219;&#21153;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#30340;&#30495;&#23454;&#25490;&#21517;&#24456;&#23569;&#35265;&#65292;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#24494;&#35843;LLMs&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26356;&#23481;&#26131;&#33719;&#24471;&#30340;&#29305;&#24449;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20998;&#26512;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;&#25490;&#21517;&#33021;&#21147;&#26159;&#21542;&#33021;&#36716;&#31227;&#21040;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#30830;&#23454;&#26159;&#36825;&#31181;&#24773;&#20917;&#65292;&#20294;&#26159;&#26410;&#23436;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15337v1 Announce Type: new  Abstract: Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but havi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;HAD&#32593;&#32476;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#21487;&#23398;&#20064;&#23383;&#20856;&#30340;LRR&#27169;&#22411;&#65292;&#31216;&#20026;LRR-Net$^+$&#65292;&#21487;&#20197;&#26356;&#36890;&#29992;&#22320;&#20809;&#35889;&#35299;&#32806;&#32972;&#26223;&#32467;&#26500;&#21644;&#30446;&#26631;&#29305;&#24615;&#65292;&#24182;&#28040;&#38500;&#30001;&#37325;&#35201;&#24178;&#25200;&#30446;&#26631;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;&#21516;&#26102;&#65292;&#36824;&#26500;&#24314;&#20102;&#29992;&#20110;&#25913;&#36827;HAD&#31639;&#27861;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;AIR-HAD&#12290;</title><link>https://arxiv.org/abs/2402.15335</link><description>&lt;p&gt;
&#20302;&#31209;&#34920;&#31034;&#36935;&#19978;&#28145;&#24230;&#23637;&#24320;&#65306;&#29992;&#20110;&#39640;&#20809;&#35889;&#24322;&#24120;&#26816;&#27979;&#30340;&#24191;&#20041;&#21644;&#21487;&#35299;&#37322;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;HAD&#32593;&#32476;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#21487;&#23398;&#20064;&#23383;&#20856;&#30340;LRR&#27169;&#22411;&#65292;&#31216;&#20026;LRR-Net$^+$&#65292;&#21487;&#20197;&#26356;&#36890;&#29992;&#22320;&#20809;&#35889;&#35299;&#32806;&#32972;&#26223;&#32467;&#26500;&#21644;&#30446;&#26631;&#29305;&#24615;&#65292;&#24182;&#28040;&#38500;&#30001;&#37325;&#35201;&#24178;&#25200;&#30446;&#26631;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;&#21516;&#26102;&#65292;&#36824;&#26500;&#24314;&#20102;&#29992;&#20110;&#25913;&#36827;HAD&#31639;&#27861;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;AIR-HAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#39640;&#20809;&#35889;&#24322;&#24120;&#26816;&#27979;&#65288;HAD&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#23384;&#22312;&#20998;&#36776;&#29575;&#20302;&#12289;&#32972;&#26223;&#31616;&#21333;&#12289;&#26816;&#27979;&#25968;&#25454;&#35268;&#27169;&#23567;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#22240;&#32032;&#20063;&#38480;&#21046;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#20302;&#31209;&#34920;&#31034;&#65288;LRR&#65289;&#27169;&#22411;&#22312;&#32972;&#26223;&#21644;&#30446;&#26631;&#29305;&#24449;&#20998;&#31163;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#23545;&#25163;&#21160;&#21442;&#25968;&#36873;&#25321;&#30340;&#20381;&#36182;&#24615;&#33021;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#26032;&#30340;&#29992;&#20110;&#25913;&#21892;&#22797;&#26434;&#22330;&#26223;&#19979;HAD&#31639;&#27861;&#40065;&#26834;&#24615;&#30340;HAD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#31616;&#31216;AIR-HAD&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;HAD&#32593;&#32476;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#21487;&#23398;&#20064;&#23383;&#20856;&#30340;LRR&#27169;&#22411;&#26469;&#23436;&#25104;&#65292;&#21629;&#21517;&#20026;LRR-Net$^+$&#65292;&#33021;&#22815;&#26356;&#36890;&#29992;&#22320;&#20809;&#35889;&#35299;&#32806;&#32972;&#26223;&#32467;&#26500;&#21644;&#30446;&#26631;&#29305;&#24615;&#65292;&#24182;&#21516;&#26102;&#28040;&#38500;&#30001;&#37325;&#35201;&#24178;&#25200;&#30446;&#26631;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;LRR-Net$^+$&#38598;&#25104;&#20102;&#20132;&#26367;&#26041;&#21521;&#27861;&#30340;&#35299;&#20915;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15335v1 Announce Type: cross  Abstract: Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from low resolution, simple background, and small size of the detection data. These factors also limit the performance of the well-known low-rank representation (LRR) models in terms of robustness on the separation of background and target features and the reliance on manual parameter selection. To this end, we build a new set of HAD benchmark datasets for improving the robustness of the HAD algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a generalized and interpretable HAD network by deeply unfolding a dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of spectrally decoupling the background structure and object properties in a more generalized fashion and eliminating the bias introduced by vital interference targets concurrently. In addition, LRR-Net$^+$ integrates the solution process of the Alternating Direction Metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.15332</link><description>&lt;p&gt;
&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#31181;&#20851;&#20110;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Categorical Deep Learning: An Algebraic Theory of Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#25351;&#23450;&#21644;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#36890;&#29992;&#26694;&#26550;&#30340;&#31435;&#22330;&#12290;&#25105;&#20204;&#35748;&#20026;&#21040;&#30446;&#21069;&#20026;&#27490;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#23581;&#35797;&#32570;&#20047;&#19968;&#31181;&#19968;&#33268;&#30340;&#26725;&#26753;&#65292;&#33021;&#22815;&#25351;&#23450;&#27169;&#22411;&#24517;&#39035;&#28385;&#36275;&#30340;&#32422;&#26463;&#24182;&#35268;&#23450;&#23427;&#20204;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;&#19987;&#27880;&#20110;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#26725;&#26753;&#65292;&#25105;&#20204;&#24314;&#35758;&#24212;&#29992;&#33539;&#30068;&#35770;&#8212;&#8212;&#20934;&#30830;&#22320;&#35828;&#65292;&#21333;&#23376;&#20540;&#20110;&#21442;&#25968;&#26144;&#23556;&#30340;&#20108;&#33539;&#30068;&#30340;&#36890;&#29992;&#20195;&#25968;&#8212;&#8212;&#20316;&#20026;&#19968;&#31181;&#21333;&#19968;&#29702;&#35770;&#65292;&#20248;&#38597;&#22320;&#21253;&#21547;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#36825;&#20004;&#31181;&#39118;&#26684;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#24674;&#22797;&#30001;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23548;&#33268;&#30340;&#32422;&#26463;&#65292;&#20197;&#21450;&#20174;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#31181;&#26550;&#26500;&#65288;&#22914;RNNs&#65289;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15332v1 Announce Type: cross  Abstract: We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#24314;&#31435;&#22522;&#20110;&#21407;&#21017;&#30340;&#20219;&#21153;&#20998;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#25968;&#23398;&#35268;&#21010;&#24418;&#24335;&#35299;&#20915;&#20102;&#36164;&#28304;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15328</link><description>&lt;p&gt;
&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#24314;&#31435;&#22522;&#20110;&#21407;&#21017;&#30340;&#20219;&#21153;&#20998;&#32452;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Task Grouping for Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15328
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#24314;&#31435;&#22522;&#20110;&#21407;&#21017;&#30340;&#20219;&#21153;&#20998;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#25968;&#23398;&#35268;&#21010;&#24418;&#24335;&#35299;&#20915;&#20102;&#36164;&#28304;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#20013;&#20219;&#21153;&#20998;&#32452;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20851;&#38190;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38480;&#21046;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20855;&#29702;&#35770;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#26500;&#24314;&#36716;&#31227;&#22686;&#30410;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#25968;&#23398;&#35268;&#21010;&#24418;&#24335;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#36164;&#28304;&#32422;&#26463;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#12289;&#32452;&#21512;&#20248;&#21270;&#22522;&#20934;&#21644;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#22522;&#32447;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;MTL&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15328v1 Announce Type: new  Abstract: This paper presents a novel approach to task grouping in Multitask Learning (MTL), advancing beyond existing methods by addressing key theoretical and practical limitations. Unlike prior studies, our approach offers a more theoretically grounded method that does not rely on restrictive assumptions for constructing transfer gains. We also propose a flexible mathematical programming formulation which can accommodate a wide spectrum of resource constraints, thus enhancing its versatility. Experimental results across diverse domains, including computer vision datasets, combinatorial optimization benchmarks and time series tasks, demonstrate the superiority of our method over extensive baselines, validating its effectiveness and general applicability in MTL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#20005;&#26684;&#35777;&#26126;&#20102;&#36807;&#24230;&#24179;&#28369;&#19982;&#25193;&#25955;&#31639;&#23376;&#30340;&#36941;&#21382;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.15326</link><description>&lt;p&gt;
&#20174;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#30340;&#35270;&#35282;&#29702;&#35299;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#20005;&#26684;&#35777;&#26126;&#20102;&#36807;&#24230;&#24179;&#28369;&#19982;&#25193;&#25955;&#31639;&#23376;&#30340;&#36941;&#21382;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#30340;&#35282;&#24230;&#23545;&#25193;&#25955;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#26032;&#30740;&#31350;&#12290;&#19982;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#20998;&#26512;&#25110;&#31890;&#23376;&#31995;&#32479;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36825;&#19968;&#29702;&#35770;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#35777;&#26126;&#36807;&#24230;&#24179;&#28369;&#19982;&#25193;&#25955;&#31639;&#23376;&#30340;&#36941;&#21382;&#24615;&#24687;&#24687;&#30456;&#20851;&#12290;&#36825;&#19968;&#21457;&#29616;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#28201;&#21644;&#30340;&#36941;&#21382;&#24615;&#30772;&#22351;&#26465;&#20214;&#65292;&#21253;&#25324;&#20808;&#21069;&#25552;&#20986;&#30340;&#21508;&#31181;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#25105;&#20204;&#29702;&#35770;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#32852;&#31995;&#65292;&#25299;&#23485;&#20102;&#29702;&#35770;&#35270;&#37326;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#36941;&#21382;&#24615;&#30772;&#22351;&#39033;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20197;&#36842;&#21033;&#20811;&#38647;&#33021;&#37327;&#34913;&#37327;&#30340;&#36807;&#24230;&#24179;&#28369;&#65292;&#24182;&#36798;&#21040;&#20102;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15326v1 Announce Type: new  Abstract: This paper presents a novel study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based GNNs. Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simul
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#35770;&#65292;&#26412;&#35770;&#25991;&#23558;Shapley&#20540;&#21644;&#20984;&#21338;&#24328;&#27010;&#24565;&#25193;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15324</link><description>&lt;p&gt;
&#22522;&#20110;Shapley&#20540;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65306;&#29702;&#35770;&#12289;&#26041;&#27861;&#21450;&#20854;&#22312;&#33021;&#28304;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15324
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#35770;&#65292;&#26412;&#35770;&#25991;&#23558;Shapley&#20540;&#21644;&#20984;&#21338;&#24328;&#27010;&#24565;&#25193;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15324v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#24050;&#32463;&#35774;&#35745;&#20102;&#35768;&#22810;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#30340;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#20123;&#20449;&#29992;&#20998;&#37197;&#26041;&#26696;&#24050;&#34987;&#35777;&#26126;&#23545;&#25913;&#21892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26377;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#65292;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#22240;&#27492;&#26080;&#27861;&#29702;&#35299;&#26234;&#33021;&#20307;&#22914;&#20309;&#21512;&#20316;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#35770;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22522;&#30784;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#19968;&#20010;&#31216;&#20026;&#20984;&#21338;&#24328;&#30340;&#28216;&#25103;&#27169;&#22411;&#21644;&#19968;&#20010;&#31216;&#20026;Shapley&#20540;&#30340;&#25903;&#20184;&#20998;&#37197;&#26041;&#26696;&#22312;&#21512;&#20316;&#21338;&#24328;&#35770;&#20013;&#25193;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21629;&#21517;&#20026;&#39532;&#23572;&#21487;&#22827;&#20984;&#21338;&#24328;&#21644;&#39532;&#27663;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15324v1 Announce Type: cross  Abstract: Multi-agent reinforcement learning is an area of rapid advancement in artificial intelligence and machine learning. One of the important questions to be answered is how to conduct credit assignment in a multi-agent system. There have been many schemes designed to conduct credit assignment by multi-agent reinforcement learning algorithms. Although these credit assignment schemes have been proved useful in improving the performance of multi-agent reinforcement learning, most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate. In this thesis, we aim at investigating the foundation of credit assignment in multi-agent reinforcement learning via cooperative game theory. We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to Markov decision process, named as Markov convex game and Markov S
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;</title><link>https://arxiv.org/abs/2402.15321</link><description>&lt;p&gt;
OpenSUN3D: &#24320;&#25918;&#35789;&#27719;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#31532;&#19968;&#27425;&#30740;&#35752;&#20250;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#27010;&#36848;&#20102;&#22312;2023&#24180;ICCV&#20250;&#35758;&#19978;&#20030;&#21150;&#30340;OpenSUN3D Workshop&#20851;&#20110;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#35752;&#20250;&#31995;&#21015;&#30340;&#30446;&#26631;&#26159;&#20026;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#25552;&#20379;&#25506;&#32034;&#21644;&#35752;&#35770;&#24179;&#21488;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#20998;&#21106;&#12289;&#26816;&#27979;&#21644;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30740;&#35752;&#20250;&#19978;&#20030;&#21150;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#23637;&#31034;&#20102;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;&#12290;&#26356;&#22810;&#35814;&#24773;&#35831;&#21442;&#38405;https://opensun3d.github.io/index_iccv23.html&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15321v1 Announce Type: cross  Abstract: This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#65292;GPTVQ&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#36824;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15319</link><description>&lt;p&gt;
GPTVQ&#65306;LLM&#37327;&#21270;&#20013;&#32500;&#24230;&#30340;&#31119;&#38899;
&lt;/p&gt;
&lt;p&gt;
GPTVQ: The Blessing of Dimensionality for LLM Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15319
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#65292;GPTVQ&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#36824;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#30340;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GPTVQ&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24555;&#36895;&#21518;&#35757;&#32451;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20132;&#26367;&#36827;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#21015;&#30340;&#37327;&#21270;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#27599;&#23618;&#36755;&#20986;&#37325;&#24314;MSE&#30340;Hessian&#20449;&#24687;&#26469;&#26356;&#26032;&#20854;&#20313;&#26410;&#37327;&#21270;&#30340;&#26435;&#37325;&#12290;&#37327;&#21270;&#30721;&#20070;&#20351;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;EM&#31639;&#27861;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#25968;&#37327;&#21270;&#21644;&#22522;&#20110;SVD&#30340;&#21387;&#32553;&#36827;&#19968;&#27493;&#21387;&#32553;&#30721;&#20070;&#12290;GPTVQ&#22312;&#35832;&#22914;Llama-v2&#21644;Mistral&#31561;&#21508;&#31181;LLMs&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#65306;&#22312;&#21333;&#20010;H100&#19978;&#65292;&#22788;&#29702;&#19968;&#20010;Llamav2-70B&#38656;&#35201;3&#33267;11&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15319v1 Announce Type: cross  Abstract: In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#20110;&#26368;&#23567;&#28145;&#24230;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26368;&#23567;&#28145;&#24230;&#19982;CPWL&#20989;&#25968;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15315</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26368;&#23567;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Minimal Depth in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#20110;&#26368;&#23567;&#28145;&#24230;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26368;&#23567;&#28145;&#24230;&#19982;CPWL&#20989;&#25968;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;ReLU&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#19982;&#34920;&#31034;&#20219;&#20309;&#36830;&#32493;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65288;CPWL&#65289;&#25152;&#38656;&#30340;&#26368;&#23567;&#28145;&#24230;&#30456;&#20851;&#30340;&#29468;&#24819;&#30340;&#20851;&#31995;&#36827;&#34892;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#29305;&#24615;&#12290;&#30740;&#31350;&#37325;&#28857;&#21253;&#25324;&#23545;&#27714;&#21644;&#21644;&#26368;&#22823;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#34920;&#31034;&#65292;&#20197;&#21450;&#23545;&#22810;&#38754;&#20307;&#31070;&#32463;&#32593;&#32476;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#27714;&#21644;&#36816;&#31639;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#25805;&#20316;&#25968;&#26368;&#23567;&#28145;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#20197;&#25214;&#21040;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#12290;&#30456;&#21453;&#65292;&#20851;&#20110;&#26368;&#22823;&#36816;&#31639;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#20165;&#20381;&#36182;&#20110;&#25805;&#20316;&#25968;&#28145;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19981;&#20250;&#26263;&#31034;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#12290;&#30740;&#31350;&#36824;&#32771;&#23519;&#20102;&#20984;CPWL&#20989;&#25968;&#20043;&#38388;&#30340;&#26368;&#23567;&#28145;&#24230;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15315v1 Announce Type: new  Abstract: A characterization of the representability of neural networks is relevant to comprehend their success in artificial intelligence. This study investigate two topics on ReLU neural network expressivity and their connection with a conjecture related to the minimum depth required for representing any continuous piecewise linear function (CPWL). The topics are the minimal depth representation of the sum and max operations, as well as the exploration of polytope neural networks. For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation. In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation. The study also examine the minimal depth relationship between convex CPWL functions. On polytope neural ne
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15313</link><description>&lt;p&gt;
ArabianGPT&#65306;&#22522;&#20110;&#21407;&#29983;&#38463;&#25289;&#20271;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ArabianGPT: Native Arabic GPT-based Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#21644;&#25289;&#19969;&#35821;&#20026;&#20027;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#26174;&#33879;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#32780;&#25104;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#19982;&#38463;&#25289;&#20271;&#35821;&#30340;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
&lt;/p&gt;</description></item><item><title>&#21453;&#20107;&#23454;&#29983;&#25104;&#38754;&#20020;&#30528;&#37197;&#23545;&#25968;&#25454;&#31232;&#32570;&#21644;&#26631;&#27880;&#20449;&#24687;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#36807;&#24230;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#20294;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#19979;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.15309</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35782;&#21035;&#24615;&#20445;&#35777;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generation with Identifiability Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15309
&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#29983;&#25104;&#38754;&#20020;&#30528;&#37197;&#23545;&#25968;&#25454;&#31232;&#32570;&#21644;&#26631;&#27880;&#20449;&#24687;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#36807;&#24230;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#20294;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#19979;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#29983;&#25104;&#26159;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#22270;&#20687;&#36716;&#25442;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#19968;&#29983;&#25104;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#28508;&#22312;&#30340;&#20998;&#35299;&#34920;&#24449;&#65292;&#22914;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#36825;&#20123;&#34920;&#24449;&#28508;&#22312;&#22320;&#25903;&#25745;&#30528;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#37197;&#23545;&#25968;&#25454;&#21644;&#26631;&#27880;&#20449;&#24687;&#30340;&#31232;&#32570;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#20998;&#35299;&#26041;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#36807;&#24230;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#27604;&#22914;&#20551;&#35774;&#20869;&#23481;&#21644;&#39118;&#26684;&#21464;&#37327;&#29420;&#31435;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#65292;&#23613;&#31649;&#36825;&#26679;&#30340;&#20551;&#35774;&#21487;&#33021;&#24182;&#19981;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20363;&#22914;&#65292;&#39135;&#29289;&#35780;&#35770;&#24448;&#24448;&#28041;&#21450;&#8220;&#32654;&#21619;&#8221;&#31561;&#35789;&#35821;&#65292;&#32780;&#30005;&#24433;&#35780;&#35770;&#36890;&#24120;&#21547;&#26377;&#8220;&#24778;&#24515;&#21160;&#39748;&#8221;&#31561;&#35789;&#35821;&#34920;&#31034;&#21516;&#26679;&#30340;&#31215;&#26497;&#24773;&#24863;&#12290;&#24403;&#25968;&#25454;&#20174;&#22810;&#20010;&#39046;&#22495;&#37319;&#26679;&#26102;&#65292;&#30001;&#20110;&#20869;&#23481;&#21644;&#39118;&#26684;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21487;&#33021;&#20250;&#26174;&#33879;&#21464;&#21270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15309v1 Announce Type: cross  Abstract: Counterfactual generation lies at the core of various machine learning tasks, including image translation and controllable text generation. This generation process usually requires the identification of the disentangled latent representations, such as content and style, that underlie the observed data. However, it becomes more challenging when faced with a scarcity of paired data and labeling information. Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and style variables, to identify the latent variables, even though such assumptions may not hold for complex data distributions. For instance, food reviews tend to involve words like tasty, whereas movie reviews commonly contain words such as thrilling for the same positive sentiment. This problem is exacerbated when data are sampled from multiple domains since the dependence between content and style may vary significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23383;&#22696;&#27700;(tokenized representation)&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#25163;&#20889;&#21576;&#29616;&#20026;&#25991;&#26412;&#24207;&#21015;&#21644;&#22270;&#20687;&#65292;&#21462;&#24471;&#20102;&#21487;&#19982;&#29616;&#26377;&#26041;&#27861;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15307</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Representing Online Handwriting for Recognition in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23383;&#22696;&#27700;(tokenized representation)&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#25163;&#20889;&#21576;&#29616;&#20026;&#25991;&#26412;&#24207;&#21015;&#21644;&#22270;&#20687;&#65292;&#21462;&#24471;&#20102;&#21487;&#19982;&#29616;&#26377;&#26041;&#27861;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24102;&#26377;&#35302;&#25720;&#23631;&#21644;&#35302;&#25511;&#31508;&#30340;&#24179;&#26495;&#30005;&#33041;&#30340;&#26222;&#21450;&#65292;&#23558;&#25163;&#20889;&#36716;&#25442;&#20026;&#25991;&#26412;&#30340;&#20851;&#38190;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#25628;&#32034;&#12289;&#32034;&#24341;&#21644;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#29616;&#22312;&#24050;&#25104;&#20026;&#22270;&#20687;&#29702;&#35299;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#20197;&#21450;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#32479;&#19968;&#26041;&#27861;&#30340;&#31616;&#27905;&#24615;&#12290;&#34429;&#28982;VLMs&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26420;&#32032;&#24212;&#29992;&#26102;(&#21363;&#23558;&#25163;&#20889;&#21576;&#29616;&#20026;&#22270;&#20687;&#24182;&#25191;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;(OCR))&#22312;&#25163;&#20889;&#35782;&#21035;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;VLMs&#20013;&#30340;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#26420;&#32032;&#30340;OCR&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#23383;&#22696;&#27700;(&#22312;&#32447;&#25163;&#20889;)&#30340;&#26032;&#22411;&#26631;&#35760;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#20316;&#20026;&#25991;&#26412;&#30340;&#19968;&#31995;&#21015;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#21015;&#30340;&#31508;&#30011;&#65292;&#20197;&#21450;&#20316;&#20026;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#34920;&#31034;&#24471;&#21040;&#30340;&#32467;&#26524;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15307v1 Announce Type: cross  Abstract: The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, fine-tuning, and inference. While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing optical character recognition (OCR). In this paper, we study online handwriting recognition with VLMs, going beyond naive OCR. We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image. We show that this representation yields results comparable to
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15301</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15301
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#22270;&#24674;&#22797;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#26159;&#22522;&#20110;&#30693;&#35782;&#25110;&#32479;&#35745;&#20272;&#35745;&#65292;&#21463;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#20851;&#20110;&#24433;&#21709;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#31185;&#23398;&#25991;&#29486;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#25512;&#23548;&#19968;&#33324;&#22240;&#26524;&#22270;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;LLMs&#31995;&#32479;&#22320;&#20998;&#26512;&#21644;&#25552;&#21462;&#26469;&#33258;&#24191;&#27867;&#30740;&#31350;&#35770;&#25991;&#38598;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20174;&#27719;&#24635;&#30340;&#25991;&#29486;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26412;&#29255;&#27573;&#12290;&#28982;&#21518;&#65292;LLM&#34987;&#29992;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#22240;&#32032;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#32852;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
&lt;/p&gt;</description></item><item><title>CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15300</link><description>&lt;p&gt;
&#35265;&#35777;&#20026;&#20449;&#65306;&#36890;&#36807;CLIP&#24341;&#23548;&#35299;&#30721;&#32531;&#35299;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15300
&lt;/p&gt;
&lt;p&gt;
CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#23481;&#26131;&#20986;&#29616;&#23545;&#35937;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#21253;&#21547;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21477;&#23376;&#32423;LVLM&#24187;&#35273;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22270;&#20687;&#30340;CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#19968;&#20010;&#27604;&#21333;&#35789;&#21487;&#33021;&#24615;&#26356;&#24378;&#22823;&#12289;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#31034;&#22120;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#35299;&#30721;&#26102;&#30340;&#23545;&#35937;&#24187;&#35273;&#12290;CGD&#21033;&#29992;CLIP&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#19982;&#22270;&#20687;&#30340;&#35270;&#35273;&#32852;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CGD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#23494;&#24230;&#20998;&#24067;&#24314;&#27169;&#30340;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#25439;&#22833;&#12289;&#23494;&#24230;&#26631;&#35760;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#25928;&#26524;</title><link>https://arxiv.org/abs/2402.15297</link><description>&lt;p&gt;
&#22522;&#20110;&#36880;&#20687;&#32032;&#23494;&#24230;&#20998;&#24067;&#24314;&#27169;&#30340;&#21322;&#30417;&#30563;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15297
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#23494;&#24230;&#20998;&#24067;&#24314;&#27169;&#30340;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#25439;&#22833;&#12289;&#23494;&#24230;&#26631;&#35760;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#65292;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#12290;&#25105;&#20204;&#23558;&#20687;&#32032;&#32423;&#23494;&#24230;&#20540;&#24314;&#27169;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30830;&#23450;&#30340;&#20540;&#36827;&#34892;&#22238;&#24402;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36880;&#20687;&#32032;&#20998;&#24067;&#21305;&#37197;&#25439;&#22833;&#65292;&#29992;&#20110;&#34913;&#37327;&#39044;&#27979;&#21644;&#30495;&#23454;&#23494;&#24230;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23494;&#24230;&#26631;&#35760;&#26469;&#22686;&#24378;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#65292;&#20351;&#35299;&#30721;&#22120;&#30340;&#21069;&#21521;&#38024;&#23545;&#19981;&#21516;&#23494;&#24230;&#21306;&#38388;&#26377;&#25152;&#19987;&#38376;&#21270;&#65307;&#31532;&#19977;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20132;&#38169;&#19968;&#33268;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#26631;&#35760;&#27604;&#20363;&#35774;&#32622;&#19979;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#12290;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312; https://g
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15297v1 Announce Type: cross  Abstract: This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled. We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value. On this basis, we propose a semi-supervised crowd-counting model. Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the transformer decoder by using density tokens to specialize the forwards of decoders w.r.t. different density intervals; Thirdly, we design the interleaving consistency self-supervised learning mechanism to learn from unlabeled data efficiently. Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings. Code will be released at https://g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#21019;&#20316;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#20027;&#35201;&#28966;&#28857;&#20197;&#21450;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15294</link><description>&lt;p&gt;
&#22312;&#20114;&#21160;&#32972;&#26223;&#19979;&#38899;&#20048;&#29983;&#25104;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Music Generation in the Context of Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15294
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#21019;&#20316;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#20027;&#35201;&#28966;&#28857;&#20197;&#21450;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;transformers&#65289;&#65292;&#24050;&#25104;&#21151;&#29992;&#20110;&#21019;&#20316;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#21253;&#25324;&#26059;&#24459;&#21644;&#22797;&#35843;&#20316;&#21697;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#39118;&#26684;&#22797;&#21046;&#65288;&#20363;&#22914;&#29983;&#25104;&#24052;&#36203;&#39118;&#26684;&#36171;&#26684;&#26354;&#65289;&#25110;&#39118;&#26684;&#36716;&#31227;&#65288;&#20363;&#22914;&#21476;&#20856;&#21040;&#29237;&#22763;&#65289;&#19978;&#65292;&#22522;&#20110;&#22823;&#37327;&#24405;&#21046;&#25110;&#36716;&#24405;&#30340;&#38899;&#20048;&#65292;&#36825;&#20063;&#20801;&#35768;&#30456;&#24403;&#30452;&#25509;&#30340;&#8220;&#34920;&#29616;&#8221;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#27169;&#22411;&#19981;&#36866;&#21512;&#36890;&#36807;&#23454;&#26102;&#20114;&#21160;&#36827;&#34892;&#20154;&#26426;&#20849;&#21019;&#65292;&#20063;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#20316;&#21697;&#23558;&#22914;&#20309;&#35780;&#20272;&#12290;&#26412;&#25991;&#20840;&#38754;&#23457;&#26597;&#20102;&#38899;&#20048;&#34920;&#31034;&#12289;&#29305;&#24449;&#20998;&#26512;&#12289;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#32479;&#35745;&#21644;&#21442;&#25968;&#24314;&#27169;&#65292;&#20197;&#21450;&#20154;&#20026;&#21644;&#33258;&#21160;&#35780;&#20272;&#25514;&#26045;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#21738;&#20123;&#26041;&#27861;&#21644;&#27169;&#22411;&#20284;&#20046;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15294v1 Announce Type: cross  Abstract: In recent years, machine learning, and in particular generative adversarial neural networks (GANs) and attention-based neural networks (transformers), have been successfully used to compose and generate music, both melodies and polyphonic pieces. Current research focuses foremost on style replication (eg. generating a Bach-style chorale) or style transfer (eg. classical to jazz) based on large amounts of recorded or transcribed music, which in turn also allows for fairly straight-forward "performance" evaluation. However, most of these models are not suitable for human-machine co-creation through live interaction, neither is clear, how such models and resulting creations would be evaluated. This article presents a thorough review of music representation, feature analysis, heuristic algorithms, statistical and parametric modelling, and human and automatic evaluation measures, along with a discussion of which approaches and models seem m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DiffusionABSA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36880;&#27493;&#25552;&#21462;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#36884;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#36827;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.15289</link><description>&lt;p&gt;
&#36880;&#27493;&#30699;&#27491;&#65306;&#29992;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DiffusionABSA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36880;&#27493;&#25552;&#21462;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#36884;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#36827;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#22312;&#39044;&#27979;&#25991;&#26412;&#20013;&#19982;&#35782;&#21035;&#26041;&#38754;&#30456;&#20851;&#30340;&#24773;&#24863;&#26497;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;ABSA&#20013;&#30340;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#25361;&#25112;&#22312;&#20110;&#31934;&#30830;&#30830;&#23450;&#26041;&#38754;&#30340;&#36793;&#30028;&#65288;&#36215;&#22987;&#21644;&#32467;&#26463;&#32034;&#24341;&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#34920;&#36798;&#24335;&#65292;&#36825;&#26159;&#30001;&#20110;&#29992;&#25143;&#30340;&#21475;&#35821;&#34920;&#36798;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionABSA&#65292;&#19968;&#31181;&#20026;ABSA&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36880;&#27493;&#25552;&#21462;&#26041;&#38754;&#12290;&#29305;&#21035;&#22320;&#65292;DiffusionABSA&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#21521;&#26041;&#38754;&#26415;&#35821;&#28155;&#21152;&#22122;&#38899;&#65292;&#38543;&#21518;&#23398;&#20064;&#19968;&#20010;&#36870;&#21521;&#36880;&#28176;&#24674;&#22797;&#36825;&#20123;&#26415;&#35821;&#30340;&#21435;&#22122;&#36807;&#31243;&#12290;&#20026;&#20102;&#20272;&#35745;&#36793;&#30028;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#36807;&#35821;&#27861;&#24863;&#30693;&#30340;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#22686;&#24378;&#30340;&#21435;&#22122;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25353;&#26102;&#38388;&#39034;&#24207;&#25429;&#25417;&#26041;&#38754;&#19982;&#21608;&#22260;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23545;&#20843;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#31361;&#26174;&#20102;DiffusionABSA&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15289v1 Announce Type: cross  Abstract: Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting the sentiment polarity associated with identified aspects within text. However, a notable challenge in ABSA lies in precisely determining the aspects' boundaries (start and end indices), especially for long ones, due to users' colloquial expressions. We propose DiffusionABSA, a novel diffusion model tailored for ABSA, which extracts the aspects progressively step by step. Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner. To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text. Empirical evaluations conducted on eight benchmark datasets underscore the compelling advantages of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#25191;&#34892;&#20809;&#36890;&#20449;&#31995;&#32479;&#22343;&#34913;&#30340;&#39640;&#21534;&#21520;&#37327;FPGA&#28436;&#31034;&#22120;&#65292;&#37319;&#29992;&#22522;&#20110;ANN&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15288</link><description>&lt;p&gt;
&#20809;&#36890;&#20449;&#20013;&#22522;&#20110;ANN&#30340;&#22343;&#34913;&#30340;&#23454;&#26102;FPGA&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15288
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#25191;&#34892;&#20809;&#36890;&#20449;&#31995;&#32479;&#22343;&#34913;&#30340;&#39640;&#21534;&#21520;&#37327;FPGA&#28436;&#31034;&#22120;&#65292;&#37319;&#29992;&#22522;&#20110;ANN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#21534;&#21520;&#37327;&#30340;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#28436;&#31034;&#22120;&#65292;&#29992;&#20110;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#22343;&#34913;&#22120;&#12290;&#35813;&#22343;&#34913;&#22120;&#23454;&#26102;&#25191;&#34892;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;30 GBd&#65292;&#20004;&#32423;&#33033;&#20914;&#25391;&#24133;&#35843;&#21046;&#65288;PAM2&#65289;&#20809;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15288v1 Announce Type: cross  Abstract: In this work, we present a high-throughput field programmable gate array (FPGA) demonstrator of an artificial neural network (ANN)-based equalizer. The equalization is performed and illustrated in real-time for a 30 GBd, two-level pulse amplitude modulation (PAM2) optical communication system.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24352;&#37327;&#30697;&#38453;&#36924;&#36817;&#21704;&#23494;&#23572;&#39039;-&#38597;&#21508;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#35299;&#20915;HJB&#26041;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#26679;&#26412;&#65292;&#19981;&#20381;&#36182;&#20110;&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#24182;&#33021;&#36991;&#20813;&#32500;&#25968;&#28798;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.15285</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#30697;&#38453;&#36924;&#36817;&#21704;&#23494;&#23572;&#39039;-&#38597;&#21508;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15285
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#30697;&#38453;&#36924;&#36817;&#21704;&#23494;&#23572;&#39039;-&#38597;&#21508;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#35299;&#20915;HJB&#26041;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#26679;&#26412;&#65292;&#19981;&#20381;&#36182;&#20110;&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#24182;&#33021;&#36991;&#20813;&#32500;&#25968;&#28798;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#27010;&#29575;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#21644;&#29983;&#25104;&#24314;&#27169;&#65288;GM&#65289;&#31561;&#39046;&#22495;&#20013;&#26159;&#19968;&#39033;&#24120;&#35265;&#25361;&#25112;&#12290; &#22312;GM&#20013;&#65292;&#29305;&#21035;&#27969;&#34892;&#30340;&#37319;&#26679;&#24037;&#20855;&#26159;&#20381;&#36182;&#20110;Ornstein-Uhlenbeck&#27491;&#21521;&#36807;&#31243;&#30340;&#23545;&#25968;&#23494;&#24230;&#30340;&#36870;&#26102;&#38388;&#25193;&#25955;&#36807;&#31243;&#12290; &#22312;Berner&#31561;&#20154;[2022]&#20013;&#65292;&#20316;&#32773;&#25351;&#20986;&#36825;&#20123;&#23545;&#25968;&#23494;&#24230;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#28304;&#33258;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#21704;&#23494;&#23572;&#39039;-&#38597;&#21508;&#27604;-&#36125;&#23572;&#26364;&#65288;HJB&#65289;&#26041;&#31243;&#26469;&#33719;&#24471;&#12290; &#34429;&#28982;&#36825;&#20010;HJB&#26041;&#31243;&#36890;&#24120;&#20351;&#29992;&#38388;&#25509;&#26041;&#27861;&#26469;&#22788;&#29702;&#65292;&#27604;&#22914;&#25919;&#31574;&#36845;&#20195;&#21644;&#23545;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#40657;&#21283;&#23376;&#26550;&#26500;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#30452;&#25509;&#26102;&#38388;&#31215;&#20998;&#26469;&#35299;&#20915;HJB&#26041;&#31243;&#65292;&#20351;&#29992;&#24352;&#37327;&#30697;&#38453;&#65288;TT&#65289;&#26684;&#24335;&#30340;&#21387;&#32553;&#22810;&#39033;&#24335;&#36827;&#34892;&#31354;&#38388;&#31163;&#25955;&#21270;&#12290; &#36825;&#31181;&#26041;&#27861;&#27809;&#26377;&#26679;&#26412;&#38656;&#27714;&#65292;&#19981;&#20381;&#36182;&#20110;&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#36991;&#20813;&#32500;&#25968;&#28798;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15285v1 Announce Type: cross  Abstract: Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool. In Berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control. While this HJB equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization. Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26102;&#31354;&#35266;&#23519;&#32773;&#8221;&#30340;&#35266;&#23519;&#32773;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#39044;&#27979;&#23398;&#20064;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#24577;&#27491;&#21017;&#21270;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.15284</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#39044;&#27979;&#23398;&#20064;&#30340;&#26102;&#31354;&#35266;&#23519;&#32773;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal Observer Design for Predictive Learning of High-Dimensional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26102;&#31354;&#35266;&#23519;&#32773;&#8221;&#30340;&#35266;&#23519;&#32773;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#39044;&#27979;&#23398;&#20064;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#24577;&#27491;&#21017;&#21270;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#26694;&#26550;&#20027;&#35201;&#26159;&#22522;&#20110;&#30452;&#35273;&#35774;&#35745;&#30340;&#12290;&#22914;&#20309;&#36827;&#34892;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#26102;&#31354;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21160;&#24577;&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26694;&#26550;&#35774;&#35745;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26102;&#31354;&#35266;&#23519;&#32773;&#8221;&#30340;&#35266;&#23519;&#32773;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#39044;&#27979;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#29305;&#28857;&#26159;&#21452;&#37325;&#30340;&#65306;&#39318;&#20808;&#65292;&#23427;&#20026;&#26102;&#31354;&#39044;&#27979;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#21644;&#25910;&#25947;&#20445;&#35777;&#65307;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21160;&#24577;&#27491;&#21017;&#21270;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#26102;&#31354;&#21160;&#24577;&#24182;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15284v1 Announce Type: cross  Abstract: Although deep learning-based methods have shown great success in spatiotemporal predictive learning, the framework of those models is designed mainly by intuition. How to make spatiotemporal forecasting with theoretical guarantees is still a challenging issue. In this work, we tackle this problem by applying domain knowledge from the dynamical system to the framework design of deep learning models. An observer theory-guided deep learning architecture, called Spatiotemporal Observer, is designed for predictive learning of high dimensional data. The characteristics of the proposed framework are twofold: firstly, it provides the generalization error bound and convergence guarantee for spatiotemporal prediction; secondly, dynamical regularization is introduced to enable the model to learn system dynamics better during training. Further experimental results show that this framework could capture the spatiotemporal dynamics and make accurate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20915;&#31574;&#26102;&#24212;&#29992;&#36845;&#20195;&#25512;&#29702;&#26469;&#24494;&#35843;&#25512;&#26029;&#30340;&#20195;&#29702;&#29366;&#24577;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;3D&#23548;&#33322;&#20219;&#21153;&#20013;&#21462;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24471;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15283</link><description>&lt;p&gt;
&#29369;&#35947;&#26102;&#65292;&#35201;&#24930;&#24930;&#24605;&#32771;&#65306;&#20855;&#26377;&#28508;&#22312;&#24819;&#35937;&#21147;&#30340;&#36845;&#20195;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15283
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20915;&#31574;&#26102;&#24212;&#29992;&#36845;&#20195;&#25512;&#29702;&#26469;&#24494;&#35843;&#25512;&#26029;&#30340;&#20195;&#29702;&#29366;&#24577;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;3D&#23548;&#33322;&#20219;&#21153;&#20013;&#21462;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24471;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#38476;&#29983;&#30340;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#20250;&#21463;&#21040;&#20854;&#19990;&#30028;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#36825;&#31867;&#20195;&#29702;&#24615;&#33021;&#30340;&#26032;&#39062;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#19982;&#35268;&#21010;&#21644;&#23398;&#20064;&#20998;&#24320;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20915;&#31574;&#26102;&#24212;&#29992;&#36845;&#20195;&#25512;&#29702;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22522;&#20110;&#26410;&#26469;&#29366;&#24577;&#34920;&#31034;&#30340;&#36830;&#36143;&#24615;&#26469;&#25913;&#36827;&#25512;&#26029;&#30340;&#20195;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#35270;&#35273;3D&#23548;&#33322;&#20219;&#21153;&#26102;&#65292;&#22312;&#37325;&#26500;&#31934;&#24230;&#21644;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#32771;&#34385;&#26356;&#22810;&#30340;&#26410;&#26469;&#29366;&#24577;&#20250;&#36827;&#19968;&#27493;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#19981;&#20250;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#35757;&#32451;&#39044;&#35780;&#20272;&#36739;&#23569;&#30340;&#20195;&#29702;&#20174;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#33719;&#30410;&#26368;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15283v1 Announce Type: cross  Abstract: In an unfamiliar setting, a model-based reinforcement learning agent can be limited by the accuracy of its world model. In this work, we present a novel, training-free approach to improving the performance of such agents separately from planning and learning. We do so by applying iterative inference at decision-time, to fine-tune the inferred agent states based on the coherence of future state representations. Our approach achieves a consistent improvement in both reconstruction accuracy and task performance when applied to visual 3D navigation tasks. We go on to show that considering more future states further improves the performance of the agent in partially-observable environments, but not in a fully-observable one. Finally, we demonstrate that agents with less training pre-evaluation benefit most from our approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#36830;&#32493;&#34920;&#31034;&#20219;&#24847;&#36816;&#21160;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#36895;&#24230;&#21644;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15281</link><description>&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#29992;&#20110;&#24555;&#36895;&#30896;&#25758;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Swept Volume Models for Fast Collision Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#36830;&#32493;&#34920;&#31034;&#20219;&#24847;&#36816;&#21160;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#36895;&#24230;&#21644;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30896;&#25758;&#26816;&#27979;&#26159;&#36816;&#21160;&#35268;&#21010;&#20013;&#26368;&#32791;&#26102;&#30340;&#25805;&#20316;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#25506;&#32034;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#30896;&#25758;&#26816;&#27979;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#20391;&#37325;&#20110;&#21033;&#29992;&#26426;&#22120;&#20154;&#20960;&#20309;&#20307;&#25110;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#25195;&#25551;&#20307;&#30340;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#39318;&#27425;&#36830;&#32493;&#34920;&#31034;&#30001;&#36215;&#22987;&#21644;&#30446;&#26631;&#37197;&#32622;&#21442;&#25968;&#21270;&#30340;&#20219;&#24847;&#36816;&#21160;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#20219;&#21153;&#31354;&#38388;&#20013;&#20219;&#24847;&#28857;&#21040;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#35745;&#31639;&#30340;&#36895;&#24230;&#19982;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#24378;&#22823;&#20934;&#30830;&#24615;&#20445;&#35777;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15281v1 Announce Type: cross  Abstract: Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that i
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#22312;&#23398;&#20064;&#20998;&#31867;&#22120;&#21518;&#20915;&#23450;&#26159;&#21542;&#21442;&#19982;&#30340;&#25112;&#30053;&#33258;&#25105;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#26524;&#21644;&#33258;&#25105;&#36873;&#25321;&#20154;&#21475;&#26500;&#25104;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20248;&#21270;&#30340;&#21487;&#24494;&#20998;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.15274</link><description>&lt;p&gt;
&#22312;&#25112;&#30053;&#33258;&#25105;&#36873;&#25321;&#19979;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification Under Strategic Self-Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15274
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#23398;&#20064;&#20998;&#31867;&#22120;&#21518;&#20915;&#23450;&#26159;&#21542;&#21442;&#19982;&#30340;&#25112;&#30053;&#33258;&#25105;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#26524;&#21644;&#33258;&#25105;&#36873;&#25321;&#20154;&#21475;&#26500;&#25104;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20248;&#21270;&#30340;&#21487;&#24494;&#20998;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#25143;&#21487;&#20197;&#20174;&#26576;&#20123;&#39044;&#27979;&#20013;&#33719;&#30410;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#20250;&#37319;&#21462;&#25112;&#30053;&#34892;&#21160;&#20197;&#33719;&#24471;&#26377;&#21033;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#25112;&#30053;&#20998;&#31867;&#30340;&#30740;&#31350;&#37117;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#34920;&#29616;&#20026;&#29305;&#24449;&#20462;&#25913;&#65292;&#32780;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#22312;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#20915;&#23450;&#26159;&#21542;&#21442;&#19982;&#65288;&#25110;&#19981;&#21442;&#19982;&#65289;&#12290;&#20026;&#20102;&#23398;&#20064;&#22686;&#21152;&#25112;&#30053;&#24847;&#35782;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#25105;&#36873;&#25321;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23398;&#20064;&#23545;&#33258;&#25105;&#36873;&#25321;&#20154;&#21475;&#26500;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22312;&#33258;&#25105;&#36873;&#25321;&#34892;&#20026;&#19979;&#23398;&#20064;&#30340;&#21487;&#24494;&#20998;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#20248;&#21270;&#12290;&#25105;&#20204;&#26368;&#21518;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#21644;&#27169;&#25311;&#34892;&#20026;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#26082;&#21487;&#20197;&#34917;&#20805;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#21448;&#21487;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15274v1 Announce Type: new  Abstract: When users stand to gain from certain predictions, they are prone to act strategically to obtain favorable predictive outcomes. Whereas most works on strategic classification consider user actions that manifest as feature modifications, we study a novel setting in which users decide -- in response to the learned classifier -- whether to at all participate (or not). For learning approaches of increasing strategic awareness, we study the effects of self-selection on learning, and the implications of learning on the composition of the self-selected population. We then propose a differentiable framework for learning under self-selective behavior, which can be optimized effectively. We conclude with experiments on real data and simulated behavior that both complement our analysis and demonstrate the utility of our approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#37096;&#32626;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#36890;&#36807;&#26032;&#22411;&#36719;&#20214;&#20869;&#26680;&#22312;off-the-shelf&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#37096;&#32626;&#65292;&#23558;&#25512;&#26029;&#24310;&#36831;&#32553;&#30701;&#20102;3.22&#20493;</title><link>https://arxiv.org/abs/2402.15273</link><description>&lt;p&gt;
&#22522;&#20110;&#32435;&#31859;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Optimized Deployment of Deep Neural Networks for Visual Pose Estimation on Nano-drones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15273
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#37096;&#32626;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#36890;&#36807;&#26032;&#22411;&#36719;&#20214;&#20869;&#26680;&#22312;off-the-shelf&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#37096;&#32626;&#65292;&#23558;&#25512;&#26029;&#24310;&#36831;&#32553;&#30701;&#20102;3.22&#20493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#33258;&#20027;&#26080;&#20154;&#39550;&#39542;&#39134;&#34892;&#22120;&#65288;UAVs&#65289;&#22240;&#20854;&#23567;&#24039;&#30340;&#23610;&#23544;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#33021;&#22815;&#25191;&#34892;&#35832;&#22914;&#23460;&#20869;&#23548;&#33322;&#25110;&#20154;&#21592;&#30417;&#25511;&#31561;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23610;&#23544;&#21644;&#31616;&#21333;&#30340;&#30005;&#23376;&#35774;&#22791;&#22312;&#23454;&#29616;&#20808;&#36827;&#30340;&#26426;&#36733;&#26234;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#30340;&#33258;&#21160;&#20248;&#21270;&#27969;&#27700;&#32447;&#12290;&#35813;&#27969;&#27700;&#32447;&#21033;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;DNN&#32467;&#26500;&#31354;&#38388;&#20013;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#25506;&#32034;&#12290;&#28982;&#21518;&#65292;&#33719;&#24471;&#30340;&#32593;&#32476;&#37096;&#32626;&#22312;&#19968;&#27454;&#37197;&#22791;&#24182;&#34892;&#36229;&#20302;&#21151;&#32791;SoC&#30340;&#29616;&#25104;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#65292;&#21033;&#29992;&#19968;&#32452;&#29992;&#20110;&#39640;&#25928;&#25191;&#34892;&#20851;&#38190;DNN&#23618;&#24207;&#21015;&#30340;&#26032;&#22411;&#36719;&#20214;&#20869;&#26680;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#23558;&#25512;&#26029;&#24310;&#36831;&#22312;&#31561;&#35823;&#24046;&#24773;&#20917;&#19979;&#32553;&#30701;&#20102;&#26368;&#22810;3.22&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15273v1 Announce Type: cross  Abstract: Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining popularity due to their small size, enabling new tasks such as indoor navigation or people monitoring. Nonetheless, their size and simple electronics pose severe challenges in implementing advanced onboard intelligence. This work proposes a new automatic optimization pipeline for visual pose estimation tasks using Deep Neural Networks (DNNs). The pipeline leverages two different Neural Architecture Search (NAS) algorithms to pursue a vast complexity-driven exploration in the DNNs' architectural space. The obtained networks are then deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low power System-on-Chip leveraging a set of novel software kernels for the efficient fused execution of critical DNN layer sequences. Our results improve the state-of-the-art reducing inference latency by up to 3.22x at iso-error.
&lt;/p&gt;</description></item><item><title>SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;</title><link>https://arxiv.org/abs/2402.15270</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#32541;&#25509;&#36817;&#24230;&#25972;&#21512;&#30340;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smoothed Graph Contrastive Learning via Seamless Proximity Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15270
&lt;/p&gt;
&lt;p&gt;
SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#36890;&#36807;&#23558;&#33410;&#28857;&#23545;&#24402;&#31867;&#20026;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#26469;&#23545;&#40784;&#33410;&#28857;&#34920;&#31034;&#65292;&#20854;&#36873;&#25321;&#36807;&#31243;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#20004;&#20010;&#22686;&#24378;&#22270;&#20013;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#12290;&#20256;&#32479;&#30340;GCL&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#32479;&#19968;&#22320;&#34701;&#20837;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#36127;&#33410;&#28857;&#34987;&#24179;&#31561;&#23545;&#24453;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#19982;&#30495;&#27491;&#27491;&#26679;&#26412;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;SGCL&#65289;&#65292;&#21033;&#29992;&#22686;&#24378;&#22270;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#27880;&#20837;&#19982;&#27491;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25509;&#36817;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#35268;&#33539;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;SGCL&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#24418;&#25104;&#20102;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15270v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale
&lt;/p&gt;</description></item><item><title>MemoryPrompt&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#65292;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#36319;&#36394;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.15268</link><description>&lt;p&gt;
MemoryPrompt: &#19968;&#31181;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#36319;&#36394;&#30340;&#36731;&#37327;&#23553;&#35013;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15268
&lt;/p&gt;
&lt;p&gt;
MemoryPrompt&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#65292;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#36319;&#36394;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22823;&#22411;&#30828;&#32534;&#30721;&#36755;&#20837;&#31383;&#21475;&#36319;&#36394;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;MemoryPrompt&#65292;&#19968;&#31181;&#26356;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#30001;&#19968;&#20010;&#23567;&#30340;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#34917;&#20805;&#65292;&#36890;&#36807;&#22312;&#20854;&#24120;&#35268;&#36755;&#20837;&#20043;&#21069;&#28155;&#21152;&#19968;&#31995;&#21015;&#21521;&#37327;&#65288;&#31867;&#20284;&#20110;&#36719;&#25552;&#31034;&#65289;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#38656;&#35201;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#23545;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36319;&#36394;&#22810;&#20010;&#20107;&#23454;&#26356;&#26032;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;MemoryPrompt&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#37027;&#20123;&#21487;&#20197;&#35775;&#38382;&#23436;&#25972;&#36755;&#20837;&#21382;&#21490;&#35760;&#24405;&#30340;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#38271;&#36317;&#31163;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;MemoryPrompt&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#24615;&#33021;&#19982;&#22312;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#19978;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#36825;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;MemoryPrompt&#22312;&#36866;&#24212;&#26032;&#20219;&#21153;&#26102;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#27492;&#19981;&#20250;&#30772;&#22351;&#38750;&#19987;&#23478;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15268v1 Announce Type: cross  Abstract: Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the un
&lt;/p&gt;</description></item><item><title>&#22312;fNIRS&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26657;&#20934;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.15266</link><description>&lt;p&gt;
fNIRS&#20013;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibration of Deep Learning Classification Models in fNIRS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15266
&lt;/p&gt;
&lt;p&gt;
&#22312;fNIRS&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26657;&#20934;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#65288;fNIRS&#65289;&#26159;&#29992;&#20110;&#30417;&#27979;&#33041;&#27963;&#21160;&#30340;&#23453;&#36149;&#26080;&#21019;&#24037;&#20855;&#12290;&#19982;&#24847;&#35782;&#27963;&#21160;&#30456;&#20851;&#30340;fNIRS&#25968;&#25454;&#20998;&#31867;&#23545;&#20110;&#25512;&#36827;&#25105;&#20204;&#23545;&#22823;&#33041;&#30340;&#29702;&#35299;&#21644;&#20419;&#36827;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#28145;&#24230;&#23398;&#20064;&#26469;&#35299;&#20915;fNIRS&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20998;&#31867;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;fNIRS&#30340;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#32622;&#20449;&#24230;&#21487;&#38752;&#24615;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20043;&#19968;&#23601;&#26159;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#26657;&#20934;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26657;&#20934;&#34701;&#20837;fNIRS&#39046;&#22495;&#65292;&#24182;&#35780;&#20272;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26657;&#20934;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#25512;&#21160;fNIRS&#39046;&#22495;&#30340;&#26657;&#20934;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15266v1 Announce Type: new  Abstract: Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22238;&#39038;&#24335;&#23398;&#20064;&#27861;&#24459;&#20462;&#27491;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#21333;&#20803;&#30340;&#21160;&#24577;&#21464;&#21270;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#32447;&#24615;&#26356;&#26032;&#35268;&#21017;&#21644;&#23567;&#20869;&#23384;&#30340;&#20248;&#21270;&#22120;&#20013;&#21462;&#24471;&#20248;&#20110;&#32463;&#20856;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15262</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#20869;&#23384;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dynamic Memory Based Adaptive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22238;&#39038;&#24335;&#23398;&#20064;&#27861;&#24459;&#20462;&#27491;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#21333;&#20803;&#30340;&#21160;&#24577;&#21464;&#21270;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#32447;&#24615;&#26356;&#26032;&#35268;&#21017;&#21644;&#23567;&#20869;&#23384;&#30340;&#20248;&#21270;&#22120;&#20013;&#21462;&#24471;&#20248;&#20110;&#32463;&#20856;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20248;&#21270;&#22120;&#23450;&#20041;&#20026;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#23384;&#20648;$k$&#20010;&#21160;&#24577;&#21464;&#21270;&#21521;&#37327;&#30340;&#20855;&#26377;&#20869;&#23384;$k$&#30340;&#20248;&#21270;&#22120;&#12290;&#32463;&#20856;&#30340;SGD&#20248;&#21270;&#22120;&#20855;&#26377;&#20869;&#23384;$0$&#65292;&#21160;&#37327;SGD&#20248;&#21270;&#22120;&#20855;&#26377;$1$&#65292;Adam&#20248;&#21270;&#22120;&#20855;&#26377;$2$&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#20248;&#21270;&#22120;&#22914;&#20309;&#21033;&#29992;&#26356;&#22810;&#20869;&#23384;&#21333;&#20803;&#65311;&#24212;&#35813;&#22312;&#20854;&#20013;&#23384;&#20648;&#21738;&#20123;&#20449;&#24687;&#65311;&#22914;&#20309;&#23558;&#23427;&#20204;&#29992;&#20110;&#23398;&#20064;&#27493;&#39588;&#65311;&#20316;&#20026;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22238;&#39038;&#24335;&#23398;&#20064;&#27861;&#24459;&#20462;&#27491;&#8221;&#25110;&#31616;&#31216;RLLC&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35745;&#31639;&#20869;&#23384;&#21333;&#20803;&#30340;&#21160;&#24577;&#21464;&#21270;&#32447;&#24615;&#32452;&#21512;&#65288;&#31216;&#20026;&#23398;&#20064;&#27861;&#21017;&#65289;&#65292;&#36825;&#20123;&#20869;&#23384;&#21333;&#20803;&#26412;&#36523;&#21487;&#33021;&#20250;&#20219;&#24847;&#28436;&#21464;&#12290;&#25105;&#20204;&#22312;&#20869;&#23384;&#21333;&#20803;&#20855;&#26377;&#32447;&#24615;&#26356;&#26032;&#35268;&#21017;&#21644;&#23567;&#20869;&#23384;&#65288;$\leq 4$&#20869;&#23384;&#21333;&#20803;&#65289;&#30340;&#20248;&#21270;&#22120;&#19978;&#23637;&#31034;&#20102;RLLC&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26631;&#20934;&#38382;&#39064;&#20013;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#34920;&#29616;&#20248;&#20110;&#19978;&#36848;&#19977;&#31181;&#32463;&#20856;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;RLLC&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15262v1 Announce Type: cross  Abstract: Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps? As an approach to the last question, we introduce a general method called "Retrospective Learning Law Correction" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promisi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2402.15259</link><description>&lt;p&gt;
&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Open Ad Hoc Teamwork with Cooperative Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#38754;&#20020;&#30528;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#19982;&#38431;&#21451;&#21327;&#20316;&#20294;&#27809;&#26377;&#20808;&#21069;&#21327;&#35843;&#25110;&#32852;&#21512;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#12290;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#26029;&#21464;&#21270;&#30340;&#38431;&#21451;&#25968;&#37327;&#30340;&#29615;&#22659;&#65292;&#21363;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;&#29616;&#26377;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#23398;&#20064;&#65288;GPL&#65289;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#22788;&#29702;&#26080;&#38480;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#65292;&#26377;&#25928;&#24212;&#23545;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;GPL&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20294;&#20854;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#23545;&#35299;&#37322;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20174;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#20026;GPL&#20013;&#37319;&#29992;&#30340;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22495;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#39640;&#20998;&#36776;&#29575;&#21513;&#20182;&#36716;&#24405;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;GuitarSet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#24405;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15258</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#21513;&#20182;&#36716;&#24405;&#30340;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
High Resolution Guitar Transcription via Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15258
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22495;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#39640;&#20998;&#36776;&#29575;&#21513;&#20182;&#36716;&#24405;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;GuitarSet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#24405;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#65288;AMT&#65289;&#23545;&#20110;&#38050;&#29748;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#24471;&#30410;&#20110;MAESTRO&#21644;MAPS&#31561;&#22823;&#22411;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#20854;&#20182;&#20048;&#22120;&#30340;&#21487;&#27604;&#25968;&#25454;&#38598;&#23578;&#19981;&#21487;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20048;&#35889;&#19982;&#36716;&#24405;&#27169;&#22411;&#30340;&#28608;&#27963;&#23545;&#20934;&#21487;&#20197;&#20026;&#38500;&#38050;&#29748;&#20197;&#22806;&#30340;&#20048;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;AMT&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#21513;&#20182;&#65292;&#23545;&#20351;&#29992;&#21830;&#29992;&#20048;&#35889;-&#38899;&#39057;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#38050;&#29748;&#36716;&#24405;&#27169;&#22411;&#26469;&#35757;&#32451;&#26032;&#30340;&#21513;&#20182;&#36716;&#24405;&#27169;&#22411;&#12290;&#26368;&#32456;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;GuitarSet&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#24405;&#32467;&#26524;&#65292;&#20248;&#20110;&#20808;&#21069;&#21457;&#34920;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15258v1 Announce Type: cross  Abstract: Automatic music transcription (AMT) has achieved high accuracy for piano due to the availability of large, high-quality datasets such as MAESTRO and MAPS, but comparable datasets are not yet available for other instruments. In recent work, however, it has been demonstrated that aligning scores to transcription model activations can produce high quality AMT training data for instruments other than piano. Focusing on the guitar, we refine this approach to training on score data using a dataset of commercially available score-audio pairs. We propose the use of a high-resolution piano transcription model to train a new guitar transcription model. The resulting model obtains state-of-the-art transcription results on GuitarSet in a zero-shot context, improving on previously published methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.15255</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#32467;&#26500;&#23398;&#20064;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Structure Learning Under Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15255
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#20250;&#24341;&#20837;&#40481;&#29983;&#34507;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#26631;&#26159;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#20294;&#40065;&#26834;&#30340;&#25554;&#34917;&#38656;&#35201;&#32771;&#34385;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#25110;&#26356;&#22909;&#22320;&#22240;&#26524;&#20851;&#31995;&#12290;&#20165;&#20165;&#29992;&#29616;&#26377;&#30340;&#25554;&#34917;&#26041;&#27861;&#22635;&#20805;&#32570;&#22833;&#20540;&#65292;&#28982;&#21518;&#22312;&#23436;&#25972;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#26500;&#23398;&#20064;&#34987;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#12290;&#36825;&#31181;&#26368;&#20248;&#36755;&#36816;&#30340;&#35266;&#28857;&#19981;&#21516;&#20110;&#29616;&#26377;&#22522;&#20110;EM&#30340;&#22522;&#20110;&#24471;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#23398;&#20064;&#25237;&#24433;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#24341;&#36215;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#21644;&#35266;&#27979;&#25968;&#25454;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15255v1 Announce Type: cross  Abstract: Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#30340;&#20132;&#26131;</title><link>https://arxiv.org/abs/2402.15247</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bargaining-based Approach for Feature Trading in Vertical Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#30340;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#36328;&#25968;&#25454;&#21644;&#20219;&#21153;&#26041;&#23545;&#30456;&#21516;&#29992;&#25143;&#38598;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#65292;VFL&#36890;&#24120;&#28041;&#21450;&#19968;&#20010;&#20219;&#21153;&#26041;&#21644;&#19968;&#20010;&#25968;&#25454;&#26041;&#12290;&#20844;&#24179;&#21644;&#32463;&#27982;&#39640;&#25928;&#30340;&#29305;&#24449;&#20132;&#26131;&#23545;VFL&#30340;&#21830;&#19994;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#20219;&#21153;&#26041;&#34987;&#35270;&#20026;&#36141;&#20080;&#25968;&#25454;&#26041;&#29305;&#24449;&#30340;&#25968;&#25454;&#28040;&#36153;&#32773;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VFL&#29305;&#24449;&#20132;&#26131;&#20570;&#27861;&#36890;&#24120;&#23558;&#25968;&#25454;&#26041;&#30340;&#25968;&#25454;&#25972;&#20307;&#23450;&#20215;&#65292;&#24182;&#20551;&#23450;&#20132;&#26131;&#21457;&#29983;&#22312;&#25191;&#34892;VFL&#20043;&#21069;&#12290;&#24573;&#30053;&#20132;&#26131;&#29305;&#24449;&#20135;&#29983;&#30340;&#24615;&#33021;&#22686;&#30410;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#25903;&#20184;&#21644;&#36807;&#24230;&#25903;&#20184;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;VFL&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#30340;&#20132;&#26131;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#22522;&#20110;&#24615;&#33021;&#22686;&#30410;&#30340;&#23450;&#20215;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15247v1 Announce Type: cross  Abstract: Vertical Federated Learning (VFL) has emerged as a popular machine learning paradigm, enabling model training across the data and the task parties with different features about the same user set while preserving data privacy. In production environment, VFL usually involves one task party and one data party. Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered as the data consumer who buys the data party's features. However, current VFL feature trading practices often price the data party's data as a whole and assume transactions occur prior to the performing VFL. Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues. In this study, we propose a bargaining-based feature trading approach in VFL to encourage economically efficient transactions. Our model incorporates performance gain-based pricing, taking int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#31574;&#30053;&#65292;&#32467;&#21512;&#26799;&#24230;&#25163;&#26415;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;GS-EMA&#65289;&#20248;&#21270;&#25216;&#26415;&#21644;&#36793;&#30028;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;BACL&#65289;&#65292;&#25552;&#39640;&#20102;&#21160;&#33033;&#30244;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15239</link><description>&lt;p&gt;
GS-EMA&#65306;&#23558;&#26799;&#24230;&#25163;&#26415;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#19982;&#36793;&#30028;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22312;&#21160;&#33033;&#30244;&#20998;&#21106;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15239
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#31574;&#30053;&#65292;&#32467;&#21512;&#26799;&#24230;&#25163;&#26415;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;GS-EMA&#65289;&#20248;&#21270;&#25216;&#26415;&#21644;&#36793;&#30028;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;BACL&#65289;&#65292;&#25552;&#39640;&#20102;&#21160;&#33033;&#30244;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#21106;&#33041;&#21160;&#33033;&#30244;&#23545;&#20110;&#20934;&#30830;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26469;&#33258;&#21508;&#31181;&#21307;&#30103;&#26426;&#26500;&#30340;&#19977;&#32500;&#26059;&#36716;&#34880;&#31649;&#36896;&#24433;&#65288;3DRA&#65289;&#25968;&#25454;&#20013;&#65292;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#39046;&#22495;&#36716;&#31227;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#20351;&#24471;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#31574;&#30053;&#65292;&#37319;&#29992;&#26799;&#24230;&#25163;&#26415;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;GS-EMA&#65289;&#20248;&#21270;&#25216;&#26415;&#19982;&#36793;&#30028;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;BACL&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20110;&#33021;&#22815;&#36866;&#24212;&#26032;&#30340;&#12289;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#65292;&#36890;&#36807;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#21508;&#31181;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#30340;&#21160;&#33033;&#30244;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#26356;&#22810;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15239v1 Announce Type: cross  Abstract: The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning. Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging. These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process. To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware contrastive learning (BACL). Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets. The results demonstrate that our proposed approach can extract more domain-inva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21487;&#20197;&#25913;&#21892;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15237</link><description>&lt;p&gt;
&#36890;&#36807;Transwarp&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21487;&#20197;&#25913;&#21892;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26088;&#22312;&#23558;&#26631;&#35760;&#28304;&#20998;&#24067;&#19982;&#26410;&#26631;&#35760;&#30446;&#26631;&#20998;&#24067;&#23545;&#40784;&#65292;&#20197;&#33719;&#24471;&#39046;&#22495;&#19981;&#21464;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;UDA&#65292;&#20197;&#32553;&#23567;&#26631;&#35760;&#28304;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#39046;&#22495;&#38388;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33041;&#34880;&#31649;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#26631;&#35760;&#30340;3DRA&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#65292;&#24182;&#25913;&#21892;&#26410;&#26631;&#35760;&#30340;MRA&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34880;&#31649;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15237v1 Announce Type: cross  Abstract: Unsupervised domain adaptation (UDA) aims to align the labelled source distribution with the unlabelled target distribution to obtain domain-invariant predictive models. Since cross-modality medical data exhibit significant intra and inter-domain shifts and most are unlabelled, UDA is more important while challenging in medical image analysis. This paper proposes a simple yet potent contrastive learning framework for UDA to narrow the inter-domain gap between labelled source and unlabelled target distribution. Our method is validated on cerebral vessel datasets. Experimental results show that our approach can learn latent features from labelled 3DRA modality data and improve vessel segmentation performance in unlabelled MRA modality data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23556;&#30005;&#21644;&#32418;&#22806;&#22270;&#20687;&#65292;&#32467;&#21512;20,000&#24352;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#38134;&#27827;&#24179;&#38754;&#20013;&#32039;&#20945;&#23556;&#30005;&#28304;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.15232</link><description>&lt;p&gt;
&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#23545;&#38134;&#27827;&#24179;&#38754;&#30340;&#32039;&#20945;&#23556;&#30005;&#28304;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of compact radio sources in the Galactic plane with supervised machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23556;&#30005;&#21644;&#32418;&#22806;&#22270;&#20687;&#65292;&#32467;&#21512;20,000&#24352;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#38134;&#27827;&#24179;&#38754;&#20013;&#32039;&#20945;&#23556;&#30005;&#28304;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22788;&#29702;&#36807;&#30340;&#25968;&#25454;&#20135;&#21697;&#20013;&#29983;&#25104;&#31185;&#23398;&#20934;&#22791;&#23601;&#32490;&#30340;&#25968;&#25454;&#26159;&#26410;&#26469;Square Kilometre Array&#65288;SKA&#65289;&#21450;&#20854;&#21069;&#36523;&#23556;&#30005;&#36830;&#32493;&#27874;&#26222;&#26597;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#36825;&#26159;&#30001;&#20110;&#39044;&#26399;&#30340;&#25968;&#25454;&#37327;&#21644;&#23454;&#29616;&#39640;&#24230;&#33258;&#21160;&#21270;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#20351;&#29992;&#23556;&#30005;&#21644;&#32418;&#22806;&#22270;&#20687;&#23545;&#38134;&#27827;&#24179;&#38754;&#20013;&#30340;&#32039;&#20945;&#23556;&#30005;&#28304;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;20,000&#24352;&#26469;&#33258;&#36807;&#21435;&#23556;&#30005;&#21644;&#32418;&#22806;&#35843;&#26597;&#20197;&#21450;&#20351;&#29992;&#28595;&#22823;&#21033;&#20122;SKA Pathfinder&#65288;ASKAP&#65289;&#36827;&#34892;&#30340;&#35797;&#39564;&#35843;&#26597;&#30340;&#19981;&#21516;&#22825;&#25991;&#31867;&#21035;&#30340;&#32039;&#20945;&#28304;&#22270;&#20687;&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#12290;&#36824;&#33719;&#24471;&#20102;&#37096;&#20998;&#25968;&#25454;&#30340;&#23556;&#30005;&#35889;&#25351;&#25968;&#20449;&#24687;&#12290;&#28982;&#21518;&#22312;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15232v1 Announce Type: cross  Abstract: Generation of science-ready data from processed data products is one of the major challenges in next-generation radio continuum surveys with the Square Kilometre Array (SKA) and its precursors, due to the expected data volume and the need to achieve a high degree of automated processing. Source extraction, characterization, and classification are the major stages involved in this process. In this work we focus on the classification of compact radio sources in the Galactic plane using both radio and infrared images as inputs. To this aim, we produced a curated dataset of ~20,000 images of compact sources of different astronomical classes, obtained from past radio and infrared surveys, and novel radio data from pilot surveys carried out with the Australian SKA Pathfinder (ASKAP). Radio spectral index information was also obtained for a subset of the data. We then trained two different classifiers on the produced dataset. The first model 
&lt;/p&gt;</description></item><item><title>&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#20851;&#27880;&#21033;&#29992;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#25968;&#25454;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#65292;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#35780;&#20272;&#26088;&#22312;&#25552;&#20986;&#36866;&#29992;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#39033;&#30740;&#31350;&#23545;&#35813;&#39046;&#22495;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#24182;&#20998;&#31867;&#20026;&#20004;&#31181;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.15231</link><description>&lt;p&gt;
&#38656;&#35201;&#36716;&#31227;&#21738;&#31181;&#27169;&#22411;&#65311;&#20851;&#20110;&#36716;&#31227;&#24615;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Which Model to Transfer? A Survey on Transferability Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15231
&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#20851;&#27880;&#21033;&#29992;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#25968;&#25454;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#65292;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#35780;&#20272;&#26088;&#22312;&#25552;&#20986;&#36866;&#29992;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#39033;&#30740;&#31350;&#23545;&#35813;&#39046;&#22495;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#24182;&#20998;&#31867;&#20026;&#20004;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#33268;&#21147;&#20110;&#21033;&#29992;&#29616;&#26377;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#20013;&#30340;&#30456;&#20851;&#30693;&#35782;&#26469;&#35299;&#20915;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#12290;&#38543;&#30528;&#24403;&#21069;&#21487;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20107;&#20808;&#35780;&#20272;&#23427;&#20204;&#26159;&#21542;&#36866;&#29992;&#20110;&#29305;&#23450;&#30446;&#26631;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#35780;&#20272;&#26159;&#19968;&#20010;&#26032;&#20852;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#36866;&#29992;&#24615;&#65292;&#32780;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#21333;&#29420;&#35757;&#32451;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#26377;&#24456;&#22810;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#33258;&#23450;&#20041;&#26415;&#35821;&#23450;&#20041;&#21644;&#23454;&#39564;&#35774;&#32622;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#35813;&#39046;&#22495;&#30340;&#29616;&#26377;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#65306;&#26080;&#28304;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#35780;&#20272;&#21644;&#26377;&#28304;&#20381;&#36182;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#35780;&#20272;&#12290;&#27599;&#20010;&#31867;&#21035;&#37117;&#32463;&#36807;&#31995;&#32479;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15231v1 Announce Type: new  Abstract: Transfer learning methods endeavor to leverage relevant knowledge from existing source pre-trained models or datasets to solve downstream target tasks. With the increase in the scale and quantity of available pre-trained models nowadays, it becomes critical to assess in advance whether they are suitable for a specific target task. Model transferability estimation is an emerging and growing area of interest, aiming to propose a metric to quantify this suitability without training them individually, which is computationally prohibitive. Despite extensive recent advances already devoted to this area, they have custom terminological definitions and experimental settings. In this survey, we present the first review of existing advances in this area and categorize them into two separate realms: source-free model transferability estimation and source-dependent model transferability estimation. Each category is systematically defined, accompanie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#65288;FRCR&#65289;&#30340;&#20004;&#38454;&#27573;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#21487;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#20026;&#22266;&#23450;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#22312;&#19981;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#32422;&#26463;&#20102;&#31561;&#20215;&#30340;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.15227</link><description>&lt;p&gt;
&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fixed Random Classifier Rearrangement for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15227
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#65288;FRCR&#65289;&#30340;&#20004;&#38454;&#27573;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#21487;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#20026;&#22266;&#23450;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#22312;&#19981;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#32422;&#26463;&#20102;&#31561;&#20215;&#30340;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#21518;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24536;&#35760;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#22312;&#35270;&#35273;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#32531;&#35299;&#36951;&#24536;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#38480;&#21046;&#20027;&#24178;&#32593;&#32476;&#65292;&#28982;&#32780;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#34987;&#20302;&#20272;&#20102;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;&#39034;&#24207;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#31561;&#20215;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#26174;&#33879;&#24433;&#21709;&#36951;&#24536;&#27700;&#24179;&#12290;&#22522;&#20110;&#36825;&#19968;&#32467;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#65288;Fixed Random Classifier Rearrangement&#65292;FRCR&#65289;&#30340;&#20004;&#38454;&#27573;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;FRCR&#29992;&#22266;&#23450;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#26367;&#25442;&#21487;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#32422;&#26463;&#20102;&#31561;&#20215;&#30340;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#65292;&#32780;&#19981;&#24433;&#21709;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15227v1 Announce Type: cross  Abstract: With the explosive growth of data, continual learning capability is increasingly important for neural networks. Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones. In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone. However, the impact of classifiers is underestimated. In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network. In second sta
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15213</link><description>&lt;p&gt;
&#32479;&#35745;&#26080;&#20559;&#22238;&#24402;&#65306;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#22238;&#24402;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Agnostic Regression: a machine learning method to validate regression models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#20998;&#26512;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#22240;&#21464;&#37327;&#65288;&#36890;&#24120;&#31216;&#20026;&#21709;&#24212;&#21464;&#37327;&#65289;&#19982;&#19968;&#20010;&#25110;&#22810;&#20010;&#33258;&#21464;&#37327;&#65288;&#21363;&#35299;&#37322;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32447;&#24615;&#22238;&#24402;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#39044;&#27979;&#12289;&#39044;&#27979;&#25110;&#22240;&#26524;&#25512;&#26029;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#25191;&#34892;&#27492;&#20219;&#21153;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#38500;&#20102;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#21508;&#31181;&#20256;&#32479;&#26041;&#27861;&#22806;&#65292;&#22914;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#12289;&#23725;&#22238;&#24402;&#25110;&#22871;&#32034;&#22238;&#24402;&#8212;&#8212;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#26159;&#26356;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#22522;&#30784;&#8212;&#8212;&#21518;&#32773;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#20294;&#27809;&#26377;&#23545;&#32479;&#35745;&#26174;&#33879;&#24615;&#36827;&#34892;&#27491;&#24335;&#23450;&#20041;&#12290;&#26368;&#22810;&#65292;&#22522;&#20110;&#32463;&#39564;&#27979;&#37327;&#65288;&#22914;&#27531;&#24046;&#25110;&#20934;&#30830;&#24230;&#65289;&#36827;&#34892;&#32622;&#25442;&#25110;&#22522;&#20110;&#32463;&#20856;&#20998;&#26512;&#65292;&#20197;&#21453;&#26144;ML&#20272;&#35745;&#23545;&#26816;&#27979;&#30340;&#26356;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23545;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;BUAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38543;&#26426;&#26631;&#31614;&#36127;&#23398;&#20064;&#26041;&#27861;&#21644;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#26088;&#22312;&#21516;&#26102;&#31579;&#36873;&#26082;&#21487;&#33021;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#21448;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#30830;&#23450;&#26368;&#26377;&#20215;&#20540;&#31034;&#20363;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15198</link><description>&lt;p&gt;
&#24320;&#25918;&#38598;&#27880;&#37322;&#30340;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Uncertainty-Based Active Learning for Open Set Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;BUAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38543;&#26426;&#26631;&#31614;&#36127;&#23398;&#20064;&#26041;&#27861;&#21644;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#26088;&#22312;&#21516;&#26102;&#31579;&#36873;&#26082;&#21487;&#33021;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#21448;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#30830;&#23450;&#26368;&#26377;&#20215;&#20540;&#31034;&#20363;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;(AL)&#38754;&#20020;&#30528;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#21363;&#30830;&#23450;&#22312;&#19968;&#20010;&#21253;&#21547;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#25968;&#25454;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#35782;&#21035;&#26368;&#26377;&#20215;&#20540;&#30340;&#31034;&#20363;&#12290;&#20256;&#32479;&#26041;&#27861;&#20248;&#20808;&#36873;&#25321;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#23384;&#22312;&#35823;&#36873;&#32622;&#20449;&#24230;&#21516;&#26679;&#36739;&#20302;&#30340;&#26410;&#30693;&#31867;&#21035;&#31034;&#20363;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#26356;&#38738;&#30544;&#26368;&#26377;&#21487;&#33021;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#65292;&#20294;&#23384;&#22312;&#36873;&#21462;&#24050;&#32463;&#25484;&#25569;&#30340;&#31616;&#21333;&#31034;&#20363;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#26597;&#35810;&#26082;&#26377;&#21487;&#33021;&#26469;&#33258;&#24050;&#30693;&#31867;&#21035;&#21448;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;\textit{&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;}&#65288;BUAL&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;\textit{&#38543;&#26426;&#26631;&#31614;&#36127;&#23398;&#20064;}&#26041;&#27861;&#23558;&#26410;&#30693;&#31867;&#21035;&#31034;&#20363;&#25512;&#21521;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;\textit{&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;}&#31574;&#30053;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15198v1 Announce Type: new  Abstract: Active learning (AL) in open set scenarios presents a novel challenge of identifying the most valuable examples in an unlabeled data pool that comprises data from both known and unknown classes. Traditional methods prioritize selecting informative examples with low confidence, with the risk of mistakenly selecting unknown-class examples with similarly low confidence. Recent methods favor the most probable known-class examples, with the risk of picking simple already mastered examples. In this paper, we attempt to query examples that are both likely from known classes and highly informative, and propose a \textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework. Specifically, we achieve this by first pushing the unknown class examples toward regions with high-confidence predictions with our proposed \textit{Random Label Negative Learning} method. Then, we propose a \textit{Bidirectional Uncertainty sampling} strategy by j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35780;&#35770;&#23478;&#22609;&#36896;&#29615;&#22659;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#24471;&#31574;&#30053;&#21487;&#20197;&#21516;&#26102;&#26397;&#30528;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#26080;&#38656;&#32422;&#26463;&#31574;&#30053;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#33258;&#28982;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.15197</link><description>&lt;p&gt;
&#23433;&#20840;&#20248;&#21270;&#30340;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35780;&#35770;&#23478;&#22609;&#36896;&#29615;&#22659;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#24471;&#31574;&#30053;&#21487;&#20197;&#21516;&#26102;&#26397;&#30528;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#26080;&#38656;&#32422;&#26463;&#31574;&#30053;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#33258;&#28982;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;&#23433;&#20840;RL&#65289;&#25351;&#30340;&#26159;&#19968;&#31867;&#25216;&#26415;&#65292;&#26088;&#22312;&#38450;&#27490;RL&#31639;&#27861;&#22312;&#35797;&#38169;&#20915;&#31574;&#21644;&#25506;&#32034;&#36807;&#31243;&#20013;&#36829;&#21453;&#32422;&#26463;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#21046;&#23450;&#30340;&#26032;&#22411;&#26080;&#27169;&#22411;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#20854;&#20013;&#31574;&#30053;&#21516;&#26102;&#26397;&#30528;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#35780;&#35770;&#23478;&#26469;&#22609;&#36896;&#29615;&#22659;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#20248;&#24615;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#23433;&#20840;&#20248;&#21270;RL&#65288;SORL&#65289;&#31639;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#30465;&#30053;&#20102;&#23545;&#31574;&#30053;&#25628;&#32034;&#31354;&#38388;&#30340;&#32422;&#26463;&#38656;&#35201;&#12290;&#36825;&#20351;&#24471;SORL&#33021;&#22815;&#22312;&#19981;&#21463;&#20005;&#26684;&#25628;&#32034;&#31354;&#38388;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#33258;&#28982;&#26435;&#34913;&#65292;&#32780;&#26080;&#38656;&#22240;&#20005;&#26684;&#25628;&#32034;&#31354;&#38388;&#32422;&#26463;&#32780;&#22312;&#23433;&#20840;&#24615;&#25110;&#26368;&#20248;&#24615;&#26041;&#38754;&#24615;&#33021;&#21463;&#25439;&#12290;&#36890;&#36807;&#23545;SORL&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15197v1 Announce Type: cross  Abstract: Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a co
&lt;/p&gt;</description></item><item><title>AffectToolbox&#26159;&#19968;&#20010;&#26032;&#22411;&#36719;&#20214;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#24773;&#24863;&#25935;&#24863;&#30740;&#31350;&#21644;&#21407;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25903;&#25345;&#65292;&#26080;&#38656;&#32534;&#31243;&#30693;&#35782;&#21363;&#21487;&#21487;&#38752;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#29366;&#24577;&#65292;&#23454;&#29616;&#22810;&#27169;&#24773;&#24863;&#35782;&#21035;&#21644;&#34701;&#21512;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.15195</link><description>&lt;p&gt;
AffectToolbox&#65306;&#27599;&#20010;&#20154;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The AffectToolbox: Affect Analysis for Everyone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15195
&lt;/p&gt;
&lt;p&gt;
AffectToolbox&#26159;&#19968;&#20010;&#26032;&#22411;&#36719;&#20214;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#24773;&#24863;&#25935;&#24863;&#30740;&#31350;&#21644;&#21407;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25903;&#25345;&#65292;&#26080;&#38656;&#32534;&#31243;&#30693;&#35782;&#21363;&#21487;&#21487;&#38752;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#29366;&#24577;&#65292;&#23454;&#29616;&#22810;&#27169;&#24773;&#24863;&#35782;&#21035;&#21644;&#34701;&#21512;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#24863;&#35745;&#31639;&#39046;&#22495;&#65292;&#30740;&#31350;&#19981;&#26029;&#20197;&#24555;&#36895;&#30340;&#36895;&#24230;&#21069;&#36827;&#65292;&#29992;&#25143;&#23545;&#29992;&#25143;&#21451;&#22909;&#24037;&#20855;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#32780;&#26131;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AffectToolbox&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#24773;&#24863;&#25935;&#24863;&#30740;&#31350;&#21644;&#21407;&#22411;&#30340;&#26032;&#22411;&#36719;&#20214;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26694;&#26550;&#25152;&#25552;&#20986;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26694;&#26550;&#36890;&#24120;&#38656;&#35201;&#28145;&#20837;&#30340;&#32534;&#31243;&#30693;&#35782;&#65292;&#24182;&#20027;&#35201;&#38754;&#21521;&#39640;&#32423;&#29992;&#25143;&#25110;&#29087;&#32451;&#24320;&#21457;&#20154;&#21592;&#12290;&#20026;&#20102;&#20419;&#36827;&#26131;&#29992;&#24615;&#65292;AffectToolbox&#19981;&#38656;&#35201;&#32534;&#31243;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26131;&#20110;&#35775;&#38382;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#20854;&#21151;&#33021;&#65292;&#21487;&#21487;&#38752;&#22320;&#20998;&#26512;&#29992;&#25143;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#20854;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22810;&#31181;&#24773;&#24863;&#28192;&#36947;&#21644;&#24773;&#24863;&#26041;&#24335;&#19978;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#65292;&#20197;&#21450;&#19968;&#20010;&#22797;&#26434;&#30340;&#34701;&#21512;&#31995;&#32479;&#65292;&#23558;&#22810;&#27169;&#24577;&#35780;&#20272;&#21512;&#24182;&#20026;&#32479;&#19968;&#32467;&#26524;&#12290;&#25972;&#20010;&#31995;&#32479;&#26159;&#20844;&#24320;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15195v1 Announce Type: cross  Abstract: In the field of affective computing, where research continually advances at a rapid pace, the demand for user-friendly tools has become increasingly apparent. In this paper, we present the AffectToolbox, a novel software system that aims to support researchers in developing affect-sensitive studies and prototypes. The proposed system addresses the challenges posed by existing frameworks, which often require profound programming knowledge and cater primarily to power-users or skilled developers. Aiming to facilitate ease of use, the AffectToolbox requires no programming knowledge and offers its functionality to reliably analyze the affective state of users through an accessible graphical user interface. The architecture encompasses a variety of models for emotion recognition on multiple affective channels and modalities, as well as an elaborate fusion system to merge multi-modal assessments into a unified result. The entire system is op
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15194</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15194
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#21644;&#34507;&#30333;&#36136;&#30340;&#20998;&#24067;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21487;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#65292;&#20294;&#25105;&#20204;&#36890;&#24120;&#26356;&#20851;&#27880;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#29983;&#25104;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#25110;&#29983;&#25104;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26576;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#65289;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#26679;&#26412;&#22810;&#26679;&#24615;&#20943;&#23569;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20986;&#29616;&#26174;&#33879;&#20559;&#24046;&#65292;&#29978;&#33267;&#30001;&#20110;&#21033;&#29992;&#19981;&#23436;&#32654;&#30340;&#22870;&#21169;&#20989;&#25968;&#32780;&#23548;&#33268;&#26679;&#26412;&#36136;&#37327;&#36739;&#24046;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#22870;&#21169;&#20989;&#25968;&#26159;&#29992;&#20110;&#36817;&#20284;&#30495;&#23454;&#8220;&#30495;&#23454;&#8221;&#22870;&#21169;&#30340;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#32463;&#24120;&#20250;&#20135;&#29983;&#12290;&#36825;&#20123;&#25361;&#25112;&#24635;&#31216;&#20026;&#8220;&#22870;&#21169;&#23849;&#28291;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15189</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Linking as Multiple Choice Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;BioEL&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32454;&#31890;&#24230;&#21644;&#38271;&#23614;&#23454;&#20307;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioELQA&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;BioELQA&#39318;&#20808;&#21033;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23558;&#25552;&#21450;&#21644;&#20505;&#36873;&#23454;&#20307;&#20849;&#21516;&#21576;&#29616;&#32473;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36755;&#20986;&#19982;&#20854;&#36873;&#23450;&#23454;&#20307;&#30456;&#20851;&#30340;&#39044;&#27979;&#31526;&#21495;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#19981;&#21516;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25429;&#25417;&#20102;&#25552;&#21450;&#21644;&#23454;&#20307;&#20043;&#38388;&#20197;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#31934;&#32454;&#20132;&#20114;&#12290;&#20026;&#20102;&#25913;&#21892;&#38271;&#23614;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#26816;&#32034;&#30456;&#20284;&#30340;&#24050;&#26631;&#35760;&#35757;&#32451;&#23454;&#20363;&#20316;&#20026;&#32447;&#32034;&#65292;&#24182;&#23558;&#36755;&#20837;&#19982;&#26816;&#32034;&#23454;&#20363;&#36830;&#25509;&#21040;&#29983;&#25104;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioELQA&#30340;&#34920;&#29616;&#20248;&#20110;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#25191;&#34892;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#26080;&#21442;&#25968;&#20048;&#35266;&#20248;&#21270;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#21033;&#26222;&#24076;&#33576;&#33258;&#21161;&#24335;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#25968;&#20540;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15188</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#31639;&#27861;&#22312;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#19979;&#36827;&#34892;&#25191;&#34892;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#25191;&#34892;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#26080;&#21442;&#25968;&#20048;&#35266;&#20248;&#21270;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#21033;&#26222;&#24076;&#33576;&#33258;&#21161;&#24335;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#25968;&#20540;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#25191;&#34892;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#21363;&#22312;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#38543;&#26426;&#20248;&#21270;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25191;&#34892;&#39118;&#38505;&#21487;&#33021;&#26159;&#38750;&#20984;&#30340;&#19968;&#33324;&#24773;&#20917;&#65292;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#25105;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#26080;&#21442;&#25968;&#20048;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#21033;&#26222;&#24076;&#33576;&#33258;&#21161;&#24335;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;&#20998;&#24067;&#26144;&#23556;&#30340;&#25935;&#24863;&#24615;&#21442;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#36827;&#34892;&#20102;&#35299;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#20048;&#35266;&#20248;&#21270;&#30340;&#26641;&#25628;&#32034;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25968;&#20540;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#20854;&#20182;&#40657;&#30418;&#20048;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15188v1 Announce Type: new  Abstract: This paper studies performative risk minimization, a formulation of stochastic optimization under decision-dependent distributions. We consider the general case where the performative risk can be non-convex, for which we develop efficient parameter-free optimistic optimization-based methods. Our algorithms significantly improve upon the existing Lipschitz bandit-based method in many aspects. In particular, our framework does not require knowledge about the sensitivity parameter of the distribution map and the Lipshitz constant of the loss function. This makes our framework practically favorable, together with the efficient optimistic optimization-based tree-search mechanism. We provide experimental results that demonstrate the numerical superiority of our algorithms over the existing method and other black-box optimistic optimization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15180</link><description>&lt;p&gt;
&#25171;&#30772;Breakout: &#29992;&#33258;&#25105;&#23436;&#21892;&#37325;&#26032;&#23450;&#20041;LM&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#24341;&#36215;&#19981;&#24555;&#30340;&#20882;&#29359;&#24615;&#35789;&#35821;&#12290;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23481;&#26131;&#34987;&#21033;&#29992;&#36827;&#34892;&#24694;&#24847;&#28389;&#29992;&#12290;&#23545;LM&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#30340;&#35757;&#32451;&#38750;&#24120;&#22797;&#26434;&#65292;&#20351;&#24471;&#38590;&#20197;&#31435;&#21363;&#24212;&#23545;&#24555;&#36895;&#21457;&#23637;&#30340;&#25915;&#20987;&#65292;&#22914;&#36234;&#29425;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26684;&#24335;&#33258;&#25105;&#23436;&#21892;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#33021;&#23454;&#29616;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#38450;&#24481;&#22522;&#32447;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#36825;&#26159;&#38024;&#23545;&#36234;&#29425;&#25915;&#20987;&#26368;&#23433;&#20840;&#30340;&#26080;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33258;&#25105;&#23436;&#21892;&#36807;&#31243;&#25928;&#29575;&#30340;&#26684;&#24335;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#36739;&#23569;&#36845;&#20195;&#20013;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#22312;&#23433;&#20840;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#65292;&#22240;&#20026;&#23427;&#20204;&#32473;&#20986;&#26356;&#26377;&#29992;&#19988;&#26356;&#23433;&#20840;&#30340;&#22238;&#22797;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#23569;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
&lt;/p&gt;</description></item><item><title>RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15179</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#25512;&#36827;&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Advancing Parameter Efficiency in Fine-tuning via Representation Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15179
&lt;/p&gt;
&lt;p&gt;
RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#33021;&#22815;&#22312;&#20165;&#26356;&#26032;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#26102;&#36798;&#21040;&#31454;&#20105;&#24615;&#32467;&#26524;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#34920;&#31034;&#32534;&#36753;&#65288;RED&#65289;&#65292;&#20854;&#25193;&#25918;&#21644;&#20559;&#32622;&#27599;&#19968;&#23618;&#20135;&#29983;&#30340;&#34920;&#31034;&#12290;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;RED&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38477;&#20302;&#20102;$25,700$&#20493;&#65292;&#24182;&#19982;LoRA&#30456;&#27604;&#38477;&#20302;&#20102;32&#20493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RED&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#19981;&#21516;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32479;&#19968;&#35299;&#37322;&#20102;Grokking&#12289;&#21452;&#37325;&#19979;&#38477;&#21644;&#26032;&#20852;&#33021;&#21147;&#36825;&#19977;&#31181;&#29616;&#35937;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#30005;&#36335;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.15175</link><description>&lt;p&gt;
&#32479;&#19968;&#29702;&#35299;Grokking&#12289;&#21452;&#38477;&#21644;&#26032;&#20852;&#33021;&#21147;&#65306;&#26469;&#33258;&#30005;&#36335;&#31454;&#20105;&#35270;&#35282;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15175
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32479;&#19968;&#35299;&#37322;&#20102;Grokking&#12289;&#21452;&#37325;&#19979;&#38477;&#21644;&#26032;&#20852;&#33021;&#21147;&#36825;&#19977;&#31181;&#29616;&#35937;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#30005;&#36335;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20123;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#22914;Grokking&#12289;&#21452;&#37325;&#19979;&#38477;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#36825;&#20123;&#25361;&#25112;&#20102;&#20154;&#31867;&#30340;&#30452;&#35273;&#65292;&#24182;&#23545;&#31070;&#32463;&#27169;&#22411;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#19977;&#31181;&#29616;&#35937;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#35760;&#24518;&#21644;&#27867;&#21270;&#30005;&#36335;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;&#36825;&#31181;&#26041;&#27861;&#26368;&#21021;&#29992;&#20110;&#35299;&#37322;Grokking&#65292;&#25105;&#20204;&#22312;&#24037;&#20316;&#20013;&#23558;&#20854;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21246;&#21202;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#27599;&#31181;&#37117;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#21452;&#37325;&#19979;&#38477;&#29616;&#35937;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#20110;&#20854;&#21457;&#29983;&#30340;&#21487;&#39564;&#35777;&#39044;&#27979;&#65292;&#22343;&#24471;&#21040;&#25105;&#20204;&#23454;&#39564;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15175v1 Announce Type: new  Abstract: Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framewo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15173</link><description>&lt;p&gt;
&#26080;&#30171;&#20154;&#24037;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20108;&#38454;&#24494;&#35843;&#65306;&#19968;&#31181;&#22522;&#20110;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32972;&#21521;&#20256;&#25773;&#36807;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;GPU&#20869;&#23384;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36716;&#21521;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21463;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#21442;&#25968;&#26354;&#29575;&#30340;&#24322;&#36136;&#24615;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#21033;&#29992;&#23545;&#35282;Hessian&#22686;&#24378;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;LLMs&#24494;&#35843;&#30340;&#24037;&#20316;&#12290;HiZOO&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#19988;&#27599;&#27493;&#21482;&#22686;&#21152;&#20102;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#12290;&#23545;&#21508;&#31181;&#27169;&#22411;&#65288;350M&#12316;66B&#21442;&#25968;&#65289;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;HiZOO&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#27493;&#39588;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;HiZOO&#22312;&#27979;&#35797;&#20989;&#25968;&#19978;&#30340;&#20248;&#21270;&#36712;&#36857;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#34920;&#31034;&#65292;&#30456;&#27604;&#26222;&#36890;MAE&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15172</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Attention-Guided Masked Autoencoders For Learning Image Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15172
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#34920;&#31034;&#65292;&#30456;&#27604;&#26222;&#36890;MAE&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Masked autoencoders (MAEs)&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19968;&#20010;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#36890;&#30693;&#37325;&#24314;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#30446;&#26631;&#21457;&#29616;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#22330;&#26223;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#23558;&#20854;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#22686;&#24378;&#20102;&#23545;&#37325;&#24314;&#30456;&#20851;&#23545;&#35937;&#30340;&#37325;&#28857;&#37325;&#24314;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#28608;&#21169;&#27169;&#22411;&#23398;&#20064;&#26356;&#27880;&#37325;&#23545;&#35937;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#24050;&#24314;&#31435;&#30340;&#36974;&#32617;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#20102;&#27604;&#26222;&#36890;MAE&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36890;&#36807;&#25913;&#36827;&#30340;&#32447;&#24615;&#25506;&#27979;&#21644;k-NN&#20998;&#31867;&#32467;&#26524;&#65292;&#21516;&#26102;&#20351;ViTs&#23545;&#19981;&#21516;&#32972;&#26223;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15172v1 Announce Type: cross  Abstract: Masked autoencoders (MAEs) have established themselves as a powerful method for unsupervised pre-training for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image, we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in unsupervised object discovery, we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects, thus effectively incentivizing the model to learn more object-focused representations without compromising the established masking strategy. Our evaluations show that our pre-trained models learn better latent representations than the vanilla MAE, demonstrated by improved linear probing and k-NN classification results on several benchmarks while at the same time making ViTs more robust against varying backgrounds.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15171</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#30340;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#29609;&#23478;&#21487;&#20197;&#20174;&#21253;&#21547;d&#20010;&#22522;&#26412;&#39033;&#30340;P&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;CUCB&#12289;ESCB&#12289;OLS-UCB&#65289;&#38656;&#35201;&#23545;&#22870;&#21169;&#20998;&#24067;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#22914;&#23376;&#39640;&#26031;&#20195;&#29702;-&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;OLS-UCB&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#22312;&#32447;&#20272;&#35745;&#12290;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#20272;&#35745;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31995;&#25968;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#24403;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#32769;&#34382;&#26426;&#21453;&#39304;&#26041;&#27861;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;P&#8811;d&#20197;&#21450;P&#8804;d&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#28857;&#24182;&#19981;&#26469;&#33258;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
&lt;/p&gt;</description></item><item><title>&#36339;&#36291;&#35843;&#35856;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24778;&#20154;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25193;&#25955;&#37319;&#26679;&#20013;UNet&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#31361;&#30772;&#20102;ODE&#37319;&#26679;&#22120;&#30340;&#38480;&#21046;</title><link>https://arxiv.org/abs/2402.15170</link><description>&lt;p&gt;
&#36339;&#36291;&#35843;&#35856;&#22312;&#25193;&#25955;&#37319;&#26679;&#20013;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15170
&lt;/p&gt;
&lt;p&gt;
&#36339;&#36291;&#35843;&#35856;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24778;&#20154;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25193;&#25955;&#37319;&#26679;&#20013;UNet&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#31361;&#30772;&#20102;ODE&#37319;&#26679;&#22120;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;UNet&#26550;&#26500;&#30340;&#25972;&#21512;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20027;&#23548;&#21147;&#37327;&#12290;UNet&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#35774;&#35745;&#26159;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#22359;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#36339;&#36291;&#36830;&#25509;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#30340;&#25463;&#24452;&#21487;&#33021;&#38480;&#21046;&#20102;&#21464;&#25442;&#30340;&#22797;&#26434;&#24615;&#12290;&#38543;&#30528;&#37319;&#26679;&#27493;&#39588;&#20943;&#23569;&#65292;&#29983;&#25104;&#36807;&#31243;&#21644;UNet&#30340;&#20316;&#29992;&#26356;&#25509;&#36817;&#20110;&#20174;&#39640;&#26031;&#20998;&#24067;&#21521;&#30446;&#26631;&#30340;&#25512;&#36827;&#36716;&#25442;&#65292;&#20026;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skip-Tuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#20196;&#20154;&#24778;&#35766;&#22320;&#26377;&#25928;&#30340;&#22522;&#20110;&#36339;&#36807;&#36830;&#25509;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;ImageNet 64&#19978;&#20351;&#29992;19&#20010;NFE&#65288;1.75&#65289;&#20026;&#39044;&#35757;&#32451;&#30340;EDM&#23454;&#29616;100%&#30340;FID&#25913;&#36827;&#65292;&#31361;&#30772;&#20102;ODE&#37319;&#26679;&#22120;&#30340;&#38480;&#21046;&#65292;&#19981;&#35770;&#37319;&#26679;&#27493;&#39588;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15170v1 Announce Type: cross  Abstract: With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surpris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22635;&#34917;&#20102;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#21508;&#24322;&#25968;&#25454;&#19978;&#25910;&#25947;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})&#12290;</title><link>https://arxiv.org/abs/2402.15166</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Split Federated Learning on Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#21508;&#24322;&#25968;&#25454;&#19978;&#25910;&#25947;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;SFL&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#36890;&#24120;&#34987;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;&#24182;&#34892;&#32852;&#37030;&#26041;&#24335;&#35757;&#32451;&#19968;&#37096;&#20998;&#65292;&#20027;&#26381;&#21153;&#22120;&#35757;&#32451;&#21478;&#19968;&#37096;&#20998;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;SFL&#31639;&#27861;&#21457;&#23637;&#30340;&#30740;&#31350;&#24456;&#22810;&#65292;&#20294;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#22312;&#25991;&#29486;&#20013;&#36824;&#26410;&#26377;&#25552;&#21450;&#65292;&#26412;&#25991;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#23545;SFL&#36827;&#34892;&#20998;&#26512;&#21487;&#33021;&#27604;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20998;&#26512;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#23458;&#25143;&#31471;&#21644;&#20027;&#26381;&#21153;&#22120;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#21452;&#36895;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#19978;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#12290;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})$&#65292;&#20854;&#20013;$T$&#34920;&#31034;SFL&#35757;&#32451;&#30340;&#24635;&#36718;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#38750;&#20984;&#30446;&#26631;&#21644;&#19968;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15166v1 Announce Type: cross  Abstract: Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and where some clients may be unavailable during trai
&lt;/p&gt;</description></item><item><title>EasyRL4Rec&#26159;&#19968;&#20010;&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#21451;&#22909;&#21644;&#39640;&#25928;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;RL&#29615;&#22659;&#12289;&#20840;&#38754;&#30340;&#26680;&#24515;&#27169;&#22359;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#26631;&#20934;&#21644;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#24110;&#21161;&#31616;&#21270;&#27169;&#22411;&#24320;&#21457;&#24182;&#25913;&#21892;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.15164</link><description>&lt;p&gt;
EasyRL4Rec&#65306;&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#21451;&#22909;&#20195;&#30721;&#24211;
&lt;/p&gt;
&lt;p&gt;
EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15164
&lt;/p&gt;
&lt;p&gt;
EasyRL4Rec&#26159;&#19968;&#20010;&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#21451;&#22909;&#21644;&#39640;&#25928;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;RL&#29615;&#22659;&#12289;&#20840;&#38754;&#30340;&#26680;&#24515;&#27169;&#22359;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#26631;&#20934;&#21644;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#24110;&#21161;&#31616;&#21270;&#27169;&#22411;&#24320;&#21457;&#24182;&#25913;&#21892;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-&#22522;&#30784;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#36234;&#26469;&#36234;&#34987;&#35748;&#21487;&#20854;&#25552;&#39640;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#26131;&#29992;&#30340;&#26694;&#26550;&#12289;&#35780;&#20272;&#26631;&#20934;&#19981;&#19968;&#33268;&#20197;&#21450;&#22797;&#21046;&#20197;&#21069;&#30340;&#24037;&#20316;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyRL4Rec&#65292;&#19968;&#20010;&#19987;&#20026;&#22522;&#20110;RL&#30340;RSs&#37327;&#36523;&#23450;&#21046;&#30340;&#29992;&#25143;&#21451;&#22909;&#21644;&#39640;&#25928;&#30340;&#24211;&#12290;EasyRL4Rec&#20855;&#26377;&#22522;&#20110;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#22810;&#26679;&#21270;&#30340;RL&#29615;&#22659;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#26680;&#24515;&#27169;&#22359;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#36873;&#39033;&#26469;&#31616;&#21270;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#24314;&#31435;&#20102;&#19968;&#33268;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#37325;&#28857;&#20851;&#27880;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#23450;&#21046;&#30340;&#29366;&#24577;&#24314;&#27169;&#21644;&#34892;&#20026;&#34920;&#31034;&#30340;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#36890;&#36807;&#19982;&#24403;&#21069;&#26041;&#27861;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#33719;&#24471;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;EasyRL4Rec&#26088;&#22312;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15164v1 Announce Type: cross  Abstract: Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly recognized for their ability to improve long-term user engagement. Yet, the field grapples with challenges such as the absence of accessible frameworks, inconsistent evaluation standards, and the complexity of replicating prior work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight, diverse RL environments built on five widely-used public datasets, and is equipped with comprehensive core modules that offer rich options to ease the development of models. It establishes consistent evaluation criteria with a focus on long-term impacts and introduces customized solutions for state modeling and action representation tailored to recommender systems. Additionally, we share valuable insights gained from extensive experiments with current methods. EasyRL4Rec aims to facil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15162</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#35843;&#30340;&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15162
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#21442;&#25968;&#21270;&#30693;&#35782;&#19982;&#36755;&#20837;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#26102;&#65292;&#24448;&#24448;&#29983;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#25105;&#20204;&#31216;&#20043;&#20026;&#20107;&#23454;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35780;&#20272;&#38598;&#65292;&#24182;&#21457;&#29616;&#20107;&#23454;&#36866;&#24212;&#24615;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24182;&#38750;&#24378;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20854;&#20013;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#31243;&#24230;&#26159;&#21487;&#35843;&#33410;&#30340;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PEGASUS &#21644; BART&#65289;&#21644;&#20004;&#20010;&#24494;&#35843;&#25968;&#25454;&#38598;&#65288;XSum &#21644; CNN/DailyMail&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#23545;&#27604;&#26041;&#27861;&#30456;&#24403;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15162v1 Announce Type: cross  Abstract: Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15160</link><description>&lt;p&gt;
&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#35760;&#24518;&#20307;&#29992;&#20110;&#20307;&#39564;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Spatially-Aware Transformer Memory for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#33410;&#35760;&#24518;&#22312;&#21508;&#31181;&#35748;&#30693;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27604;&#22914;&#33021;&#22815;&#22312;&#22836;&#33041;&#20013;&#22238;&#24518;&#36807;&#21435;&#20107;&#20214;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#24378;&#35843;&#31354;&#38388;&#19978;&#19979;&#25991;&#22312;&#24773;&#33410;&#35760;&#24518;&#30340;&#24418;&#25104;&#21644;&#26816;&#32034;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24403;&#21069;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#24773;&#33410;&#35760;&#24518;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#23384;&#20648;&#26102;&#38388;&#39034;&#24207;&#20307;&#39564;&#30340;&#21464;&#21387;&#22120;&#65292;&#36825;&#24573;&#30053;&#20102;&#31354;&#38388;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;&#22522;&#30784;&#32467;&#26500;&#25193;&#23637;&#21040;&#38500;&#20102;&#20165;&#26377;&#26102;&#38388;&#39034;&#24207;&#20043;&#22806;&#30340;&#31354;&#38388;&#36724;&#65292;&#24182;&#30001;&#27492;&#33021;&#22815;&#33719;&#24471;&#21738;&#20123;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#32771;&#34385;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#30340;&#22330;&#25152;&#20013;&#24515;&#24773;&#33410;&#35760;&#24518;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#65292;&#23548;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Self-Adaptive Reconstruction Contrastive Sentence Embeddings&#65288;SARCSE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#24314;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#20196;&#29260;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#20445;&#30041;&#26356;&#22810;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#24314;&#25439;&#22833;&#26469;&#32531;&#35299;&#23545;&#20196;&#29260;&#39057;&#29575;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.15153</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#33258;&#36866;&#24212;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Self-Adaptive Reconstruction Contrastive Sentence Embeddings&#65288;SARCSE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#24314;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#20196;&#29260;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#20445;&#30041;&#26356;&#22810;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#24314;&#25439;&#22833;&#26469;&#32531;&#35299;&#23545;&#20196;&#29260;&#39057;&#29575;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#20219;&#21153;&#26088;&#22312;&#23558;&#21477;&#23376;&#36716;&#25442;&#20026;&#35821;&#20041;&#21521;&#37327;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#30452;&#25509;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27966;&#29983;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20196;&#29260;&#20559;&#24046;&#65292;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21477;&#23376;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#23548;&#33268;&#39044;&#27979;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#24314;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#65288;SARCSE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#24314;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#20196;&#29260;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#32858;&#21512;&#20196;&#29260;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#32454;&#31890;&#24230;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#24314;&#25439;&#22833;&#26469;&#32531;&#35299;&#23545;&#20196;&#29260;&#39057;&#29575;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24378;&#22522;&#20934;SimCSE&#30456;&#27604;&#65292;SARCSE&#22312;7&#20010;STS&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15153v1 Announce Type: new  Abstract: Unsupervised sentence embeddings task aims to convert sentences to semantic vector representations. Most previous works directly use the sentence representations derived from pretrained language models. However, due to the token bias in pretrained language models, the models can not capture the fine-grained semantics in sentences, which leads to poor predictions. To address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in sentences with an AutoEncoder to help the model to preserve more fine-grained semantics during tokens aggregating. In addition, we proposed a self-adaptive reconstruction loss to alleviate the token bias towards frequency. Experimental results show that SARCSE gains significant improvements compared with the strong baseline SimCSE on the 7 STS tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15152</link><description>&lt;p&gt;
&#20851;&#20110;Sharpness-Aware&#26368;&#23567;&#21270;&#21644;&#23545;&#25239;&#35757;&#32451;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Duality Between Sharpness-Aware Minimization and Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15152
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(Adversarial Training, AT)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#34987;&#35748;&#20026;&#26159;&#23545;&#25239;&#25915;&#20987;&#20013;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#20043;&#19968;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#19968;&#31181;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#24517;&#28982;&#20250;&#38477;&#20302;&#24178;&#20928;&#20934;&#30830;&#24615;&#12290;&#19982;&#23545;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#19981;&#21516;&#65292;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#65292;&#20197;&#23547;&#25214;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26354;&#38754;&#24182;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#26088;&#22312;&#25552;&#39640;&#24178;&#20928;&#20934;&#30830;&#24615;&#65292;&#20854;&#22312;&#22686;&#24378;&#23545;&#25239;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;SAM&#21644;AT&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20174;SAM&#20013;&#27966;&#29983;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#29702;&#35299;SAM&#30340;&#36825;&#31181;&#24847;&#22806;&#29305;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;SAM&#22914;&#20309;&#38544;&#24335;&#23398;&#20064;&#26356;&#40065;&#26834;&#29305;&#24449;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#35265;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15152v1 Announce Type: cross  Abstract: Adversarial Training (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against adversarial attacks, yet suffers from a fundamental tradeoff that inevitably decreases clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing adversarial robustness remains unexplored. In this work, considering the duality between SAM and AT, we investigate the adversarial robustness derived from SAM. Intriguingly, we find that using SAM alone can improve adversarial robustness. To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TREC&#65292;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#23569;&#26679;&#26412;&#36861;&#28335;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;&#23545;APT&#25915;&#20987;&#27963;&#21160;&#20013;&#25112;&#26415;/&#25216;&#26415;&#30340;&#35782;&#21035;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#32454;&#31890;&#24230;APT&#25216;&#26415;&#21644;&#21464;&#31181;APT&#25915;&#20987;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15147</link><description>&lt;p&gt;
TREC: &#36890;&#36807;&#23569;&#26679;&#26412;&#36861;&#28335;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;APT&#25112;&#26415;/&#25216;&#26415;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TREC&#65292;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#23569;&#26679;&#26412;&#36861;&#28335;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;&#23545;APT&#25915;&#20987;&#27963;&#21160;&#20013;&#25112;&#26415;/&#25216;&#26415;&#30340;&#35782;&#21035;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#32454;&#31890;&#24230;APT&#25216;&#26415;&#21644;&#21464;&#31181;APT&#25915;&#20987;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
APT&#65288;&#39640;&#32423;&#25345;&#32493;&#24615;&#23041;&#32961;&#65289;&#20855;&#26377;&#25345;&#20037;&#24615;&#12289;&#38544;&#31192;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#29305;&#24449;&#65292;&#26159;&#23545;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#30340;&#26368;&#22823;&#23041;&#32961;&#20043;&#19968;&#12290; &#20026;&#24212;&#23545;&#27492;&#23041;&#32961;&#65292;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#28335;&#28304;&#22270;&#26469;&#25429;&#25417;&#20027;&#26426;&#20013;&#31995;&#32479;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;APT&#26816;&#27979;&#12290; &#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#21482;&#26816;&#27979;&#21333;&#20010;&#25915;&#20987;&#20107;&#20214;&#19981;&#21516;&#65292;&#29702;&#35299;&#32452;&#32455;&#21644;&#23436;&#25104;APT&#25915;&#20987;&#27963;&#21160;&#25152;&#24212;&#29992;&#30340;&#25112;&#26415;/&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;Kill-Chain&#12289;ATT&amp;CK&#65289;&#23545;&#23433;&#20840;&#36816;&#33829;&#26356;&#20026;&#37325;&#35201;&#12290; &#29616;&#26377;&#30740;&#31350;&#23581;&#35797;&#25163;&#21160;&#35774;&#35745;&#19968;&#32452;&#35268;&#21017;&#65292;&#23558;&#20302;&#32423;&#31995;&#32479;&#20107;&#20214;&#26144;&#23556;&#21040;&#39640;&#32423;APT&#25112;&#26415;/&#25216;&#26415;&#12290; &#20294;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#31895;&#31890;&#24230;&#19988;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#27492;&#21482;&#33021;&#35782;&#21035;APT&#25112;&#26415;&#65292;&#26080;&#27861;&#36776;&#35782;&#32454;&#31890;&#24230;&#30340;APT&#25216;&#26415;&#21644;&#31361;&#21464;&#30340;APT&#25915;&#20987;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;TREC&#65292;&#36825;&#26159;&#39318;&#20010;&#23581;&#35797;&#35748;&#30693;AP
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15147v1 Announce Type: cross  Abstract: APT (Advanced Persistent Threat) with the characteristics of persistence, stealth, and diversity is one of the greatest threats against cyber-infrastructure. As a countermeasure, existing studies leverage provenance graphs to capture the complex relations between system entities in a host for effective APT detection. In addition to detecting single attack events as most existing work does, understanding the tactics / techniques (e.g., Kill-Chain, ATT&amp;CK) applied to organize and accomplish the APT attack campaign is more important for security operations. Existing studies try to manually design a set of rules to map low-level system events to high-level APT tactics / techniques. However, the rule based methods are coarse-grained and lack generalization ability, thus they can only recognize APT tactics and cannot identify fine-grained APT techniques and mutant APT attacks. In this paper, we propose TREC, the first attempt to recognize AP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;BMS&#31639;&#27861;&#35299;&#37322;&#20026;&#20248;&#21270;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#27169;&#31946;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#20998;&#26512;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#32467;&#26524;&#65292;&#26412;&#30740;&#31350;&#19981;&#20165;&#35206;&#30422;&#25968;&#25454;&#25910;&#25947;&#21040;&#21333;&#19968;&#28857;&#30340;&#24773;&#20917;&#65292;&#36824;&#25552;&#20379;&#20102;&#22810;&#28857;&#25910;&#25947;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#24555;&#12290;</title><link>https://arxiv.org/abs/2402.15146</link><description>&lt;p&gt;
&#27169;&#31946;&#22343;&#20540;&#28418;&#31227;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Blurring Mean Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;BMS&#31639;&#27861;&#35299;&#37322;&#20026;&#20248;&#21270;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#27169;&#31946;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#20998;&#26512;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#32467;&#26524;&#65292;&#26412;&#30740;&#31350;&#19981;&#20165;&#35206;&#30422;&#25968;&#25454;&#25910;&#25947;&#21040;&#21333;&#19968;&#28857;&#30340;&#24773;&#20917;&#65292;&#36824;&#25552;&#20379;&#20102;&#22810;&#28857;&#25910;&#25947;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#22343;&#20540;&#28418;&#31227;&#65288;BMS&#65289;&#31639;&#27861;&#26159;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26159;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;&#65292;&#25968;&#25454;&#28857;&#26681;&#25454;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#27169;&#31946;&#30340;&#25910;&#25947;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#23558;BMS&#31639;&#27861;&#35299;&#37322;&#20026;&#20248;&#21270;&#36807;&#31243;&#26469;&#20998;&#26512;BMS&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#36825;&#22312;&#29616;&#26377;&#30340;&#25910;&#25947;&#30740;&#31350;&#20013;&#26159;&#24050;&#30693;&#30340;&#65292;&#20294;&#21364;&#34987;&#23569;&#26377;&#21033;&#29992;&#12290;&#29616;&#26377;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#22810;&#32500;&#25968;&#25454;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#21482;&#28085;&#30422;&#20102;&#25152;&#26377;&#27169;&#31946;&#25968;&#25454;&#28857;&#24207;&#21015;&#25910;&#25947;&#21040;&#21333;&#19968;&#28857;&#30340;&#24773;&#20917;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#65292;&#21363;&#20351;&#36825;&#20123;&#24207;&#21015;&#21487;&#20197;&#25910;&#25947;&#21040;&#22810;&#20010;&#28857;&#65292;&#20174;&#32780;&#20135;&#29983;&#22810;&#20010;&#31751;&#12290;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#36827;&#19968;&#27493;&#21033;&#29992;&#25910;&#25947;&#28857;&#30340;&#20960;&#20309;&#29305;&#24449;&#34920;&#26126;&#65292;BMS&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15146v1 Announce Type: new  Abstract: Blurring mean shift (BMS) algorithm, a variant of the mean shift algorithm, is a kernel-based iterative method for data clustering, where data points are clustered according to their convergent points via iterative blurring. In this paper, we analyze convergence properties of the BMS algorithm by leveraging its interpretation as an optimization procedure, which is known but has been underutilized in existing convergence studies. Whereas existing results on convergence properties applicable to multi-dimensional data only cover the case where all the blurred data point sequences converge to a single point, this study provides a convergence guarantee even when those sequences can converge to multiple points, yielding multiple clusters. This study also shows that the convergence of the BMS algorithm is fast by further leveraging geometrical characterization of the convergent points.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#24182;&#34892;&#21270;&#24369;&#21040;&#24378;Boosting&#31639;&#27861;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#23545;Boosting&#30340;&#36731;&#24494;&#24182;&#34892;&#21270;&#20063;&#38656;&#35201;&#22312;&#35757;&#32451;&#22797;&#26434;&#24230;&#19978;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;</title><link>https://arxiv.org/abs/2402.15145</link><description>&lt;p&gt;
&#24179;&#34892;Boosting&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Cost of Parallelizing Boosting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15145
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24182;&#34892;&#21270;&#24369;&#21040;&#24378;Boosting&#31639;&#27861;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#23545;Boosting&#30340;&#36731;&#24494;&#24182;&#34892;&#21270;&#20063;&#38656;&#35201;&#22312;&#35757;&#32451;&#22797;&#26434;&#24230;&#19978;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24182;&#34892;&#21270;&#24369;&#21040;&#24378;Boosting&#31639;&#27861;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#24310;&#32493;&#20102;Karbasi&#21644;Larsen&#26368;&#36817;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#21452;&#37325;&#30340;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#19979;&#30028;&#65292;&#34920;&#26126;&#21363;&#20351;&#26159;&#23545;Boosting&#30340;&#36731;&#24494;&#24182;&#34892;&#21270;&#20063;&#38656;&#35201;&#22312;&#35757;&#32451;&#22797;&#26434;&#24230;&#19978;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;$\gamma$&#20026;&#24369;&#23398;&#20064;&#22120;&#20248;&#20110;&#38543;&#26426;&#29468;&#27979;&#30340;&#20248;&#21183;&#12290;&#33879;&#21517;&#30340;\textsc{AdaBoost}&#31639;&#27861;&#36890;&#36807;&#19982;&#24369;&#23398;&#20064;&#22120;&#36827;&#34892;$\tilde{O}(1 / \gamma^2)$&#36718;&#20132;&#20114;&#26469;&#20135;&#29983;&#20934;&#30830;&#30340;&#20551;&#35774;&#65292;&#20854;&#20013;&#27599;&#36718;&#37117;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;Karbasi&#21644;Larsen&#34920;&#26126;&#8220;&#26174;&#33879;&#8221;&#30340;&#24182;&#34892;&#21270;&#24517;&#39035;&#23548;&#33268;&#25351;&#25968;&#32423;&#22686;&#38271;&#65306;&#20219;&#20309;Boosting&#31639;&#27861;&#35201;&#20040;&#19982;&#24369;&#23398;&#20064;&#22120;&#20132;&#20114;$\Omega(1 / \gamma)$&#36718;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#22797;&#26434;&#24230;&#19978;&#20986;&#29616;$\exp(d/\gamma)$&#30340;&#22686;&#38271;&#65292;&#20854;&#20013;$d$&#26159;&#20551;&#35774;&#31867;&#30340;VC&#32500;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20219;&#20309;Boosti
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15145v1 Announce Type: new  Abstract: We study the cost of parallelizing weak-to-strong boosting algorithms for learning, following the recent work of Karbasi and Larsen. Our main results are two-fold:   - First, we prove a tight lower bound, showing that even "slight" parallelization of boosting requires an exponential blow-up in the complexity of training.   Specifically, let $\gamma$ be the weak learner's advantage over random guessing. The famous \textsc{AdaBoost} algorithm produces an accurate hypothesis by interacting with the weak learner for $\tilde{O}(1 / \gamma^2)$ rounds where each round runs in polynomial time.   Karbasi and Larsen showed that "significant" parallelization must incur exponential blow-up: Any boosting algorithm either interacts with the weak learner for $\Omega(1 / \gamma)$ rounds or incurs an $\exp(d / \gamma)$ blow-up in the complexity of training, where $d$ is the VC dimension of the hypothesis class. We close the gap by showing that any boosti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#19978;&#21033;&#29992;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#26469;&#26816;&#27979;&#36923;&#36753;&#24322;&#24120;&#30340;&#31616;&#21333;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.15143</link><description>&lt;p&gt;
PUAD&#65306;&#31283;&#20581;&#24322;&#24120;&#26816;&#27979;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PUAD: Frustratingly Simple Method for Robust Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#19978;&#21033;&#29992;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#26469;&#26816;&#27979;&#36923;&#36753;&#24322;&#24120;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20934;&#30830;&#24555;&#36895;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#26159;&#23454;&#26102;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#30446;&#21069;&#26377;&#24456;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#24320;&#21457;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#32467;&#26500;&#24615;&#25110;&#36923;&#36753;&#24615;&#24322;&#24120;&#65292;&#36825;&#20004;&#32773;&#26412;&#36136;&#19978;&#26159;&#19981;&#21516;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#22823;&#22810;&#25968;&#26263;&#21547;&#24322;&#24120;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#24322;&#24120;&#20301;&#32622;&#26469;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#22914;&#38169;&#35823;&#30340;&#29289;&#20307;&#25968;&#37327;&#31561;&#36923;&#36753;&#24322;&#24120;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#34987;&#31354;&#38388;&#29305;&#24449;&#22270;&#25152;&#34920;&#31034;&#65292;&#38656;&#35201;&#21478;&#19968;&#31181;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#26469;&#26816;&#27979;&#36923;&#36753;&#24322;&#24120;&#30340;&#21487;&#33021;&#24615;&#65292;&#35813;&#26041;&#27861;&#32858;&#21512;&#20102;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#20316;&#20026;&#28436;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31616;&#21333;&#30340;&#29305;&#24449;&#31354;&#38388;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15143v1 Announce Type: cross  Abstract: Developing an accurate and fast anomaly detection model is an important task in real-time computer vision applications. There has been much research to develop a single model that detects either structural or logical anomalies, which are inherently distinct. The majority of the existing approaches implicitly assume that the anomaly can be represented by identifying the anomalous location. However, we argue that logical anomalies, such as the wrong number of objects, can not be well-represented by the spatial feature maps and require an alternative approach. In addition, we focused on the possibility of detecting logical anomalies by using an out-of-distribution detection approach on the feature space, which aggregates the spatial information of the feature map. As a demonstration, we propose a method that incorporates a simple out-of-distribution detection method on the feature space against state-of-the-art reconstruction-based approa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#30340;&#20276;&#38543;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#20276;&#38543;&#24418;&#24335;&#19982;&#21453;&#21521;&#20256;&#25773;&#32467;&#26524;&#24182;&#19981;&#31561;&#20215;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20160;&#20040;&#24773;&#20917;&#19979;&#20276;&#38543;&#24418;&#24335;&#20250;&#20135;&#29983;&#19982;&#21453;&#21521;&#20256;&#25773;&#30456;&#21516;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15141</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#30340;&#20276;&#38543;&#26041;&#27861;&#30340;&#27880;&#35299;
&lt;/p&gt;
&lt;p&gt;
A note on the adjoint method for neural ordinary differential equation network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15141
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#30340;&#20276;&#38543;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#20276;&#38543;&#24418;&#24335;&#19982;&#21453;&#21521;&#20256;&#25773;&#32467;&#26524;&#24182;&#19981;&#31561;&#20215;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20160;&#20040;&#24773;&#20917;&#19979;&#20276;&#38543;&#24418;&#24335;&#20250;&#20135;&#29983;&#19982;&#21453;&#21521;&#20256;&#25773;&#30456;&#21516;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#25200;&#21160;&#21644;&#31639;&#23376;&#20276;&#38543;&#26041;&#27861;&#65292;&#20005;&#26684;&#32473;&#20986;&#20102;&#27491;&#30830;&#30340;&#20276;&#38543;&#24418;&#24335;&#12290;&#20174;&#25512;&#23548;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#32467;&#26524;&#65306;1) &#25439;&#22833;&#26799;&#24230;&#19981;&#26159;&#19968;&#20010;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#32780;&#26159;&#19968;&#20010;&#31215;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21407;&#22240;&#65307;2) &#20256;&#32479;&#30340;&#20276;&#38543;&#24418;&#24335;&#19982;&#21453;&#21521;&#20256;&#25773;&#32467;&#26524;&#19981;&#31561;&#20215;&#12290;3) &#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#24403;&#31163;&#25955;&#20276;&#38543;&#31639;&#23376;&#19982;&#31163;&#25955;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20855;&#26377;&#30456;&#21516;&#26041;&#26696;&#26102;&#65292;&#20276;&#38543;&#24418;&#24335;&#25165;&#33021;&#32473;&#20986;&#19982;BP&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15141v1 Announce Type: cross  Abstract: Perturbation and operator adjoint method are used to give the right adjoint form rigourously. From the derivation, we can have following results: 1) The loss gradient is not an ODE, it is an integral and we shows the reason; 2) The traditional adjoint form is not equivalent with the back propagation results. 3) The adjoint operator analysis shows that if and only if the discrete adjoint has the same scheme with the discrete neural ODE, the adjoint form would give the same results as BP does.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#22810;&#38454;&#24207;&#20869;&#21644;&#24207;&#38388;&#30340;&#22797;&#26434;&#32806;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.15134</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Coupling Network For Multivariate Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15134
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#22810;&#38454;&#24207;&#20869;&#21644;&#24207;&#38388;&#30340;&#22797;&#26434;&#32806;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#30340;MTS&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20998;&#21035;&#24314;&#27169;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#24182;&#24573;&#30053;&#20102;&#23384;&#22312;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#22810;&#38454;&#20132;&#20114;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#24182;&#30456;&#24212;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#25429;&#25417;&#22797;&#26434;&#22810;&#38454;&#24207;&#20869;&#21644;&#24207;&#38388;&#32806;&#21512;&#30340;&#20840;&#38754;&#20851;&#31995;&#23398;&#20064;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#19968;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;MTS&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;&#65292;&#31216;&#20026;DeepCN&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#19987;&#29992;&#20110;&#26126;&#30830;&#25506;&#32034;&#22810;&#38454;&#24207;&#20869;&#21644;&#38388;&#30340;&#32806;&#21512;&#26426;&#21046;&#30340;&#32806;&#21512;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15134v1 Announce Type: cross  Abstract: Multivariate time series (MTS) forecasting is crucial in many real-world applications. To achieve accurate MTS forecasting, it is essential to simultaneously consider both intra- and inter-series relationships among time series data. However, previous work has typically modeled intra- and inter-series relationships separately and has disregarded multi-order interactions present within and between time series data, which can seriously degrade forecasting accuracy. In this paper, we reexamine intra- and inter-series relationships from the perspective of mutual information and accordingly construct a comprehensive relationship learning mechanism tailored to simultaneously capture the intricate multi-order intra- and inter-series couplings. Based on the mechanism, we propose a novel deep coupling network for MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated to explicitly exploring the multi-order intra- and in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;STS&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15132</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Sentence Embeddings with an Automatically Generated NLI Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15132
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;STS&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#21516;&#26679;&#25104;&#31435;&#65292;&#20854;&#20013;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;PromptEOL &#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;PromptEOL &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21033;&#29992;&#20102;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25968;&#25454;&#38598;&#30340;&#25163;&#21160;&#26631;&#27880;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;PromptEOL&#12290;&#22312;STS&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#36798;&#21040;&#20102;82.21&#30340;&#24179;&#22343;Spearman&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;&#65292;&#20174;&#32780;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#35268;&#27169;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24323;&#26435;&#36873;&#39033;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28176;&#36817;&#21644;&#31859;&#36855;&#35834;&#19979;&#26368;&#20248;&#12290;</title><link>https://arxiv.org/abs/2402.15127</link><description>&lt;p&gt;
&#20855;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Armed Bandits with Abstention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15127
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24323;&#26435;&#36873;&#39033;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28176;&#36817;&#21644;&#31859;&#36855;&#35834;&#19979;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#25193;&#23637;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#39069;&#22806;&#30340;&#25112;&#30053;&#20803;&#32032;&#65306;&#24323;&#26435;&#36873;&#39033;&#12290;&#22312;&#36825;&#20010;&#22686;&#24378;&#26694;&#26550;&#20013;&#65292;&#20195;&#29702;&#19981;&#20165;&#38656;&#35201;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36873;&#25321;&#19968;&#20010;&#33218;&#65292;&#36824;&#21487;&#20197;&#36873;&#25321;&#22312;&#35266;&#23519;&#20043;&#21069;&#25918;&#24323;&#25509;&#21463;&#38543;&#26426;&#30636;&#26102;&#22870;&#21169;&#12290;&#24403;&#36873;&#25321;&#24323;&#26435;&#26102;&#65292;&#20195;&#29702;&#35201;&#20040;&#36973;&#21463;&#22266;&#23450;&#30340;&#21518;&#24724;&#65292;&#35201;&#20040;&#33719;&#24471;&#19968;&#23450;&#30340;&#22870;&#21169;&#20445;&#35777;&#12290;&#37492;&#20110;&#36825;&#31181;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25506;&#35752;&#26159;&#21542;&#21487;&#20197;&#24320;&#21457;&#20986;&#26082;&#28176;&#36817;&#21448;&#31859;&#36855;&#35834;&#19979;&#26368;&#20248;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#21644;&#20998;&#26512;&#31639;&#27861;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#21518;&#24724;&#28385;&#36275;&#30456;&#24212;&#30340;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#24323;&#26435;&#36873;&#39033;&#30340;&#22909;&#22788;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#37327;&#21270;&#35265;&#35299;&#65292;&#20026;&#22312;&#20854;&#20182;&#20855;&#26377;&#36825;&#31181;&#36873;&#39033;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#20013;&#36827;&#19968;&#27493;&#25506;&#32034;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15127v1 Announce Type: new  Abstract: We introduce a novel extension of the canonical multi-armed bandit problem that incorporates an additional strategic element: abstention. In this enhanced framework, the agent is not only tasked with selecting an arm at each time step, but also has the option to abstain from accepting the stochastic instantaneous reward before observing it. When opting for abstention, the agent either suffers a fixed regret or gains a guaranteed reward. Given this added layer of complexity, we ask whether we can develop efficient algorithms that are both asymptotically and minimax optimal. We answer this question affirmatively by designing and analyzing algorithms whose regrets meet their corresponding information-theoretic lower bounds. Our results offer valuable quantitative insights into the benefits of the abstention option, laying the groundwork for further exploration in other online decision-making problems with such an option. Numerical results f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#21487;&#35757;&#32451;SVGD&#31639;&#27861;&#21152;&#36895;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#20256;&#32479;SVGD&#21464;&#20307;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.15125</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#21152;&#36895;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15125
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#21487;&#35757;&#32451;SVGD&#31639;&#27861;&#21152;&#36895;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#20256;&#32479;SVGD&#21464;&#20307;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;SVGD&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#23637;&#24320;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#34701;&#20837;SVGD&#30340;&#26032;&#22411;&#21487;&#35757;&#32451;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20419;&#36827;&#20102;&#23545;SVGD&#30340;&#20869;&#37096;&#21442;&#25968;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#21487;&#35757;&#32451;SVGD&#31639;&#27861;&#65292;&#25105;&#20204;&#23545;&#19977;&#39033;&#20219;&#21153;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#65306;&#23545;&#19968;&#32500;&#39640;&#26031;&#28151;&#21512;&#36827;&#34892;&#37319;&#26679;&#65292;&#36827;&#34892;&#36125;&#21494;&#26031;&#36923;&#36753;&#22238;&#24402;&#20197;&#21450;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#27604;SVGD&#30340;&#20256;&#32479;&#21464;&#20307;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15125v1 Announce Type: new  Abstract: Stein variational gradient descent (SVGD) is a prominent particle-based variational inference method used for sampling a target distribution. SVGD has attracted interest for application in machine-learning techniques such as Bayesian inference. In this paper, we propose novel trainable algorithms that incorporate a deep-learning technique called deep unfolding,into SVGD. This approach facilitates the learning of the internal parameters of SVGD, thereby accelerating its convergence speed. To evaluate the proposed trainable SVGD algorithms, we conducted numerical simulations of three tasks: sampling a one-dimensional Gaussian mixture, performing Bayesian logistic regression, and learning Bayesian neural networks. The results show that our proposed algorithms exhibit faster convergence than the conventional variants of SVGD.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#27493;&#37325;&#36848;&#29983;&#25104;&#36807;&#31243;&#23545;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#23545;&#37322;&#20041;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15120</link><description>&lt;p&gt;
&#20351;&#29992;&#20004;&#27493;&#37325;&#36848;&#23545;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#37325;&#36848;&#29983;&#25104;&#36807;&#31243;&#23545;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#23545;&#37322;&#20041;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Contrastive language-image pre-training (CLIP)&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#65292;&#20854;&#20013;&#27169;&#22411;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#35270;&#35273;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#22788;&#29702;&#36755;&#20837;&#26597;&#35810;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#65288;&#22914;&#37322;&#20041;&#65289;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#38480;&#21046;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#24212;&#29992;&#20013;&#29992;&#25143;&#26597;&#35810;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;CLIP&#27169;&#22411;&#23545;&#37322;&#20041;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#20010;&#20004;&#27493;&#37322;&#20041;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32593;&#39029;&#35268;&#27169;&#30340;&#22270;&#20687;&#26631;&#39064;&#20013;&#33258;&#21160;&#21019;&#24314;&#20004;&#31867;&#37322;&#20041;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#29983;&#25104;&#30340;&#37322;&#20041;&#26469;&#24494;&#35843;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#27169;&#22411;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15120v1 Announce Type: cross  Abstract: Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;SciML&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#22312;UQ&#20219;&#21153;&#20013;&#21033;&#29992;SciML&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.15115</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15115
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;SciML&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#22312;UQ&#20219;&#21153;&#20013;&#21033;&#29992;SciML&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#25191;&#34892;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65306;&#23558;SciML&#19982;UQ&#26080;&#32541;&#38598;&#25104;&#65292;&#20174;&#32780;&#33021;&#22815;&#26377;&#25928;&#22320;&#37327;&#21270;SciML&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;SciML&#26469;&#25913;&#21892;UQ&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;&#35813;&#26367;&#20195;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#22810;&#31181;&#29289;&#29702;&#32422;&#26463;&#65292;&#22914;&#25903;&#37197;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#21450;&#20854;&#30456;&#20851;&#30340;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#32422;&#26463;&#65292;&#19981;&#31561;&#24335;&#22411;&#32422;&#26463;&#65288;&#22914;&#21333;&#35843;&#24615;&#65292;&#20984;&#24615;&#65292;&#38750;&#36127;&#24615;&#31561;&#65289;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#39069;&#22806;&#20808;&#39564;&#20449;&#24687;&#20197;&#36741;&#21161;&#26377;&#38480;&#25968;&#25454;&#12290;&#36825;&#30830;&#20445;&#20102;&#29289;&#29702;&#19978;&#21512;&#29702;&#30340;&#39044;&#27979;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#26114;&#36149;&#35745;&#31639;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15115v1 Announce Type: cross  Abstract: We present a novel physics-constrained polynomial chaos expansion as a surrogate modeling method capable of performing both scientific machine learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method possesses a unique capability: it seamlessly integrates SciML into UQ and vice versa, which allows it to quantify the uncertainties in SciML tasks effectively and leverage SciML for improved uncertainty assessment during UQ-related tasks. The proposed surrogate model can effectively incorporate a variety of physical constraints, such as governing partial differential equations (PDEs) with associated initial and boundary conditions constraints, inequality-type constraints (e.g., monotonicity, convexity, non-negativity, among others), and additional a priori information in the training process to supplement limited data. This ensures physically realistic predictions and significantly reduces the need for expensive comp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MSPipe&#65292;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15113</link><description>&lt;p&gt;
MSPipe: &#36890;&#36807;&#24847;&#35782;&#21040;&#38472;&#26087;&#24615;&#30340;&#31649;&#36947;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24615;GNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MSPipe&#65292;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#22411;&#26102;&#38388;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MTGNNs&#65289;&#26159;&#19968;&#31867;&#21033;&#29992;&#33410;&#28857;&#35760;&#24518;&#27169;&#22359;&#25429;&#33719;&#21644;&#20445;&#30041;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#26102;&#38388;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#23545;&#20110;&#26080;&#35760;&#24518;&#30340;&#23545;&#24212;&#32593;&#32476;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;MTGNNs&#20013;&#65292;&#20026;&#20102;&#33719;&#21462;&#26368;&#26032;&#30340;&#20449;&#24687;&#65292;&#35760;&#24518;&#27169;&#22359;&#30340;&#36845;&#20195;&#35835;&#21462;&#21644;&#26356;&#26032;&#36807;&#31243;&#38656;&#35201;&#36981;&#24490;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#24182;&#38480;&#21046;&#20102;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#38745;&#24577;GNNs&#30340;&#20248;&#21270;&#19981;&#36866;&#29992;&#20110;MTGNNs&#65292;&#22240;&#20026;&#20004;&#32773;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#32570;&#20047;&#35760;&#24518;&#27169;&#22359;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24182;&#26410;&#26377;&#25928;&#22320;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20351;&#20854;&#23545;MTGNN&#35757;&#32451;&#26080;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSPipe&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#21487;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15113v1 Announce Type: new  Abstract: Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, they do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for MTGNNs that maximizes training throughput while maintaining model accuracy. Our design
&lt;/p&gt;</description></item><item><title>Chu-ko-nu&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21311;&#21517;&#35748;&#35777;&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#31192;&#23494;&#23494;&#38053;&#32452;&#20214;&#26469;&#31361;&#30772;&#20102;&#20849;&#20139;&#20256;&#36755;&#30340;&#27010;&#29575;P&#38480;&#21046;</title><link>https://arxiv.org/abs/2402.15111</link><description>&lt;p&gt;
Chu-ko-nu&#65306;&#19968;&#31181;&#21487;&#38752;&#12289;&#39640;&#25928;&#19988;&#25903;&#25345;&#21311;&#21517;&#35748;&#35777;&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#23454;&#29616;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15111
&lt;/p&gt;
&lt;p&gt;
Chu-ko-nu&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21311;&#21517;&#35748;&#35777;&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#31192;&#23494;&#23494;&#38053;&#32452;&#20214;&#26469;&#31361;&#30772;&#20102;&#20849;&#20139;&#20256;&#36755;&#30340;&#27010;&#29575;P&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33021;&#22815;&#36890;&#36807;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#23545;&#23458;&#25143;&#31471;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#27599;&#36718;&#24517;&#39035;&#25191;&#34892;&#26114;&#36149;&#30340;&#21047;&#26032;&#35774;&#32622;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#38656;&#35201;&#22312;&#19981;&#21516;&#36718;&#27425;&#24314;&#31435;&#26032;&#30340;&#29420;&#31435;&#20110;&#36755;&#20837;&#30340;&#23494;&#38053;&#12290;&#26368;&#26032;&#30740;&#31350;Flamingo&#65288;S&amp;P 2023&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#20256;&#36755;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#23494;&#38053;&#65292;&#20197;&#25903;&#25345;&#26381;&#21153;&#22120;&#25345;&#32493;&#25191;&#34892;&#22810;&#36718;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#23427;&#25552;&#20986;&#30340;&#20849;&#20139;&#20256;&#36755;&#26426;&#21046;&#20165;&#33021;&#20197;P&#27010;&#29575;&#23454;&#29616;&#65292;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#19988;&#25903;&#25345;&#21311;&#21517;&#35748;&#35777;&#30340;&#21517;&#20026;Chu-ko-nu&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#20849;&#20139;&#20256;&#36755;&#26041;&#38754;&#65292;Chu-ko-nu&#36890;&#36807;&#34917;&#20805;&#31192;&#23494;&#23494;&#38053;&#32452;&#20214;&#30340;&#37325;&#26032;&#20998;&#37197;&#36807;&#31243;&#65292;&#31361;&#30772;&#20102;&#27010;&#29575;P&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15111v1 Announce Type: cross  Abstract: Secure aggregation enables federated learning (FL) to perform collaborative training of clients from local gradient updates without exposing raw data. However, existing secure aggregation schemes inevitably perform an expensive fresh setup per round because each client needs to establish fresh input-independent secrets over different rounds. The latest research, Flamingo (S&amp;P 2023), designed a share-transfer-based reusable secret key to support the server continuously performing multiple rounds of aggregation. Nevertheless, the share transfer mechanism it proposed can only be achieved with P probability, which has limited reliability. To tackle the aforementioned problems, we propose a more reliable and anonymously authenticated scheme called Chu-ko-nu for multi-round secure aggregation. Specifically, in terms of share transfer, Chu-ko-nu breaks the probability P barrier by supplementing a redistribution process of secret key component
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15109</link><description>&lt;p&gt;
&#25233;&#21046;&#26679;&#26412;&#36129;&#29486;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning by Suppressing Sample Contribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#26159;&#25351;&#20174;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#28041;&#21450;&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#12290;&#26412;&#25991;&#20174;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#35265;&#25968;&#25454;&#23545;&#27169;&#22411;&#36129;&#29486;&#30340;&#22522;&#26412;&#21306;&#21035;&#20837;&#25163;&#65306;&#35757;&#32451;&#25968;&#25454;&#23545;&#26368;&#32456;&#27169;&#22411;&#26377;&#36129;&#29486;&#65292;&#32780;&#26410;&#35265;&#25968;&#25454;&#27809;&#26377;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21457;&#29616;&#36755;&#20837;&#25935;&#24863;&#24230;&#21487;&#20197;&#36817;&#20284;&#34913;&#37327;&#36129;&#29486;&#65292;&#24182;&#23454;&#38469;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#31216;&#20026;MU-Mis&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#36827;&#34892;&#26426;&#22120;&#36951;&#24536;&#65289;&#65292;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MU-Mis&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MU&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MU-Mis&#19982;MU&#30340;&#24212;&#29992;&#26356;&#21152;&#23494;&#20999;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15109v1 Announce Type: new  Abstract: Machine Unlearning (MU) is to forget data from a well-trained model, which is practically important due to the "right to be forgotten". In this paper, we start from the fundamental distinction between training data and unseen data on their contribution to the model: the training data contributes to the final model while the unseen data does not. We theoretically discover that the input sensitivity can approximately measure the contribution and practically design an algorithm, called MU-Mis (machine unlearning via minimizing input sensitivity), to suppress the contribution of the forgetting data. Experimental results demonstrate that MU-Mis outperforms state-of-the-art MU methods significantly. Additionally, MU-Mis aligns more closely with the application of MU as it does not require the use of remaining data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15106</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Distributed Training with Message Passing Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#32780;&#25193;&#23637;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;Nystrom-&#36817;&#20284;&#37319;&#26679;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DS-MPNN&#65288;&#20854;&#20013;D&#21644;S&#20998;&#21035;&#20195;&#34920;&#20998;&#24067;&#24335;&#21644;&#37319;&#26679;&#65289;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;$O(10^5)$&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26696;&#20363;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65306;&#65288;a&#65289;Darcy&#27969;&#25968;&#25454;&#38598;&#21644;&#65288;b&#65289;2-D&#26426;&#32764;&#30340;&#31283;&#24577;RANS&#27169;&#25311;&#65292;&#25552;&#20379;&#20102;&#19982;&#21333;GPU&#23454;&#29616;&#21644;&#22522;&#20110;&#33410;&#28857;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#27604;&#36739;&#12290;DS-MPNN&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#21333;GPU&#23454;&#29616;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#23481;&#32435;&#27604;&#21333;&#20010;GPU&#23454;&#29616;&#26356;&#22810;&#25968;&#37327;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15106v1 Announce Type: new  Abstract: In this study, we introduce a domain-decomposition-based distributed training and inference approach for message-passing neural networks (MPNN). Our objective is to address the challenge of scaling edge-based graph neural networks as the number of nodes increases. Through our distributed training approach, coupled with Nystr\"om-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based graph convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15102</link><description>&lt;p&gt;
&#36712;&#36857;&#24335;&#36845;&#20195;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#31454;&#26631;
&lt;/p&gt;
&lt;p&gt;
Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15102
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#65292;&#24191;&#21578;&#20027;&#21442;&#19982;&#24191;&#21578;&#31454;&#25293;&#20197;&#33719;&#21462;&#24191;&#21578;&#26426;&#20250;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#38656;&#27714;&#26041;&#24179;&#21488;(DSPs)&#25552;&#20379;&#30340;&#33258;&#21160;&#31454;&#26631;&#24037;&#20855;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#31454;&#26631;&#31639;&#27861;&#36890;&#24120;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;RL&#30340;&#33258;&#21160;&#31454;&#26631;&#31574;&#30053;&#26159;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#37096;&#32626;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21487;&#20197;&#24182;&#34892;&#37096;&#32626;&#22810;&#20010;&#33258;&#21160;&#31454;&#26631;&#20195;&#29702;&#20197;&#25910;&#38598;&#22823;&#37327;&#20132;&#20114;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#21033;&#29992;&#31163;&#32447;RL&#31639;&#27861;&#35757;&#32451;&#26032;&#31574;&#30053;&#12290;&#35757;&#32451;&#21518;&#30340;&#31574;&#30053;&#38543;&#21518;&#21487;&#20197;&#37096;&#32626;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#36845;&#20195;&#35757;&#32451;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36845;&#20195;&#31163;&#32447;RL&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#36845;&#20195;&#31163;&#32447;RL&#26694;&#26550;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#20854;&#26681;&#28304;&#22312;&#20110;&#30001;&#20110;&#20869;&#22312;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15102v1 Announce Type: cross  Abstract: In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inhe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#22312;&#38381;&#28304;&#21644;&#24320;&#28304;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#38381;&#28304;&#36719;&#20214;&#25968;&#25454;&#19978;&#65292;C#&#30340;&#24615;&#33021;&#21464;&#21270;&#19981;&#22823;&#12290;</title><link>https://arxiv.org/abs/2402.15100</link><description>&lt;p&gt;
&#30740;&#31350;LLM&#22312;&#38381;&#28304;&#21644;&#24320;&#28304;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Studying LLM Performance on Closed- and Open-source Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#22312;&#38381;&#28304;&#21644;&#24320;&#28304;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#38381;&#28304;&#36719;&#20214;&#25968;&#25454;&#19978;&#65292;C#&#30340;&#24615;&#33021;&#21464;&#21270;&#19981;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#25968;&#25454;&#38656;&#27714;&#26497;&#39640;&#65292;&#20027;&#35201;&#26159;&#22312;&#20855;&#26377;&#23485;&#26494;&#35768;&#21487;&#30340;&#24320;&#28304;&#65288;OSS&#65289;&#20195;&#30721;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#24456;&#22810;&#36719;&#20214;&#24320;&#21457;&#20173;&#28982;&#21457;&#29983;&#22312;&#30408;&#21033;/&#19987;&#26377;&#39046;&#22495;&#65292;&#20854;&#20013;&#27491;&#22312;&#24320;&#21457;&#30340;&#20195;&#30721;&#19981;&#26159;&#20063;&#20174;&#26410;&#22312;&#20844;&#20849;&#39046;&#22495;&#20013;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24320;&#21457;&#20154;&#21592;&#22312;LLMs&#22312;&#19981;&#29087;&#24713;&#27491;&#22312;&#24320;&#21457;&#30340;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24037;&#20316;&#21644;&#20351;&#29992;&#12290;&#22312;&#36825;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#30340;&#24615;&#33021;&#26159;&#21542;&#21644;&#22312;OSS&#20195;&#30721;&#19978;&#19968;&#26679;&#22909;&#21602;&#65311;&#33509;&#19981;&#26159;&#65292;&#26377;&#20160;&#20040;&#19981;&#21516;&#65311;&#24403;&#24615;&#33021;&#19981;&#21516;&#26102;&#65292;&#21487;&#33021;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#65292;&#26159;&#21542;&#26377;&#35299;&#20915;&#26041;&#27861;&#65311;&#26412;&#25991;&#20351;&#29992;&#26469;&#33258;&#24494;&#36719;&#30340;&#19987;&#26377;&#38381;&#28304;&#36719;&#20214;&#25968;&#25454;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#22823;&#37096;&#20998;&#19987;&#26377;&#20195;&#30721;&#37319;&#29992;C#&#21644;C++&#12290;&#25105;&#20204;&#21457;&#29616;C#&#30340;&#24615;&#33021;&#20174;OSS --&gt; proprietary&#21464;&#21270;&#19981;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15100v1 Announce Type: cross  Abstract: Large Language models (LLMs) are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use LLMs, in settings where the models may not be as familiar with the code under development. In such settings, do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C# and C++. We find that performance for C# changes little from OSS --&gt; proprieta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#35299;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#21253;&#25324;&#21508;&#31181;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#22788;&#29702;&#24230;&#37327;&#31354;&#38388;&#30340;&#36924;&#36817;&#29702;&#35770;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.15097</link><description>&lt;p&gt;
&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning solution operators of PDEs defined on varying domains via MIONet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15097
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#35299;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#21253;&#25324;&#21508;&#31181;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#22788;&#29702;&#24230;&#37327;&#31354;&#38388;&#30340;&#36924;&#36817;&#29702;&#35770;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;MIONet&#23398;&#20064;&#23450;&#20041;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;PDE&#30340;&#35299;&#31639;&#23376;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;MIONet&#30340;&#36924;&#36817;&#29702;&#35770;&#25193;&#23637;&#21040;&#36827;&#19968;&#27493;&#22788;&#29702;&#24230;&#37327;&#31354;&#38388;&#65292;&#24314;&#31435;MIONet&#21487;&#20197;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#36924;&#36817;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#26144;&#23556;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#20123;&#36866;&#24403;&#21306;&#22495;&#30340;&#38598;&#21512;&#65292;&#24182;&#20026;&#36825;&#20010;&#38598;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#24230;&#37327;&#65292;&#20174;&#32780;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#28385;&#36275;MIONet&#30340;&#36924;&#36817;&#26465;&#20214;&#12290;&#22522;&#20110;&#29702;&#35770;&#22522;&#30784;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;PDE&#30340;&#35299;&#26144;&#23556;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#21253;&#25324;&#24494;&#20998;&#31639;&#23376;&#30340;&#21442;&#25968;&#65292;&#21491;&#25163;&#36793;&#39033;&#65292;&#36793;&#30028;&#26465;&#20214;&#20197;&#21450;&#22495;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;2D&#27850;&#26494;&#26041;&#31243;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20854;&#20013;&#22495;&#21644;&#21491;&#25163;&#36793;&#39033;&#26159;&#21464;&#21270;&#30340;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15097v1 Announce Type: new  Abstract: In this work, we propose a method to learn the solution operators of PDEs defined on varying domains via MIONet, and theoretically justify this method. We first extend the approximation theory of MIONet to further deal with metric spaces, establishing that MIONet can approximate mappings with multiple inputs in metric spaces. Subsequently, we construct a set consisting of some appropriate regions and provide a metric on this set thus make it a metric space, which satisfies the approximation condition of MIONet. Building upon the theoretical foundation, we are able to learn the solution mapping of a PDE with all the parameters varying, including the parameters of the differential operator, the right-hand side term, the boundary condition, as well as the domain. Without loss of generality, we for example perform the experiments for 2-d Poisson equations, where the domains and the right-hand side terms are varying. The results provide insig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#22810;&#27169;&#24577;Transformer (LoCoMT) &#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#27169;&#24335;&#32473;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#65292;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;</title><link>https://arxiv.org/abs/2402.15096</link><description>&lt;p&gt;
&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#20445;&#35777;&#30340;&#22810;&#27169;&#24577;Transformer
&lt;/p&gt;
&lt;p&gt;
Multimodal Transformer With a Low-Computational-Cost Guarantee
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15096
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#22810;&#27169;&#24577;Transformer (LoCoMT) &#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#27169;&#24335;&#32473;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#65292;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#22914;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#21644;&#21160;&#20316;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;Transformer&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23384;&#22312;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20302;&#25104;&#26412;&#22810;&#27169;&#24577;Transformer&#65288;LoCoMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26368;&#23567;&#24615;&#33021;&#25439;&#22833;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;LoCoMT&#33021;&#22815;&#28789;&#27963;&#25511;&#21046;&#22810;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#30830;&#20445;&#30456;&#23545;&#20110;&#29616;&#26377;&#22810;&#27169;&#24577;Transformer&#21464;&#20307;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20004;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;Audioset&#21644;MedVidCL&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoCoMT&#19981;&#20165;&#20943;&#23569;&#20102;GFLOPs&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15096v1 Announce Type: new  Abstract: Transformer-based models have significantly improved performance across a range of multimodal understanding tasks, such as visual question answering and action recognition. However, multimodal Transformers significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different multimodal attention patterns to each attention head, LoCoMT can flexibly control multimodal signals and theoretically ensures a reduced computational cost compared to existing multimodal Transformer variants. Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs bu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21305;&#37197;&#36890;&#36807;&#28508;&#22312;&#33410;&#28857;&#25490;&#21015;&#30456;&#20851;&#30340;&#20004;&#20010;&#39640;&#26031;&#20960;&#20309;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#20026;&#20302;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#21644;&#20960;&#20046;&#31934;&#30830;&#24674;&#22797;&#24314;&#31435;&#20102;&#20449;&#24687;&#38408;&#20540;&#65292;&#24182;&#36827;&#34892;&#20102;&#26757;&#23665;&#31639;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.15095</link><description>&lt;p&gt;
&#29992;&#20110;&#21305;&#37197;&#20302;&#32500;&#24773;&#20917;&#19979;&#30340;&#30456;&#20851;&#39640;&#26031;&#20960;&#20309;&#27169;&#22411;&#30340;&#26757;&#23665;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Umeyama algorithm for matching correlated Gaussian geometric models in the low-dimensional regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21305;&#37197;&#36890;&#36807;&#28508;&#22312;&#33410;&#28857;&#25490;&#21015;&#30456;&#20851;&#30340;&#20004;&#20010;&#39640;&#26031;&#20960;&#20309;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#20026;&#20302;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#21644;&#20960;&#20046;&#31934;&#30830;&#24674;&#22797;&#24314;&#31435;&#20102;&#20449;&#24687;&#38408;&#20540;&#65292;&#24182;&#36827;&#34892;&#20102;&#26757;&#23665;&#31639;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#21305;&#37197;&#20004;&#20010;&#30456;&#20851;&#30340;&#38543;&#26426;&#20960;&#20309;&#22270;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#28508;&#22312;&#33410;&#28857;&#25490;&#21015;&#30456;&#20851;&#30340;&#20004;&#20010;&#39640;&#26031;&#20960;&#20309;&#27169;&#22411;&#30340;&#21305;&#37197;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;$\{1,\ldots,n\}$&#19978;&#30340;&#19968;&#20010;&#26410;&#30693;&#25490;&#21015;$\pi^*$&#65292;&#20197;&#21450;&#32473;&#23450;$n$&#23545;&#22312;$\mathbb{R}^d$&#20013;&#36890;&#36807;&#22122;&#22768;&#21442;&#25968;$\sigma$&#30456;&#20851;&#30340;&#39640;&#26031;&#21521;&#37327;$\{X_{\pi^*(i)},Y_i\}$&#65292;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#36793;&#26435;&#37325;$A_{i,j}=\langle X_i,X_j \rangle$&#65292;$B_{i,j}=\langle Y_i,Y_j \rangle$&#30340;&#20004;&#31181;&#31867;&#22411;&#30340;(&#30456;&#20851;&#30340;)&#21152;&#26435;&#23436;&#20840;&#22270;&#12290;&#30446;&#26631;&#26159;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#30697;&#38453;$A$&#21644;$B$&#24674;&#22797;&#38544;&#34255;&#30340;&#39030;&#28857;&#23545;&#24212;$\pi^*$&#12290;&#22312;&#32500;&#25968;&#20026;$d=O(\log n)$&#30340;&#20302;&#32500;&#24773;&#20917;&#19979;&#65292;Wang, Wu, Xu&#21644;Yolou [WWXY22+]&#24314;&#31435;&#20102;&#21305;&#37197;&#30456;&#20851;&#30340;&#39640;&#26031;&#20960;&#20309;&#27169;&#22411;&#20013;&#31934;&#30830;&#21644;&#20960;&#20046;&#31934;&#30830;&#24674;&#22797;&#30340;&#20449;&#24687;&#38408;&#20540;&#12290;&#20182;&#20204;&#36824;&#23545;&#32463;&#20856;&#30340;&#26757;&#23665;&#31639;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15095v1 Announce Type: cross  Abstract: Motivated by the problem of matching two correlated random geometric graphs, we study the problem of matching two Gaussian geometric models correlated through a latent node permutation. Specifically, given an unknown permutation $\pi^*$ on $\{1,\ldots,n\}$ and given $n$ i.i.d. pairs of correlated Gaussian vectors $\{X_{\pi^*(i)},Y_i\}$ in $\mathbb{R}^d$ with noise parameter $\sigma$, we consider two types of (correlated) weighted complete graphs with edge weights given by $A_{i,j}=\langle X_i,X_j \rangle$, $B_{i,j}=\langle Y_i,Y_j \rangle$. The goal is to recover the hidden vertex correspondence $\pi^*$ based on the observed matrices $A$ and $B$. For the low-dimensional regime where $d=O(\log n)$, Wang, Wu, Xu, and Yolou [WWXY22+] established the information thresholds for exact and almost exact recovery in matching correlated Gaussian geometric models. They also conducted numerical experiments for the classical Umeyama algorithm. In o
&lt;/p&gt;</description></item><item><title>AttributionBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#21482;&#33021;&#36798;&#21040;80%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15089</link><description>&lt;p&gt;
AttributionBench&#65306;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#26377;&#22810;&#38590;&#65311;
&lt;/p&gt;
&lt;p&gt;
AttributionBench: How Hard is Automatic Attribution Evaluation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15089
&lt;/p&gt;
&lt;p&gt;
AttributionBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#21482;&#33021;&#36798;&#21040;80%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#25552;&#20379;&#24341;&#29992;&#35777;&#25454;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#24402;&#22240;&#65292;&#21363;&#29983;&#25104;&#21709;&#24212;&#20013;&#30340;&#27599;&#20010;&#22768;&#26126;&#26159;&#21542;&#37117;&#24471;&#21040;&#20854;&#24341;&#29992;&#35777;&#25454;&#30340;&#20805;&#20998;&#25903;&#25345;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#35780;&#20272;&#30340;&#36825;&#31181;&#39564;&#35777;&#24378;&#35843;&#20102;&#23545;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#22522;&#20934;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AttributionBench&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#22522;&#20934;&#65292;&#30001;&#21508;&#31181;&#29616;&#26377;&#30340;&#24402;&#22240;&#25968;&#25454;&#38598;&#32534;&#21046;&#32780;&#25104;&#12290;&#25105;&#20204;&#22312;AttributionBench&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;LLM&#20063;&#26159;&#22914;&#27492;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#32463;&#36807;&#20248;&#21270;&#30340;GPT-3.5&#22312;&#20108;&#20803;&#20998;&#31867;&#20844;&#24335;&#19979;&#20063;&#21482;&#33021;&#36798;&#21040;&#32422;80%&#30340;&#23439;F1&#20998;&#25968;&#12290;&#26356; than 300 error c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15089v1 Announce Type: cross  Abstract: Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error c
&lt;/p&gt;</description></item><item><title>PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15082</link><description>&lt;p&gt;
PEMT: &#22810;&#20219;&#21153;&#30456;&#20851;&#24615;&#24341;&#23548;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15082
&lt;/p&gt;
&lt;p&gt;
PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#20316;&#20026;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#24050;&#32463;&#23835;&#36215;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#21040;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20197;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#36866;&#37197;&#22120;&#65292;&#35201;&#20040;&#20174;&#28304;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#30693;&#35782;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#20219;&#21153;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEMT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21019;&#26032;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#12290;PEMT&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26694;&#26550;&#25193;&#23637;&#20026;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#30340;&#21152;&#26435;&#32452;&#21512;&#20197;&#25429;&#33719;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#36825;&#20123;&#26435;&#37325;&#30001;&#19968;&#20010;&#38376;&#25511;&#21333;&#20803;&#30830;&#23450;&#65292;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#27979;&#37327;&#30446;&#26631;&#20219;&#21153;&#21644;&#27599;&#20010;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#20559;&#22909;&#23398;&#20064;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#25104;&#26412;&#33258;&#36866;&#24212;&#30340;&#34917;&#25937;&#24314;&#35758;&#65292;&#36890;&#36807;&#35774;&#35745;&#38382;&#31572;&#26694;&#26550;&#21644;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#29983;&#25104;&#34917;&#25937;&#26469;&#32771;&#34385;&#20027;&#20307;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.15073</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#20559;&#22909;&#24341;&#23548;&#23454;&#29616;&#25104;&#26412;&#33258;&#36866;&#24212;&#34917;&#25937;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#20559;&#22909;&#23398;&#20064;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#25104;&#26412;&#33258;&#36866;&#24212;&#30340;&#34917;&#25937;&#24314;&#35758;&#65292;&#36890;&#36807;&#35774;&#35745;&#38382;&#31572;&#26694;&#26550;&#21644;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#29983;&#25104;&#34917;&#25937;&#26469;&#32771;&#34385;&#20027;&#20307;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15073v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#31639;&#27861;&#24615;&#34917;&#25937;&#24314;&#35758;&#26159;&#21521;&#20027;&#20307;&#25512;&#33616;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;&#34892;&#21160;&#65292;&#20197;&#25197;&#36716;&#19981;&#21033;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20915;&#31574;&#12290;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#29983;&#25104;&#34917;&#25937;&#24314;&#35758;&#26102;&#37117;&#26159;&#22522;&#20110;&#23545;&#25104;&#26412;&#20989;&#25968;&#30340;&#23436;&#20840;&#20102;&#35299;&#36825;&#19968;&#20551;&#35774;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#36341;&#20013;&#65292;&#20027;&#20307;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#23548;&#33268;&#23545;&#20027;&#20307;&#30340;&#22522;&#30784;&#25104;&#26412;&#20989;&#25968;&#23384;&#22312;&#19981;&#23436;&#25972;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20559;&#22909;&#23398;&#20064;&#25972;&#21512;&#21040;&#34917;&#25937;&#20135;&#29983;&#38382;&#39064;&#20013;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38382;&#31572;&#26694;&#26550;&#65292;&#20197;&#39034;&#24207;&#22320;&#25913;&#36827;&#20027;&#20307;&#30340;&#39532;&#27663;&#30697;&#38453;&#25104;&#26412;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#21644;&#22522;&#20110;&#22270;&#30340;&#25104;&#26412;&#33258;&#36866;&#24212;&#34917;&#25937;&#20004;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#34917;&#25937;&#24314;&#35758;&#65292;&#30830;&#20445;&#26377;&#25928;&#24615;&#30340;&#21516;&#26102;&#32771;&#34385;&#25104;&#26412;&#30697;&#38453;&#30340;&#25972;&#20010;&#32622;&#20449;&#21306;&#38388;&#12290;&#25968;&#20540;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15073v1 Announce Type: new  Abstract: Algorithmic recourse recommends a cost-efficient action to a subject to reverse an unfavorable machine learning classification decision. Most existing methods in the literature generate recourse under the assumption of complete knowledge about the cost function. In real-world practice, subjects could have distinct preferences, leading to incomplete information about the underlying cost function of the subject. This paper proposes a two-step approach integrating preference learning into the recourse generation problem. In the first step, we design a question-answering framework to refine the confidence set of the Mahalanobis matrix cost of the subject sequentially. Then, we generate recourse by utilizing two methods: gradient-based and graph-based cost-adaptive recourse that ensures validity while considering the whole confidence set of the cost matrix. The numerical evaluation demonstrates the benefits of our approach over state-of-the-a
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Co-Boosting&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38598;&#25104;&#27169;&#22411;&#30456;&#20114;&#22686;&#24378;&#65292;&#20419;&#36827;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.15070</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21644;&#38598;&#25104;&#21327;&#21516;&#22686;&#24378;&#22686;&#24378;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15070
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Co-Boosting&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38598;&#25104;&#27169;&#22411;&#30456;&#20114;&#22686;&#24378;&#65292;&#20419;&#36827;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#65288;OFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#21333;&#19968;&#36890;&#20449;&#36718;&#27425;&#23454;&#29616;&#20840;&#23616;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;OFL&#20013;&#65292;&#26381;&#21153;&#22120;&#27169;&#22411;&#36890;&#36807;&#20174;&#25152;&#26377;&#23458;&#25143;&#31471;&#27169;&#22411;&#65288;&#38598;&#25104;&#65289;&#20013;&#25552;&#28860;&#30693;&#35782;&#36827;&#34892;&#32858;&#21512;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#20063;&#36127;&#36131;&#21512;&#25104;&#29992;&#20110;&#25552;&#28860;&#30340;&#26679;&#26412;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#20808;&#36827;&#30340;&#20316;&#21697;&#34920;&#26126;&#65292;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#38598;&#25104;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#25512;&#24191;OFL&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;Co-Boosting&#65292;&#20854;&#20013;&#21512;&#25104;&#25968;&#25454;&#21644;&#38598;&#25104;&#27169;&#22411;&#30456;&#20114;&#36880;&#27493;&#22686;&#24378;&#23545;&#26041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Co-Boosting&#21033;&#29992;&#24403;&#21069;&#30340;&#38598;&#25104;&#27169;&#22411;&#20197;&#23545;&#25239;&#26041;&#24335;&#21512;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#22256;&#38590;&#26679;&#26412;&#28982;&#21518;&#34987;&#29992;&#26469;&#36890;&#36807;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#38598;&#25104;&#26435;&#37325;&#26469;&#25552;&#21319;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;Co-Boosting&#21608;&#26399;&#24615;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15070v1 Announce Type: new  Abstract: One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodicall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.15062</link><description>&lt;p&gt;
&#21035;&#32781;&#33457;&#25307;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35843;&#25972;&#20197;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#22238;&#31572;&#38382;&#39064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38382;&#39064;&#27809;&#26377;&#26126;&#30830;&#31572;&#26696;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33258;&#20449;&#36807;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#21521;&#36825;&#20123;&#26410;&#30693;&#38382;&#39064;&#25552;&#20379;&#34394;&#26500;&#31572;&#26696;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#25506;&#35752;&#25298;&#32477;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#26412;&#36523;&#26469;&#22686;&#24378;&#20854;&#23545;&#19981;&#21516;&#31867;&#22411;&#26410;&#30693;&#38382;&#39064;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#25298;&#32477;&#22238;&#31572;&#65292;&#36824;&#33021;&#22815;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Self-Align&#26041;&#27861;&#39318;&#20808;&#37319;&#29992;&#20004;&#38454;&#27573;&#31867;&#24863;&#30693;&#33258;&#25105;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#26410;&#30693;&#38382;&#39064;-&#22238;&#24212;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#24046;&#24322;&#39537;&#21160;&#30340;&#33258;&#25105;&#25972;&#29702;&#65292;&#36873;&#25321;&#21512;&#26684;&#25968;&#25454;&#23545;LLM&#26412;&#36523;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35843;&#25972;&#23545;&#26410;&#30693;&#38382;&#39064;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15062v1 Announce Type: new  Abstract: Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown q
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15061</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models for Domain-specific Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39046;&#22495;&#29305;&#23450;MT&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#22522;&#20110;LLMs&#30340;MT&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#39640;&#25928;&#22320;&#20026;&#39046;&#22495;&#29305;&#23450;MT&#20219;&#21153;&#24494;&#35843;&#36890;&#29992;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15061v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we constru
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#26465;&#24418;&#30721;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#20934;&#25345;&#20037;&#21516;&#35843;&#19982;&#22270;&#20687;&#25345;&#20037;&#21516;&#35843;&#32467;&#21512;&#65292;&#21487;&#20197;&#37327;&#21270;&#20219;&#24847;&#32500;&#24230;&#20004;&#20010;&#28857;&#38598;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#24341;&#20837;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;&#26469;&#37327;&#21270;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15058</link><description>&lt;p&gt;
&#28151;&#21512;&#26465;&#24418;&#30721;&#65306;&#37327;&#21270;&#28857;&#20113;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mixup Barcodes: Quantifying Geometric-Topological Interactions between Point Clouds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#26465;&#24418;&#30721;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#20934;&#25345;&#20037;&#21516;&#35843;&#19982;&#22270;&#20687;&#25345;&#20037;&#21516;&#35843;&#32467;&#21512;&#65292;&#21487;&#20197;&#37327;&#21270;&#20219;&#24847;&#32500;&#24230;&#20004;&#20010;&#28857;&#38598;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#24341;&#20837;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;&#26469;&#37327;&#21270;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26631;&#20934;&#25345;&#20037;&#21516;&#35843;&#19982;&#22270;&#20687;&#25345;&#20037;&#21516;&#35843;&#30456;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#24449;&#24418;&#29366;&#21644;&#23427;&#20204;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#65306;&#65288;1&#65289;&#28151;&#21512;&#26465;&#24418;&#30721;&#65292;&#25429;&#25417;&#20219;&#24847;&#32500;&#24230;&#20004;&#20010;&#28857;&#38598;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;&#65288;&#28151;&#21512;&#65289;&#65307;&#65288;2&#65289;&#31616;&#21333;&#30340;&#24635;&#28151;&#21512;&#21644;&#24635;&#30334;&#20998;&#27604;&#28151;&#21512;&#32479;&#35745;&#37327;&#65292;&#20316;&#20026;&#19968;&#20010;&#21333;&#19968;&#25968;&#23383;&#26469;&#37327;&#21270;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65307;&#65288;3&#65289;&#19968;&#20010;&#29992;&#20110;&#25805;&#20316;&#19978;&#36848;&#24037;&#20855;&#30340;&#36719;&#20214;&#24037;&#20855;&#12290;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23558;&#35813;&#24037;&#20855;&#24212;&#29992;&#21040;&#19968;&#20010;&#28304;&#33258;&#26426;&#22120;&#23398;&#20064;&#30340;&#38382;&#39064;&#19978;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#21035;&#23884;&#20837;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25299;&#25169;&#28151;&#21512;&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#24449;&#20302;&#32500;&#21644;&#39640;&#32500;&#25968;&#25454;&#20132;&#20114;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#19982;&#25345;&#20037;&#21516;&#35843;&#30340;&#20856;&#22411;&#29992;&#27861;&#30456;&#27604;&#65292;&#36825;&#20010;&#26032;&#24037;&#20855;&#23545;&#20110;&#25299;&#25169;&#29305;&#24449;&#30340;&#20960;&#20309;&#20301;&#32622;&#26356;&#20026;&#25935;&#24863;&#65292;&#36825;&#36890;&#24120;&#26159;&#21487;&#21462;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15058v1 Announce Type: cross  Abstract: We combine standard persistent homology with image persistent homology to define a novel way of characterizing shapes and interactions between them. In particular, we introduce: (1) a mixup barcode, which captures geometric-topological interactions (mixup) between two point sets in arbitrary dimension; (2) simple summary statistics, total mixup and total percentage mixup, which quantify the complexity of the interactions as a single number; (3) a software tool for playing with the above.   As a proof of concept, we apply this tool to a problem arising from machine learning. In particular, we study the disentanglement in embeddings of different classes. The results suggest that topological mixup is a useful method for characterizing interactions for low and high-dimensional data. Compared to the typical usage of persistent homology, the new tool is sensitive to the geometric locations of the topological features, which is often desirabl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15055</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#35299;&#37322;&#19978;&#19979;&#25991;&#26597;&#25214;&#65306;&#25506;&#31350;&#27880;&#24847;&#21147;-MLP&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#21644;Multilayer Perceptron&#20013;&#19987;&#38376;&#39044;&#27979;&#29305;&#23450;token&#30340;"next-token"&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20419;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;LLM&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#21487;&#20197;&#38416;&#26126;&#28608;&#27963;&#26576;&#20123;next-token&#31070;&#32463;&#20803;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30830;&#23450;&#20102;&#35782;&#21035;&#19982;&#39044;&#27979;&#29305;&#23450;token&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#30340;attention heads&#65292;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#28608;&#27963;&#30456;&#20851;&#32852;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#36739;&#26089;&#30340;&#23618;&#20013;&#22987;&#32456;&#28608;&#27963;&#30456;&#21516;next-token&#31070;&#32463;&#20803;&#30340;attention heads&#12290;&#25506;&#32034;&#36825;&#20123;&#19981;&#21516;&#30340;&#28608;&#27963;&#27169;&#24335;&#25581;&#31034;&#20102;&#20026;&#19981;&#21516;&#35821;&#35328;&#19978;&#19979;&#25991;&#19987;&#38376;&#21270;&#30340;&#22836;&#19982;&#29983;&#25104;&#26576;&#20123;tokens&#30456;&#20851;&#32852;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#35299;&#37322;&#21644;&#25506;&#27979;&#23396;&#31435;&#30340;&#32452;&#20214;&#65292;&#20197;&#38416;&#26126;&#27880;&#24847;&#21147;&#22914;&#20309;&#20351;LLMs&#20013;&#30340;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15055v1 Announce Type: cross  Abstract: In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#26500;&#36896;&#30340;MI&#19979;&#38480;&#30340;&#36138;&#23146;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#27169;&#22411;&#20248;&#21270;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#33394;</title><link>https://arxiv.org/abs/2402.15053</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#30340;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15053
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#26500;&#36896;&#30340;MI&#19979;&#38480;&#30340;&#36138;&#23146;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#27169;&#22411;&#20248;&#21270;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#19968;&#20010;&#36739;&#22823;&#30340;&#20505;&#36873;&#27744;&#20013;&#36873;&#25321;$k$&#20010;&#23454;&#39564;&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#25152;&#36873;&#23376;&#38598;&#19982;&#22522;&#30784;&#21442;&#25968;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#12290;&#30001;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#22312;&#38750;&#32447;&#24615;/&#38750;&#39640;&#26031;&#35774;&#32622;&#20013;&#35780;&#20272;MI&#30340;&#22256;&#38590;&#24615;&#65292;&#25214;&#21040;&#30830;&#20999;&#35299;&#20915;&#26041;&#26696;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36890;&#36807;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#26500;&#36896;&#30340;&#26032;&#30340;&#35745;&#31639;&#24265;&#20215;&#30340;MI&#19979;&#38480;&#30340;&#36138;&#23146;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;&#20855;&#26377;&#38750;&#21152;&#24615;&#22122;&#22768;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#26368;&#20248;&#35774;&#35745;&#22312;&#20869;&#30340;&#21508;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38543;&#26426;&#36873;&#25321;&#31574;&#30053;&#12289;&#39640;&#26031;&#36924;&#36817;&#21644;&#23884;&#22871;&#33945;&#29305;&#21345;&#27931;&#65288;NMC&#65289;MI&#20272;&#31639;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15053v1 Announce Type: cross  Abstract: We study the problem of selecting $k$ experiments from a larger candidate pool, where the goal is to maximize mutual information (MI) between the selected subset and the underlying parameters. Finding the exact solution is to this combinatorial optimization problem is computationally costly, not only due to the complexity of the combinatorial search but also the difficulty of evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches based on new computationally inexpensive lower bounds for MI, constructed via log-Sobolev inequalities. We demonstrate that our method outperforms random selection strategies, Gaussian approximations, and nested Monte Carlo (NMC) estimators of MI in various settings, including optimal design for nonlinear models with non-additive noise.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#20154;&#33080;&#26631;&#35760;&#26816;&#27979;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#22411;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#36830;&#20307;&#26550;&#26500;&#35757;&#32451;&#26426;&#21046;&#21644;DCCA&#25439;&#22833;&#26469;&#23454;&#29616;&#23545;&#38754;&#37096;&#32467;&#26500;&#29702;&#35299;&#30340;&#25552;&#39640;</title><link>https://arxiv.org/abs/2402.15044</link><description>&lt;p&gt;
&#20154;&#33080;&#26631;&#35760;&#26816;&#27979;&#30340;&#20449;&#20219;&#28966;&#28857;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Fiducial Focus Augmentation for Facial Landmark Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#20154;&#33080;&#26631;&#35760;&#26816;&#27979;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#22411;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#36830;&#20307;&#26550;&#26500;&#35757;&#32451;&#26426;&#21046;&#21644;DCCA&#25439;&#22833;&#26469;&#23454;&#29616;&#23545;&#38754;&#37096;&#32467;&#26500;&#29702;&#35299;&#30340;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20154;&#33080;&#26631;&#35760;&#26816;&#27979;&#65288;FLD&#65289;&#20219;&#21153;&#30340;&#34920;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#26816;&#27979;&#26631;&#35760;&#65292;&#22914;&#22836;&#37096;&#23039;&#21183;&#21464;&#21270;&#12289;&#22840;&#24352;&#34920;&#24773;&#25110;&#19981;&#22343;&#21248;&#29031;&#26126;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;&#39640;&#24230;&#21464;&#21270;&#21644;&#26679;&#26412;&#19981;&#36275;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;FLD&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#22411;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#38754;&#37096;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#26032;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#36830;&#20307;&#26550;&#26500;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;DCCA&#65289;&#30340;&#25439;&#22833;&#26469;&#23454;&#29616;&#20174;&#36755;&#20837;&#22270;&#20687;&#30340;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#39640;&#32423;&#29305;&#24449;&#34920;&#31034;&#30340;&#38598;&#20307;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15044v1 Announce Type: cross  Abstract: Deep learning methods have led to significant improvements in the performance on the facial landmark detection (FLD) task. However, detecting landmarks in challenging settings, such as head pose changes, exaggerated expressions, or uneven illumination, continue to remain a challenge due to high variability and insufficient samples. This inadequacy can be attributed to the model's inability to effectively acquire appropriate facial structure information from the input images. To address this, we propose a novel image augmentation technique specifically designed for the FLD task to enhance the model's understanding of facial structures. To effectively utilize the newly proposed augmentation technique, we employ a Siamese architecture-based training mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to achieve collective learning of high-level feature representations from two different views of the input images. Furthe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.15043</link><description>&lt;p&gt;
KIEval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#35780;&#20272;&#34987;&#22840;&#22823;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#26088;&#22312;&#26816;&#27979;&#21463;&#27745;&#26579;&#30340;&#25991;&#26412;&#65292;&#20294;&#20391;&#37325;&#20110;&#37327;&#21270;&#27745;&#26579;&#31243;&#24230;&#32780;&#38750;&#20934;&#30830;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KIEval&#65292;&#36825;&#26159;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;LLM&#39537;&#21160;&#30340;&#8220;&#20132;&#20114;&#32773;&#8221;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25239;&#27745;&#26579;&#35780;&#20272;&#12290;&#20174;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#24120;&#35268;LLM&#22522;&#20934;&#38382;&#39064;&#24320;&#22987;&#65292;KIEval&#21033;&#29992;&#21160;&#24577;&#29983;&#25104;&#30340;&#12289;&#22810;&#36718;&#12289;&#20197;&#30693;&#35782;&#20026;&#37325;&#28857;&#30340;&#23545;&#35805;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#21542;&#20165;&#26159;&#22522;&#20934;&#31572;&#26696;&#30340;&#22238;&#24518;&#65292;&#36824;&#26159;&#34920;&#26126;&#20102;&#28145;&#20837;&#29702;&#35299;&#24182;&#33021;&#22312;&#26356;&#22797;&#26434;&#30340;&#23545;&#35805;&#20013;&#24212;&#29992;&#30693;&#35782;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19971;&#20010;&#39046;&#20808;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;KI
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32467;&#21512;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20174;&#23721;&#30707;&#34180;&#29255;&#22270;&#20687;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.15039</link><description>&lt;p&gt;
&#33258;&#21160;&#25551;&#36848;&#23721;&#30707;&#34180;&#29255;&#65306;&#19968;&#20010;Web&#24212;&#29992;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Descripci\'on autom\'atica de secciones delgadas de rocas: una aplicaci\'on Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32467;&#21512;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20174;&#23721;&#30707;&#34180;&#29255;&#22270;&#20687;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#21644;&#34920;&#24449;&#21508;&#31181;&#23721;&#30707;&#31867;&#22411;&#26159;&#22320;&#36136;&#23398;&#20197;&#21450;&#30719;&#19994;&#12289;&#30707;&#27833;&#12289;&#29615;&#22659;&#12289;&#24037;&#19994;&#21644;&#24314;&#31569;&#31561;&#39046;&#22495;&#30340;&#22522;&#26412;&#27963;&#21160;&#20043;&#19968;&#12290;&#20256;&#32479;&#19978;, &#20154;&#31867;&#19987;&#23478;&#36127;&#36131;&#20351;&#29992;&#21407;&#20301;&#37319;&#38598;&#30340;&#23721;&#30707;&#26679;&#26412;&#25110;&#22312;&#23454;&#39564;&#23460;&#20934;&#22791;&#30340;&#26679;&#26412;&#20998;&#26512;&#21644;&#35299;&#37322;&#26377;&#20851;&#31867;&#22411;&#12289;&#32452;&#25104;&#12289;&#36136;&#22320;&#12289;&#24418;&#29366;&#21644;&#20854;&#20182;&#23646;&#24615;&#30340;&#32454;&#33410;&#12290;&#32467;&#26524;&#22240;&#32463;&#39564;&#32780;&#20027;&#35266;, &#38500;&#20102;&#28040;&#32791;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#26412;&#25552;&#35758;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32467;&#21512;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20174;&#23721;&#30707;&#34180;&#29255;&#22270;&#20687;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#25551;&#36848;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#25991;&#26412;&#25551;&#36848;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23558;&#30001;EfficientNetB7&#25552;&#21462;&#30340;&#22270;&#20687;&#30456;&#20851;&#29305;&#24449;&#19982;&#29983;&#25104;&#30340;&#25991;&#26412;&#25551;&#36848;&#30456;&#20851;&#32852;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15039v1 Announce Type: cross  Abstract: The identification and characterization of various rock types is one of the fundamental activities for geology and related areas such as mining, petroleum, environment, industry and construction. Traditionally, a human specialist is responsible for analyzing and explaining details about the type, composition, texture, shape and other properties using rock samples collected in-situ or prepared in a laboratory. The results become subjective based on experience, in addition to consuming a large investment of time and effort. The present proposal uses artificial intelligence techniques combining computer vision and natural language processing to generate a textual and verbal description from a thin section image of rock. We build a dataset of images and their respective textual descriptions for the training of a model that associates the relevant features of the image extracted by EfficientNetB7 with the textual description generated by a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#20026;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#65292;&#36890;&#36807;&#35774;&#35745;&#30446;&#26631;&#26500;&#24314;&#30340;&#26799;&#24230;&#24341;&#23548;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#30340;&#23436;&#21892;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.15038</link><description>&lt;p&gt;
&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154; manipulator &#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Dynamics-Guided Diffusion Model for Robot Manipulator Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#20026;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#65292;&#36890;&#36807;&#35774;&#35745;&#30446;&#26631;&#26500;&#24314;&#30340;&#26799;&#24230;&#24341;&#23548;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#30340;&#23436;&#21892;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#12290;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#35757;&#32451;&#19981;&#21516;&#30340;&#35774;&#35745;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#20010;&#36328;&#20219;&#21153;&#20849;&#20139;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#32593;&#32476;&#12290;&#23545;&#20110;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#31216;&#20026;&#30446;&#26631;&#30456;&#20114;&#20316;&#29992;&#37197;&#32622;&#25991;&#20214;&#30340;&#20010;&#21035;&#36816;&#21160;&#30446;&#26631;&#65292;&#20854;&#20013;&#27599;&#20010;&#20010;&#21035;&#36816;&#21160;&#21487;&#20197;&#30001;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#24314;&#27169;&#12290;&#20174;&#30446;&#26631;&#21644;&#39044;&#27979;&#30340;&#30456;&#20114;&#20316;&#29992;&#37197;&#32622;&#25991;&#20214;&#26500;&#24314;&#30340;&#35774;&#35745;&#30446;&#26631;&#20026;&#20219;&#21153;&#30340;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#25552;&#20379;&#20102;&#26799;&#24230;&#24341;&#23548;&#12290;&#36825;&#20010;&#35774;&#35745;&#36807;&#31243;&#34987;&#25191;&#34892;&#20026;&#19968;&#31181;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#35774;&#35745;&#30446;&#26631;&#20316;&#20026;&#20998;&#31867;&#22120;&#24341;&#23548;&#12290;&#25105;&#20204;&#22312;&#21482;&#20351;&#29992;&#24320;&#29615;&#24179;&#34892;&#22841;&#29226;&#36816;&#21160;&#30340;&#26080;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#65292;&#22312;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15038v1 Announce Type: cross  Abstract: We present Dynamics-Guided Diffusion Model, a data-driven framework for generating manipulator geometry designs for a given manipulation task. Instead of training different design models for each task, our approach employs a learned dynamics network shared across tasks. For a new manipulation task, we first decompose it into a collection of individual motion targets which we call target interaction profile, where each individual motion can be modeled by the shared dynamics network. The design objective constructed from the target and predicted interaction profiles provides a gradient to guide the refinement of finger geometry for the task. This refinement process is executed as a classifier-guided diffusion process, where the design objective acts as the classifier guidance. We evaluate our framework on various manipulation tasks, under the sensor-less setting using only an open-loop parallel jaw motion. Our generated designs outperfor
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#20154;&#24212;&#35813;&#36890;&#36807;&#20272;&#35745;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#25512;&#26029;&#36890;&#36807;&#32451;&#20064;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#24182;&#23558;&#20854;&#25918;&#32622;&#22312;&#20219;&#21153;&#20998;&#37197;&#20013;&#65292;&#20197;&#36873;&#25321;&#35201;&#32451;&#20064;&#30340;&#25216;&#33021;&#26469;&#26368;&#22823;&#21270;&#26410;&#26469;&#20219;&#21153;&#25104;&#21151;&#30340;&#39044;&#26399;&#12290;</title><link>https://arxiv.org/abs/2402.15025</link><description>&lt;p&gt;
&#32451;&#20064;&#26041;&#33021;&#33268;&#23436;&#32654;&#65306;&#35268;&#21010;&#23398;&#20064;&#25216;&#33021;&#21442;&#25968;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Practice Makes Perfect: Planning to Learn Skill Parameter Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15025
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#24212;&#35813;&#36890;&#36807;&#20272;&#35745;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#25512;&#26029;&#36890;&#36807;&#32451;&#20064;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#24182;&#23558;&#20854;&#25918;&#32622;&#22312;&#20219;&#21153;&#20998;&#37197;&#20013;&#65292;&#20197;&#36873;&#25321;&#35201;&#32451;&#20064;&#30340;&#25216;&#33021;&#26469;&#26368;&#22823;&#21270;&#26410;&#26469;&#20219;&#21153;&#25104;&#21151;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#20915;&#31574;&#26041;&#27861;&#26159;&#23558;&#21442;&#25968;&#21270;&#25216;&#33021;&#24207;&#21015;&#36215;&#26469;&#65292;&#32771;&#34385;&#21040;&#26426;&#22120;&#20154;&#26368;&#21021;&#20250;&#37197;&#22791;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25216;&#33021;&#24211;&#12289;&#19968;&#20010;AI&#35268;&#21010;&#22120;&#29992;&#20110;&#26681;&#25454;&#30446;&#26631;&#23545;&#25216;&#33021;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#19988;&#20855;&#26377;&#29992;&#20110;&#36873;&#25321;&#25216;&#33021;&#21442;&#25968;&#30340;&#38750;&#24120;&#26222;&#36941;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#19968;&#26086;&#37096;&#32626;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#36890;&#36807;&#23558;&#20854;&#25216;&#33021;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#19987;&#38376;&#21270;&#21040;&#20854;&#29615;&#22659;&#20013;&#30340;&#29305;&#23450;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#26469;&#36805;&#36895;&#33258;&#20027;&#22320;&#23398;&#20064;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#36873;&#25321;&#35201;&#32451;&#20064;&#30340;&#25216;&#33021;&#20197;&#26368;&#22823;&#21270;&#26410;&#26469;&#20219;&#21153;&#25104;&#21151;&#30340;&#39044;&#26399;&#12290;&#25105;&#20204;&#25552;&#20986;&#26426;&#22120;&#20154;&#24212;&#35813;&#20272;&#35745;&#27599;&#20010;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#25512;&#26029;&#33021;&#21147;&#65288;&#21363;&#8220;&#36890;&#36807;&#32451;&#20064;&#33021;&#21147;&#20250;&#25552;&#21319;&#22810;&#23569;&#65311;&#8221;&#65289;&#65292;&#24182;&#23558;&#35813;&#25216;&#33021;&#32622;&#20110;&#20219;&#21153;&#20998;&#37197;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15025v1 Announce Type: cross  Abstract: One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together parameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills to practice to maximize expected future task success. We propose that the robot should estimate the competence of each skill, extrapolate the competence (asking: "how much would the competence improve through practice?"), and situate the skill in the task distribution throu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15020</link><description>&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#20581;&#22766;&#26463;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistically-sound beam search with masked language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#30340;&#26463;&#25628;&#32034;&#23384;&#22312;&#25361;&#25112;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24207;&#21015;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;readily available&#12290;&#28982;&#32780;&#65292;&#20272;&#31639;&#36825;&#26679;&#30340;&#20998;&#24067;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24212;&#29992;&#65292;&#21253;&#25324;&#34507;&#30333;&#24037;&#31243;&#21644;&#21476;&#20195;&#25991;&#26412;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20351;&#29992;MLMs&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20351;&#29992;&#26631;&#20934;&#26463;&#25628;&#32034;&#23545;MLMs&#25191;&#34892;&#25991;&#26412;&#22635;&#20805;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#12290;&#24403;&#36825;&#20123;&#26465;&#20214;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20462;&#25913;&#65292;&#32780;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#39044;&#26399;&#26465;&#20214;&#19979;&#23427;&#20248;&#20110;&#21069;&#36848;&#30340;&#26463;&#25628;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27604;&#36739;&#22810;&#20010;&#39046;&#22495;&#20013;&#20960;&#31181;&#20351;&#29992;MLMs&#36827;&#34892;&#22635;&#20805;&#30340;&#26041;&#27861;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#65288;CTS&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#20379;&#28304;&#22495;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#30417;&#30563;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#22495;&#22806;&#65288;OOD&#65289;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15019</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#21644;&#20869;&#23481;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#29992;&#20110;&#22495;&#22806;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15019
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#65288;CTS&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#20379;&#28304;&#22495;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#30417;&#30563;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#22495;&#22806;&#65288;OOD&#65289;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19978;&#65292;&#32780;&#19981;&#26159;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#21518;&#32773;&#26159;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#35201;&#27714;&#12290;&#28201;&#24230;&#32553;&#25918;&#65288;TS&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#22312;&#39046;&#22495;&#20869;&#29615;&#22659;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#21364;&#19981;&#26159;&#65292;&#22240;&#20026;&#20107;&#20808;&#24456;&#38590;&#33719;&#21462;&#26410;&#35265;&#39046;&#22495;&#30340;&#39564;&#35777;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#32553;&#25918;&#31574;&#30053;&#65292;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#65288;CTS&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#28304;&#22495;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#30417;&#30563;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;OOD&#26657;&#20934;&#24615;&#33021;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#35266;&#23519;&#21040;&#30340;&#21457;&#29616;&#65292;&#30001;&#20110;&#19981;&#19968;&#33268;&#30340;&#26679;&#26412;&#39044;&#27979;&#23548;&#33268;&#30340;&#36807;&#24230;&#33258;&#20449;&#26159;OOD&#26657;&#20934;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15019v1 Announce Type: cross  Abstract: Research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence stemming from inconsistent sample predictions is the main obstacle to OOD calibration, we propose to 
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#21487;&#33021;&#20250;&#23548;&#33268;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#20063;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15018</link><description>&lt;p&gt;
LLM&#23545;&#20840;&#29699;&#34920;&#31034;&#30340;&#24847;&#22806;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unintended Impacts of LLM Alignment on Global Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15018
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#21487;&#33021;&#20250;&#23548;&#33268;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#20063;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#38754;&#21521;&#29992;&#25143;&#30340;&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#20043;&#21069;&#65292;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#21508;&#31181;&#31243;&#24207;&#65288;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;&#30446;&#21069;&#23545;&#36825;&#20123;&#31243;&#24207;&#30340;&#35780;&#20272;&#20391;&#37325;&#20110;&#36981;&#24490;&#25351;&#23548;&#12289;&#25512;&#29702;&#21644;&#30495;&#23454;&#24615;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#24182;&#38750;&#26222;&#36941;&#65292;&#23545;&#29305;&#23450;&#20559;&#22909;&#38598;&#36827;&#34892;&#23545;&#40784;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#22806;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#19977;&#20010;&#20840;&#29699;&#34920;&#31034;&#32500;&#24230;&#65306;&#33521;&#35821;&#26041;&#35328;&#12289;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20840;&#29699;&#21508;&#22269;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#31243;&#24207;&#22312;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#20135;&#29983;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#40784;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23548;&#33268;&#36825;&#20123;&#24847;&#22806;&#24433;&#21709;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#20026;&#26356;&#20844;&#24179;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable 
&lt;/p&gt;</description></item><item><title>&#22810;&#20219;&#21153;&#24494;&#35843;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#36866;&#24212;&#38480;&#21046;&#26631;&#31614;&#25968;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#33021;&#22815;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15017</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15017
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24494;&#35843;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#36866;&#24212;&#38480;&#21046;&#26631;&#31614;&#25968;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#33021;&#22815;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26377;&#25928;&#22320;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25968;&#25454;&#26631;&#31614;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#19988;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#26368;&#36817;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;&#19968;&#31181;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#26159;&#65292;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#19968;&#31995;&#21015;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#20877;&#36866;&#24212;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;&#30340;&#29702;&#35770;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#30456;&#20851;&#20219;&#21153;&#38598;&#65292;&#36825;&#31181;&#22810;&#20219;&#21153;&#24494;&#35843;&#21487;&#20197;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#19982;&#30452;&#25509;&#36866;&#24212;&#30456;&#21516;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#25351;&#26631;&#37327;&#21270;&#20102;&#24494;&#35843;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15017v1 Announce Type: cross  Abstract: Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our 
&lt;/p&gt;</description></item><item><title>&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.15010</link><description>&lt;p&gt;
&#27861;&#35821;&#21307;&#29992;&#21475;&#32617;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#21270;&#26377;&#22810;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Important Is Tokenization in French Medical Masked Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15010
&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23376;&#35789;&#30340;&#26631;&#35760;&#21270;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#20854;&#25104;&#21151;&#30340;&#30830;&#20999;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#65292;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#12290;&#36825;&#22312;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#31649;&#29702;&#24418;&#24577;&#32032;&#32452;&#21512;&#30340;&#29305;&#23450;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
&lt;/p&gt;</description></item><item><title>opp/ai&#26694;&#26550;&#32467;&#21512;&#20102;zkML&#30340;&#38544;&#31169;&#21644;opML&#30340;&#25928;&#29575;&#65292;&#20026;&#21306;&#22359;&#38142;&#19978;&#30340;AI&#26381;&#21153;&#25552;&#20379;&#20102;&#24179;&#34913;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15006</link><description>&lt;p&gt;
opp/ai&#65306;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20048;&#35266;&#38544;&#31169;&#20445;&#25252;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
opp/ai: Optimistic Privacy-Preserving AI on Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15006
&lt;/p&gt;
&lt;p&gt;
opp/ai&#26694;&#26550;&#32467;&#21512;&#20102;zkML&#30340;&#38544;&#31169;&#21644;opML&#30340;&#25928;&#29575;&#65292;&#20026;&#21306;&#22359;&#38142;&#19978;&#30340;AI&#26381;&#21153;&#25552;&#20379;&#20102;&#24179;&#34913;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#34701;&#21512;&#27491;&#22312;&#37325;&#22609;&#25968;&#23383;&#19990;&#30028;&#65292;&#20026;&#21306;&#22359;&#38142;&#24179;&#21488;&#19978;&#25552;&#20379;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;AI&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;AI&#23545;&#21306;&#22359;&#38142;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#24341;&#21457;&#20102;&#38544;&#31169;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#37325;&#22823;&#25285;&#24551;&#12290;&#20048;&#35266;&#38544;&#31169;&#20445;&#25252;&#20154;&#24037;&#26234;&#33021;&#65288;opp/ai&#65289;&#26694;&#26550;&#34987;&#24341;&#20837;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24320;&#21019;&#24615;&#26041;&#26696;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#35813;&#26694;&#26550;&#23558;Zero-Knowledge Machine Learning&#65288;zkML&#65289;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#65292;&#23558;Optimistic Machine Learning&#65288;opML&#65289;&#29992;&#20110;&#25552;&#39640;&#25928;&#29575;&#65292;&#25171;&#36896;&#20102;&#38024;&#23545;&#21306;&#22359;&#38142;AI&#26381;&#21153;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;opp/ai&#26694;&#26550;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;zkML&#30340;&#38544;&#31169;&#21151;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15006v1 Announce Type: cross  Abstract: The convergence of Artificial Intelligence (AI) and blockchain technology is reshaping the digital world, offering decentralized, secure, and efficient AI services on blockchain platforms. Despite the promise, the high computational demands of AI on blockchain raise significant privacy and efficiency concerns. The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a pioneering solution to these issues, striking a balance between privacy protection and computational efficiency. The framework integrates Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine Learning (opML) for efficiency, creating a hybrid model tailored for blockchain AI services. This study presents the opp/ai framework, delves into the privacy features of zkML, and assesses the framework's performance and adaptability across different scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#24343;&#33713;&#26126;&#27721;&#22982;&#24515;&#33039;&#30149;&#25968;&#25454;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20843;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;/&#27979;&#35797;&#22330;&#26223;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21457;&#29616;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#35757;&#32451;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2402.15005</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#30340;&#27604;&#36739;&#21450;&#20854;&#22312;&#24343;&#33713;&#26126;&#27721;&#22982;&#24515;&#33039;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Comparison of Machine Learning Classification Algorithms and Application to the Framingham Heart Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#24343;&#33713;&#26126;&#27721;&#22982;&#24515;&#33039;&#30149;&#25968;&#25454;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20843;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;/&#27979;&#35797;&#22330;&#26223;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21457;&#29616;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#35757;&#32451;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#20250;&#25918;&#22823;&#31038;&#20250;&#19981;&#20844;&#27491;&#21644;&#20581;&#24247;&#19981;&#24179;&#31561;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#19968;&#20123;&#19968;&#33324;&#21270;&#38556;&#30861;&#65292;&#20351;&#29992;&#24343;&#33713;&#26126;&#27721;&#22982;&#20896;&#24515;&#30149;&#25968;&#25454;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#36873;&#25321;&#27010;&#29575;&#25130;&#26029;&#20197;&#23558;&#22238;&#24402;&#27169;&#22411;&#36716;&#25442;&#20026;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20843;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#22312;&#22235;&#31181;&#35757;&#32451;/&#27979;&#35797;&#22330;&#26223;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#30340;&#25277;&#26679;&#20998;&#24067;&#65292;&#20197;&#27979;&#35797;&#23427;&#20204;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;&#21644;&#24310;&#32493;&#20559;&#35265;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#35757;&#32451;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15005v1 Announce Type: new  Abstract: The use of machine learning algorithms in healthcare can amplify social injustices and health inequities. While the exacerbation of biases can occur and compound during the problem selection, data collection, and outcome definition, this research pertains to some generalizability impediments that occur during the development and the post-deployment of machine learning classification algorithms. Using the Framingham coronary heart disease data as a case study, we show how to effectively select a probability cutoff to convert a regression model for a dichotomous variable into a classifier. We then compare the sampling distribution of the predictive performance of eight machine learning classification algorithms under four training/testing scenarios to test their generalizability and their potential to perpetuate biases. We show that both the Extreme Gradient Boosting, and Support Vector Machine are flawed when trained on an unbalanced data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.15000</link><description>&lt;p&gt;
&#21010;&#20998;&#36824;&#26159;&#24449;&#26381;&#65311;&#20320;&#24212;&#35813;&#25552;&#28860;LLM&#30340;&#21738;&#19968;&#37096;&#20998;&#65311;
&lt;/p&gt;
&lt;p&gt;
Divide-or-Conquer? Which Part Should You Distill Your LLM?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34987;&#40723;&#21169;&#20808;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#30340;&#23376;&#20219;&#21153;&#26102;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#31574;&#30053;&#65292;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#35813;&#31574;&#30053;&#33021;&#22815;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#19982;&#35299;&#20915;&#38382;&#39064;&#30456;&#27604;&#65292;&#20998;&#35299;&#38454;&#27573;&#26356;&#23481;&#26131;&#34987;&#25552;&#28860;&#20026;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#21069;&#32773;&#21482;&#38656;&#35201;&#23398;&#20064;&#19968;&#33324;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#28860;&#36825;&#20004;&#31181;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#25512;&#29702;&#32467;&#26524;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#21487;&#20197;&#25552;&#28860;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#65292;&#24182;&#21516;&#26102;&#22312;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#35201;&#25552;&#28860;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#23601;&#26356;&#22256;&#38590;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15000v1 Announce Type: new  Abstract: Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14992</link><description>&lt;p&gt;
&#23567;&#22411;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
tinyBenchmarks: evaluating LLMs with fewer examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#23548;&#33268;&#21019;&#24314;&#20102;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24443;&#24213;&#27979;&#35797;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#31034;&#20363;&#65292;&#20351;&#24471;&#35780;&#20272;LLMs&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;MMLU&#19978;&#30340;&#24615;&#33021;&#65288;&#19968;&#20010;&#21253;&#21547;14K&#20010;&#31034;&#20363;&#30340;&#27969;&#34892;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#21482;&#38656;&#35201;&#22312;100&#20010;&#31934;&#24515;&#25361;&#36873;&#30340;&#31034;&#20363;&#19978;&#35780;&#20272;&#36825;&#20010;LLMs&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#24037;&#20855;&#21644;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22411;&#29256;&#26412;&#65306;Open LLM Leaderboard&#12289;MMLU&#12289;HELM&#21644;AlpacaEval 2.0&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#24494;&#22411;&#22522;&#20934;&#27979;&#35797;&#36275;&#20197;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#37325;&#29616;&#21407;&#22987;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#32972;&#26223;&#24773;&#22659;&#20013;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#39564;&#35777;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25429;&#25417;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14991</link><description>&lt;p&gt;
&#37327;&#23376;&#29702;&#35770;&#19982;&#24773;&#22659;&#26368;&#20248;&#36755;&#36816;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Theory and Application of Contextual Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#32972;&#26223;&#24773;&#22659;&#20013;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#39564;&#35777;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25429;&#25417;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;Optimal Transport&#65292;OT&#65289;&#25512;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22312;&#27979;&#37327;&#25968;&#25454;&#65288;$\mu$&#65292;$\nu$&#65289;&#19982;&#19978;&#19979;&#25991;&#21464;&#37327; $p_i$ &#32806;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#21162;&#21147;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21487;&#33021;&#30475;&#19981;&#35265;&#30340;&#19978;&#19979;&#25991;&#21442;&#25968;&#21270;&#30340;&#20840;&#23616;&#36755;&#36816;&#26144;&#23556;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;Brenier&#23450;&#29702;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#21452;&#38543;&#26426;&#30697;&#38453;&#21644;&#37193;&#31639;&#31526;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#65292;&#20174;&#32780;&#25214;&#21040;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#37327;&#23376;&#35745;&#31639;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#39564;&#35777;&#65292;&#39044;&#27979;&#36890;&#36807;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#20316;&#20026;&#32972;&#26223;&#24773;&#22659;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#65292;&#29978;&#33267;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14991v1 Announce Type: new  Abstract: Optimal Transport (OT) has fueled machine learning (ML) applications across many domains. In cases where paired data measurements ($\mu$, $\nu$) are coupled to a context variable $p_i$ , one may aspire to learn a global transportation map that can be parameterized through a potentially unseen con-text. Existing approaches utilize Neural OT and largely rely on Brenier's theorem. Here, we propose a first-of-its-kind quantum computing formulation for amortized optimization of contextualized transportation plans. We exploit a direct link between doubly stochastic matrices and unitary operators thus finding a natural connection between OT and quantum computation. We verify our method on synthetic and real data, by predicting variations in cell type distributions parameterized through drug dosage as context. Our comparisons to several baselines reveal that our method can capture dose-induced variations in cell distributions, even to some exten
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#21487;&#39564;&#35777;&#23398;&#20064;&#20174;&#22522;&#26412;&#38598;&#25104;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32423;&#25552;&#21319;&#26641;&#38598;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20266;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#39564;&#35777;&#40065;&#26834;&#24615;&#65292;&#23545;&#22522;&#20110;$L_p$-&#33539;&#25968;&#30340;&#25915;&#20987;&#32773;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14988</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#30340;&#25552;&#21319;&#26641;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Verifiable Boosted Tree Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#21487;&#39564;&#35777;&#23398;&#20064;&#20174;&#22522;&#26412;&#38598;&#25104;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32423;&#25552;&#21319;&#26641;&#38598;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20266;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#39564;&#35777;&#40065;&#26834;&#24615;&#65292;&#23545;&#22522;&#20110;$L_p$-&#33539;&#25968;&#30340;&#25915;&#20987;&#32773;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#23398;&#20064;&#20513;&#23548;&#35757;&#32451;&#26131;&#20110;&#36827;&#34892;&#39640;&#25928;&#23433;&#20840;&#39564;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#23450;&#31867;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#21363;&#31216;&#20026;&#22823;&#24191;&#27867;&#38598;&#25104;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#38024;&#23545;&#20219;&#20309;&#22522;&#20110;&#33539;&#25968;&#30340;&#25915;&#20987;&#32773;&#36827;&#34892;&#40065;&#26834;&#24615;&#39564;&#35777;&#12290;&#26412;&#30740;&#31350;&#23558;&#21487;&#39564;&#35777;&#23398;&#20064;&#20174;&#22522;&#26412;&#38598;&#25104;&#26041;&#27861;&#65288;&#21363;&#30828;&#22810;&#25968;&#25237;&#31080;&#65289;&#25193;&#23637;&#21040;&#39640;&#32423;&#25552;&#21319;&#26641;&#38598;&#25104;&#65292;&#27604;&#22914;&#37027;&#20123;&#20351;&#29992;XGBoost&#25110;LightGBM&#35757;&#32451;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27491;&#24335;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32771;&#34385;&#22522;&#20110;$L_\infty$-&#33539;&#25968;&#30340;&#25915;&#20987;&#32773;&#26102;&#65292;&#40065;&#26834;&#24615;&#39564;&#35777;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23454;&#29616;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#33539;&#25968;&#30340;&#25915;&#20987;&#32773;&#26469;&#35828;&#20173;&#28982;&#26159;NP&#38590;&#30340;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20266;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#39564;&#35777;&#38024;&#23545;&#22522;&#20110;$L_p$-&#33539;&#25968;&#30340;&#25915;&#20987;&#32773;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;$p \in \mathbb{N} \cup \{0\}$&#65292;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14988v1 Announce Type: new  Abstract: Verifiable learning advocates for training machine learning models amenable to efficient security verification. Prior research demonstrated that specific classes of decision tree ensembles -- called large-spread ensembles -- allow for robustness verification in polynomial time against any norm-based attacker. This study expands prior work on verifiable learning from basic ensemble methods (i.e., hard majority voting) to advanced boosted tree ensembles, such as those trained using XGBoost or LightGBM. Our formal results indicate that robustness verification is achievable in polynomial time when considering attackers based on the $L_\infty$-norm, but remains NP-hard for other norm-based attackers. Nevertheless, we present a pseudo-polynomial time algorithm to verify robustness against attackers based on the $L_p$-norm for any $p \in \mathbb{N} \cup \{0\}$, which in practice grants excellent performance. Our experimental evaluation shows th
&lt;/p&gt;</description></item><item><title>&#22312;&#25968;&#25454;&#26159;&#33391;&#22909;&#25351;&#23450;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#19982;&#24179;&#26041;&#25439;&#22833;&#30340;&#24615;&#33021;&#65292;&#24403;&#31867;&#26159;&#21487;&#20174; iid &#25968;&#25454;&#20013;&#23398;&#20064;&#26102;&#65292;ERM&#33021;&#22815;&#23454;&#29616;&#27425;&#32447;&#24615;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.14987</link><description>&lt;p&gt;
&#22312;&#24179;&#28369;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Performance of Empirical Risk Minimization with Smoothed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14987
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#26159;&#33391;&#22909;&#25351;&#23450;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#19982;&#24179;&#26041;&#25439;&#22833;&#30340;&#24615;&#33021;&#65292;&#24403;&#31867;&#26159;&#21487;&#20174; iid &#25968;&#25454;&#20013;&#23398;&#20064;&#26102;&#65292;ERM&#33021;&#22815;&#23454;&#29616;&#27425;&#32447;&#24615;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#24320;&#22312;&#24207;&#36143;&#20915;&#31574;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#22256;&#38590;&#32467;&#26524;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#24179;&#28369;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#20854;&#20013;&#20551;&#35774;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#20998;&#24067;&#22312;&#32473;&#23450;&#21382;&#21490;&#26465;&#20214;&#19979;&#30456;&#23545;&#20110;&#22522;&#30784;&#24230;&#37327;&#20855;&#26377;&#26377;&#30028;&#30340;&#20284;&#28982;&#27604;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#35777;&#26126;&#20102;&#24179;&#28369;&#24615;&#30340;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#20551;&#35774;&#22522;&#30784;&#24230;&#37327;&#23545;&#23398;&#20064;&#32773;&#26159;&#24050;&#30693;&#30340;&#65292;&#35201;&#20040;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#20165;&#36866;&#29992;&#20110;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#65292;&#21363;&#22522;&#30784;&#24230;&#37327;&#23545;&#23398;&#20064;&#32773;&#26159;\emph{&#26410;&#30693;}&#30340;&#24773;&#20917;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#25968;&#25454;&#26126;&#30830;&#23450;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#25968;&#25454;&#26159;&#33391;&#22909;&#25351;&#23450;&#26102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#19982;&#24179;&#26041;&#25439;&#22833;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#21482;&#35201;&#31867;&#26159;&#21487;&#20174;iid&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#65292;ERM&#23601;&#33021;&#22815;&#23454;&#29616;&#27425;&#32447;&#24615;&#35823;&#24046;&#65307;&#29305;&#21035;&#26159;&#65292;&#24403;&#25968;&#25454;&#26159;iid&#26102;&#65292;ERM&#23454;&#29616;&#30340;&#38169;&#35823;&#23610;&#24230;&#20026;$\tilde O(
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14987v1 Announce Type: cross  Abstract: In order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history. While previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases. This work investigates the more general setting where the base measure is \emph{unknown} to the learner, focusing in particular on the performance of Empirical Risk Minimization (ERM) with square loss when the data are well-specified and smooth. We show that in this setting, ERM is able to achieve sublinear error whenever a class is learnable with iid data; in particular, ERM achieves error scaling as $\tilde O(
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#36328;&#22810;&#20010;&#20445;&#38505;&#34892;&#19994;&#25968;&#25454;&#38598;&#23398;&#20064;&#21333;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36180;&#20607;&#25439;&#22833;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14983</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#20316;&#20449;&#24687;&#20849;&#20139;--&#20197;&#20445;&#38505;&#34892;&#19994;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Privacy-Enhancing Collaborative Information Sharing through Federated Learning -- A Case of the Insurance Industry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14983
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#36328;&#22810;&#20010;&#20445;&#38505;&#34892;&#19994;&#25968;&#25454;&#38598;&#23398;&#20064;&#21333;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36180;&#20607;&#25439;&#22833;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#23637;&#31034;&#20102;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20445;&#38505;&#34892;&#19994;&#25968;&#25454;&#38598;&#20043;&#38388;&#23398;&#20064;&#21333;&#19968;&#27169;&#22411;&#32780;&#26080;&#38656;&#23558;&#25968;&#25454;&#38598;&#20174;&#19968;&#23478;&#20844;&#21496;&#20998;&#20139;&#32473;&#21478;&#19968;&#23478;&#20844;&#21496;&#65292;&#20197;&#25913;&#36827;&#36180;&#20607;&#25439;&#22833;&#24314;&#27169;&#30340;&#22909;&#22788;&#12290;FL&#30340;&#24212;&#29992;&#35299;&#20915;&#20102;&#20004;&#20010;&#26368;&#32039;&#36843;&#30340;&#38382;&#39064;&#65306;&#25968;&#25454;&#37327;&#26377;&#38480;&#21644;&#25968;&#25454;&#31181;&#31867;&#32321;&#22810;&#65292;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#38544;&#31169;&#39038;&#34385;&#12289;&#36180;&#20607;&#20107;&#20214;&#30340;&#32597;&#35265;&#24615;&#12289;&#32570;&#20047;&#20449;&#24687;&#24615;&#35780;&#32423;&#22240;&#32032;&#31561;&#24341;&#36215;&#30340;&#12290;&#22312;&#27599;&#19968;&#36718;FL&#20013;&#65292;&#21512;&#20316;&#32773;&#20351;&#29992;&#20182;&#20204;&#30340;&#26412;&#22320;&#31169;&#26377;&#25968;&#25454;&#35745;&#31639;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#36825;&#20123;&#35265;&#35299;&#34987;&#32467;&#21512;&#36215;&#26469;&#26356;&#26032;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#31181;&#35265;&#35299;&#30340;&#32858;&#21512;&#30456;&#23545;&#20110;&#22312;&#27599;&#20010;&#21512;&#20316;&#32773;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22686;&#21152;&#23545;&#36180;&#20607;&#25439;&#22833;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#21327;&#20316;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#31163;&#24320;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14983v1 Announce Type: new  Abstract: The report demonstrates the benefits (in terms of improved claims loss modeling) of harnessing the value of Federated Learning (FL) to learn a single model across multiple insurance industry datasets without requiring the datasets themselves to be shared from one company to another. The application of FL addresses two of the most pressing concerns: limited data volume and data variety, which are caused by privacy concerns, the rarity of claim events, the lack of informative rating factors, etc.. During each round of FL, collaborators compute improvements on the model using their local private data, and these insights are combined to update a global model. Such aggregation of insights allows for an increase to the effectiveness in forecasting claims losses compared to models individually trained at each collaborator. Critically, this approach enables machine learning collaboration without the need for raw data to leave the compute infrast
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.14982</link><description>&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#22312;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#23637;&#29616;&#20986;&#19981;&#21516;&#27169;&#24335;&#65306;&#21021;&#27493;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14982
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#24182;&#27809;&#26377;&#26174;&#31034;&#20986;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#20043;&#38388;&#30340;&#28165;&#26224;&#19981;&#21516;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22823;&#33041;&#27963;&#21160;&#65292;&#36890;&#36807; EEG &#27979;&#37327;&#65292;&#22312;&#20010;&#20307;&#25509;&#35302;&#34394;&#20551;&#19982;&#30495;&#23454;&#38899;&#39057;&#26102;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#36825;&#20123;&#21021;&#27493;&#35777;&#25454;&#20026;&#26410;&#26469;&#22312;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14982v1 Announce Type: cross  Abstract: In this paper we study the variations in human brain activity when listening to real and fake audio. Our preliminary results suggest that the representations learned by a state-of-the-art deepfake audio detection algorithm, do not exhibit clear distinct patterns between real and fake audio. In contrast, human brain activity, as measured by EEG, displays distinct patterns when individuals are exposed to fake versus real audio. This preliminary evidence enables future research directions in areas such as deepfake audio detection.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#36873;&#25321;&#23545;&#35757;&#32451;&#36951;&#20256;&#25968;&#25454;&#38598;&#19978;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24322;&#24120;&#20540;&#21644;&#20542;&#26012;&#20250;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14980</link><description>&lt;p&gt;
&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#12289;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#36951;&#20256;&#25968;&#25454;&#19978;&#25552;&#39640;&#20998;&#31867;&#21644;&#22238;&#24402;&#24615;&#33021;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Data Preprocessing Methods, Feature Selection Techniques and Machine Learning Models for Improved Classification and Regression Performance on Imbalanced Genetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14980
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#36873;&#25321;&#23545;&#35757;&#32451;&#36951;&#20256;&#25968;&#25454;&#38598;&#19978;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24322;&#24120;&#20540;&#21644;&#20542;&#26012;&#20250;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#27979;&#24207;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#22823;&#37327;&#22522;&#22240;&#32452;&#25968;&#25454;&#30340;&#25910;&#38598;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#26377;&#20852;&#36259;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#22522;&#22240;&#31361;&#21464;&#30340;&#33268;&#30149;&#24615;&#25110;&#20020;&#24202;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36951;&#20256;&#25968;&#25454;&#38598;&#21253;&#21547;&#19981;&#24179;&#34913;&#30340;&#30446;&#26631;&#21464;&#37327;&#65292;&#36825;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#25361;&#25112;&#65306;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#35266;&#23519;&#32467;&#26524;&#20542;&#26012;/&#19981;&#24179;&#34913;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#36951;&#20256;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#39640;&#22522;&#25968;&#21644;&#20542;&#26012;&#30340;&#39044;&#27979;&#21464;&#37327;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#30740;&#31350;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#21644;&#27169;&#22411;&#36873;&#25321;&#23545;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;5&#25240;&#20132;&#21449;&#39564;&#35777;&#27979;&#37327;&#24615;&#33021;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#32452;&#21512;&#19979;&#30340;&#24179;&#22343;r&#24179;&#26041;&#21644;&#20934;&#30830;&#29575;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#27979;&#21464;&#37327;&#25110;&#30446;&#26631;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#20540;/&#20542;&#26012;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14980v1 Announce Type: cross  Abstract: Rapid advancements in genome sequencing have led to the collection of vast amounts of genomics data. Researchers may be interested in using machine learning models on such data to predict the pathogenicity or clinical significance of a genetic mutation. However, many genetic datasets contain imbalanced target variables that pose challenges to machine learning models: observations are skewed/imbalanced in regression tasks or class-imbalanced in classification tasks. Genetic datasets are also often high-cardinal and contain skewed predictor variables, which poses further challenges. We aimed to investigate the effects of data preprocessing, feature selection techniques, and model selection on the performance of models trained on these datasets. We measured performance with 5-fold cross-validation and compared averaged r-squared and accuracy metrics across different combinations of techniques. We found that outliers/skew in predictor or t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.14979</link><description>&lt;p&gt;
&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#26159;&#19968;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Models for Human Preferences is a Causal Inference Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#21830;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#25991;&#26412;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#20174;&#30452;&#25509;&#32467;&#26524;&#25968;&#25454;&#38598;&#20013;&#38024;&#23545;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#30001;&#19968;&#27573;&#25991;&#26412;&#21644;&#19968;&#20010;&#34913;&#37327;&#35835;&#32773;&#21709;&#24212;&#30340;&#30456;&#20851;&#25968;&#20540;&#32467;&#26524;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#24212;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#27491;&#30830;&#23398;&#20064;&#25991;&#26412;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27491;&#24335;&#21270;&#20102;&#36825;&#20010;&#22240;&#26524;&#35821;&#35328;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;--&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;(CPO)--&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26080;&#20559;&#26367;&#20195;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#30340;CPO(DR-CPO)&#25193;&#23637;CPO&#65292;&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26126;&#26174;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante
&lt;/p&gt;</description></item><item><title>Mudjacking&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26041;&#27861;&#26469;&#20462;&#34917;&#22522;&#30784;&#27169;&#22411;&#20197;&#28040;&#38500;&#21518;&#38376;&#28431;&#27934;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21644;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14977</link><description>&lt;p&gt;
Mudjacking&#65306;&#20462;&#34917;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14977
&lt;/p&gt;
&lt;p&gt;
Mudjacking&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26041;&#27861;&#26469;&#20462;&#34917;&#22522;&#30784;&#27169;&#22411;&#20197;&#28040;&#38500;&#21518;&#38376;&#28431;&#27934;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21644;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#25903;&#26609;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#29992;&#20316;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#26500;&#24314;&#21508;&#31181;&#19979;&#28216;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#36973;&#21463;&#21518;&#38376;&#25915;&#20987;&#65292;&#32780;&#21518;&#38376;&#22522;&#30784;&#27169;&#22411;&#26159;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#21333;&#28857;&#25925;&#38556;&#65292;&#20363;&#22914;&#65292;&#22810;&#20010;&#19979;&#28216;&#20998;&#31867;&#22120;&#21516;&#26102;&#32487;&#25215;&#21518;&#38376;&#28431;&#27934;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mudjacking&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20462;&#34917;&#22522;&#30784;&#27169;&#22411;&#20197;&#28040;&#38500;&#21518;&#38376;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#26086;&#37096;&#32626;&#20102;&#24102;&#26377;&#21518;&#38376;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#26816;&#27979;&#21040;&#23884;&#20837;&#35823;&#20998;&#31867;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#65292;Mudjacking&#20250;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#20197;&#28040;&#38500;&#21518;&#38376;&#12290;&#25105;&#20204;&#23558;&#20462;&#34917;&#22522;&#30784;&#27169;&#22411;&#24418;&#24335;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;Mudjacking&#65292;&#20351;&#29992;&#20102;&#21313;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14977v1 Announce Type: cross  Abstract: Foundation model has become the backbone of the AI ecosystem. In particular, a foundation model can be used as a general-purpose feature extractor to build various downstream classifiers. However, foundation models are vulnerable to backdoor attacks and a backdoored foundation model is a single-point-of-failure of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor vulnerabilities simultaneously. In this work, we propose Mudjacking, the first method to patch foundation models to remove backdoors. Specifically, given a misclassified trigger-embedded input detected after a backdoored foundation model is deployed, Mudjacking adjusts the parameters of the foundation model to remove the backdoor. We formulate patching a foundation model as an optimization problem and propose a gradient descent based method to solve it. We evaluate Mudjacking on both vision and language foundation models, eleven benchmark datasets, f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#22522;&#30784;&#28508;&#31354;&#38388;&#20869;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23450;&#24615;&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#26174;&#31034;&#20102;&#23578;&#26410;&#35299;&#20915;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14976</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#22522;&#30784;&#28508;&#31354;&#38388;&#20869;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation within Deep Foundation Latent Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#22522;&#30784;&#28508;&#31354;&#38388;&#20869;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23450;&#24615;&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#26174;&#31034;&#20102;&#23578;&#26410;&#35299;&#20915;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;ViT&#25110;Dino-V2&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#38656;&#25110;&#24456;&#23569;&#24494;&#35843;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#32452;&#21407;&#22411;&#32593;&#32476;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22522;&#30784;&#27169;&#22411;&#22312;&#26080;&#38656;&#22312;&#28304;&#22495;&#25110;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#20197;&#21450;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#23450;&#24615;&#35299;&#37322;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24314;&#35758;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#26377;&#24453;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14976v1 Announce Type: cross  Abstract: The vision transformer-based foundation models, such as ViT or Dino-V2, are aimed at solving problems with little or no finetuning of features. Using a setting of prototypical networks, we analyse to what extent such foundation models can solve unsupervised domain adaptation without finetuning over the source or target domain. Through quantitative analysis, as well as qualitative interpretations of decision making, we demonstrate that the suggested method can improve upon existing baselines, as well as showcase the limitations of such approach yet to be solved.
&lt;/p&gt;</description></item><item><title>&#21457;&#23637;&#20102;&#19968;&#20010;&#20351;&#29992;&#31354;&#38388;&#38598;&#21512;&#26694;&#26550;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#28857;&#30340;&#25490;&#21015;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#21306;&#20998;&#20004;&#20010;&#31867;&#21035;&#65292;&#23545;&#20110;&#32959;&#30244;&#23398;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2402.14974</link><description>&lt;p&gt;
&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23454;&#29616;&#31354;&#38388;&#36879;&#26126;&#30340;AI&#20998;&#31867;&#65306;MxIF&#32959;&#30244;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14974
&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20102;&#19968;&#20010;&#20351;&#29992;&#31354;&#38388;&#38598;&#21512;&#26694;&#26550;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#28857;&#30340;&#25490;&#21015;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#21306;&#20998;&#20004;&#20010;&#31867;&#21035;&#65292;&#23545;&#20110;&#32959;&#30244;&#23398;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26469;&#33258;&#19981;&#21516;&#22320;&#28857;&#31867;&#22411;&#30340;&#22810;&#31867;&#21035;&#28857;&#38598;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#31354;&#38388;&#36879;&#26126;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#28857;&#30340;&#25490;&#21015;&#21306;&#20998;&#20004;&#20010;&#31867;&#21035;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#27604;&#22914;&#32959;&#30244;&#23398;&#65292;&#29992;&#20110;&#20998;&#26512;&#20813;&#30123;-&#32959;&#30244;&#20851;&#31995;&#21644;&#35774;&#35745;&#26032;&#30340;&#20813;&#30123;&#27835;&#30103;&#26041;&#27861;&#12290;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#32771;&#34385;&#31354;&#38388;&#21464;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38656;&#27714;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#25216;&#26415;&#35201;&#27714;&#23494;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25110;&#32773;&#22312;&#22788;&#29702;&#21333;&#20010;&#22320;&#28857;&#31867;&#22411;&#20869;&#30340;&#26174;&#33879;&#31354;&#38388;&#21464;&#24322;&#24615;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26041;&#27861;&#27809;&#26377;&#35774;&#35745;&#29992;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#28857;&#38598;&#12290;&#29616;&#26377;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;DNN&#26041;&#27861;&#23616;&#38480;&#20110;&#19968;&#20992;&#20999;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31354;&#38388;&#38598;&#21512;&#26694;&#26550;&#65292;&#26126;&#30830;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#36317;&#31163;&#23398;&#20064;&#29575;&#21644;&#31354;&#38388;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14974v1 Announce Type: cross  Abstract: Given multi-category point sets from different place-types, our goal is to develop a spatially-lucid classifier that can distinguish between two classes based on the arrangements of their points. This problem is important for many applications, such as oncology, for analyzing immune-tumor relationships and designing new immunotherapies. It is challenging due to spatial variability and interpretability needs. Previously proposed techniques require dense training data or have limited ability to handle significant spatial variability within a single place-type. Most importantly, these deep neural network (DNN) approaches are not designed to work in non-Euclidean space, particularly point sets. Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches. We explore a spatial ensemble framework that explicitly uses different training strategies, including weighted-distance learning rate and spatial domain adaptation, on v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20809;&#28369;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#65288;SATL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#38454;&#27573;&#22343;&#37319;&#29992;&#39640;&#26031;&#26680;&#65292;&#20351;&#20272;&#35745;&#22120;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;/&#28304;&#21450;&#20854;&#20559;&#31227;&#20989;&#25968;&#30340;&#26410;&#30693;&#20809;&#28369;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14966</link><description>&lt;p&gt;
&#20809;&#28369;&#33258;&#36866;&#24212;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smoothness Adaptive Hypothesis Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20809;&#28369;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#65288;SATL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#38454;&#27573;&#22343;&#37319;&#29992;&#39640;&#26031;&#26680;&#65292;&#20351;&#20272;&#35745;&#22120;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;/&#28304;&#21450;&#20854;&#20559;&#31227;&#20989;&#25968;&#30340;&#26410;&#30693;&#20809;&#28369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20110;&#26680;&#30340;&#20004;&#38454;&#27573;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#22312;&#19981;&#21516;&#38454;&#27573;&#22343;&#37319;&#29992;&#30456;&#21516;&#30340;&#26680;&#27491;&#21017;&#21270;&#65292;&#24182;&#20381;&#36182;&#20110;&#20989;&#25968;&#30340;&#24050;&#30693;&#20809;&#28369;&#24615;&#26469;&#23454;&#29616;&#26368;&#20248;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#26410;&#33021;&#36866;&#24212;&#30446;&#26631;/&#28304;&#21450;&#20854;&#20559;&#31227;&#20043;&#38388;&#30340;&#21464;&#21270;&#21644;&#26410;&#30693;&#20809;&#28369;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#20809;&#28369;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#65288;SATL&#65289;&#65292;&#19968;&#20010;&#22522;&#20110;&#20004;&#38454;&#27573;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#30340;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#22312;&#30446;&#26631;&#19987;&#29992;KRR&#23398;&#20064;&#20013;&#37319;&#29992;&#38169;&#35823;&#25351;&#23450;&#30340;&#22266;&#23450;&#24102;&#23485;&#39640;&#26031;&#26680;&#21487;&#20197;&#23454;&#29616;&#26497;&#23567;&#21270;&#26368;&#20248;&#24615;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;Sobolev&#20809;&#28369;&#24615;&#30340;&#33258;&#36866;&#24212;&#36807;&#31243;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;SATL&#22312;&#20004;&#38454;&#27573;&#22343;&#37319;&#29992;&#39640;&#26031;&#26680;&#65292;&#20197;&#20351;&#20272;&#35745;&#37327;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;/&#28304;&#21450;&#20854;&#20559;&#31227;&#20989;&#25968;&#30340;&#26410;&#30693;&#20809;&#28369;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#23398;&#20064;&#38382;&#39064;&#22312;&#36807;&#37327;&#39118;&#38505;&#20013;&#30340;&#26497;&#23567;&#20540;&#19979;&#38480;&#65292;&#24182;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14966v1 Announce Type: cross  Abstract: Many existing two-phase kernel-based hypothesis transfer learning algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. Therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset in practice. In this paper, we address these problems by proposing Smoothness Adaptive Transfer Learning (SATL), a two-phase kernel ridge regression(KRR)-based algorithm. We first prove that employing the misspecified fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax optimality and derive an adaptive procedure to the unknown Sobolev smoothness. Leveraging these results, SATL employs Gaussian kernels in both phases so that the estimators can adapt to the unknown smoothness of the target/source and their offset function. We derive the minimax lower bound of the learning problem in excess risk and show that
&lt;/p&gt;</description></item><item><title>SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14961</link><description>&lt;p&gt;
&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Elastic Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14961
&lt;/p&gt;
&lt;p&gt;
SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20197;&#20197;&#22266;&#23450;&#25511;&#21046;&#39057;&#29575;&#25191;&#34892;&#21160;&#20316;&#30340;&#25511;&#21046;&#22120;&#12290;&#37492;&#20110;RL&#31639;&#27861;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#23427;&#20204;&#23545;&#25511;&#21046;&#39057;&#29575;&#30340;&#36873;&#25321;&#30340;&#24433;&#21709;&#35270;&#32780;&#19981;&#35265;&#65306;&#25214;&#21040;&#27491;&#30830;&#30340;&#25511;&#21046;&#39057;&#29575;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#24230;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#29978;&#33267;&#23548;&#33268;&#26080;&#27861;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#24377;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SEAC&#65289;, &#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;SEAC&#23454;&#29616;&#20102;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#65292;&#21363;&#20855;&#26377;&#24050;&#30693;&#21464;&#21270;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20801;&#35768;&#20195;&#29702;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#20854;&#25511;&#21046;&#39057;&#29575;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;SEAC&#20165;&#22312;&#24517;&#35201;&#26102;&#24212;&#29992;&#25511;&#21046;&#65292;&#26368;&#23567;&#21270;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SEAC&#22312;&#29275;&#39039;&#36816;&#21160;&#23398;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#21644;&#19977;&#32500;&#36187;&#36710;&#35270;&#39057;&#28216;&#25103;Trackmania&#20013;&#30340;&#33021;&#21147;&#12290;SEAC&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;SAC&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#20849;&#21516;&#31283;&#23450;&#26426;&#21046;&#26159;&#36890;&#36807;&#24341;&#20837;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#36991;&#20813;&#22349;&#32553;&#65292;&#36825;&#20123;&#26041;&#27861;&#23613;&#31649;&#26377;&#19981;&#21516;&#30340;&#20844;&#24335;&#65292;&#20294;&#38544;&#24335;&#20248;&#21270;&#20102;&#31867;&#20284;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14957</link><description>&lt;p&gt;
&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#32972;&#21518;&#30340;&#20849;&#21516;&#31283;&#23450;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Common Stability Mechanism behind most Self-Supervised Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14957
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#20849;&#21516;&#31283;&#23450;&#26426;&#21046;&#26159;&#36890;&#36807;&#24341;&#20837;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#36991;&#20813;&#22349;&#32553;&#65292;&#36825;&#20123;&#26041;&#27861;&#23613;&#31649;&#26377;&#19981;&#21516;&#30340;&#20844;&#24335;&#65292;&#20294;&#38544;&#24335;&#20248;&#21270;&#20102;&#31867;&#20284;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#35265;&#35777;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39046;&#22495;&#30340;&#24040;&#22823;&#36827;&#23637;&#65292;&#20854;&#25104;&#21151;&#24402;&#21151;&#20110;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#21516;&#26102;&#36991;&#20813;&#22349;&#32553;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#20123;&#19981;&#21516;SSL&#25216;&#26415;&#30340;&#31283;&#23450;&#26426;&#21046;&#65292;&#35752;&#35770;&#20102;&#20687;SimCLR&#36825;&#26679;&#30340;&#23545;&#27604;&#25216;&#26415;&#12289;&#20687;BYOL&#12289;SWAV&#12289;SimSiam&#12289;Barlow Twins&#21644;DINO&#36825;&#26679;&#30340;&#38750;&#23545;&#27604;&#25216;&#26415;&#30340;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35770;&#28857;&#65292;&#21363;&#23613;&#31649;&#26377;&#19981;&#21516;&#30340;&#20844;&#24335;&#65292;&#36825;&#20123;&#26041;&#27861;&#38544;&#24335;&#20248;&#21270;&#20102;&#31867;&#20284;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21363;&#26368;&#23567;&#21270;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14957v1 Announce Type: cross  Abstract: Last couple of years have witnessed a tremendous progress in self-supervised learning (SSL), the success of which can be attributed to the introduction of useful inductive biases in the learning process to learn meaningful visual representations while avoiding collapse. These inductive biases and constraints manifest themselves in the form of different optimization formulations in the SSL techniques, e.g. by utilizing negative examples in a contrastive formulation, or exponential moving average and predictor in BYOL and SimSiam. In this paper, we provide a framework to explain the stability mechanism of these different SSL techniques: i) we discuss the working mechanism of contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV, SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite different formulations these methods implicitly optimize a similar objective function, i.e. minimizing the magnitu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#32447;&#24615;&#27880;&#24847;&#21147;&#21644;&#32447;&#24615;MLP&#32452;&#20214;&#30340;&#32447;&#24615;Transformer&#22359;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#65292;&#24182;&#19988;&#19982;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#26377;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.14951</link><description>&lt;p&gt;
&#19968;&#20010;&#32447;&#24615;Transformer&#22359;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;MLP&#32452;&#20214;&#21644;&#19968;&#27493;GD&#21021;&#22987;&#21270;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14951
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#32447;&#24615;&#27880;&#24847;&#21147;&#21644;&#32447;&#24615;MLP&#32452;&#20214;&#30340;&#32447;&#24615;Transformer&#22359;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#65292;&#24182;&#19988;&#19982;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#26377;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#21512;&#32447;&#24615;&#27880;&#24847;&#21147;&#32452;&#20214;&#21644;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#32452;&#20214;&#30340;&#32447;&#24615;Transformer&#22359;&#65288;LTB&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#21644;&#38750;&#38646;&#22343;&#20540;&#30340;&#32447;&#24615;&#22238;&#24402;&#30340;ICL&#65292;&#25105;&#20204;&#34920;&#26126;LTB&#21487;&#20197;&#23454;&#29616;&#20960;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;ICL&#39118;&#38505;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#20351;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#24517;&#39035;&#20135;&#29983;&#19981;&#21487;&#36991;&#20813;&#30340;&#38468;&#21152;&#36817;&#20284;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;LTB&#19982;&#20855;&#26377;&#21487;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#65288;$\mathsf{GD}-\mathbf{\beta}$&#65289;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20174;&#27599;&#20010;$\mathsf{GD}-\mathbf{\beta}$&#20272;&#35745;&#22120;&#21487;&#20197;&#36890;&#36807;LTB&#20272;&#35745;&#22120;&#23454;&#29616;&#65292;&#21040;&#26368;&#23567;&#21270;&#31867;&#20869;ICL&#39118;&#38505;&#30340;&#27599;&#20010;&#26368;&#20248;LTB&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;$\mathsf{GD}-\mathbf{\beta}$&#20272;&#35745;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;$\mathsf{GD}-\mathbf{\beta}$&#20272;&#35745;&#22120;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#39640;&#25928;&#22320;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14951v1 Announce Type: cross  Abstract: We study the \emph{in-context learning} (ICL) ability of a \emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator. Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be efficiently optimized with gradient f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#21551;&#29992;&#27880;&#24847;&#21147;&#30340;Transformer&#20316;&#20026;&#24037;&#20855;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#20934;&#30830;&#20998;&#31867;&#32771;&#34385;&#27979;&#37327;&#22122;&#22768;&#12289;&#30452;&#27969;&#20559;&#31227;&#21644;&#30005;&#21387;&#20449;&#21495;&#24133;&#24230;&#39057;&#29575;&#21464;&#21270;&#31561;&#26465;&#20214;&#19979;&#30340;&#30005;&#33021;&#36136;&#37327;&#20107;&#20214;&#65292;&#26080;&#38656;&#39069;&#22806;&#29305;&#24449;&#25552;&#21462;&#65292;&#31934;&#24230;&#39640;&#20110;&#20854;&#20182;&#23398;&#20064;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14949</link><description>&lt;p&gt;
&#21033;&#29992;AI Transformer&#27169;&#22411;&#22686;&#24378;&#30005;&#33021;&#36136;&#37327;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Power Quality Event Classification with AI Transformer Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#21551;&#29992;&#27880;&#24847;&#21147;&#30340;Transformer&#20316;&#20026;&#24037;&#20855;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#20934;&#30830;&#20998;&#31867;&#32771;&#34385;&#27979;&#37327;&#22122;&#22768;&#12289;&#30452;&#27969;&#20559;&#31227;&#21644;&#30005;&#21387;&#20449;&#21495;&#24133;&#24230;&#39057;&#29575;&#21464;&#21270;&#31561;&#26465;&#20214;&#19979;&#30340;&#30005;&#33021;&#36136;&#37327;&#20107;&#20214;&#65292;&#26080;&#38656;&#39069;&#22806;&#29305;&#24449;&#25552;&#21462;&#65292;&#31934;&#24230;&#39640;&#20110;&#20854;&#20182;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#31934;&#30830;&#20998;&#31867;&#30005;&#33021;&#36136;&#37327;&#20107;&#20214;&#65288;PQEs&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#22312;&#20551;&#35774;&#29702;&#24819;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#65292;&#32780;&#29616;&#23454;&#20013;&#25105;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#27979;&#37327;&#22122;&#22768;&#12289;&#30452;&#27969;&#20559;&#31227;&#20197;&#21450;&#30005;&#21387;&#20449;&#21495;&#24133;&#24230;&#21644;&#39057;&#29575;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#22312;&#20808;&#21069;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;PQE&#20998;&#31867;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21551;&#29992;&#27880;&#24847;&#21147;&#30340;Transformer&#20316;&#20026;&#24037;&#20855;&#26469;&#20934;&#30830;&#20998;&#31867;&#32771;&#34385;&#19978;&#36848;&#22240;&#32032;&#30340;PQE&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#30452;&#25509;&#22312;&#30005;&#21387;&#20449;&#21495;&#19978;&#25805;&#20316;&#65292;&#26080;&#38656;&#21333;&#29420;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#25110;&#35745;&#31639;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#12290;&#23427;&#21487;&#20197;&#22312;&#19978;&#36848;&#26465;&#20214;&#19979;&#31934;&#30830;&#20998;&#31867;PQEs&#65292;&#20934;&#30830;&#29575;&#22312;99.81%&#33267;91.43%&#20043;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14949v1 Announce Type: new  Abstract: Recently, there has been a growing interest in utilizing machine learning for accurate classification of power quality events (PQEs). However, most of these studies are performed assuming an ideal situation, while in reality, we can have measurement noise, DC offset, and variations in the voltage signal's amplitude and frequency. Building on the prior PQE classification works using deep learning, this paper proposes a deep-learning framework that leverages attention-enabled Transformers as a tool to accurately classify PQEs under the aforementioned considerations. The proposed framework can operate directly on the voltage signals with no need for a separate feature extraction or calculation phase. Our results show that the proposed framework outperforms recently proposed learning-based techniques. It can accurately classify PQEs under the aforementioned conditions with an accuracy varying between 99.81%$-$91.43% depending on the signal-t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14948</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14948
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#36828;&#31243;&#30417;&#30563;&#65288;DS-NER&#65289;&#26694;&#26550;&#19979;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#26631;&#31614;&#36136;&#37327;&#21463;&#21040;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#22914;&#20551;&#38451;&#24615;&#12289;&#20551;&#38452;&#24615;&#21644;&#27491;&#21521;&#31867;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#24403;&#21069;DS-NER&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;QTL&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#31526;&#21512;&#39044;&#26399;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#26222;&#36941;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#65288;CuPUL&#65289;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31574;&#30053;&#24615;&#22320;&#20174;&#8220;&#26131;&#8221;&#21644;&#26356;&#28165;&#27905;&#30340;&#26679;&#26412;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#22024;&#26434;&#26679;&#26412;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;CuPUL&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#24433;&#21709;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#23545;&#25163;&#30693;&#35782;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26679;&#26412;&#28216;&#25103;&#26631;&#20934;&#21270;&#25915;&#20987;&#65292;&#23545;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#25915;&#20987;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#23545;&#25163;&#30693;&#35782;&#65292;&#24471;&#20986;&#26032;&#30340;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.14937</link><description>&lt;p&gt;
SoK: &#20998;&#26512;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;&#30740;&#31350;&#23545;&#25163;&#30693;&#35782;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SoK: Analyzing Adversarial Examples: A Framework to Study Adversary Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#23545;&#25163;&#30693;&#35782;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26679;&#26412;&#28216;&#25103;&#26631;&#20934;&#21270;&#25915;&#20987;&#65292;&#23545;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#25915;&#20987;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#23545;&#25163;&#30693;&#35782;&#65292;&#24471;&#20986;&#26032;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#26159;&#24694;&#24847;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20869;&#23481;&#65292;&#20250;&#24341;&#21457;&#35823;&#20998;&#31867;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#24050;&#32463;&#30740;&#31350;&#20102;&#36817;&#21313;&#24180;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21457;&#36215;&#25915;&#20987;&#26102;&#23545;&#23545;&#25163;&#30693;&#35782;&#30340;&#30740;&#31350;&#21644;&#24418;&#24335;&#21270;&#23384;&#22312;&#32570;&#20047;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#38590;&#20197;&#27604;&#36739;&#30340;&#23041;&#32961;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#24335;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21463;&#21551;&#21457;&#20110;&#24207;&#29702;&#35770;&#24037;&#20316;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#23545;&#25163;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#26679;&#26412;&#28216;&#25103;&#65292;&#21463;&#21040;&#23494;&#30721;&#28216;&#25103;&#30340;&#21551;&#21457;&#65292;&#29992;&#20197;&#26631;&#20934;&#21270;&#25915;&#20987;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#25915;&#20987;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#23545;&#25915;&#20987;&#32773;&#30340;&#30693;&#35782;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#36825;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#27719;&#32534;&#20102;&#32467;&#26524;&#65292;&#26082;&#30830;&#35748;&#20102;&#20851;&#20110;&#23545;&#25163;&#30693;&#35782;&#30340;&#29616;&#26377;&#35266;&#28857;&#65292;&#20363;&#22914;&#20851;&#20110;&#34987;&#25915;&#20987;&#27169;&#22411;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#20063;&#35753;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26032;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14937v1 Announce Type: new  Abstract: Adversarial examples are malicious inputs to machine learning models that trigger a misclassification. This type of attack has been studied for close to a decade, and we find that there is a lack of study and formalization of adversary knowledge when mounting attacks. This has yielded a complex space of attack research with hard-to-compare threat models and attacks. We focus on the image classification domain and provide a theoretical framework to study adversary knowledge inspired by work in order theory. We present an adversarial example game, inspired by cryptographic games, to standardize attacks. We survey recent attacks in the image classification domain and classify their adversary's knowledge in our framework. From this systematization, we compile results that both confirm existing beliefs about adversary knowledge, such as the potency of information about the attacked model as well as allow us to derive new conclusions on the di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#25935;&#24863;&#32676;&#20307;&#23450;&#20041;&#25110;&#39069;&#22806;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36229;&#21442;&#25968;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20445;&#35777;&#20219;&#20309;&#36275;&#22815;&#22823;&#30340;&#20154;&#32676;&#23376;&#38598;&#33021;&#33719;&#24471;&#33267;&#23569;&#26368;&#20302;&#25928;&#29992;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14929</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#35775;&#38382;&#25935;&#24863;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32852;&#37030;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Federated Fairness without Access to Sensitive Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#25935;&#24863;&#32676;&#20307;&#23450;&#20041;&#25110;&#39069;&#22806;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36229;&#21442;&#25968;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20445;&#35777;&#20219;&#20309;&#36275;&#22815;&#22823;&#30340;&#20154;&#32676;&#23376;&#38598;&#33021;&#33719;&#24471;&#33267;&#23569;&#26368;&#20302;&#25928;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#22312;&#35757;&#32451;&#26399;&#38388;&#23384;&#22312;&#39044;&#23450;&#20041;&#21644;&#26631;&#35760;&#30340;&#25935;&#24863;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#26032;&#20852;&#27861;&#35268;&#21040;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#21160;&#24577;&#21644;&#20301;&#32622;&#20381;&#36182;&#24615;&#31561;&#22810;&#31181;&#22240;&#32032;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#21512;&#36866;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20445;&#35777;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#39044;&#23450;&#20041;&#30340;&#25935;&#24863;&#32676;&#20307;&#30340;&#23450;&#20041;&#25110;&#39069;&#22806;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#20801;&#35768;&#32852;&#37030;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#30830;&#20445;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20165;&#21463;&#21040;&#32676;&#20307;&#22823;&#23567;&#32422;&#26463;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#36275;&#22815;&#22823;&#30340;&#20154;&#32676;&#23376;&#38598;&#37117;&#20445;&#35777;&#33021;&#20174;&#27169;&#22411;&#20013;&#33719;&#24471;&#33267;&#23569;&#30340;&#26368;&#20302;&#25928;&#29992;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#28085;&#30422;&#20102;&#29616;&#26377;&#26041;&#27861;&#20316;&#20026;&#29305;&#27530;&#26696;&#20363;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14929v1 Announce Type: cross  Abstract: Current approaches to group fairness in federated learning assume the existence of predefined and labeled sensitive groups during training. However, due to factors ranging from emerging regulations to dynamics and location-dependency of protected groups, this assumption may be unsuitable in many real-world scenarios. In this work, we propose a new approach to guarantee group fairness that does not rely on any predefined definition of sensitive groups or additional labels. Our objective allows the federation to learn a Pareto efficient global model ensuring worst-case group fairness and it enables, via a single hyper-parameter, trade-offs between fairness and utility, subject only to a group size constraint. This implies that any sufficiently large subset of the population is guaranteed to receive at least a minimum level of utility performance from the model. The proposed objective encompasses existing approaches as special cases, such
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#23398;&#20064;&#23567;&#22411;&#33258;&#21160;&#36710;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#23454;&#29616;&#39640;&#36895;&#22278;&#24418;&#23548;&#33322;&#21644;&#33258;&#20027;&#28418;&#31227;&#65292;&#24110;&#21161;&#36710;&#36742;&#23398;&#20064;&#19990;&#30028;&#29366;&#24577;&#24182;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;</title><link>https://arxiv.org/abs/2402.14928</link><description>&lt;p&gt;
&#23398;&#20064;&#36870;&#36816;&#21160;&#23398;&#20197;&#23454;&#29616;&#33258;&#21160;&#36710;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learning Inverse Kinodynamics for Autonomous Vehicle Drifting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14928
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#23398;&#20064;&#23567;&#22411;&#33258;&#21160;&#36710;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#23454;&#29616;&#39640;&#36895;&#22278;&#24418;&#23548;&#33322;&#21644;&#33258;&#20027;&#28418;&#31227;&#65292;&#24110;&#21161;&#36710;&#36742;&#23398;&#20064;&#19990;&#30028;&#29366;&#24577;&#24182;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23567;&#22411;&#33258;&#21160;&#36710;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#20854;&#23545;&#36816;&#21160;&#35268;&#21010;&#29305;&#21035;&#26159;&#33258;&#20027;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#36816;&#21160;&#35268;&#21010;&#26102;&#65292;&#23384;&#22312;&#35768;&#22810;&#23548;&#33268;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#35745;&#21010;&#30340;&#20869;&#23481;&#36890;&#24120;&#19982;&#23454;&#38469;&#27773;&#36710;&#19978;&#25191;&#34892;&#30340;&#20869;&#23481;&#19981;&#21516;&#12290;&#22522;&#20110;&#24815;&#24615;&#27979;&#37327;&#21644;&#25191;&#34892;&#21629;&#20196;&#23398;&#20064;&#21160;&#21147;&#23398;&#35268;&#21010;&#22120;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#23398;&#20064;&#19990;&#30028;&#29366;&#24577;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#30446;&#20809;&#36716;&#21521;&#28418;&#31227;&#39046;&#22495;&#65307;&#28418;&#31227;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#28436;&#32451;&#65292;&#38656;&#35201;&#36275;&#22815;&#24179;&#28369;&#30340;&#34920;&#38754;&#12289;&#36275;&#22815;&#39640;&#30340;&#36895;&#24230;&#21644;&#36895;&#24230;&#30340;&#24613;&#21095;&#21464;&#21270;&#12290;&#25105;&#20204;&#23581;&#35797;&#23398;&#20064;&#36825;&#20123;&#28418;&#31227;&#28436;&#32451;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#23581;&#35797;&#20943;&#23567;&#36710;&#36742;&#30340;&#20391;&#28369;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#39640;&#36895;&#22278;&#24418;&#33322;&#34892;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#26657;&#27491;&#33258;&#20027;&#39640;&#36895;&#28418;&#31227;&#19978;&#30340;&#38556;&#30861;&#29289;&#26469;&#36991;&#20813;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14928v1 Announce Type: cross  Abstract: In this work, we explore a data-driven learning-based approach to learning the kinodynamic model of a small autonomous vehicle, and observe the effect it has on motion planning, specifically autonomous drifting. When executing a motion plan in the real world, there are numerous causes for error, and what is planned is often not what is executed on the actual car. Learning a kinodynamic planner based off of inertial measurements and executed commands can help us learn the world state. In our case, we look towards the realm of drifting; it is a complex maneuver that requires a smooth enough surface, high enough speed, and a drastic change in velocity. We attempt to learn the kinodynamic model for these drifting maneuvers, and attempt to tighten the slip of the car. Our approach is able to learn a kinodynamic model for high-speed circular navigation, and is able to avoid obstacles on an autonomous drift at high speed by correcting an exec
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36866;&#21512;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#19982;&#26641;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#65288;&#26799;&#24230;&#65289;&#25552;&#21319;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14926</link><description>&lt;p&gt;
&#25552;&#21319;&#20851;&#31995;&#23398;&#20064;&#30340;&#20840;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Boosting gets full Attention for Relational Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14926
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36866;&#21512;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#19982;&#26641;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#65288;&#26799;&#24230;&#65289;&#25552;&#21319;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20934;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#24120;&#24120;&#20250;&#36935;&#21040;&#24179;&#38754;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#21363;&#30001;&#19968;&#20010;$m \times d$&#65288;&#34892;&#65292;&#21015;&#65289;&#25991;&#20214;&#32452;&#25104;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#24456;&#22810;&#24773;&#20917;&#26159;&#30001;&#19968;&#32452;&#24102;&#26377;&#32467;&#26500;&#20851;&#31995;&#30340;&#34920;&#26684;&#25551;&#36848;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36866;&#21512;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#19982;&#26641;&#27169;&#22411;&#32467;&#21512;&#22312;&#65288;&#26799;&#24230;&#65289;&#25552;&#21319;&#30340;&#35757;&#32451;&#32972;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14926v1 Announce Type: new  Abstract: More often than not in benchmark supervised ML, tabular data is flat, i.e. consists of a single $m \times d$ (rows, columns) file, but cases abound in the real world where observations are described by a set of tables with structural relationships. Neural nets-based deep models are a classical fit to incorporate general topological dependence among description features (pixels, words, etc.), but their suboptimality to tree-based models on tabular data is still well documented. In this paper, we introduce an attention mechanism for structured data that blends well with tree-based models in the training context of (gradient) boosting. Each aggregated model is a tree whose training involves two steps: first, simple tabular models are learned descending tables in a top-down fashion with boosting's class residuals on tables' features. Second, what has been learned progresses back bottom-up via attention and aggregation mechanisms, progressive
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.14925</link><description>&lt;p&gt;
&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Unbiased Sparsification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21521;&#37327;$p\in \mathbb{R}^n$&#30340;&#26080;&#20559;$m$-&#31232;&#30095;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#24179;&#22343;&#20540;&#20026;$p$&#65292;&#26368;&#22810;&#26377;$m&lt;n$&#20010;&#38750;&#38646;&#22352;&#26631;&#30340;&#38543;&#26426;&#21521;&#37327;$Q\in \mathbb{R}^n&#12290; &#26080;&#20559;&#31232;&#30095;&#21270;&#21487;&#20197;&#21387;&#32553;&#21407;&#22987;&#21521;&#37327;&#32780;&#19981;&#24341;&#20837;&#20559;&#24046;&#65307;&#23427;&#20986;&#29616;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#65292;&#27604;&#22914;&#32852;&#37030;&#23398;&#20064;&#21644;&#37319;&#26679;&#31232;&#30095;&#27010;&#29575;&#20998;&#24067;&#12290; &#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26080;&#20559;&#31232;&#30095;&#21270;&#36824;&#24212;&#35813;&#26368;&#23567;&#21270;&#19968;&#20010;&#24230;&#37327;$Q$&#19982;&#21407;&#22987;$p$&#20043;&#38388;&#36317;&#31163;&#26377;&#22810;&#36828;&#30340;&#20998;&#35010;&#20989;&#25968;$\mathsf{Div}(Q,p)$&#30340;&#26399;&#26395;&#20540;&#12290; &#22914;&#26524;$Q$&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#37027;&#20040;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#25928;&#12290; &#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#25551;&#36848;&#20102;&#23545;&#20110;&#26082;&#26159;&#25490;&#21015;&#19981;&#21464;&#21448;&#26159;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#30340;&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;&#12290; &#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25490;&#21015;&#19981;&#21464;&#20998;&#35010;&#20989;&#25968;&#30340;&#34920;&#24449;&#23545;&#20110;&#20998;&#35010;&#20989;&#25968;&#30340;&#36873;&#25321;&#26159;&#20581;&#22766;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#30340;&#26368;&#20248;$Q$&#30340;&#31867;&#19982;&#25105;&#20204;&#30340;&#31867;&#37325;&#21512;&#20102;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14925v1 Announce Type: cross  Abstract: An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of op
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#30693;&#35782;&#33976;&#39311;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#27604;&#36739;&#65292;&#21253;&#25324;&#20248;&#21270;&#30340;&#28201;&#24230;&#21644;&#26435;&#37325;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#20197;&#21450;&#25968;&#25454;&#20998;&#21306;KD&#65292;&#25581;&#31034;&#20102;&#26368;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14922</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#23454;&#36341;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Practical Insights into Knowledge Distillation for Pre-Trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14922
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#30693;&#35782;&#33976;&#39311;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#27604;&#36739;&#65292;&#21253;&#25324;&#20248;&#21270;&#30340;&#28201;&#24230;&#21644;&#26435;&#37325;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#20197;&#21450;&#25968;&#25454;&#20998;&#21306;KD&#65292;&#25581;&#31034;&#20102;&#26368;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23545;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36807;&#31243;&#30340;&#22686;&#24378;&#65292;&#36825;&#26159;&#30693;&#35782;&#20256;&#36755;&#20013;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#24182;&#23545;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#37319;&#29992;&#20102;&#35768;&#22810;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#20294;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#20102;&#35299;&#30693;&#35782;&#33976;&#39311;&#30340;&#24212;&#29992;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#22810;&#31181;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#65292;&#21253;&#25324;&#26631;&#20934;KD&#12289;&#32463;&#36807;&#20248;&#21270;&#28201;&#24230;&#21644;&#26435;&#37325;&#21442;&#25968;&#35843;&#25972;&#30340;KD&#12289;&#28145;&#24230;&#30456;&#20114;&#23398;&#20064;&#20197;&#21450;&#25968;&#25454;&#20998;&#21306;KD&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#65292;&#20197;&#30830;&#23450;&#27599;&#31181;&#26041;&#27861;&#26368;&#26377;&#25928;&#30340;&#24773;&#22659;&#12290;&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#32467;&#21512;&#24191;&#27867;&#30340;&#32593;&#26684;&#25628;&#32034;&#35780;&#20272;&#26469;&#33719;&#21462;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14922v1 Announce Type: cross  Abstract: This research investigates the enhancement of knowledge distillation (KD) processes in pre-trained models, an emerging field in knowledge transfer with significant implications for distributed training and federated learning environments. These environments benefit from reduced communication demands and accommodate various model architectures. Despite the adoption of numerous KD approaches for transferring knowledge among pre-trained models, a comprehensive understanding of KD's application in these scenarios is lacking. Our study conducts an extensive comparison of multiple KD techniques, including standard KD, tuned KD (via optimized temperature and weight parameters), deep mutual learning, and data partitioning KD. We assess these methods across various data distribution strategies to identify the most effective contexts for each. Through detailed examination of hyperparameter tuning, informed by extensive grid search evaluations, w
&lt;/p&gt;</description></item><item><title>MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14905</link><description>&lt;p&gt;
MobileLLM&#65306;&#20248;&#21270;&#20122;&#21313;&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#35774;&#22791;&#31471;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14905
&lt;/p&gt;
&lt;p&gt;
MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36843;&#20999;&#38656;&#27714;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20113;&#25104;&#26412;&#21644;&#24310;&#36831;&#38382;&#39064;&#19981;&#26029;&#22686;&#21152;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#19981;&#21040;&#21313;&#20159;&#21442;&#25968;&#30340;&#39030;&#32423;LLMs&#65292;&#36825;&#26159;&#31227;&#21160;&#37096;&#32626;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#24378;&#35843;&#25968;&#25454;&#21644;&#21442;&#25968;&#25968;&#37327;&#22312;&#30830;&#23450;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20122;&#21313;&#20159;&#35268;&#27169;LLMs&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#65292;&#20877;&#21152;&#19978;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32593;&#32476;&#65292;&#31216;&#20026;MobileLLM&#65292;&#20854;&#22312;&#23558;&#36817;125M/350M&#20808;&#36827;&#27169;&#22411;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#24778;&#20154;&#30340;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#21363;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#65292;&#19988;&#20165;&#20855;&#26377;&#26497;&#23567;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;MobileLLM-L
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14904</link><description>&lt;p&gt;
&#25968;&#23383;&#27700;&#21360;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25918;&#23556;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watermarking Makes Language Models Radioactive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#36755;&#20837;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#20197;&#19968;&#23450;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#36825;&#31181;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#35757;&#32451;&#25968;&#25454;&#30041;&#19979;&#30340;&#30165;&#36857;&#27604;&#25104;&#21592;&#25512;&#26029;&#26356;&#23481;&#26131;&#26816;&#27979;&#19988;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23558;&#27745;&#26579;&#27700;&#24179;&#19982;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12289;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27604;&#20363;&#21644;&#24494;&#35843;&#36807;&#31243;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;5&#65285;&#30340;&#35757;&#32451;&#25991;&#26412;&#34987;&#25968;&#23383;&#27700;&#21360;&#26631;&#35760;&#65292;&#35757;&#32451;&#22312;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#21512;&#25104;&#25351;&#20196;&#19978;&#20173;&#28982;&#21487;&#20197;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#65288;p&#20540;&lt;1e-5&#65289;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;LLM&#27700;&#21360;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#30830;&#23450;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;LLM&#30340;&#36755;&#20986;&#26159;&#21542;&#34987;&#29992;&#26469;&#23545;&#21478;&#19968;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value &lt; 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;tokenization&#23545;&#25968;&#20540;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#37319;&#29992;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#26041;&#24335;&#21487;&#26174;&#33879;&#25552;&#39640;&#31639;&#26415;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14903</link><description>&lt;p&gt;
Tokenization&#35745;&#25968;&#65306;Tokenization&#23545;&#21069;&#27839;LLMs&#20013;&#31639;&#26415;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;tokenization&#23545;&#25968;&#20540;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#37319;&#29992;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#26041;&#24335;&#21487;&#26174;&#33879;&#25552;&#39640;&#31639;&#26415;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tokenization&#65292;&#21363;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#25104;&#36755;&#20837;token&#30340;&#36807;&#31243;&#65292;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31649;&#36947;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#25110;&#26377;&#23475;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#26469;&#28304;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;LLMs&#20542;&#21521;&#20110;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#29305;&#23450;&#30340;&#36755;&#20837;&#39046;&#22495;&#12290;&#38543;&#30528;LLMs&#29992;&#20110;&#25512;&#29702;&#30340;&#22686;&#21152;&#65292;&#21508;&#31181;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;tokenization&#26041;&#26696;&#24471;&#21040;&#20102;&#37319;&#29992;&#65292;&#20687;LLaMa&#21644;PaLM&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#36873;&#25321;&#20102;&#21333;&#20010;&#25968;&#23383;tokenization&#65292;&#32780;GPT-3.5&#21644;GPT-4&#20026;&#27599;&#20010;1&#20301;&#12289;2&#20301;&#21644;3&#20301;&#25968;&#23383;&#37117;&#26377;&#21333;&#29420;&#30340;token&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31639;&#26415;&#20219;&#21153;&#30740;&#31350;&#36825;&#31181;&#36873;&#25321;&#23545;&#25968;&#20540;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;GPT-3.5&#21644;-4&#30340;&#20174;&#24038;&#21040;&#21491;&#21644;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#65292;&#21457;&#29616;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#65288;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#36887;&#21495;&#20998;&#31163;&#25968;&#23383;&#65289;&#23548;&#33268;&#20102;&#22823;&#24133;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20351;&#29992;&#26631;&#20934;&#30340;&#20174;&#24038;&#21040;&#21491;tokenization&#26102;&#27169;&#22411;&#23384;&#22312;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14903v1 Announce Type: new  Abstract: Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-r
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.14899</link><description>&lt;p&gt;
&#20572;&#27490;&#25512;&#29702;&#65281;&#24403;&#22810;&#27169;&#24577;LLMs&#19982;&#20018;&#32852;&#25512;&#29702;&#36935;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#29702;&#35299;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#20256;&#32479;&#35270;&#35273;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20018;&#32852;&#25512;&#29702;&#65288;CoT&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;MLLMs&#19978;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#25552;&#20379;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20851;&#20110;MLLMs&#22312;CoT&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;MLLMs&#29992;&#23545;&#25239;&#24615;&#22270;&#20687;&#25512;&#26029;&#38169;&#35823;&#31572;&#26696;&#26102;&#25512;&#29702;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#37319;&#29992;CoT&#25512;&#29702;&#26102;MLLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;CoT&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25269;&#25239;&#20102;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;CoT&#24341;&#36215;&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoT&#25512;&#29702;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14897</link><description>&lt;p&gt;
Chain-of-Thought&#19981;&#24544;&#35802;&#20316;&#20026;&#20266;&#35013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Unfaithfulness as Disguised Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14897
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought (CoT)&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;LLM&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;CoT&#24544;&#23454;&#24230;&#30340;&#20195;&#29702;&#65292;arXiv:2307.13702&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#27169;&#22411;&#20381;&#36182;&#20854;CoT&#29983;&#25104;&#31572;&#26696;&#30340;&#25351;&#26631;&#12290;&#22312;&#19968;&#20010;&#19987;&#26377;&#27169;&#22411;&#31995;&#21015;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#27169;&#22411;&#22823;&#23567;&#19982;&#20854;&#24544;&#23454;&#24230;&#27979;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;-&#21453;&#21521;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#19988;130&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#27604;&#20110;&#23610;&#23544;&#20171;&#20110;8.1&#20159;&#21040;1750&#20159;&#21442;&#25968;&#20043;&#38388;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#22686;&#21152;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#20316;&#20026;&#25152;&#26377;LLM&#30340;&#29305;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#31995;&#21015;&#30340;&#27169;&#22411;&#22797;&#21046;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20182;&#20204;&#25253;&#21578;&#30340;CoT&#24544;&#23454;&#24230;&#30340;&#32553;&#25918;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#25913;&#21464;&#35774;&#23450;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#24335;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14895</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#27515;&#65292;&#25968;&#25454;&#22686;&#24378;&#19975;&#23681;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is Dead, Long Live Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14895
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#20010;&#32321;&#33635;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#19981;&#26029;&#25552;&#20986;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#21019;&#24314;&#20154;&#24037;&#25968;&#25454;&#65292;&#24050;&#32463;&#22312;&#23567;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#33267;&#23569;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#32467;&#26524;&#65292;&#34920;&#26126;&#32463;&#20856;&#30340;&#25968;&#25454;&#22686;&#24378;&#21482;&#26159;&#19968;&#31181;&#26356;&#22909;&#22320;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#24335;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#20043;&#21069;&#33457;&#26356;&#22810;&#26102;&#38388;&#36827;&#34892;&#24494;&#35843;&#20250;&#25269;&#28040;&#20854;&#25928;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#22240;&#20026;&#23427;&#22238;&#31572;&#20102;&#26368;&#36817;&#20960;&#24180;&#30041;&#19979;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#21363;&#65306;&#21738;&#31181;DA&#25216;&#26415;&#34920;&#29616;&#26368;&#20339;&#65288;&#21482;&#35201;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#35757;&#32451;&#38598;&#36275;&#22815;&#25509;&#36817;&#65292;&#19981;&#20250;&#25439;&#23475;&#35757;&#32451;&#65289;&#65292;&#20026;&#20160;&#20040;DA&#34920;&#29616;&#20986;&#31215;&#26497;&#30340;&#32467;&#26524;&#65288;&#31616;&#21270;&#32593;&#32476;&#35757;&#32451;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#35805;&#20195;&#29702;&#65288;&#22914;ChatGPT&#25110;LLama2&#65289;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20174;&#32780;&#24471;&#20986;&#20102;&#32467;&#35770;&#65292;&#27492;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14895v1 Announce Type: cross  Abstract: Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation is simply a way of performing better fine-tuning, and that spending more time fine-tuning before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate data close enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and few-shot data generation via conversational agents such as ChatGPT or LLama2 can increase performances, concluding that this f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22788;&#29702;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#37197;&#30005;&#31995;&#32479;&#20013;&#25925;&#38556;&#30340;&#20934;&#30830;&#39044;&#27979;</title><link>https://arxiv.org/abs/2402.14894</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#37197;&#30005;&#32593;&#20998;&#24067;&#24335;&#21457;&#30005;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Ground-Fault Location Method in Distribution Power System With Distributed Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22788;&#29702;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#37197;&#30005;&#31995;&#32479;&#20013;&#25925;&#38556;&#30340;&#20934;&#30830;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#37197;&#30005;&#32423;&#21035;&#30340;&#22686;&#21152;&#24341;&#20837;&#20102;&#22810;&#26041;&#21521;&#21151;&#29575;&#27969;&#65292;&#20351;&#24471;&#36807;&#26102;&#30340;&#20256;&#32479;&#25925;&#38556;&#23450;&#20301;&#25216;&#26415;&#38590;&#20197;&#36866;&#29992;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#20197;&#30830;&#20445;&#24555;&#36895;&#20934;&#30830;&#30340;&#25925;&#38556;&#23450;&#20301;&#65292;&#20174;&#32780;&#22686;&#24378;&#30005;&#21147;&#31995;&#32479;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37197;&#30005;&#31995;&#32479;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#12290;&#22312;Matlab/Simulink&#20013;&#24314;&#27169;&#20102;&#19968;&#20010;11&#33410;&#28857; 20 kV&#30340;&#30005;&#21147;&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#25311;&#25509;&#22320;&#25925;&#38556;&#12290;&#22312;&#19981;&#21516;&#20301;&#32622;&#21644;&#19981;&#21516;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#19979;&#20135;&#29983;&#20102;&#25925;&#38556;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#20998;&#26512;&#31995;&#32479;&#21464;&#30005;&#31449;&#30340;&#26102;&#22495;&#25925;&#38556;&#19977;&#30456;&#30005;&#21387;&#12290;&#26368;&#32456;&#21033;&#29992;&#22788;&#29702;&#21518;&#30340;&#25968;&#25454;&#30340;&#32479;&#35745;&#37327;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#26469;&#25214;&#21040;&#35745;&#31639;&#30005;&#21387;&#29305;&#24449;&#21644;&#25925;&#38556;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19977;&#20010;ANNs&#21487;&#20197;&#39044;&#27979;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14894v1 Announce Type: cross  Abstract: The recent increase in renewable energy penetration at the distribution level introduces a multi-directional power flow that outdated traditional fault location techniques. To this extent, the development of new methods is needed to ensure fast and accurate fault localization and, hence, strengthen power system reliability. This paper proposes a data-driven ground fault location method for the power distribution system. An 11-bus 20 kV power system is modeled in Matlab/Simulink to simulate ground faults. The faults are generated at different locations and under various system operational states. Time-domain faulted three-phase voltages at the system substation are then analyzed with discrete wavelet transform. Statistical quantities of the processed data are eventually used to train an Artificial Neural Network (ANN) to find a mapping between computed voltage features and faults. Specifically, three ANNs allow the prediction of faulted
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;SigNova&#65292;&#29992;&#20110;&#26816;&#27979;&#23556;&#30005;&#22825;&#25991;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#37319;&#29992;&#29305;&#24449;&#36716;&#25442;&#25552;&#21462;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#24182;&#35745;&#31639;&#26032;&#39062;&#24615;&#35780;&#20998;&#65292;&#20197;&#35782;&#21035;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#30340;&#35266;&#23519;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.14892</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#23545;&#23556;&#30005;&#22825;&#25991;&#25968;&#25454;&#36827;&#34892;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Novelty Detection on Radio Astronomy Data using Signatures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14892
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;SigNova&#65292;&#29992;&#20110;&#26816;&#27979;&#23556;&#30005;&#22825;&#25991;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#37319;&#29992;&#29305;&#24449;&#36716;&#25442;&#25552;&#21462;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#24182;&#35745;&#31639;&#26032;&#39062;&#24615;&#35780;&#20998;&#65292;&#20197;&#35782;&#21035;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#30340;&#35266;&#23519;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SigNova&#65292;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#27969;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#23613;&#31649;&#25105;&#20204;&#26368;&#21021;&#30340;&#20363;&#23376;&#20391;&#37325;&#20110;&#22312;&#23556;&#30005;&#22825;&#25991;&#23398;&#39046;&#22495;&#20869;&#26816;&#27979;&#25968;&#23383;&#20449;&#21495;&#20013;&#30340;&#23556;&#39057;&#24178;&#25200;&#65288;RFI&#65289;&#65292;&#20294;&#37325;&#35201;&#30340;&#26159;&#35201;&#27880;&#24847;&#65292;SigNova&#30340;&#36866;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#20219;&#20309;&#31867;&#22411;&#30340;&#27969;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#24449;&#36716;&#25442;&#20174;&#35266;&#27979;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#32452;&#35268;&#33539;&#30340;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#21487;&#21464;&#38271;&#24230;&#30340;&#21487;&#35265;&#24615;&#26679;&#26412;&#34920;&#31034;&#20026;&#26377;&#38480;&#32500;&#29305;&#24449;&#21521;&#37327;&#12290;&#20854;&#27425;&#65292;&#27599;&#20010;&#29305;&#24449;&#21521;&#37327;&#34987;&#20998;&#37197;&#19968;&#20010;&#26032;&#39062;&#24615;&#35780;&#20998;&#65292;&#35745;&#31639;&#20026;&#21040;&#26080;RFI&#35757;&#32451;&#38598;&#20013;&#26368;&#36817;&#37051;&#30340;&#39532;&#27663;&#36317;&#31163;&#12290;&#36890;&#36807;&#35774;&#23450;&#36825;&#20123;&#20998;&#25968;&#30340;&#38408;&#20540;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20559;&#31163;&#26080;RFI&#21487;&#35265;&#24615;&#26679;&#26412;&#39044;&#26399;&#34892;&#20026;&#30340;&#35266;&#23519;&#33539;&#22260;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20005;&#26684;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14892v1 Announce Type: cross  Abstract: We introduce SigNova, a new semi-supervised framework for detecting anomalies in streamed data. While our initial examples focus on detecting radio-frequency interference (RFI) in digitized signals within the field of radio astronomy, it is important to note that SigNova's applicability extends to any type of streamed data. The framework comprises three primary components. Firstly, we use the signature transform to extract a canonical collection of summary statistics from observational sequences. This allows us to represent variable-length visibility samples as finite-dimensional feature vectors. Secondly, each feature vector is assigned a novelty score, calculated as the Mahalanobis distance to its nearest neighbor in an RFI-free training set. By thresholding these scores we identify observation ranges that deviate from the expected behavior of RFI-free visibility samples without relying on stringent distributional assumptions. Thirdl
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14890</link><description>&lt;p&gt;
Vygotsky Distance: &#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vygotsky Distance: Measure for Benchmark Task Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14890
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#24037;&#20855;&#21644;&#23454;&#36341;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#20934;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#31216;&#20043;&#20026;"&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;"&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#8220;&#23398;&#29983;&#8221;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20219;&#21153;&#26412;&#36523;&#30340;&#23646;&#24615;&#12290;&#22914;&#26524;&#20004;&#20010;&#20219;&#21153;&#22312;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#19978;&#24444;&#27492;&#25509;&#36817;&#65292;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978; tend to have similar relative performance&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20102;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#30340;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#22312;&#19981;&#32463;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14888</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient data selection employing Semantic Similarity-based Graph Structures for model training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#30340;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#22312;&#19981;&#32463;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#20984;&#26174;&#20102;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#25152;&#38656;&#22823;&#37327;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#35757;&#32451;&#27492;&#31867;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;SeSaME&#8221;&#30340;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#23427;&#20165;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#65292;&#26080;&#38656;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14888v1 Announce Type: cross  Abstract: Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated speech recognition (ASR) models, which excessively rely on text-to-speech (TTS) calls when using augmented data. SeSaME learns to categorize new incoming data points into speech recognition difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#26174;&#33879;&#20943;&#23569;&#32039;&#24613;&#20572;&#36710;&#27425;&#25968;&#65292;&#38477;&#20302;&#20132;&#36890;&#25317;&#22581;&#65292;&#25913;&#21892;&#20132;&#36890;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.14886</link><description>&lt;p&gt;
&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;
&lt;/p&gt;
&lt;p&gt;
Applying Reinforcement Learning to Optimize Traffic Light Cycles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#26174;&#33879;&#20943;&#23569;&#32039;&#24613;&#20572;&#36710;&#27425;&#25968;&#65292;&#38477;&#20302;&#20132;&#36890;&#25317;&#22581;&#65292;&#25913;&#21892;&#20132;&#36890;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#30340;&#25163;&#21160;&#20248;&#21270;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#26102;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#22478;&#24066;&#31227;&#21160;&#27169;&#25311;&#22120;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;Q&#32593;&#32476;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#32039;&#24613;&#20572;&#36710;&#27425;&#25968;&#20943;&#23569;&#20102;44.16%&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12289;&#25913;&#21892;&#20132;&#36890;&#27969;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36884;&#24452;&#21644;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14886v1 Announce Type: cross  Abstract: Manual optimization of traffic light cycles is a complex and time-consuming task, necessitating the development of automated solutions. In this paper, we propose the application of reinforcement learning to optimize traffic light cycles in real-time. We present a case study using the Simulation Urban Mobility simulator to train a Deep Q-Network algorithm. The experimental results showed 44.16% decrease in the average number of Emergency stops, showing the potential of our approach to reduce traffic congestion and improve traffic flow. Furthermore, we discuss avenues for future research and enhancements to the reinforcement learning model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.14883</link><description>&lt;p&gt;
&#21452;I&#27700;&#21360;&#65306;&#20445;&#25252;LLM&#24494;&#35843;&#27169;&#22411;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#65292;&#19994;&#20027;&#32463;&#24120;&#36890;&#36807;LLM&#25152;&#26377;&#32773;&#25110;&#20113;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;API&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#21462;&#23450;&#21046;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#23384;&#22312;&#30528;&#27169;&#22411;&#34987;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#20250;&#32473;&#19994;&#20027;&#24102;&#26469;&#20005;&#37325;&#30340;&#32463;&#27982;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#20445;&#25252;&#36825;&#20123;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#24050;&#25104;&#20026;&#32039;&#36843;&#30340;&#23454;&#38469;&#38656;&#27714;&#65292;&#20294;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32039;&#36843;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#22522;&#20110;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#65292;&#20998;&#21035;&#22312;&#25351;&#20196;&#21644;&#36755;&#20837;&#20013;&#35302;&#21457;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#23558;&#23450;&#21046;&#30340;&#21518;&#38376;&#26679;&#26412;&#32435;&#20837;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#27880;&#20837;&#20102;&#29305;&#23450;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14883v1 Announce Type: cross  Abstract: To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28385;&#36275;&#36816;&#21160;&#23398;&#21644;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#22810;&#36830;&#26438;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.14882</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28385;&#36275;&#30446;&#26631;&#26465;&#20214;&#30340;&#22235;&#36830;&#26438;&#26426;&#26500;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28385;&#36275;&#36816;&#21160;&#23398;&#21644;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#22810;&#36830;&#26438;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26500;&#26159;&#21508;&#31181;&#26426;&#26800;&#31995;&#32479;&#20013;&#35774;&#35745;&#29992;&#20110;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#28385;&#36275;&#29305;&#23450;&#36816;&#21160;&#23398;&#25110;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#26426;&#26500;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#28385;&#36275;&#36816;&#21160;&#23398;&#21644;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#22810;&#36830;&#26438;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22522;&#20110;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(cGAN)&#65292;&#24182;&#32463;&#36807;&#38024;&#23545;&#26426;&#26500;&#21512;&#25104;&#30340;&#20462;&#25913;&#65292;&#20854;&#35757;&#32451;&#30446;&#30340;&#26159;&#23398;&#20064;&#26426;&#26500;&#30340;&#35201;&#27714;&#19982;&#36830;&#26438;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#21512;&#25104;&#28385;&#36275;&#35201;&#27714;&#30340;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14882v1 Announce Type: cross  Abstract: Mechanisms are essential components designed to perform specific tasks in various mechanical systems. However, designing a mechanism that satisfies certain kinematic or quasi-static requirements is a challenging task. The kinematic requirements may include the workspace of a mechanism, while the quasi-static requirements of a mechanism may include its torque transmission, which refers to the ability of the mechanism to transfer power and torque effectively. In this paper, we propose a deep learning-based generative model for generating multiple crank-rocker four-bar linkage mechanisms that satisfy both the kinematic and quasi-static requirements aforementioned. The proposed model is based on a conditional generative adversarial network (cGAN) with modifications for mechanism synthesis, which is trained to learn the relationship between the requirements of a mechanism with respect to linkage lengths. The results demonstrate that the pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14878</link><description>&lt;p&gt;
&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#30340;&#33021;&#25928;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: &#20869;&#23384;&#20013;&#23398;&#20064;&#65288;LIM&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;Paradigm&#65292;&#26088;&#22312;&#20811;&#26381;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20869;&#23384;&#29942;&#39048;&#12290;&#34429;&#28982;&#35745;&#31639;&#20110;&#20869;&#23384;&#65288;CIM&#65289;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#20869;&#23384;&#22681;&#38382;&#39064;&#65288;&#21363;&#30001;&#20110;&#37325;&#22797;&#20869;&#23384;&#35835;&#21462;&#35775;&#38382;&#32780;&#28040;&#32791;&#30340;&#33021;&#37327;&#65289;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20197;&#35757;&#32451;&#25152;&#38656;&#30340;&#31934;&#24230;&#37325;&#22797;&#20869;&#23384;&#20889;&#20837;&#26102;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#26356;&#26032;&#22681;&#65289;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#32771;&#34385;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20043;&#38388;&#20256;&#36755;&#20449;&#24687;&#26102;&#25152;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#25972;&#21512;&#22681;&#65289;&#12290;LIM&#33539;&#24335;&#25552;&#20986;&#65292;&#22914;&#26524;&#29289;&#29702;&#20869;&#23384;&#30340;&#33021;&#37327;&#23631;&#38556;&#34987;&#33258;&#36866;&#24212;&#35843;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#22120;&#26356;&#26032;&#21644;&#25972;&#21512;&#30340;&#21160;&#24577;&#19982;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;AI&#27169;&#22411;&#30340;Lyapunov&#21160;&#24577;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20123;&#29942;&#39048;&#20063;&#21487;&#20197;&#34987;&#20811;&#26381;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#19981;&#21516;LIM&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#32791;&#30340;&#26032;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 Announce Type: cross  Abstract: Learning-in-memory (LIM) is a recently proposed paradigm to overcome fundamental memory bottlenecks in training machine learning systems. While compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access) they are agnostic to the energy dissipated due to repeated memory writes at the precision required for training (the update-wall), and they don't account for the energy dissipated when transferring information between short-term and long-term memories (the consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can be overcome if the energy barrier of physical memories is adaptively modulated such that the dynamics of memory updates and consolidation match the Lyapunov dynamics of gradient-descent training of an AI model. In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM app
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#22024;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#20542;&#35206;&#65292;&#21253;&#25324;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#30340;&#20542;&#35206;&#21644;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2402.14877</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#30340;&#20542;&#35206;&#21644;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Machine-learning prediction of tipping and collapse of the Atlantic Meridional Overturning Circulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#22024;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#20542;&#35206;&#65292;&#21253;&#25324;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#30340;&#20542;&#35206;&#21644;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;(AMOC)&#30340;&#26368;&#26032;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#20854;&#28508;&#22312;&#20542;&#35206;&#30340;&#25285;&#24551;&#65292;&#36825;&#26159;&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#21271;&#22823;&#35199;&#27915;&#28129;&#27700;&#36755;&#20837;&#22686;&#21152;&#30340;&#19968;&#20010;&#20020;&#30028;&#28857;&#12290;&#39044;&#27979;&#30340;&#23849;&#28291;&#26102;&#38388;&#31383;&#21475;&#22823;&#32422;&#22312;&#26412;&#19990;&#32426;&#20013;&#21494;&#65292;&#26368;&#26089;&#21487;&#33021;&#22312;&#22823;&#32422;&#20004;&#24180;&#21518;&#24320;&#22987;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#23545;&#31995;&#32479;&#20174;&#19968;&#20010;&#31283;&#23450;&#24179;&#34913;&#29366;&#24577;&#36716;&#21464;&#21040;&#21478;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#30340;&#20020;&#30028;&#28857;&#30340;&#39044;&#27979;&#23545;&#20110;&#24191;&#27867;&#39046;&#22495;&#37117;&#26159;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22024;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#20542;&#35206;&#65292;&#24182;&#22312;&#22810;&#20010;&#31995;&#32479;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#21253;&#25324;AMOC&#12289;&#29983;&#24577;&#32593;&#32476;&#12289;&#30005;&#21147;&#31995;&#32479;&#21644;&#27668;&#20505;&#27169;&#22411;&#12290;&#23545;&#20110;AMOC&#65292;&#25105;&#20204;&#22522;&#20110;&#27169;&#25311;&#25351;&#32441;&#25968;&#25454;&#21644;&#28023;&#34920;&#28201;&#24230;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#23558;&#28508;&#22312;&#20542;&#35206;&#30340;&#26102;&#38388;&#31383;&#21475;&#32622;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14877v1 Announce Type: cross  Abstract: Recent research on the Atlantic Meridional Overturning Circulation (AMOC) raised concern about its potential collapse through a tipping point due to the climate-change caused increase in the freshwater input into the North Atlantic. The predicted time window of collapse is centered about the middle of the century and the earliest possible start is approximately two years from now. More generally, anticipating a tipping point at which the system transitions from one stable steady state to another is relevant to a broad range of fields. We develop a machine-learning approach to predicting tipping in noisy dynamical systems with a time-varying parameter and test it on a number of systems including the AMOC, ecological networks, an electrical power system, and a climate model. For the AMOC, our prediction based on simulated fingerprint data and real data of the sea surface temperature places the time window of a potential collapse between 
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.14875</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#21547;&#20041;&#26159;&#20160;&#20040;&#65311;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Auditing Large Language Models for Race and Gender Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14875
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#23457;&#35745;&#35774;&#35745;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;GPT-4&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#21457;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#20026;&#20010;&#20154;&#25552;&#20379;&#24314;&#35758;&#65292;&#27604;&#22914;&#22312;&#36141;&#36710;&#35848;&#21028;&#25110;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#24314;&#35758;&#31995;&#32479;&#24615;&#22320;&#23545;&#19982;&#31181;&#26063;&#23569;&#25968;&#32676;&#20307;&#21644;&#22899;&#24615;&#24120;&#35265;&#30456;&#20851;&#30340;&#21517;&#23383;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#24471;&#21040;&#30340;&#32467;&#26524;&#26368;&#19981;&#21033;&#12290;&#36825;&#20123;&#20559;&#35265;&#22312;42&#20010;&#25552;&#31034;&#27169;&#26495;&#21644;&#22810;&#20010;&#27169;&#22411;&#20013;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#20107;&#20214;&#12290;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#25968;&#20540;&#12289;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#38170;&#28857;&#21487;&#20197;&#25104;&#21151;&#25269;&#28040;&#20559;&#35265;&#65292;&#32780;&#23450;&#24615;&#32454;&#33410;&#30340;&#24433;&#21709;&#24182;&#19981;&#19968;&#33268;&#65292;&#29978;&#33267;&#21487;&#33021;&#20250;&#21152;&#21095;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#36827;&#34892;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14874</link><description>&lt;p&gt;
&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65306;&#21033;&#29992;&#23545;&#27604;&#35299;&#30721;&#21644;&#33976;&#39311;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#36739;&#23567;&#30340;&#19994;&#20313;&#27169;&#22411;&#25110;&#38544;&#34255;&#29366;&#24577;&#24046;&#24322;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DCD&#37319;&#29992;&#20102;&#23545;&#27604;&#24335;&#24605;&#32500;&#24341;&#23548;&#21644;&#20808;&#36827;&#30340;&#33976;&#39311;&#25216;&#26415;&#65292;&#21253;&#25324;Dropout&#21644;&#37327;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21644;&#19994;&#20313;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;DCD&#28040;&#38500;&#20102;&#23545;&#19994;&#20313;&#27169;&#22411;&#30340;&#38656;&#27714;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DCD&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;GSM8K&#21644;StrategyQA&#25968;&#25454;&#38598;&#20013;&#22343;&#36229;&#36807;&#20102;CD&#21644;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#65288;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#26102;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F-&#24230;&#37327;&#20540;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#29992;&#35789;&#30340;&#22788;&#29702;&#26041;&#24335;&#23545;&#25991;&#26412;&#20998;&#31867;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.14867</link><description>&lt;p&gt;
&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#23545;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#30340;&#21152;&#26435;&#26041;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of term weighting approach with and without stop words removing on Arabic text classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#65288;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#26102;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F-&#24230;&#37327;&#20540;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#29992;&#35789;&#30340;&#22788;&#29702;&#26041;&#24335;&#23545;&#25991;&#26412;&#20998;&#31867;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#25991;&#26412;&#26159;&#19968;&#31181;&#23558;&#25991;&#26723;&#20998;&#31867;&#20026;&#39044;&#20808;&#24314;&#31435;&#30340;&#32676;&#32452;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#31867;&#20043;&#21069;&#65292;&#25991;&#26412;&#25991;&#26723;&#24517;&#39035;&#20197;&#36866;&#21512;&#25968;&#25454;&#25366;&#25496;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#20934;&#22791;&#21644;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#21019;&#24314;&#20102;&#35768;&#22810;&#26415;&#35821;&#21152;&#26435;&#31574;&#30053;&#26469;&#22686;&#24378;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#30340;&#21151;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#23545;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#19968;&#27425;&#21024;&#38500;&#20572;&#29992;&#35789;&#21644;&#19981;&#21024;&#38500;&#20572;&#29992;&#35789;&#12290;&#20026;&#20102;&#35780;&#20272;&#20808;&#21069;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;322&#20221;&#25991;&#26723;&#30340;&#38463;&#25289;&#20271;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#20845;&#20010;&#20027;&#39064;&#65288;&#20892;&#19994;&#12289;&#32463;&#27982;&#12289;&#20581;&#24247;&#12289;&#25919;&#27835;&#12289;&#31185;&#23398;&#21644;&#20307;&#32946;&#65289;&#65292;&#27599;&#20010;&#20027;&#39064;&#21253;&#21547;50&#20221;&#25991;&#26723;&#65292;&#21807;&#29420;&#20581;&#24247;&#31867;&#21035;&#38500;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14867v1 Announce Type: cross  Abstract: Classifying text is a method for categorizing documents into pre-established groups. Text documents must be prepared and represented in a way that is appropriate for the algorithms used for data mining prior to classification. As a result, a number of term weighting strategies have been created in the literature to enhance text categorization algorithms' functionality. This study compares the effects of Binary and Term frequency weighting feature methodologies on the text's classification method when stop words are eliminated once and when they are not. In recognition of assessing the effects of prior weighting of features approaches on classification results in terms of accuracy, recall, precision, and F-measure values, we used an Arabic data set made up of 322 documents divided into six main topics (agriculture, economy, health, politics, science, and sport), each of which contains 50 documents, with the exception of the health categ
&lt;/p&gt;</description></item><item><title>APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;</title><link>https://arxiv.org/abs/2402.14866</link><description>&lt;p&gt;
APTQ: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14866
&lt;/p&gt;
&lt;p&gt;
APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#36127;&#36733;&#21644;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#23545;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;APTQ&#65288;Attention-aware Post-Training Mixed-Precision Quantization&#65289;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#32780;&#19988;&#39318;&#27425;&#32771;&#34385;&#20102;&#27880;&#24847;&#21147;&#36755;&#20986;&#23545;&#25972;&#20010;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;Hessian&#36857;&#20316;&#20026;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25935;&#24863;&#24230;&#24230;&#37327;&#65292;&#30830;&#20445;&#32463;&#36807;&#29702;&#24615;&#30340;&#31934;&#24230;&#38477;&#20302;&#33021;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;APTQ&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;C4&#25968;&#25454;&#38598;&#20013;&#20197;&#24179;&#22343;4&#20301;&#23485;&#24230;&#33719;&#24471;5.22&#22256;&#24785;&#24230;&#65292;&#20960;&#20046;&#31561;&#25928;&#20110;&#20840;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;APTQ&#22312;LLaMa-7B&#21644;LLaMa-1&#20013;&#20197;&#24179;&#22343;3.8&#20301;&#23485;&#24230;&#36798;&#21040;&#20102;68.24&#65285;&#21644;70.48&#65285;&#30340;&#26368;&#20808;&#36827;&#38646;-shot&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14865</link><description>&lt;p&gt;
DyVal 2: &#20803;&#25506;&#27979;&#20195;&#29702;&#21160;&#24577;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#35774;&#35745;&#20102;&#20351;&#29992;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#31639;&#27861;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;&#21327;&#35758;&#26080;&#27861;&#36731;&#26494;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21482;&#33021;&#25552;&#20379;&#25972;&#20307;&#22522;&#20934;&#32467;&#26524;&#65292;&#19981;&#33021;&#25903;&#25345;&#23545;LLMs&#33021;&#21147;&#36827;&#34892;&#32454;&#31890;&#24230;&#21644;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#24515;&#29702;&#27979;&#37327;&#23398;&#21551;&#21457;&#30340;&#36890;&#29992;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#12290; MPA &#26159; DyVal 2 &#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#33258;&#28982;&#22320;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340; DyVal&#12290; MPA &#35774;&#35745;&#20102;&#25506;&#27979;&#21644;&#35780;&#21028;&#20195;&#29702;&#65292;&#20197;&#33258;&#21160;&#23558;&#21407;&#22987;&#35780;&#20272;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#36981;&#24490;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#22312;&#19977;&#20010;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#19978;&#30340;&#24212;&#29992;: &#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
&lt;/p&gt;</description></item><item><title>SISSA&#25552;&#20986;&#20102;&#22522;&#20110;SOME/IP&#36890;&#20449;&#27969;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#20998;&#26512;&#36710;&#36733;&#21151;&#33021;&#23433;&#20840;&#21644;&#32593;&#32476;&#23433;&#20840;&#65292;&#35299;&#20915;&#20102;SOME/IP&#32570;&#20047;&#23433;&#20840;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#30828;&#20214;&#25925;&#38556;&#21644;&#20116;&#31181;&#28508;&#22312;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.14862</link><description>&lt;p&gt;
SISSA: &#23454;&#26102;&#30417;&#25511;&#36710;&#36733;SOME/IP&#20197;&#22826;&#32593;&#27969;&#37327;&#30340;&#30828;&#20214;&#21151;&#33021;&#23433;&#20840;&#21644;&#32593;&#32476;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14862
&lt;/p&gt;
&lt;p&gt;
SISSA&#25552;&#20986;&#20102;&#22522;&#20110;SOME/IP&#36890;&#20449;&#27969;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#20998;&#26512;&#36710;&#36733;&#21151;&#33021;&#23433;&#20840;&#21644;&#32593;&#32476;&#23433;&#20840;&#65292;&#35299;&#20915;&#20102;SOME/IP&#32570;&#20047;&#23433;&#20840;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#30828;&#20214;&#25925;&#38556;&#21644;&#20116;&#31181;&#28508;&#22312;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Scalable Service-Oriented Middleware over IP&#65288;SOME/IP&#65289;&#26159;&#27773;&#36710;&#24320;&#25918;&#31995;&#32479;&#26550;&#26500;&#65288;AUTOSAR&#65289;&#20013;&#30340;&#20197;&#22826;&#32593;&#36890;&#20449;&#26631;&#20934;&#21327;&#35758;&#65292;&#20419;&#36827;ECU&#19982;ECU&#20043;&#38388;&#36890;&#36807;IP&#22534;&#26632;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;SOME/IP&#32570;&#20047;&#20581;&#22766;&#30340;&#23433;&#20840;&#26550;&#26500;&#65292;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;ECU&#30340;&#38543;&#26426;&#30828;&#20214;&#25925;&#38556;&#20250;&#30772;&#22351;SOME/IP&#36890;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SISSA&#65292;&#19968;&#31181;&#22522;&#20110;SOME/IP&#36890;&#20449;&#27969;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#20998;&#26512;&#36710;&#36733;&#21151;&#33021;&#23433;&#20840;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SISSA&#20351;&#29992;Weibull&#20998;&#24067;&#23545;&#30828;&#20214;&#25925;&#38556;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35299;&#20915;&#21253;&#25324;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#12289;&#20013;&#38388;&#20154;&#21644;&#24322;&#24120;&#36890;&#20449;&#27969;&#31243;&#22312;&#20869;&#30340;&#20116;&#31181;&#28508;&#22312;SOME/IP&#36890;&#20449;&#25915;&#20987;&#65292;&#20551;&#35774;&#24694;&#24847;&#29992;&#25143;&#35775;&#38382;&#36710;&#36733;&#32593;&#32476;&#12290;&#38543;&#21518;&#65292;SISSA&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#20027;&#24178;&#32467;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;SOM&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14862v1 Announce Type: cross  Abstract: Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet communication standard protocol in the Automotive Open System Architecture (AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However, SOME/IP lacks a robust security architecture, making it susceptible to potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP communication. In this paper, we propose SISSA, a SOME/IP communication traffic-based approach for modeling and analyzing in-vehicle functional safety and cyber security. Specifically, SISSA models hardware failures with the Weibull distribution and addresses five potential attacks on SOME/IP communication, including Distributed Denial-of-Services, Man-in-the-Middle, and abnormal communication processes, assuming a malicious user accesses the in-vehicle network. Subsequently, SISSA designs a series of deep learning models with various backbones to extract features from SOM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;CloudNine&#8221;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#27668;&#35937;&#35266;&#27979;&#23545;&#29305;&#23450;&#22825;&#27668;&#39044;&#27979;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2402.14861</link><description>&lt;p&gt;
CloudNine&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#27668;&#35937;&#35266;&#27979;&#23545;&#22825;&#27668;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;CloudNine&#8221;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#27668;&#35937;&#35266;&#27979;&#23545;&#29305;&#23450;&#22825;&#27668;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#35937;&#35266;&#27979;&#23545;&#22825;&#27668;&#39044;&#25253;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#20256;&#24863;&#22120;&#31867;&#22411;&#12289;&#20301;&#32622;&#12289;&#26102;&#38388;&#21644;&#20854;&#20182;&#29615;&#22659;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#23450;&#37327;&#20998;&#26512;&#35266;&#27979;&#24433;&#21709;&#23545;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;CloudNine&#8221;&#30340;&#26032;&#31995;&#32479;&#65292;&#22522;&#20110;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;XGNNs&#65289;&#20998;&#26512;&#21333;&#20010;&#35266;&#27979;&#23545;&#29305;&#23450;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#23558;&#22522;&#20110;XGNN&#30340;&#22823;&#27668;&#29366;&#24577;&#20272;&#35745;&#27169;&#22411;&#19982;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#22312;&#22320;&#29699;&#31995;&#32479;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#25628;&#32034;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14861v1 Announce Type: cross  Abstract: The impact of meteorological observations on weather forecasting varies with sensor type, location, time, and other environmental factors. Thus, quantitative analysis of observation impacts is crucial for effective and efficient development of weather forecasting systems. However, the existing impact analysis methods are difficult to be widely applied due to their high dependencies on specific forecasting systems. Also, they cannot provide observation impacts at multiple spatio-temporal scales, only global impacts of observation types. To address these issues, we present a novel system called ``CloudNine,'' which allows analysis of individual observations' impacts on specific predictions based on explainable graph neural networks (XGNNs). Combining an XGNN-based atmospheric state estimation model with a numerical weather prediction model, we provide a web application to search for observations in the 3D space of the Earth system and to
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.14859</link><description>&lt;p&gt;
&#20869;&#22312;&#30340;&#29436;&#65306;&#36890;&#36807;MLLM&#25805;&#20316;&#21592;&#21521;MLLM&#31038;&#20250;&#20013;&#28183;&#20837;&#24694;&#24847;
&lt;/p&gt;
&lt;p&gt;
The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14859
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#22788;&#29702;&#21644;&#21709;&#24212;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#65292;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#19981;&#26029;&#23450;&#20041;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26032;&#36793;&#30028;&#12290;&#38543;&#30528;&#36825;&#20123;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24418;&#25104;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#30340;&#21327;&#20316;&#32593;&#32476;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#12298;&#20869;&#22312;&#30340;&#29436;&#12299;&#25506;&#35752;&#20102;MLLM&#31038;&#20250;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#28431;&#27934; - &#24694;&#24847;&#20869;&#23481;&#30340;&#38388;&#25509;&#20256;&#25773;&#12290;&#19982;&#30452;&#25509;&#20026;MLLM&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#20010;MLLM&#20195;&#29702;&#22914;&#20309;&#34987;&#24494;&#22937;&#22320;&#24433;&#21709;&#65292;&#20197;&#29983;&#25104;&#20877;&#27425;&#35825;&#20351;&#31038;&#20250;&#20013;&#20854;&#20182;MLLM&#20195;&#29702;&#36755;&#20986;&#24694;&#24847;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#36825;&#31181;&#24494;&#22937;&#32780;&#24378;&#26377;&#21147;&#30340;&#38388;&#25509;&#24433;&#21709;&#26041;&#27861;&#26631;&#24535;&#30528;&#19982;MLLM&#30456;&#20851;&#30340;&#23433;&#20840;&#39118;&#38505;&#30340;&#26174;&#33879;&#21319;&#32423;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#20960;&#20046;&#27809;&#26377;&#25110;&#26159;&#26681;&#26412;&#27809;&#26377;&#35775;&#38382;MLLM&#21442;&#25968;&#65292;&#19968;&#20010;MLLM&#20195;&#29702;&#65292;&#24403;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#22312;&#31243;&#24207;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;HumanEval&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#26356;&#22810;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14852</link><description>&lt;p&gt;
&#26368;&#26032;GPT&#27169;&#22411;&#19978;&#30340;HumanEval -- 2024
&lt;/p&gt;
&lt;p&gt;
HumanEval on Latest GPT Models -- 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14852
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#22312;&#31243;&#24207;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;HumanEval&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#26356;&#22810;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;&#65292;&#25105;&#20204;&#27491;&#22312;&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#26469;&#25512;&#36827;&#31243;&#24207;&#21512;&#25104;&#12290;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26174;&#33879;&#25913;&#36827;&#20102;&#36825;&#19968;&#30446;&#30340;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#36827;&#23637;&#26356;&#26131;&#20110;&#35775;&#38382;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#23558;&#36825;&#20123;&#27169;&#22411;&#36830;&#25509;&#21040;Human Eval&#30340;&#23384;&#20648;&#24211;&#12290;&#35813;&#25968;&#25454;&#38598;&#26368;&#21021;&#26159;&#20026;&#19982;&#21517;&#20026;CODEGEN&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#25968;&#25454;&#19978;&#20351;&#29992;&#32780;&#24320;&#21457;&#30340;&#12290;&#36890;&#36807;&#23637;&#31034;&#36825;&#20123;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#19982;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#22312;HumanEval&#20219;&#21153;&#19978;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20026;&#24320;&#21457;&#26356;&#22810;&#30340;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#21019;&#36896;&#20102;&#21487;&#33021;&#12290;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;160&#20010;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#38598;&#65292;&#36825;&#20123;&#38382;&#39064;&#38598;&#34987;&#20998;&#35299;&#25104;&#22810;&#27493;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36825;&#26174;&#33879;&#25913;&#36827;&#20102;&#21333;&#36718;&#36755;&#20837;&#19978;&#30340;&#31243;&#24207;&#32508;&#21512;&#12290;&#25152;&#26377;&#20195;&#30721;&#22343;&#20197;&#24320;&#28304;&#26041;&#24335;&#21457;&#24067;&#22312;https://github.com/daniel442li/gpt-human-eval&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14852v1 Announce Type: cross  Abstract: In 2023, we are using the latest models of GPT-4 to advance program synthesis. The large language models have significantly improved the state-of-the-art for this purpose. To make these advancements more accessible, we have created a repository that connects these models to Huamn Eval. This dataset was initally developed to be used with a language model called CODEGEN on natural and programming language data. The utility of these trained models is showcased by demonstrating their competitive performance in zero-shot Python code generation on HumanEval tasks compared to previous state-of-the-art solutions. Additionally, this gives way to developing more multi-step paradigm synthesis. This benchmark features 160 diverse problem sets factorized into multistep prompts that our analysis shows significantly improves program synthesis over single-turn inputs. All code is open source at https://github.com/daniel442li/gpt-human-eval .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14849</link><description>&lt;p&gt;
&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#23545;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Asynchronous and Segmented Bidirectional Encoding for NMT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#36136;&#37327;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#23454;&#26045;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#35299;&#30721;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#20174;&#24038;&#21040;&#21491;&#25110;&#20174;&#21491;&#21040;&#24038;&#30340;&#21333;&#21521;&#32763;&#35793;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#21477;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;IWSLT2017&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21152;&#36895;&#32763;&#35793;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21333;&#21521;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14849v1 Announce Type: cross  Abstract: With the rapid advancement of Neural Machine Translation (NMT), enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the Transformer in various aspects, they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the Transformer, implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left, our method demonstrates heightened efficiency and improved translation quality, particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy, especially surpassing traditional unidirection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#26426;&#38382;&#39064;&#35843;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#26368;&#20339;&#38382;&#39064;&#21010;&#20998;&#26041;&#24335;&#20197;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#12290;</title><link>https://arxiv.org/abs/2402.14847</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#26426;&#38382;&#39064;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#30340;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep learning-driven scheduling algorithm for a single machine problem minimizing the total tardiness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#26426;&#38382;&#39064;&#35843;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#26368;&#20339;&#38382;&#39064;&#21010;&#20998;&#26041;&#24335;&#20197;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#19968;&#20010;&#33879;&#21517;&#30340;NP&#38590;&#39064;&#21333;&#26426;&#35843;&#24230;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#20272;&#35745;&#37327;&#26469;&#29992;&#20110;&#22522;&#20110;Lawler&#20998;&#35299;&#21644;Della Croce&#31561;&#20154;&#25552;&#20986;&#30340;&#23545;&#31216;&#20998;&#35299;&#30340;&#21333;&#36941;&#35843;&#24230;&#31639;&#27861;&#12290;&#23454;&#36136;&#19978;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20272;&#35745;&#38382;&#39064;&#21010;&#20998;&#20026;&#23376;&#38382;&#39064;&#30340;&#26368;&#20339;&#26041;&#24335;&#26469;&#24341;&#23548;&#31639;&#27861;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#20943;&#23569;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#22343;&#26368;&#20248;&#24615;&#24046;&#36317;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#35757;&#32451;&#38454;&#27573;&#27010;&#25324;&#20449;&#24687;&#21040;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#20363;&#12290;&#21363;&#20351;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#23454;&#20363;&#20174;75
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14847v1 Announce Type: cross  Abstract: In this paper, we investigate the use of the deep learning method for solving a well-known NP-hard single machine scheduling problem with the objective of minimizing the total tardiness. We propose a deep neural network that acts as a polynomial-time estimator of the criterion value used in a single-pass scheduling algorithm based on Lawler's decomposition and symmetric decomposition proposed by Della Croce et al. Essentially, the neural network guides the algorithm by estimating the best splitting of the problem into subproblems. The paper also describes a new method for generating the training data set, which speeds up the training dataset generation and reduces the average optimality gap of solutions. The experimental results show that our machine learning-driven approach can efficiently generalize information from the training phase to significantly larger instances. Even though the instances used in the training phase have from 75
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20445;&#25345;&#20854;&#24615;&#33021;&#24182;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.14845</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Purifying Large Language Models by Ensembling a Small Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14845
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20445;&#25345;&#20854;&#24615;&#33021;&#24182;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20174;&#22806;&#37096;&#65288;&#19981;&#21463;&#20449;&#20219;&#65289;&#26469;&#28304;&#25910;&#38598;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;&#21644;&#31934;&#24515;&#31574;&#21010;&#65292;&#20294;&#24050;&#26377;&#25253;&#36947;&#26174;&#31034;&#26500;&#24314;&#33391;&#22909;&#30340;LLMs&#23384;&#22312;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;/&#25110;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#65292;&#36825;&#23558;&#38459;&#30861;LLMs&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#33391;&#24615;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#38598;&#25104;&#26469;&#20928;&#21270;LLMs&#20813;&#21463;&#26410;&#32463;&#31579;&#36873;&#25968;&#25454;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#20174;&#32463;&#39564;&#35777;&#23454;&#65292;LLMs&#19982;SLMs&#38598;&#25104;&#21487;&#20197;&#26377;&#25928;&#20445;&#25345;LLMs&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14845v1 Announce Type: cross  Abstract: The emerging success of large language models (LLMs) heavily relies on collecting abundant training data from external (untrusted) sources. Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs. In this study, we propose a simple and easily implementable method for purifying LLMs from the negative effects caused by uncurated data, namely, through ensembling LLMs with benign and small language models (SLMs). Aside from theoretical guarantees, we perform comprehensive experiments to empirically confirm the efficacy of ensembling LLMs with SLMs, which can effectively preserve the performance of LLMs while mitigating issues such as copyright infringement, data poisoning, and privacy violations.
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#30417;&#30563;&#23398;&#20064;&#21644;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#27773;&#36710;&#31199;&#36161;&#34892;&#19994;&#30340;&#21160;&#24577;&#23450;&#20215;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21033;&#28070;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14844</link><description>&lt;p&gt;
&#21160;&#24577;&#23450;&#20215;&#30340;&#26032;&#26102;&#20195;&#65306;&#21327;&#21516;&#30417;&#30563;&#23398;&#20064;&#19982;&#20108;&#27425;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
The New Era of Dynamic Pricing: Synergizing Supervised Learning and Quadratic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14844
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#30417;&#30563;&#23398;&#20064;&#21644;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#27773;&#36710;&#31199;&#36161;&#34892;&#19994;&#30340;&#21160;&#24577;&#23450;&#20215;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21033;&#28070;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#20108;&#27425;&#35268;&#21010;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#29992;&#20110;&#22312;&#27773;&#36710;&#31199;&#36161;&#34892;&#19994;&#20013;&#20248;&#21270;&#21160;&#24577;&#23450;&#20215;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#20215;&#26684;&#24377;&#24615;&#30340;&#21160;&#24577;&#24314;&#27169;&#65292;&#20511;&#21161;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#25351;&#26631;&#65292;&#22914;P&#20540;&#12289;&#21516;&#26041;&#24046;&#24615;&#12289;&#35823;&#24046;&#27491;&#24577;&#24615;&#36827;&#34892;&#20248;&#21270;&#12290;&#24403;&#36825;&#20123;&#25351;&#26631;&#30340;&#22522;&#26412;&#20551;&#35774;&#25104;&#31435;&#26102;&#65292;&#23427;&#20204;&#23545;&#25351;&#23548;&#20108;&#27425;&#35268;&#21010;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#31243;&#24207;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#26377;&#38480;&#30446;&#26631;&#38598;&#20248;&#21270;&#21033;&#28070;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14844v1 Announce Type: cross  Abstract: In this paper, we explore a novel combination of supervised learning and quadratic programming to refine dynamic pricing models in the car rental industry. We utilize dynamic modeling of price elasticity, informed by ordinary least squares (OLS) metrics such as p-values, homoscedasticity, error normality. These metrics, when their underlying assumptions hold, are integral in guiding a quadratic programming agent. The program is tasked with optimizing margin for a given finite set target.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREC&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#21644;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14843</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#35843;&#33410;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text Diffusion with Reinforced Conditioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14843
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREC&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#21644;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#30001;&#20110;&#22312;&#36845;&#20195;&#25913;&#36827;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#23427;&#20204;&#23545;&#23454;&#29616;&#26356;&#22909;&#30340;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#29983;&#25104;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22788;&#29702;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#30340;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#23545;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TREC&#30340;&#26032;&#22411;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#32531;&#35299;&#20102;&#36864;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TREC&#22312;&#33258;&#22238;&#24402;&#12289;&#38750;&#33258;&#22238;&#24402;&#21644;&#25193;&#25955;&#22522;&#32447;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#23450;&#24615;&#20998;&#26512;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14843v1 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis sh
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24230;&#30340;AI&#27169;&#22411;&#65292;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14838</link><description>&lt;p&gt;
RFBES&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#24212;&#29992;&#65306;&#25506;&#31350;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24230;&#30340;AI&#27169;&#22411;&#65292;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#24182;&#19988;LLMs&#24050;&#34987;&#29992;&#20110;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#19981;&#21516;&#20219;&#21153;&#20013;&#29983;&#25104;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35895;&#27468;&#21644;OpenAI&#31561;&#30693;&#21517;&#20844;&#21496;&#30340;&#21442;&#19982;&#65292;LLMs&#29616;&#22312;&#26356;&#26131;&#33719;&#24471;&#65292;&#20154;&#20204;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#21306;&#21035;&#12290;&#26412;&#25991;&#20174;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#38382;&#39064;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#39640;&#20934;&#30830;&#24230;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#65292;&#26080;&#35770;&#26159;&#22810;&#35821;&#35328;&#36824;&#26159;&#21333;&#35821;&#20219;&#21153;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#20351;&#29992;&#35821;&#20041;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#26356;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22312;&#21477;&#27861;&#26041;&#27861;&#19978;&#36824;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#23558;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#19968;&#20010;&#33391;&#22909;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14838v1 Announce Type: cross  Abstract: Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs have been used to generate texts in different languages and for different tasks. Additionally, due to the participation of remarkable companies such as Google and OpenAI, LLMs are now more accessible, and people can easily use them. However, an important issue is how we can detect AI-generated texts from human-written ones. In this article, we have investigated the problem of AI-generated text detection from two different aspects: semantics and syntax. Finally, we presented an AI model that can distinguish AI-generated texts from human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset. According to our results, using a semantic approach would be more helpful for detection. However, there is a lot of room for improvement in the syntactic approach, and it would be a good approach for future work.
&lt;/p&gt;</description></item><item><title>&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14837</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#30340;&#23454;&#35777;&#20998;&#31867;&#65306;&#20174;&#19994;&#32773;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14837
&lt;/p&gt;
&lt;p&gt;
&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#36817;&#29992;&#25552;&#31034;&#35821;&#26469;&#32534;&#31243;&#36825;&#20123;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#23545;&#20110;&#24076;&#26395;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;LLMs&#65292;&#32534;&#21046;&#19968;&#20010;&#20840;&#38754;&#30340;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#24182;&#24314;&#31435;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#36328;&#23398;&#31185;&#20998;&#31867;&#26694;&#26550;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#19968;&#20123;&#26368;&#30693;&#21517;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#23398;&#26415;&#21644;&#23454;&#36341;&#35282;&#24230;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#19971;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#27599;&#20010;&#31867;&#21035;&#65292;&#26088;&#22312;&#28548;&#28165;&#23427;&#20204;&#30340;&#29420;&#29305;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20197;&#20026;&#21516;&#34892;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#26694;&#26550;&#65292;&#24110;&#21161;&#20182;&#20204;&#29702;&#35299;&#21644;&#24402;&#31867;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14837v1 Announce Type: cross  Abstract: Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniqu
&lt;/p&gt;</description></item><item><title>MIKE&#26159;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#22522;&#20934;&#20027;&#35201;&#20391;&#37325;&#20110;&#31895;&#31890;&#24230;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#24418;&#24335;&#20197;&#35780;&#20272;&#32534;&#36753;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14835</link><description>&lt;p&gt;
MIKE&#65306;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14835
&lt;/p&gt;
&lt;p&gt;
MIKE&#26159;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#22522;&#20934;&#20027;&#35201;&#20391;&#37325;&#20110;&#31895;&#31890;&#24230;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#24418;&#24335;&#20197;&#35780;&#20272;&#32534;&#36753;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#32534;&#36753;&#26159;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21151;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#24403;&#21069;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#30693;&#35782;&#19978;&#65292;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#22823;&#22810;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MIKE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14835v1 Announce Type: cross  Abstract: Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations
&lt;/p&gt;</description></item><item><title>CliqueParcel&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14833</link><description>&lt;p&gt;
CliqueParcel&#65306;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#25928;&#29575;&#21644;&#24544;&#23454;&#24230;&#30340;&#25209;&#22788;&#29702;LLM&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14833
&lt;/p&gt;
&lt;p&gt;
CliqueParcel&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;LLM&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CliqueParcel&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20248;&#21270;&#25512;&#29702;&#25928;&#29575;&#30340;&#31574;&#30053;&#36890;&#24120;&#20250;&#23545;&#36755;&#20986;&#36136;&#37327;&#36827;&#34892;&#22949;&#21327;&#65292;&#23548;&#33268;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#25110;&#36755;&#20986;&#32570;&#20047;&#32454;&#33410;&#12290;CliqueParcel&#26159;&#25105;&#20204;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#22238;&#24212;&#12290;&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65288;&#21363;&#24544;&#23454;&#24230;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#20026;&#20102;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25490;&#38500;&#30001;&#20110;&#38271;&#24230;&#32553;&#30701;&#32780;&#23548;&#33268;&#30340;&#36816;&#34892;&#26102;&#38388;&#20943;&#23569;&#26469;&#37325;&#26032;&#23450;&#20041;&#25928;&#29575;&#27979;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25928;&#29575;&#21644;&#24544;&#23454;&#24230;&#20043;&#38388;&#30340;&#20840;&#38754;&#26435;&#34913;&#65292;&#20197;&#38416;&#26126;&#8220;&#25240;&#20215;&#36755;&#20986;&#8221;&#38382;&#39064;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14833v1 Announce Type: cross  Abstract: Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference.   To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31383;&#21475;&#26041;&#27861;&#21644;&#33410;&#28857;&#20248;&#21270;&#20998;&#26512;EHG&#20449;&#21495;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20248;&#21270;&#22922;&#23072;&#21644;&#20998;&#23081;&#20013;&#30340;&#23376;&#23467;&#21516;&#27493;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.14827</link><description>&lt;p&gt;
&#36890;&#36807;&#31383;&#21475;&#36873;&#25321;&#21644;&#33410;&#28857;&#20248;&#21270;&#20248;&#21270;&#22922;&#23072;&#21644;&#20998;&#23081;&#20013;&#30340;&#23376;&#23467;&#21516;&#27493;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Optimizing Uterine Synchronization Analysis in Pregnancy and Labor through Window Selection and Node Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31383;&#21475;&#26041;&#27861;&#21644;&#33410;&#28857;&#20248;&#21270;&#20998;&#26512;EHG&#20449;&#21495;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20248;&#21270;&#22922;&#23072;&#21644;&#20998;&#23081;&#20013;&#30340;&#23376;&#23467;&#21516;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#20135;&#26159;&#20840;&#29699;5&#23681;&#20197;&#19979;&#20799;&#31461;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35760;&#24405;&#22312;&#21171;&#21160;&#21644;&#22922;&#23072;&#26399;&#38388;&#27597;&#20146;&#33145;&#37096;&#30340;EHG&#20449;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;EHG&#20449;&#21495;&#21453;&#26144;&#20102;&#35825;&#23548;&#23376;&#23467;&#32908;&#30005;&#26426;&#25910;&#32553;&#30340;&#30005;&#27963;&#21160;&#12290;&#30001;&#20110;EHG&#20449;&#21495;&#34987;&#35748;&#20026;&#26159;&#38750;&#24179;&#31283;&#20449;&#21495;&#65292;&#24182;&#19988;&#25105;&#20204;&#39044;&#26399;&#22312;&#25910;&#32553;&#36807;&#31243;&#20013;&#36830;&#25509;&#24615;&#20250;&#21457;&#29983;&#25913;&#21464;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#31383;&#21475;&#26041;&#27861;&#22312;&#23454;&#38469;&#20449;&#21495;&#19978;&#65292;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#29992;&#20110;&#20998;&#31867;&#30340;&#26368;&#20339;&#31383;&#21475;&#21644;&#26368;&#20339;&#33410;&#28857;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14827v1 Announce Type: cross  Abstract: Preterm labor (PL) has globally become the leading cause of death in children under the age of 5 years. To address this problem, this paper will provide a new approach by analyzing the EHG signals, which are recorded on the abdomen of the mother during labor and pregnancy. The EHG signal reflects the electrical activity that induces the mechanical contraction of the myometrium. Because EHGs are known to be non-stationary signals, and because we anticipate connectivity to alter during contraction, we applied the windowing approach on real signals to help us identify the best windows and the best nodes with the most significant data to be used for classification. The suggested pipeline includes i) divide the 16 EHG signals that are recorded from the abdomen of pregnant women in N windows; ii) apply the connectivity matrices on each window; iii) apply the Graph theory-based measures on the connectivity matrices on each window; iv) apply t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25506;&#32034;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14825</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21450;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Deepfake Detection and the Impact of Limited Computing Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25506;&#32034;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;deepfakes&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35782;&#21035;&#25216;&#26415;&#12290;&#20026;&#20102;&#30830;&#20445;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#65292;&#25511;&#21046;&#34394;&#20551;&#20449;&#24687;&#21644;&#22823;&#35268;&#27169;&#25805;&#32437;&#65292;&#21457;&#29616;&#24182;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#22330;&#26223;&#19979;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#28145;&#24230;&#20266;&#36896;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#20998;&#26512;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#36825;&#20123;&#38480;&#21046;&#26465;&#20214;&#19979;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25506;&#32034;&#25552;&#39640;&#25928;&#29575;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14825v1 Announce Type: cross  Abstract: The rapid development of technologies and artificial intelligence makes deepfakes an increasingly sophisticated and challenging-to-identify technique. To ensure the accuracy of information and control misinformation and mass manipulation, it is of paramount importance to discover and develop artificial intelligence models that enable the generic detection of forged videos. This work aims to address the detection of deepfakes across various existing datasets in a scenario with limited computing resources. The goal is to analyze the applicability of different deep learning techniques under these restrictions and explore possible approaches to enhance their efficiency.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.14601</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#25945;&#32946;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bringing Generative AI to Adaptive Learning in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14601
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#28608;&#22686;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21457;&#23637;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#36825;&#19968;&#27010;&#24565;&#22312;&#25945;&#32946;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#39046;&#22495;&#30340;&#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#28508;&#21147;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#32467;&#21512;&#23558;&#20026;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#24418;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#33021;&#22815;&#36827;&#34892;&#26799;&#24230;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#25216;&#26415;&#23548;&#20986;&#20102;&#23545;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#37327;&#21270;&#20102;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#24102;&#26469;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14434</link><description>&lt;p&gt;
&#24182;&#34892;&#20013;&#28857;&#38543;&#26426;&#21270;&#30340; Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Parallelized Midpoint Randomization for Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14434
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#33021;&#22815;&#36827;&#34892;&#26799;&#24230;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#25216;&#26415;&#23548;&#20986;&#20102;&#23545;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#37327;&#21270;&#20102;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#24102;&#26469;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#21487;&#20197;&#36827;&#34892;&#26799;&#24230;&#30340;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#30001;&#24179;&#28369;&#21644;&#24378;log-&#20985;&#23494;&#24230;&#34920;&#24449;&#30340;&#30446;&#26631;&#20998;&#24067;&#19978;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36816;&#29992;&#26368;&#36817;&#24320;&#21457;&#29992;&#20110;&#20998;&#26512;&#20854;&#32431;&#39034;&#24207;&#29256;&#26412;&#30340;&#35777;&#26126;&#25216;&#26415;&#12290;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#30028;&#38480;&#37327;&#21270;&#20102;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#25152;&#23454;&#29616;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#65292;&#36825;&#21487;&#33021;&#26159;&#30456;&#24403;&#21487;&#35266;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14434v1 Announce Type: cross  Abstract: We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible. Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities. We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version. Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities. These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#30340;&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#28508;&#31354;&#38388;&#22810;&#32423;&#30417;&#30563;&#32593;&#32476;(SPDNet)&#65292;&#22686;&#24378;&#20998;&#21106;&#20197;&#26356;&#20934;&#30830;&#20272;&#35745;EAT&#20307;&#31215;</title><link>https://arxiv.org/abs/2402.14349</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#30340;&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#30340;&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#28508;&#31354;&#38388;&#22810;&#32423;&#30417;&#30563;&#32593;&#32476;(SPDNet)&#65292;&#22686;&#24378;&#20998;&#21106;&#20197;&#26356;&#20934;&#30830;&#20272;&#35745;EAT&#20307;&#31215;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;(EAT)&#26159;&#19968;&#31181;&#21487;&#20197;&#20998;&#27852;&#22823;&#37327;&#33026;&#32852;&#32032;&#20174;&#32780;&#24433;&#21709;&#24515;&#32908;&#21644;&#20896;&#29366;&#21160;&#33033;&#30340;&#20869;&#33039;&#33026;&#32938;&#12290;EAT&#30340;&#20307;&#31215;&#21644;&#23494;&#24230;&#21487;&#20197;&#20316;&#20026;&#29420;&#31435;&#39118;&#38505;&#26631;&#35760;&#30340;&#27979;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30913;&#20849;&#25391;&#22270;&#20687;&#27979;&#37327;&#20307;&#31215;&#26159;&#35780;&#20272;EAT&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;EAT&#19982;&#24515;&#21253;&#31215;&#28082;&#20043;&#38388;&#23545;&#27604;&#24230;&#20302;&#20197;&#21450;&#36816;&#21160;&#20266;&#24433;&#30340;&#23384;&#22312;&#65292;&#20998;&#21106;EAT&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#28508;&#31354;&#38388;&#22810;&#32423;&#30417;&#30563;&#32593;&#32476;(SPDNet)&#65292;&#37319;&#29992;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#20197;&#22686;&#24378;&#20998;&#21106;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;EAT&#20307;&#31215;&#12290;&#32593;&#32476;&#39318;&#20808;&#36890;&#36807;&#22312;&#29305;&#24449;&#28508;&#31354;&#38388;&#20013;&#23558;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#20026;&#39640;&#26031;&#20998;&#24067;&#26469;&#35299;&#20915;&#30001;&#20110;&#24320;&#25918;&#24335;&#21307;&#30103;&#29615;&#22659;&#20013;&#21307;&#23398;&#22270;&#20687;&#30340;&#20302;&#36136;&#37327;&#25110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#32780;&#23548;&#33268;EAT&#36793;&#32536;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#20854;&#36125;&#21494;&#26031;&#20272;&#35745;&#20316;&#20026;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14349v1 Announce Type: cross  Abstract: Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularizatio
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;</title><link>https://arxiv.org/abs/2402.14285</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#25193;&#25955;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14285
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#29983;&#25104;&#38050;&#29748;&#21367;&#35889;&#65289;&#65292;&#25216;&#26415;&#37325;&#28857;&#25918;&#22312;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#19978;&#12290;&#38899;&#20048;&#35268;&#21017;&#36890;&#24120;&#20197;&#31526;&#21495;&#24418;&#24335;&#34920;&#36798;&#22312;&#38899;&#31526;&#29305;&#24449;&#19978;&#65292;&#22914;&#38899;&#31526;&#23494;&#24230;&#25110;&#21644;&#24358;&#36827;&#34892;&#65292;&#35768;&#22810;&#35268;&#21017;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#22312;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#24341;&#23548;&#25193;&#25955;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;&#25511;&#21046;&#24341;&#23548;&#65288;SCG&#65289;&#65292;&#23427;&#20165;&#38656;&#35201;&#23545;&#35268;&#21017;&#20989;&#25968;&#36827;&#34892;&#21069;&#21521;&#35780;&#20272;&#65292;&#21487;&#20197;&#19982;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#19968;&#36215;&#24037;&#20316;&#65292;&#20174;&#32780;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#30340;&#26080;&#35757;&#32451;&#24341;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#21487;&#20197;&#19982;SCG&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#32452;&#21512;&#12290;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#26631;&#20934;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14148</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19982;&#25705;&#25830;&#65306;&#28369;&#21160;&#12289;&#20445;&#25345;&#12289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Networks and Friction: Slide, Hold, Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14148
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#26550;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20855;&#26377;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#23450;&#24459;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;&#29992;&#20110;&#35757;&#32451;&#32593;&#32476;&#30340;&#25968;&#25454;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#26041;&#31243;&#32467;&#21512;&#29366;&#24577;&#28436;&#21270;&#32769;&#21270;&#23450;&#24459;&#29983;&#25104;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#21046;&#23450;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26126;&#30830;&#32771;&#34385;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#26465;&#20214;&#12289;&#30452;&#25509;&#25928;&#24212;&#20197;&#21450;&#29366;&#24577;&#21464;&#37327;&#30340;&#28436;&#21464;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;GRU&#26550;&#26500;&#30340;RNN&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#39044;&#27979;&#25705;&#25830;&#31995;&#25968;&#30001;&#20110;&#36895;&#24230;&#36339;&#36291;&#32780;&#20135;&#29983;&#30340;&#21464;&#21270;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14148v1 Announce Type: cross  Abstract: In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#27979;&#37327;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Explainable Classification Techniques for Quantum Dot Device Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#31283;&#20581;&#29305;&#24449;&#34920;&#31034;&#38656;&#27714;&#22686;&#21152;&#65306;&#22270;&#20687;&#37319;&#38598;&#65292;&#22312;&#24191;&#20041;&#19978;&#25351;&#20108;&#32500;&#25968;&#25454;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;&#27492;&#32771;&#34385;&#30340;&#37327;&#23376;&#20449;&#24687;&#31185;&#23398;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#20256;&#32479;&#22270;&#20687;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#27491;&#22312;&#36805;&#36895;&#34987;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#25152;&#21462;&#20195;&#65292;&#21518;&#32773;&#24448;&#24448;&#20197;&#29306;&#29298;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#25442;&#21462;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23637;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#30410;&#22788;&#65292;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.13352</link><description>&lt;p&gt;
KetGPT -- &#20351;&#29992;Transformer&#23545;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#65292;&#34920;&#31034;&#20026;&#37327;&#23376;&#30005;&#36335;&#65292;&#21487;&#29992;&#20316;&#35780;&#20272;&#37327;&#23376;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#65292;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#30005;&#36335;&#24182;&#19981;&#26159;&#20195;&#34920;&#24615;&#22522;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#37327;&#23376;&#31995;&#32479;&#21046;&#36896;&#30340;&#30495;&#23454;&#37327;&#23376;&#31639;&#27861;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#8220;&#26377;&#29992;&#8221;&#30340;&#37327;&#23376;&#22522;&#20934;&#26500;&#25104;&#20102;&#25512;&#21160;&#37327;&#23376;&#32534;&#35793;&#22120;&#21644;&#30828;&#20214;&#24320;&#21457;&#19982;&#27604;&#36739;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KetGPT&#65292;&#19968;&#31181;&#20197;OpenQASM&#35821;&#35328;&#29983;&#25104;&#21512;&#25104;&#30005;&#36335;&#30340;&#24037;&#20855;&#65292;&#20854;&#32467;&#26500;&#26159;&#22522;&#20110;&#25512;&#23548;&#33258;&#37327;&#23376;&#30005;&#36335;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#22797;&#26434;&#35821;&#20041;bug&#65292;&#36890;&#36807;&#26032;&#30340;&#26597;&#35810;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#36317;&#31163;&#20195;&#30721;&#20851;&#31995;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13291</link><description>&lt;p&gt;
DeepCode AI Fix: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13291
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#22797;&#26434;&#35821;&#20041;bug&#65292;&#36890;&#36807;&#26032;&#30340;&#26597;&#35810;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#36317;&#31163;&#20195;&#30721;&#20851;&#31995;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#39046;&#22495;&#33021;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#23613;&#31649;&#26377;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#65292;&#21019;&#24314;&#19968;&#20010;&#23545;&#20110;&#22797;&#26434;&#35821;&#20041;&#38169;&#35823;&#65288;&#22914;&#23433;&#20840;&#28431;&#27934;&#65289;&#25928;&#26524;&#33391;&#22909;&#30340;&#31995;&#32479;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#26159;&#21033;&#29992;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#32534;&#31243;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#20195;&#30721;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#23398;&#20064;&#38271;&#36317;&#31163;&#30340;&#20195;&#30721;&#20851;&#31995;&#65292;&#36825;&#26159;&#19968;&#20010;&#22825;&#28982;&#20381;&#36182;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#22797;&#26434;&#31243;&#24207;&#38169;&#35823;&#21644;&#20854;&#23545;&#24212;&#20462;&#22797;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#19988;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#24182;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#21644;&#24494;&#35843;LLMs&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#21033;&#29992;&#31243;&#24207;&#20998;&#26512;&#26469;&#38480;&#21046;LLMs&#30340;&#20851;&#27880;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13291v1 Announce Type: cross  Abstract: The automated program repair field has attracted substantial interest over the years, but despite significant research efforts, creating a system that works well for complex semantic bugs such as security vulnerabilities has proven difficult. A promising direction to solve this challenge is by leveraging large language models (LLMs), which are increasingly used to solve various programming tasks. In this paper, we investigate the effectiveness of LLMs for solving code-repair task. We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a large, clean dataset for complex program bugs and their corresponding fixes is non-trivial. We propose a technique to address these challenges with a new approach for querying and fine-tuning LLMs. The idea is to use program analysis to limit the LLM's attention me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.13277</link><description>&lt;p&gt;
&#20351;&#29992;SMOTETomek&#22312;WSNs&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek in WSNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13277
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;(WSNs)&#22312;&#22522;&#30784;&#35774;&#26045;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#22266;&#23450;&#21644;&#31227;&#21160;&#20256;&#24863;&#22120;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#33258;&#32452;&#32455;&#24182;&#24314;&#31435;&#22810;&#36339;&#36830;&#25509;&#36827;&#34892;&#36890;&#20449;&#65292;&#20849;&#21516;&#24863;&#30693;&#12289;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20256;&#36755;&#26377;&#20851;&#21608;&#22260;&#29615;&#22659;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#65292;WSNs&#38754;&#20020;&#30528;&#21487;&#33021;&#30772;&#22351;&#21151;&#33021;&#30340;&#24555;&#36895;&#21644;&#26377;&#23475;&#30340;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;WSN&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#36935;&#21040;&#20102;&#20302;&#26816;&#27979;&#29575;&#12289;&#35745;&#31639;&#24320;&#38144;&#21644;&#35823;&#25253;&#35686;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#28304;&#20110;&#20256;&#24863;&#22120;&#33410;&#28857;&#36164;&#28304;&#32422;&#26463;&#12289;&#25968;&#25454;&#20887;&#20313;&#20197;&#21450;&#32593;&#32476;&#20869;&#39640;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#34701;&#21512;&#21512;&#25104;&#20102;&#23569;&#25968;&#23454;&#20363;&#24182;&#28040;&#38500;&#20102;Tomek&#38142;&#25509;&#65292;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13277v1 Announce Type: cross  Abstract: Wireless Sensor Networks (WSNs) play a pivotal role as infrastructures, encompassing both stationary and mobile sensors. These sensors self-organize and establish multi-hop connections for communication, collectively sensing, gathering, processing, and transmitting data about their surroundings. Despite their significance, WSNs face rapid and detrimental attacks that can disrupt functionality. Existing intrusion detection methods for WSNs encounter challenges such as low detection rates, computational overhead, and false alarms. These issues stem from sensor node resource constraints, data redundancy, and high correlation within the network. To address these challenges, we propose an innovative intrusion detection approach that integrates Machine Learning (ML) techniques with the Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend synthesizes minority instances and eliminates Tomek links, result
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SzCORE&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;&#36755;&#20837;&#20869;&#23481;&#12289;&#24615;&#33021;&#24230;&#37327;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.13005</link><description>&lt;p&gt;
SzCORE&#65306;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#30340;&#30315;&#30187;&#31038;&#21306;&#24320;&#28304;&#30740;&#31350;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13005
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SzCORE&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;&#36755;&#20837;&#20869;&#23481;&#12289;&#24615;&#33021;&#24230;&#37327;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23478;&#24237;&#21644;&#38271;&#26399;&#33041;&#30005;&#22270;&#30417;&#27979;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#39640;&#36136;&#37327;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#36843;&#20999;&#12290;&#36825;&#20123;&#31639;&#27861;&#39564;&#35777;&#26041;&#27861;&#30340;&#24322;&#36136;&#24615;&#24433;&#21709;&#20102;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35813;&#24322;&#36136;&#24615;&#20027;&#35201;&#28041;&#21450;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#24615;&#33021;&#24230;&#37327;&#31561;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24314;&#31435;EEG&#22522;&#30784;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#39564;&#35777;&#30340;&#26631;&#20934;&#21270;&#12290;&#22522;&#20110;&#29616;&#26377;&#25351;&#21335;&#21644;&#24314;&#35758;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#32452;&#20851;&#20110;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;EEG&#25968;&#25454;&#36755;&#20837;&#20869;&#23481;&#12289;&#30315;&#30187;&#27880;&#37322;&#36755;&#20837;&#21644;&#36755;&#20986;&#12289;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#20197;&#21450;&#24615;&#33021;&#24230;&#37327;&#30340;&#24314;&#35758;&#21644;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13005v1 Announce Type: cross  Abstract: The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#23545;&#31216;&#26694;&#26550;&#65292;&#21487;&#25913;&#36827;&#29616;&#26377;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312; StarGAN V2 &#19978;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12531</link><description>&lt;p&gt;
&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#25913;&#36827;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Deep Generative Models on Many-To-One Image-to-Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12531
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#23545;&#31216;&#26694;&#26550;&#65292;&#21487;&#25913;&#36827;&#29616;&#26377;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312; StarGAN V2 &#19978;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12531v1 &#36890;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#20013;&#30340;&#22810;&#20010;&#24212;&#29992;&#65292;&#24050;&#24212;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290; &#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290; &#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20855;&#26377;&#23545;&#31216;&#35774;&#32622;&#12290; &#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#25152;&#26377;&#39046;&#22495;&#37117;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#25110;&#20165;&#19968;&#20010;&#27169;&#24577;&#12290; &#20294;&#26159;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#23384;&#22312;&#20004;&#20010;&#22495;&#20043;&#38388;&#30340;&#22810;&#23545;&#19968;&#20851;&#31995;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;Colorized MNIST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;Color-Recall&#20998;&#25968;&#65292;&#23427;&#21487;&#20197;&#20026;&#22312;&#22810;&#23545;&#19968;&#32763;&#35793;&#19978;&#35780;&#20272;&#27169;&#22411;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#23545;&#31216;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#30340;&#34920;&#29616;&#12290; &#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040; StarGAN V2 &#19978;&#65292;&#24182;&#34920;&#26126;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12531v1 Announce Type: cross  Abstract: Deep generative models have been applied to multiple applications in image- to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to- image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12269</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#30417;&#30563;&#22270;&#39044;&#27979;&#65288;SGP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#25439;&#22833;&#65292;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#25439;&#22833;&#65288;PM-FGW&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#21033;&#29992;&#22270;&#34920;&#31034;&#65292;&#27604;&#22914;&#37051;&#25509;&#21644;&#29305;&#24449;&#30697;&#38453;&#12290;PM-FGW&#20855;&#26377;SGP&#30340;&#25152;&#26377;&#29702;&#24819;&#23646;&#24615;&#65306;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#22635;&#20805;&#34920;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#25513;&#30721;&#21521;&#37327;&#22788;&#29702;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#37096;&#20998;&#65292;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;image2graph&#65289;&#21644;&#20004;&#20010;&#30495;&#23454;&#20219;&#21153;&#65292;&#22270;&#20687;&#21040;&#22320;&#22270;&#21644;&#25351;&#32441;&#21040;&#20998;&#23376; - &#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#27604;&#31454;&#20105;&#32773;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12269v1 Announce Type: new  Abstract: We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#27010;&#24565;&#21407;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11641</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35282;&#25506;&#32034;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#27010;&#24565;&#21407;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#26159;&#24120;&#29992;&#30340;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#38754;&#23545;&#22810;&#26679;&#30340;&#23398;&#20064;&#20219;&#21153;&#12289;&#22270;&#39046;&#22495;&#21644;&#22797;&#26434;&#30340;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#20256;&#32479;&#30340;&#35774;&#35745;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#23545;&#20154;&#31867;&#19987;&#23478;&#25552;&#20986;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#21407;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#12290;&#20174;&#8220;&#22312;&#21738;&#37324;&#8221;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22235;&#20010;&#20851;&#38190;&#30340;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#22270;&#25968;&#25454;&#29305;&#24449;&#24037;&#31243;&#12289;&#27169;&#22411;&#36873;&#25321;&#19982;&#20248;&#21270;&#12289;&#37096;&#32626;&#19982;&#26381;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22312;&#36825;&#20123;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11641v1 Announce Type: new  Abstract: Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives. From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the ``how'' perspective, we
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.08576</link><description>&lt;p&gt;
&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#20013;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regret Minimization in Stackelberg Games with Side Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08576
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#22522;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;Stackelberg&#21338;&#24328;&#26159;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;Stackelberg&#21338;&#24328;&#31639;&#27861;&#26159;&#31639;&#27861;&#21338;&#24328;&#35770;&#30340;&#26368;&#22823;&#25104;&#21151;&#20043;&#19968;&#65292;&#22240;&#20026;Stackelberg&#21338;&#24328;&#30340;&#31639;&#27861;&#24050;&#32463;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#34987;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22330;&#23433;&#20840;&#12289;&#21453;&#30423;&#29454;&#21644;&#32593;&#32476;&#29359;&#32618;&#39044;&#38450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#26410;&#33021;&#32771;&#34385;&#21040;&#27599;&#20010;&#29609;&#23478;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65288;&#20363;&#22914;&#20132;&#36890;&#27169;&#24335;&#65292;&#22825;&#27668;&#26465;&#20214;&#65292;&#32593;&#32476;&#25317;&#22622;&#65289;&#65292;&#36825;&#26159;&#29616;&#23454;&#30340;&#26174;&#33879;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#21040;&#20004;&#20010;&#29609;&#23478;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#24773;&#20917;&#24418;&#24335;&#21270;&#20026;&#24102;&#26377;&#20391;&#20449;&#24687;&#30340;Stackelberg&#21338;&#24328;&#65292;&#20854;&#20013;&#20004;&#20010;&#29609;&#23478;&#22312;&#36827;&#34892;&#28216;&#25103;&#20043;&#21069;&#37117;&#35266;&#23519;&#21040;&#19968;&#20010;&#22806;&#37096;&#29615;&#22659;&#12290;&#28982;&#21518;&#65292;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#21487;&#33021;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#23545;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#21644;&#19978;&#19979;&#25991;&#37117;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on t
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.08082</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08082
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#25968;&#23398;&#22522;&#30784;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SGMs&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#30340;&#36817;&#20284;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20851;&#20110;&#27010;&#29575;&#20998;&#24067;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#30456;&#23545;&#23494;&#24230;&#19982;&#26631;&#20934;&#39640;&#26031;&#27979;&#24230;&#30340;&#30456;&#23545;&#23494;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#23545;&#25968;&#30456;&#23545;&#23494;&#24230;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23616;&#37096;&#36924;&#36817;&#65292;&#24182;&#19988;&#32593;&#32476;&#21442;&#25968;&#21487;&#20197;&#36866;&#24403;&#22320;&#21463;&#38480;&#65292;&#37027;&#20040;&#36890;&#36807;&#32463;&#39564;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#26576;&#20123;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#25512;&#23548;&#20986;&#19982;&#27491;&#21521;&#36807;&#31243;&#30456;&#20851;&#30340;&#30495;&#23454;&#24471;&#20998;&#20989;&#25968;&#30340;&#32500;&#24230;&#26080;&#20851;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07703</link><description>&lt;p&gt;
&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Sequential Decision-Making with Unknown Delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#39046;&#22495;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26694;&#26550;&#35299;&#20915;&#20102;&#20855;&#26377;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#30340;&#21453;&#39304;&#21487;&#33021;&#20197;&#26410;&#30693;&#24310;&#36831;&#21040;&#36798;&#12290;&#19982;&#20043;&#21069;&#20165;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20110;&#36817;&#20284;&#35299;&#30340;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25509;&#25910;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22810;&#21151;&#33021;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#33539;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#23436;&#25972;&#25439;&#22833;&#20989;&#25968;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#35268;&#33539;&#21270;&#39046;&#23548;&#31639;&#27861;&#26063;&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#30456;&#24212;&#20915;&#31574;&#28857;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#20540;&#20449;&#24687;&#21453;&#39304;&#30340;&#31616;&#21270;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#12290;&#23545;&#20110;&#27599;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.05980</link><description>&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#65311;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Large Code Models Understand Programming Concepts? A Black-box Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#25104;&#21151;&#20063;&#20351;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#24037;&#20316;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#21644;&#32534;&#36753;&#31561;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20026;&#20160;&#20040;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#24213;&#23618;&#31243;&#24207;&#30340;&#36923;&#36753;&#32467;&#26500;&#29702;&#35299;&#31243;&#24230;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#32534;&#31243;&#27010;&#24565;&#35859;&#35789;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#65288;CACP&#65289;&#20316;&#20026;&#19968;&#31181;&#21453;&#20107;&#23454;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#12290;&#21482;&#36890;&#36807;&#40657;&#30418;&#35775;&#38382;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;CACP&#35780;&#20272;&#20102;&#21313;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#20010;&#19981;&#21516;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models' success on text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.
&lt;/p&gt;</description></item><item><title>SDEMG&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#21435;&#22122;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SDEMG&#32988;&#36807;&#20102;&#20854;&#20182;&#27604;&#36739;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03808</link><description>&lt;p&gt;
SDEMG: &#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03808
&lt;/p&gt;
&lt;p&gt;
SDEMG&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#21435;&#22122;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SDEMG&#32988;&#36807;&#20102;&#20854;&#20182;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#34987;&#30417;&#27979;&#30340;&#32908;&#32905;&#38752;&#36817;&#24515;&#33039;&#26102;&#65292;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#35760;&#24405;&#21487;&#33021;&#21463;&#21040;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#30340;&#24433;&#21709;&#12290;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#20449;&#21495;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22914;&#39640;&#36890;&#28388;&#27874;&#22120;&#21644;&#27169;&#26495;&#20943;&#27861;&#65292;&#32780;&#19968;&#20123;&#26041;&#27861;&#21017;&#36890;&#36807;&#27714;&#21462;&#26144;&#23556;&#20989;&#25968;&#20174;&#24102;&#26377;ECG&#24178;&#25200;&#30340;sEMG&#65288;&#22122;&#22768;sEMG&#65289;&#20013;&#24674;&#22797;&#20986;&#24178;&#20928;&#30340;sEMG&#20449;&#21495;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#33879;&#21517;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#22122;&#22768;&#36755;&#20837;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SDEMG&#65292;&#29992;&#20316;sEMG&#20449;&#21495;&#21435;&#22122;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;SDEMG&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#24320;&#25918;&#25509;&#28304;&#30340;Non-Invasive Adaptive Prosthetics&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#20197;&#21450;&#26469;&#33258;MIT-BIH Normal Sinus Rhythm&#25968;&#25454;&#24211;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#20102;&#38477;&#22122;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SDEMG&#32988;&#36807;&#20102;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart. Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference). Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data. In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising. To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database. The experiment result indicates that SDEMG outperformed comparative methods a
&lt;/p&gt;</description></item><item><title>CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00450</link><description>&lt;p&gt;
CPT: &#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#33021; &#21147;&#36882;&#36827;&#24335;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
CPT: Competence-progressive Training Strategy for Few-shot Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00450
&lt;/p&gt;
&lt;p&gt;
CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#25104;&#21151;&#20173;&#28982;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#33410;&#28857;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#26631;&#31614;&#31232;&#30095;&#65292;&#24378;&#35843;&#20102;GNN&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20256;&#32479;&#30340;&#24773;&#33410;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#38480;&#21046;&#65306;&#38543;&#26426;&#21644;&#22343;&#21248;&#20219;&#21153;&#20998;&#37197;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#65292;&#24573;&#35270;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#27700;&#24179;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#20803;&#23398;&#20064;&#22120;&#36807;&#26089;&#22320;&#38754;&#20020;&#22797;&#26434;&#20219;&#21153;&#65292;&#38459;&#30861;&#20102;&#27491;&#24120;&#30340;&#23398;&#20064;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20803;&#23398;&#20064;&#22120;&#24212;&#35813;&#20174;&#31616;&#21333;&#27010;&#24565;&#24320;&#22987;&#65292;&#36880;&#28176;&#36827;&#20837;&#26356;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#23601;&#20687;&#20154;&#31867;&#23398;&#20064;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#38590;&#24230;&#19982;&#20803;&#23398;&#20064;&#22120;&#30340;&#36882;&#36827;&#33021;&#21147;&#30456;&#21305;&#37197;&#65292;&#22686;&#24378;&#20102;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17791</link><description>&lt;p&gt;
&#19981;&#24102;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Transformers without Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#21387;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#21333;&#29420;&#20351;&#29992;&#36824;&#26159;&#19982;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MP-GNN&#65289;&#32467;&#21512;&#12290;&#23558;&#22270;&#24402;&#32435;&#20559;&#35265;&#34701;&#20837;&#22825;&#28982;&#19982;&#32467;&#26500;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20197;&#32467;&#26500;&#25110;&#20301;&#32622;&#32534;&#30721;&#65288;PEs&#65289;&#30340;&#24418;&#24335;&#65292;&#26159;&#23454;&#29616;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#26159;&#26840;&#25163;&#30340;&#65292;&#20154;&#20204;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#23581;&#35797;&#26469;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#65292;&#21253;&#25324;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#12289;&#30456;&#23545;&#38543;&#26426;&#34892;&#36208;&#27010;&#29575;&#65288;RRWP&#65289;&#12289;&#31354;&#38388;&#32534;&#30721;&#12289;&#20013;&#24515;&#24230;&#32534;&#30721;&#12289;&#36793;&#32536;&#32534;&#30721;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32534;&#30721;&#21487;&#33021;&#26681;&#26412;&#19981;&#38656;&#35201;&#65292;&#21482;&#35201;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Eigenformer&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#65292;&#20102;&#35299;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13933</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structured Probabilistic Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13933
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#65292;&#29992;&#20110;&#20174;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20013;&#23398;&#20064;&#32039;&#20945;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;SPC&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#32534;&#30721;&#25216;&#26415;&#65292;&#20855;&#26377;&#26469;&#33258;&#30446;&#26631;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27010;&#29575;&#32534;&#30721;&#22312;&#19968;&#20010;&#27169;&#22359;&#20013;&#21516;&#26102;&#36827;&#34892;&#20449;&#24687;&#32534;&#30721;&#21644;&#20219;&#21153;&#39044;&#27979;&#65292;&#20197;&#26356;&#20805;&#20998;&#22320;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#31354;&#38388;&#30340;&#21464;&#20998;&#25512;&#26029;&#26469;&#20943;&#23569;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25511;&#21046;&#27010;&#29575;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#65292;&#20197;&#20419;&#36827;&#31867;&#21035;&#20043;&#38388;&#30340;&#22343;&#21248;&#24615;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#65292;SPC&#21487;&#20197;&#20445;&#25345;&#28508;&#22312;&#32534;&#30721;&#30340;&#39640;&#26031;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36127;&#33976;&#39311;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#38024;&#23545;&#26497;&#31471;&#22024;&#26434;&#23458;&#25143;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#22312;&#22024;&#26434;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#23458;&#25143;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.12703</link><description>&lt;p&gt;
&#36890;&#36807;&#36127;&#33976;&#39311;&#38024;&#23545;&#26497;&#31471;&#22024;&#26434;&#23458;&#25143;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Extremely Noisy Clients via Negative Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12703
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36127;&#33976;&#39311;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#38024;&#23545;&#26497;&#31471;&#22024;&#26434;&#23458;&#25143;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#22312;&#22024;&#26434;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#23458;&#25143;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#22312;&#21512;&#20316;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#36890;&#24120;&#22312;&#22788;&#29702;&#22024;&#26434;&#26631;&#31614;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#20808;&#36827;&#30340;&#20316;&#21697;&#25552;&#20986;&#36890;&#36807;&#19968;&#31181;&#20551;&#35774;&#36739;&#24369;&#26631;&#31614;&#22024;&#26434;&#24615;&#30340;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#26469;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#27745;&#26579;&#30340;&#23458;&#25143;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#34987;&#36829;&#21453;&#65292;&#23548;&#33268;&#26497;&#31471;&#22024;&#26434;&#27604;&#20363;&#65292;&#20363;&#22914;&#22823;&#20110;90%&#12290;&#20026;&#20102;&#35299;&#20915;&#26497;&#31471;&#22024;&#26434;&#23458;&#25143;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#19968;&#31181;&#24754;&#35266;&#32467;&#35770;&#65306;&#20943;&#23569;&#22312;&#22024;&#26434;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#23458;&#25143;&#30340;&#26435;&#37325;&#32988;&#36807;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#12290;&#20026;&#20102;&#21033;&#29992;&#22312;&#22024;&#26434;&#23458;&#25143;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#36127;&#33976;&#39311;&#65288;FedNed&#65289;&#12290;FedNed&#39318;&#20808;&#35782;&#21035;&#22024;&#26434;&#23458;&#25143;&#65292;&#24182;&#37319;&#29992;&#32780;&#19981;&#26159;&#20002;&#24323;&#36825;&#20123;&#22024;&#26434;&#23458;&#25143;&#65292;&#20197;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#29305;&#21035;&#26159;&#65292;&#34987;&#35782;&#21035;&#20026;&#22024;&#26434;&#30340;&#23458;&#25143;&#38656;&#35201;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12703v2 Announce Type: replace  Abstract: Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels. Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise. However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., $&gt;$90%. To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies. To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed). FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner. In particular, clients identified as noisy ones are required to train models using 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;</title><link>https://arxiv.org/abs/2312.06522</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26631;&#31614;&#24179;&#28369;&#22312;&#22686;&#24378;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06522
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#65292;&#20197;&#26377;&#25928;&#23545;&#25239;&#27169;&#22411;&#36807;&#25311;&#21512;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26631;&#31614;&#24179;&#28369;&#22914;&#20309;&#22686;&#24378;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#32454;&#33268;&#20998;&#26512;&#21364;&#24456;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22312;&#20843;&#20010;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;TextCNN&#12289;BERT&#21644;RoBERTa&#65289;&#20197;&#21450;&#20004;&#31181;&#23398;&#20064;&#26041;&#26696;&#19979;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#28145;&#20837;&#20998;&#26512;&#12290;&#36890;&#36807;&#35843;&#25972;&#24179;&#28369;&#21442;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#27599;&#20010;&#27169;&#22411;&#26550;&#26500;&#30340;&#20960;&#20046;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#30340;&#22909;&#22788;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06522v2 Announce Type: replace-cross  Abstract: Label smoothing is a widely used technique in various domains, such as text classification, image classification and speech recognition, known for effectively combating model overfitting. However, there is little fine-grained analysis on how label smoothing enhances text sentiment classification. To fill in the gap, this article performs a set of in-depth analyses on eight datasets for text sentiment classification and three deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning. By tuning the smoothing parameters, we can achieve improved performance on almost all datasets for each model architecture. We further investigate the benefits of label smoothing, finding that label smoothing can accelerate the convergence of deep models and make samples of different labels easily distinguishable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2312.04772</link><description>&lt;p&gt;
&#22312;&#24207;&#36143;&#20915;&#31574;&#20013;&#35760;&#20303;&#20844;&#24179;&#65306;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#20915;&#31574;&#21046;&#23450;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38024;&#23545;&#21333;&#19968;&#20915;&#31574;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#26412;&#25991;&#22312;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#21463;&#21040;&#20915;&#31574;&#32467;&#26524;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#30340;&#21382;&#21490;&#65292;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35762;&#65292;&#23427;&#26159;&#22266;&#26377;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#36890;&#24120;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#36807;&#31243;&#32467;&#26463;&#26102;&#12290;&#20026;&#20102;&#25512;&#36827;&#25105;&#20204;&#23545;&#36825;&#31867;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39034;&#24207;&#20915;&#31574;&#32972;&#26223;&#19979;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#38271;&#26399;&#20844;&#24179;&#24615;&#12289;&#20219;&#24847;&#26102;&#21051;&#20844;&#24179;&#24615;&#12289;&#21608;&#26399;&#24615;&#20844;&#24179;&#24615;&#21644;&#26377;&#30028;&#20844;&#24179;&#24615;&#31561;&#27010;&#24565;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#36825;&#22914;&#20309;&#25903;&#25345;&#21046;&#23450;&#20844;&#24179;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04772v3 Announce Type: replace  Abstract: Fair decision making has largely been studied with respect to a single decision. In this paper we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points within the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We further explore the interplay between non-Markovian fairness and memory, and how this can support construction of fair policies
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.01957</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#36807;&#31934;&#28860;&#30340;LLMs&#33258;&#25105;&#25209;&#35780;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;(dSC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;Gibbs&#37319;&#26679;&#22120;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#28982;&#21518;&#23558;&#20854;&#33976;&#39311;&#25104;&#19968;&#20010;&#24494;&#35843;&#27169;&#22411;&#12290;&#21482;&#38656;&#35201;&#21512;&#25104;&#25968;&#25454;&#65292;dSC&#22312;&#28041;&#21450;&#23433;&#20840;&#24615;&#12289;&#24773;&#24863;&#21644;&#38544;&#31169;&#25511;&#21046;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#20316;&#20026;&#23545;&#40784;LLMs&#30340;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20195;&#30721;&#22312;\url{https://github.com/vicgalle/distilled-self-critique}&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#35843;&#24230;&#26041;&#26696;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#26102;&#20195;&#23450;&#20041;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20449;&#24687;&#26102;&#20195;&#26469;&#25913;&#21892;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.00279</link><description>&lt;p&gt;
&#22522;&#20110;&#24180;&#40836;&#30340;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#35843;&#24230;&#65306;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00279
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#35843;&#24230;&#26041;&#26696;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#26102;&#20195;&#23450;&#20041;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20449;&#24687;&#26102;&#20195;&#26469;&#25913;&#21892;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21508;&#31181;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#24050;&#32463;&#37096;&#32626;&#65292;&#36896;&#31119;&#20110;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25910;&#38598;&#30340;&#29615;&#22659;&#20449;&#24687;&#30340;&#26032;&#40092;&#24230;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#20854;&#20449;&#24687;&#26102;&#20195;&#65288;AoI&#65289;&#26469;&#34913;&#37327;&#12290;&#22312;&#20256;&#32479;AoI&#30340;&#23450;&#20041;&#20013;&#65292;&#20551;&#23450;&#29366;&#24577;&#20449;&#24687;&#21487;&#20197;&#34987;&#31215;&#26497;&#37319;&#26679;&#24182;&#30452;&#25509;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;MEC&#21551;&#29992;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26399;&#26395;&#30340;&#29366;&#24577;&#20449;&#24687;&#20197;&#20107;&#20214;&#39537;&#21160;&#30340;&#26041;&#24335;&#26356;&#26032;&#65292;&#24182;&#38656;&#35201;&#25968;&#25454;&#22788;&#29702;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#28385;&#36275;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;AoI&#23450;&#20041;&#65292;&#24182;&#22522;&#20110;&#37325;&#26032;&#23450;&#20041;&#30340;AoI&#65292;&#20026;MEC&#31995;&#32479;&#21046;&#23450;&#20102;&#19968;&#20010;&#22312;&#32447;AoI&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20174;&#32780;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;RL&#31639;&#27861;&#23384;&#22312;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00279v2 Announce Type: replace  Abstract: With the rapid development of Mobile Edge Computing (MEC), various real-time applications have been deployed to benefit people's daily lives. The performance of these applications relies heavily on the freshness of collected environmental information, which can be quantified by its Age of Information (AoI). In the traditional definition of AoI, it is assumed that the status information can be actively sampled and directly used. However, for many MEC-enabled applications, the desired status information is updated in an event-driven manner and necessitates data processing. To better serve these applications, we propose a new definition of AoI and, based on the redefined AoI, we formulate an online AoI minimization problem for MEC systems. Notably, the problem can be interpreted as a Markov Decision Process (MDP), thus enabling its solution through Reinforcement Learning (RL) algorithms. Nevertheless, the traditional RL algorithms are d
&lt;/p&gt;</description></item><item><title>InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12943</link><description>&lt;p&gt;
InteRACT&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12943
&lt;/p&gt;
&lt;p&gt;
InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20013;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#24182;&#30456;&#24212;&#35843;&#25972;&#20854;&#34892;&#21160;&#65292;&#20197;&#24179;&#31283;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24847;&#22270;&#21453;&#36807;&#26469;&#21448;&#21462;&#20915;&#20110;&#26426;&#22120;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#65292;&#36896;&#25104;&#20102;&#19968;&#20010;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26159;&#35757;&#32451;&#29420;&#31435;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#30340;&#36793;&#38469;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#32570;&#20047;&#37197;&#23545;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26465;&#20214;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#33021;&#21542;&#36716;&#32780;&#21033;&#29992;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#65311;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#21160;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;-&#20154;&#31867;&#21040;&#20154;&#31867;-&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;InteRACT&#65292;&#35813;&#26550;&#26500;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20999;&#30340;&#24605;&#32771;&#8221;&#65288;XoT&#65289;&#30340;&#26032;&#22411;&#24605;&#32771;&#20419;&#36827;&#26041;&#27861;&#65292;&#20511;&#21161;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#23558;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#24605;&#24819;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.04254</link><description>&lt;p&gt;
&#19968;&#20999;&#30340;&#24605;&#32771;&#65306;&#25171;&#30772;&#24429;&#32599;&#26031;&#19977;&#35282;&#23450;&#24459;&#20197;&#29983;&#25104;&#24605;&#24819;
&lt;/p&gt;
&lt;p&gt;
Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04254
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20999;&#30340;&#24605;&#32771;&#8221;&#65288;XoT&#65289;&#30340;&#26032;&#22411;&#24605;&#32771;&#20419;&#36827;&#26041;&#27861;&#65292;&#20511;&#21161;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#23558;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#24605;&#24819;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#35821;&#35328;&#24207;&#21015;&#65288;&#21363;&#8220;&#24605;&#24819;&#8221;&#65289;&#24443;&#24213;&#25913;&#21464;&#20102;&#20915;&#31574;&#12290;&#19968;&#20010;&#26377;&#25928;&#30340;&#24605;&#24819;&#35774;&#35745;&#24212;&#35813;&#32771;&#34385;&#19977;&#20010;&#20851;&#38190;&#35270;&#35282;&#65306;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24605;&#24819;&#26368;&#22810;&#21482;&#33021;&#20307;&#29616;&#36825;&#20123;&#23646;&#24615;&#20013;&#30340;&#20004;&#20010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20999;&#30340;&#24605;&#32771;&#8221;&#65288;XoT&#65289;&#30340;&#26032;&#22411;&#24605;&#32771;&#20419;&#36827;&#26041;&#27861;&#65292;&#20197;&#25171;&#30772;&#29616;&#26377;&#24605;&#32771;&#33539;&#24335;&#30340;&#8220;&#24429;&#32599;&#26031;&#19977;&#35282;&#23450;&#24459;&#8221;&#12290;XoT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#23558;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#24605;&#24819;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;MCTS-LLM&#21327;&#20316;&#24605;&#32771;&#20462;&#35746;&#26694;&#26550;&#65292;&#36825;&#31181;&#26041;&#27861;&#33258;&#20027;&#29983;&#20135;&#39640;&#36136;&#37327;&#30340;&#32508;&#21512;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04254v3 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as "thoughts". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called "Everything of Thoughts" (XoT) to defy the law of "Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitiv
&lt;/p&gt;</description></item><item><title>&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;</title><link>https://arxiv.org/abs/2311.02181</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#32852;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Joint Problems in Learning Multiple Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02181
&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#26159;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#20174;&#36890;&#36807;&#20195;&#35874;&#20135;&#29289;&#27987;&#24230;&#33719;&#24471;&#30340;&#23450;&#37327;&#20010;&#24615;&#21270;&#20195;&#35874;&#27169;&#22411;&#21040;&#37327;&#23376;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#29366;&#24577;&#21028;&#21035;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21464;&#31181;&#65292;&#21363;&#32473;&#23450;&#19968;&#32452;&#36712;&#36857;&#21644;&#19968;&#20123;&#37096;&#20998;&#65292;&#25105;&#20204;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#27169;&#22411;&#65292;&#20197;&#20351;&#24471;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#25910;&#25947;&#30340;&#26041;&#27861;&#21644;EM&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#38468;&#19978;&#20102;&#26377;&#21069;&#26223;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02181v2 Announce Type: replace-cross  Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results.
&lt;/p&gt;</description></item><item><title>&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#36827;&#34892;&#20102;&#20262;&#29702;&#35752;&#35770;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#26816;&#26597;&#34920;&#65292;&#20197;&#25512;&#21160;&#20262;&#29702;&#35752;&#35770;&#30340;&#20840;&#38754;&#35780;&#20272;&#21644;&#36879;&#26126;&#35760;&#24405;&#12290;</title><link>https://arxiv.org/abs/2311.02107</link><description>&lt;p&gt;
&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#20262;&#29702;&#32771;&#34385;&#19982;&#35780;&#20272;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02107
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#36827;&#34892;&#20102;&#20262;&#29702;&#35752;&#35770;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#26816;&#26597;&#34920;&#65292;&#20197;&#25512;&#21160;&#20262;&#29702;&#35752;&#35770;&#30340;&#20840;&#38754;&#35780;&#20272;&#21644;&#36879;&#26126;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#26032;&#20852;&#25216;&#26415;&#22522;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#23545;&#28508;&#22312;&#20262;&#29702;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#20294;&#20262;&#29702;&#35752;&#35770;&#23578;&#26410;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#27491;&#22312;&#36827;&#34892;&#30340;&#20262;&#29702;&#35752;&#35770;&#24120;&#24120;&#24573;&#35270;&#20854;&#20182;&#31867;&#22411;&#30340;GenAI&#65292;&#36825;&#20123;GenAI&#24050;&#34987;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#36827;&#34892;&#30740;&#31350;&#21644;&#23454;&#38469;&#30446;&#30340;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19968;&#20123;&#20262;&#29702;&#38382;&#39064;&#24182;&#26292;&#38706;&#20102;&#20854;&#20182;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#21307;&#30103;&#20445;&#20581;&#20013;GenAI&#20262;&#29702;&#35752;&#35770;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#20197;&#20840;&#38754;&#20998;&#26512;&#24403;&#21069;&#30740;&#31350;&#20013;&#30340;&#24046;&#36317;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#35758;&#36890;&#36807;&#21046;&#23450;&#19968;&#20221;&#26816;&#26597;&#34920;&#26469;&#20943;&#23569;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#21644;&#36879;&#26126;&#35760;&#24405;GenAI&#30740;&#31350;&#20013;&#30340;&#20262;&#29702;&#35752;&#35770;&#12290;&#36825;&#20221;&#26816;&#26597;&#34920;&#21487;&#20197;&#36731;&#26494;&#25972;&#21512;&#21040;&#24403;&#21069;&#30340;&#21516;&#34892;&#35780;&#23457;&#21644;&#21457;&#24067;&#31995;&#32479;&#20013;&#65292;&#20197;&#22686;&#24378;GenAI&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02107v2 Announce Type: replace-cross  Abstract: The widespread use of ChatGPT and other emerging technology powered by generative artificial intelligence (GenAI) has drawn much attention to potential ethical issues, especially in high-stakes applications such as healthcare, but ethical discussions are yet to translate into operationalisable solutions. Furthermore, ongoing ethical discussions often neglect other types of GenAI that have been used to synthesise data (e.g., images) for research and practical purposes, which resolved some ethical issues and exposed others. We conduct a scoping review of ethical discussions on GenAI in healthcare to comprehensively analyse gaps in the current research, and further propose to reduce the gaps by developing a checklist for comprehensive assessment and transparent documentation of ethical discussions in GenAI research. The checklist can be readily integrated into the current peer review and publication system to enhance GenAI researc
&lt;/p&gt;</description></item><item><title>UniTime&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#36328;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32479;&#19968;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#25968;&#25454;&#29305;&#24449;&#24046;&#24322;&#12289;&#25968;&#25454;&#21306;&#20998;&#22256;&#38590;&#21644;&#25910;&#25947;&#36895;&#24230;&#19981;&#21516;&#31561;&#25361;&#25112;</title><link>https://arxiv.org/abs/2310.09751</link><description>&lt;p&gt;
UniTime&#65306;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#36328;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32479;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09751
&lt;/p&gt;
&lt;p&gt;
UniTime&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#36328;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32479;&#19968;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#25968;&#25454;&#29305;&#24449;&#24046;&#24322;&#12289;&#25968;&#25454;&#21306;&#20998;&#22256;&#38590;&#21644;&#25910;&#25947;&#36895;&#24230;&#19981;&#21516;&#31561;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#24403;&#20195;&#32593;&#32476;&#25216;&#26415;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19982;&#20026;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#39046;&#22495;&#21019;&#24314;&#19987;&#29992;&#27169;&#22411;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#20513;&#23548;&#36328;&#36234;&#39046;&#22495;&#36793;&#30028;&#30340;&#32479;&#19968;&#27169;&#22411;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#19968;&#20010;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#27169;&#22411;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19981;&#21516;&#39046;&#22495;&#22312;&#25968;&#25454;&#29305;&#24449;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20363;&#22914;&#21464;&#37327;&#25968;&#37327;&#65292;&#36825;&#32473;&#29616;&#26377;&#27169;&#22411;&#24102;&#26469;&#20102;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#20123;&#22240;&#32032;&#19978;&#24378;&#21152;&#20102;&#19981;&#28789;&#27963;&#30340;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#21487;&#33021;&#22312;&#21306;&#20998;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#23548;&#33268;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#31532;&#19977;&#65292;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#24230;&#20063;&#21487;&#33021;&#23548;&#33268;&#23454;&#35777;&#24615;&#33021;&#21463;&#25439;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniTime&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UniTime&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09751v3 Announce Type: replace  Abstract: Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36807;&#31243;&#20013;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#35760;&#24518;&#22312;&#21508;&#31181;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.06714</link><description>&lt;p&gt;
&#25506;&#32034;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring Memorization in Fine-tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06714
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36807;&#31243;&#20013;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#35760;&#24518;&#22312;&#21508;&#31181;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#34920;&#29616;&#20986;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#38544;&#31169;&#21644;&#29256;&#26435;&#25285;&#24551;&#12290; &#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26102;&#30340;&#35760;&#24518;&#29616;&#35937;&#12290; &#25105;&#20204;&#20351;&#29992;&#24320;&#28304;&#21644;&#25105;&#20204;&#33258;&#24049;&#30340;&#24494;&#35843;LMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#21516;&#24494;&#35843;&#20219;&#21153;&#20013;&#65292;&#35760;&#24518;&#21576;&#29616;&#20986;&#36739;&#24378;&#30340;&#24046;&#24322;&#24615;&#12290; &#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#25552;&#20379;&#20102;&#23545;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#30340;&#30452;&#35266;&#35299;&#37322;&#65292;&#24182;&#25581;&#31034;&#20102;&#35760;&#24518;&#21644;&#27880;&#24847;&#21147;&#20998;&#25968;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06714v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models' (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention scor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#31574;&#30053;&#21512;&#24182;&#35299;&#20915;&#26426;&#22120;&#20154;&#32676;&#20307;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;Meta-World&#29615;&#22659;&#20013;&#23558;50&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#34892;&#20026;&#25972;&#21512;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2310.01362</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#21512;&#24182;&#23454;&#29616;&#33328;&#38431;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fleet Learning via Policy Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#31574;&#30053;&#21512;&#24182;&#35299;&#20915;&#26426;&#22120;&#20154;&#32676;&#20307;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;Meta-World&#29615;&#22659;&#20013;&#23558;50&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#34892;&#20026;&#25972;&#21512;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#32676;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#20135;&#29983;&#30340;&#22823;&#37327;&#24322;&#26500;&#27969;&#25968;&#25454;&#23384;&#20648;&#25110;&#20256;&#36755;&#19978;&#30340;&#22256;&#38590;&#65292;&#26426;&#22120;&#20154;&#22242;&#38431;&#38656;&#35201;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24322;&#26500;&#32463;&#39564;&#26469;&#20849;&#21516;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#25216;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36825;&#31181;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#31574;&#30053;&#21512;&#24182;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22312;&#33328;&#38431;&#29615;&#22659;&#20013;&#39640;&#25928;&#21512;&#24182;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLEET-MERGE&#65292;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#25511;&#21046;&#31574;&#30053;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#23454;&#20363;&#65292;&#32771;&#34385;&#20102;&#21442;&#25968;&#21270;&#25511;&#21046;&#31574;&#30053;&#20013;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;FLEET-MERGE&#22312;Meta-World&#29615;&#22659;&#20013;&#23545;50&#20010;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#31574;&#30053;&#34892;&#20026;&#36827;&#34892;&#20102;&#25972;&#21512;&#65292;&#24182;&#19988;&#20960;&#20046;&#22312;&#25152;&#26377;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01362v2 Announce Type: replace-cross  Abstract: Fleets of robots ingest massive amounts of heterogeneous streaming data silos generated by interacting with their environments, far more than what can be stored or transmitted with ease. At the same time, teams of robots should co-acquire diverse skills through their heterogeneous experiences in varied settings. How can we enable such fleet-level learning without having to transmit or centralize fleet-scale data? In this paper, we investigate policy merging (PoMe) from such distributed heterogeneous datasets as a potential solution. To efficiently merge policies in the fleet setting, we propose FLEET-MERGE, an instantiation of distributed learning that accounts for the permutation invariance that arises when parameterizing the control policies with recurrent neural networks. We show that FLEET-MERGE consolidates the behavior of policies trained on 50 tasks in the Meta-World environment, with good performance on nearly all train
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;T-SPEAR&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;(TGNN)&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#20013;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10779</link><description>&lt;p&gt;
&#30683;&#19982;&#30462;&#65306;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#19982;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;T-SPEAR&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;(TGNN)&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#20013;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#22270;&#26159;&#21160;&#24577;&#30340;&#65292;&#19981;&#26029;&#38543;&#30528;&#26032;&#30340;&#20132;&#20114;&#32780;&#21457;&#23637;&#65292;&#27604;&#22914;&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#37329;&#34701;&#20132;&#26131;&#12290;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;(TGNN)&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#21160;&#24577;&#22270;&#20013;&#19981;&#26029;&#28436;&#21464;&#30340;&#27169;&#24335;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#20294;&#23427;&#20204;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;T-SPEAR&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;TGNN&#30340;&#33030;&#24369;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19968;&#20010;&#38024;&#23545;&#38142;&#36335;&#39044;&#27979;&#30340;TGNN&#21463;&#23475;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#65292;&#25105;&#20204;&#21521;&#25968;&#25454;&#27880;&#20837;&#36793;&#32536;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#22235;&#20010;&#32422;&#26463;&#26465;&#20214;&#26041;&#38754;&#26159;&#19981;&#21487;&#23519;&#35273;&#30340;&#65292;&#20294;&#36275;&#20197;&#23548;&#33268;&#21463;&#23475;&#27169;&#22411;&#30340;&#25925;&#38556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10779v2 Announce Type: replace  Abstract: Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks. Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model. Moreover, we propose a robust training appr
&lt;/p&gt;</description></item><item><title>FedDefender&#26159;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#38024;&#23545;&#24615;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#24046;&#20998;&#27979;&#35797;&#26469;&#35782;&#21035;&#28508;&#22312;&#21253;&#21547;&#21518;&#38376;&#30340;&#24694;&#24847;&#23458;&#25143;&#65292;&#26377;&#25928;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#21040;10%&#12290;</title><link>https://arxiv.org/abs/2307.08672</link><description>&lt;p&gt;
FedDefender&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
FedDefender: Backdoor Attack Defense in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.08672
&lt;/p&gt;
&lt;p&gt;
FedDefender&#26159;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#38024;&#23545;&#24615;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#24046;&#20998;&#27979;&#35797;&#26469;&#35782;&#21035;&#28508;&#22312;&#21253;&#21547;&#21518;&#38376;&#30340;&#24694;&#24847;&#23458;&#25143;&#65292;&#26377;&#25928;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#21040;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL)&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#20351;&#24471;&#20010;&#20307;&#23458;&#25143;&#65288;&#20363;&#22914;&#29992;&#25143;&#21442;&#19982;&#32773;&#12289;&#36793;&#32536;&#35774;&#22791;&#25110;&#32452;&#32455;&#65289;&#33021;&#22815;&#22312;&#23433;&#20840;&#29615;&#22659;&#20013;&#22522;&#20110;&#26412;&#22320;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#19982;&#32858;&#21512;&#22120;&#20849;&#20139;&#35757;&#32451;&#27169;&#22411;&#20197;&#21327;&#20316;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDefender&#65292;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#38024;&#23545;&#24615;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#23427;&#21033;&#29992;&#24046;&#20998;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#30456;&#21516;&#36755;&#20837;&#30340;&#23458;&#25143;&#27169;&#22411;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#24046;&#20998;&#27979;&#35797;&#26469;&#35782;&#21035;&#28508;&#22312;&#21253;&#21547;&#21518;&#38376;&#30340;&#24694;&#24847;&#23458;&#25143;&#12290;&#25105;&#20204;&#20351;&#29992;MNIST&#21644;FashionMNIST&#25968;&#25454;&#38598;&#20197;&#21450;20&#20010;&#21644;30&#20010;&#23458;&#25143;&#23545;FedDefender&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;FedDefender&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#27492;&#31867;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#38477;&#20302;&#21040;10%&#65292;&#32780;&#19981;&#20250;&#24694;&#21270;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.08672v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a privacy-preserving distributed machine learning technique that enables individual clients (e.g., user participants, edge devices, or organizations) to train a model on their local data in a secure environment and then share the trained model with an aggregator to build a global model collaboratively. In this work, we propose FedDefender, a defense mechanism against targeted poisoning attacks in FL by leveraging differential testing. Our proposed method fingerprints the neuron activations of clients' models on the same input and uses differential testing to identify a potentially malicious client containing a backdoor. We evaluate FedDefender using MNIST and FashionMNIST datasets with 20 and 30 clients, and our results demonstrate that FedDefender effectively mitigates such attacks, reducing the attack success rate (ASR) to 10\% without deteriorating the global model performance.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#30740;&#31350;&#34920;&#26126;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#20250;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21253;&#25324;&#28145;&#24230;&#34920;&#31034;&#21644;&#27973;&#23618;&#65292;&#25552;&#20986;&#20102;BlockGCL&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;</title><link>https://arxiv.org/abs/2306.02117</link><description>&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#65306;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#22121;&#26790;&#65311;
&lt;/p&gt;
&lt;p&gt;
Oversmoothing: A Nightmare for Graph Contrastive Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02117
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#30740;&#31350;&#34920;&#26126;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#20250;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21253;&#25324;&#28145;&#24230;&#34920;&#31034;&#21644;&#27973;&#23618;&#65292;&#25552;&#20986;&#20102;BlockGCL&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#32593;&#32476;&#28145;&#24230;&#30340;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#27491;&#26085;&#30410;&#25104;&#20026;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#12290;&#20316;&#20026;GNNs&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#23578;&#19981;&#28165;&#26970;GCL&#26159;&#21542;&#20250;&#32487;&#25215;GNNs&#30340;&#36807;&#24230;&#24179;&#28369;&#32570;&#38519;&#12290;&#26412;&#25991;&#20174;&#36807;&#24230;&#24179;&#28369;&#30340;&#35282;&#24230;&#23545;GCL&#36827;&#34892;&#20102;&#22522;&#30784;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;GCL&#20013;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#20063;&#20250;&#23548;&#33268;&#23427;&#20204;&#30340;&#28145;&#24230;&#34920;&#31034;&#36807;&#24230;&#24179;&#28369;&#65292;&#32780;&#19988;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#27973;&#23618;&#20063;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#22312;GCL&#20013;&#31216;&#20026;&#8220;&#38271;&#36317;&#31163;&#39269;&#39295;&#8221;&#65292;&#21363;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#36739;&#20302;&#23618;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#30417;&#30563;&#30340;&#20805;&#20998;&#25351;&#23548;&#32780;&#36973;&#21463;&#36864;&#21270;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BlockGCL&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22359;wi
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02117v2 Announce Type: replace-cross  Abstract: Oversmoothing is a common phenomenon observed in graph neural networks (GNNs), in which an increase in the network depth leads to a deterioration in their performance. Graph contrastive learning (GCL) is emerging as a promising way of leveraging vast unlabeled graph data. As a marriage between GNNs and contrastive learning, it remains unclear whether GCL inherits the same oversmoothing defect from GNNs. This work undertakes a fundamental analysis of GCL from the perspective of oversmoothing on the first hand. We demonstrate empirically that increasing network depth in GCL also leads to oversmoothing in their deep representations, and surprisingly, the shallow ones. We refer to this phenomenon in GCL as `long-range starvation', wherein lower layers in deep networks suffer from degradation due to the lack of sufficient guidance from supervision. Based on our findings, we present BlockGCL, a remarkably simple yet effective blockwi
&lt;/p&gt;</description></item><item><title>SparDL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31232;&#30095;&#36890;&#20449;&#26694;&#26550;&#65292;&#20351;&#29992;Spar-Reduce-Scatter&#21644;Spar-All-Gather&#31639;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#26799;&#24230;&#32047;&#31215;&#22256;&#22659;&#65292;&#36991;&#20813;&#20381;&#36182;&#20302;&#25928;&#36890;&#20449;&#31639;&#27861;&#21644;&#39069;&#22806;&#20256;&#36755;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2304.00737</link><description>&lt;p&gt;
SparDL&#65306;&#39640;&#25928;&#31232;&#30095;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SparDL: Distributed Deep Learning Training with Efficient Sparse Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.00737
&lt;/p&gt;
&lt;p&gt;
SparDL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31232;&#30095;&#36890;&#20449;&#26694;&#26550;&#65292;&#20351;&#29992;Spar-Reduce-Scatter&#21644;Spar-All-Gather&#31639;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#26799;&#24230;&#32047;&#31215;&#22256;&#22659;&#65292;&#36991;&#20813;&#20381;&#36182;&#20302;&#25928;&#36890;&#20449;&#31639;&#27861;&#21644;&#39069;&#22806;&#20256;&#36755;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Top-k&#31232;&#30095;&#21270;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#37327;&#65292;&#28982;&#32780;&#30001;&#20110;&#31232;&#30095;&#26799;&#24230;&#32047;&#31215;&#65288;SGA&#65289;&#22256;&#22659;&#65292;Top-k&#31232;&#30095;&#21270;&#30340;&#24615;&#33021;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;SGA&#22256;&#22659;&#65292;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#28982;&#32780;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#65292;&#20363;&#22914;&#20381;&#36182;&#20302;&#25928;&#30340;&#36890;&#20449;&#31639;&#27861;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#20256;&#36755;&#27493;&#39588;&#12290;&#21463;&#29616;&#26377;&#26041;&#27861;&#23616;&#38480;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#31232;&#30095;&#36890;&#20449;&#26694;&#26550;&#65292;&#31216;&#20026;SparDL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SparDL&#20351;&#29992;&#20102;Spar-Reduce-Scatter&#31639;&#27861;&#65292;&#22522;&#20110;&#39640;&#25928;&#30340;Reduce-Scatter&#27169;&#22411;&#22788;&#29702;SGA&#22256;&#22659;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36890;&#20449;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#24310;&#36831;&#25104;&#26412;&#24182;&#25552;&#39640;SparDL&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Spar-All-Gather&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.00737v2 Announce Type: replace  Abstract: Top-k sparsification has recently been widely used to reduce the communication volume in distributed deep learning. However, due to the Sparse Gradient Accumulation (SGA) dilemma, the performance of top-k sparsification still has limitations. Recently, a few methods have been put forward to handle the SGA dilemma. Regrettably, even the state-of-the-art method suffers from several drawbacks, e.g., it relies on an inefficient communication algorithm and requires extra transmission steps. Motivated by the limitations of existing methods, we propose a novel efficient sparse communication framework, called SparDL. Specifically, SparDL uses the Spar-Reduce-Scatter algorithm, which is based on an efficient Reduce-Scatter model, to handle the SGA dilemma without additional communication operations. Besides, to further reduce the latency cost and improve the efficiency of SparDL, we propose the Spar-All-Gather algorithm. Moreover, we propose 
&lt;/p&gt;</description></item><item><title>&#20248;&#21270;&#21160;&#24577;&#21457;&#29616;&#20102;&#22312;&#23398;&#20064;&#19981;&#21516;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857;&#27969;&#24418;&#26102;&#22914;&#20309;&#24179;&#34913;&#31867;&#21035;&#20998;&#31163;&#21644;&#29305;&#24449;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#23545;&#31435;&#20542;&#21521;&#65292;&#36890;&#36807;&#38750;&#21333;&#35843;&#36235;&#21183;&#23637;&#29616;&#20102;&#36825;&#31181;&#26435;&#34913;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2303.05161</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#31867;&#21035;&#27969;&#24418;&#30340;&#21453;&#28436;&#21160;&#21147;&#23398;&#25581;&#31034;&#20102;&#28508;&#22312;&#27867;&#21270;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Inversion dynamics of class manifolds in deep learning reveals tradeoffs underlying generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05161
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#21160;&#24577;&#21457;&#29616;&#20102;&#22312;&#23398;&#20064;&#19981;&#21516;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857;&#27969;&#24418;&#26102;&#22914;&#20309;&#24179;&#34913;&#31867;&#21035;&#20998;&#31163;&#21644;&#29305;&#24449;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#23545;&#31435;&#20542;&#21521;&#65292;&#36890;&#36807;&#38750;&#21333;&#35843;&#36235;&#21183;&#23637;&#29616;&#20102;&#36825;&#31181;&#26435;&#34913;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#23454;&#29616;&#25509;&#36817;&#38646;&#35757;&#32451;&#35823;&#24046;&#65292;&#21069;&#39304;&#32593;&#32476;&#30340;&#23618;&#24517;&#39035;&#35299;&#24320;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857;&#30340;&#27969;&#24418;&#65292;&#20197;&#20419;&#36827;&#21306;&#20998;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#30340;&#31867;&#21035;&#20998;&#31163;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#22240;&#20026;&#33391;&#22909;&#30340;&#27867;&#21270;&#38656;&#35201;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#36825;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#28151;&#26434;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#20248;&#21270;&#21160;&#24577;&#22914;&#20309;&#25214;&#21040;&#24179;&#34913;&#36825;&#20123;&#23545;&#31435;&#20542;&#21521;&#30340;&#34920;&#31034;&#65292;&#24182;&#21576;&#29616;&#20986;&#38750;&#21333;&#35843;&#36235;&#21183;&#12290;&#24555;&#36895;&#20998;&#31163;&#38454;&#27573;&#20043;&#21518;&#65292;&#26356;&#24930;&#30340;&#37325;&#26032;&#25490;&#21015;&#65288;&#22312;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#38388;&#20445;&#25345;&#19968;&#33268;&#65289;&#22686;&#21152;&#20102;&#31867;&#21035;&#28151;&#26434;&#24615;&#12290;&#21453;&#28436;&#26102;&#30340;&#35757;&#32451;&#35823;&#24046;&#22312;&#23376;&#37319;&#26679;&#12289;&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#22120;&#20043;&#38388;&#20445;&#25345;&#31283;&#23450;&#65292;&#36825;&#23558;&#20854;&#29305;&#24449;&#21270;&#20026;&#20165;&#21462;&#20915;&#20110;&#25968;&#25454;&#32467;&#26500;&#65288;&#21450;&#26550;&#26500;&#19978;&#30340;&#24494;&#24369;&#23646;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05161v2 Announce Type: replace  Abstract: To achieve near-zero training error in a classification problem, the layers of a feed-forward network have to disentangle the manifolds of data points with different labels, to facilitate the discrimination. However, excessive class separation can bring to overfitting since good generalisation requires learning invariant features, which involve some level of entanglement. We report on numerical experiments showing how the optimisation dynamics finds representations that balance these opposing tendencies with a non-monotonic trend. After a fast segregation phase, a slower rearrangement (conserved across data sets and architectures) increases the class entanglement.The training error at the inversion is stable under subsampling, and across network initialisations and optimisers, which characterises it as a property solely of the data structure and (very weakly) of the architecture. The inversion is the manifestation of tradeoffs elicit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#37329;&#34701;&#39046;&#22495;&#30740;&#31350;&#20102;&#29305;&#24449;&#36873;&#25321;&#19982;&#36864;&#28779;&#26041;&#27861;&#65292;&#20026;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2303.02223</link><description>&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#19982;&#36864;&#28779;&#29992;&#20110;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Feature Selection with Annealing for Forecasting Financial Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#37329;&#34701;&#39046;&#22495;&#30740;&#31350;&#20102;&#29305;&#24449;&#36873;&#25321;&#19982;&#36864;&#28779;&#26041;&#27861;&#65292;&#20026;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#24066;&#22330;&#21644;&#21152;&#23494;&#36135;&#24065;&#30340;&#39044;&#27979;&#23545;&#25237;&#36164;&#32773;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#20182;&#20204;&#28212;&#26395;&#25913;&#36827;&#36141;&#20080;&#25110;&#25345;&#26377;&#31574;&#30053;&#65292;&#20197;&#22686;&#21152;&#30408;&#21033;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25112;&#30053;&#36755;&#20837;&#36755;&#20986;&#29305;&#24449;&#26144;&#23556;&#25216;&#26415;&#65292;&#29992;&#20110;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02223v3 Announce Type: replace  Abstract: Stock market and cryptocurrency forecasting is very important to investors as they aspire to achieve even the slightest improvement to their buy or hold strategies so that they may increase profitability. However, obtaining accurate and reliable predictions is challenging, noting that accuracy does not equate to reliability, especially when financial time-series forecasting is applied owing to its complex and chaotic tendencies. To mitigate this complexity, this study provides a comprehensive method for forecasting financial time series based on tactical input output feature mapping techniques using machine learning (ML) models. During the prediction process, selecting the relevant indicators is vital to obtaining the desired results. In the financial field, limited attention has been paid to this problem with ML solutions. We investigate the use of feature selection with annealing (FSA) for the first time in this field, and we apply
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#35268;&#21017;&#38388;&#38548;&#25968;&#25454;&#19978;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#28857;&#38388;&#36317;&#25351;&#25968;&#32423;&#23567;&#30340;&#24773;&#20917;&#19979;&#38656;&#35201;$\Omega(N)$&#20010;&#21442;&#25968;&#65292;&#21516;&#26102;&#25351;&#20986;&#29616;&#26377;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2302.00834</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#35268;&#21017;&#38388;&#38548;&#25968;&#25454;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#25554;&#20540;&#30340;&#23574;&#38160;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#35268;&#21017;&#38388;&#38548;&#25968;&#25454;&#19978;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#28857;&#38388;&#36317;&#25351;&#25968;&#32423;&#23567;&#30340;&#24773;&#20917;&#19979;&#38656;&#35201;$\Omega(N)$&#20010;&#21442;&#25968;&#65292;&#21516;&#26102;&#25351;&#20986;&#29616;&#26377;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#28145;&#24230;ReLU&#32593;&#32476;&#22914;&#20309;&#22312;&#21333;&#20301;&#29699;&#20013;&#30340;$N$&#20010;&#25968;&#25454;&#28857;&#19978;&#36827;&#34892;&#20540;&#30340;&#25554;&#20540;&#65292;&#36825;&#20123;&#28857;&#20043;&#38388;&#30456;&#36317;$\delta$&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;$\delta$&#22312;$N$&#25351;&#25968;&#32423;&#23567;&#30340;&#21306;&#22495;&#20013;&#38656;&#35201;$\Omega(N)$&#20010;&#21442;&#25968;&#65292;&#36825;&#32473;&#20986;&#20102;&#35813;&#21306;&#22495;&#30340;&#23574;&#38160;&#32467;&#26524;&#65292;&#22240;&#20026;$O(N)$&#20010;&#21442;&#25968;&#24635;&#26159;&#36275;&#22815;&#30340;&#12290; &#36825;&#20063;&#34920;&#26126;&#29992;&#20110;&#35777;&#26126;VC&#32500;&#24230;&#19979;&#30028;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#20110;&#19981;&#35268;&#21017;&#38388;&#38548;&#30340;&#25968;&#25454;&#28857;&#12290;&#26368;&#21518;&#65292;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#23884;&#20837;&#31471;&#28857;&#22788;&#20026;Sobolev&#31354;&#38388;&#23454;&#29616;&#30340;&#36817;&#20284;&#36895;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00834v2 Announce Type: replace  Abstract: We study the interpolation power of deep ReLU neural networks. Specifically, we consider the question of how efficiently, in terms of the number of parameters, deep ReLU networks can interpolate values at $N$ datapoints in the unit ball which are separated by a distance $\delta$. We show that $\Omega(N)$ parameters are required in the regime where $\delta$ is exponentially small in $N$, which gives the sharp result in this regime since $O(N)$ parameters are always sufficient. This also shows that the bit-extraction technique used to prove lower bounds on the VC dimension cannot be applied to irregularly spaced datapoints. Finally, as an application we give a lower bound on the approximation rates that deep ReLU neural networks can achieve for Sobolev spaces at the embedding endpoint.
&lt;/p&gt;</description></item><item><title>FedDebug&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#39046;&#22495;&#25512;&#21160;&#20102;FL&#35843;&#35797;&#12290;FedDebug&#36890;&#36807;&#21033;&#29992;&#35760;&#24405;&#21644;&#37325;&#29616;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#20013;&#23454;&#26102;&#21327;&#21516;&#35757;&#32451;&#30340;&#20132;&#20114;&#24335;&#35843;&#35797;&#12290;</title><link>https://arxiv.org/abs/2301.03553</link><description>&lt;p&gt;
FedDebug: &#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#30340;&#31995;&#32479;&#21270;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedDebug: Systematic Debugging for Federated Learning Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03553
&lt;/p&gt;
&lt;p&gt;
FedDebug&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#39046;&#22495;&#25512;&#21160;&#20102;FL&#35843;&#35797;&#12290;FedDebug&#36890;&#36807;&#21033;&#29992;&#35760;&#24405;&#21644;&#37325;&#29616;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#20013;&#23454;&#26102;&#21327;&#21516;&#35757;&#32451;&#30340;&#20132;&#20114;&#24335;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#23458;&#25143;&#31471;&#29420;&#31435;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#24182;&#19982;&#20013;&#22830;&#32858;&#21512;&#22120;&#20849;&#20139;&#65292;&#20197;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#26080;&#27861;&#35775;&#38382;&#23458;&#25143;&#31471;&#25968;&#25454;&#21644;&#21327;&#21516;&#35757;&#32451;&#20351;FL&#22312;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;FL&#29305;&#24449;&#20026;&#35843;&#35797;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#24403;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#26102;&#65292;&#35782;&#21035;&#36131;&#20219;&#36718;&#27425;&#21644;&#23458;&#25143;&#31471;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#24320;&#21457;&#32773;&#20511;&#21161;&#20351;&#29992;&#23376;&#23458;&#25143;&#31471;&#30340;&#35797;&#38169;&#35843;&#35797;&#65292;&#24076;&#26395;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25110;&#35753;&#26410;&#26469;&#30340;FL&#36718;&#27425;&#37325;&#26032;&#35843;&#25972;&#27169;&#22411;&#65292;&#36825;&#20123;&#36807;&#31243;&#32791;&#26102;&#19988;&#25104;&#26412;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.03553v2 Announce Type: replace-cross  Abstract: In Federated Learning (FL), clients independently train local models and share them with a central aggregator to build a global model. Impermissibility to access clients' data and collaborative training make FL appealing for applications with data-privacy concerns, such as medical imaging. However, these FL characteristics pose unprecedented challenges for debugging. When a global model's performance deteriorates, identifying the responsible rounds and clients is a major pain point. Developers resort to trial-and-error debugging with subsets of clients, hoping to increase the global model's accuracy or let future FL rounds retune the model, which are time-consuming and costly.   We design a systematic fault localization framework, FedDebug, that advances the FL debugging on two novel fronts. First, FedDebug enables interactive debugging of realtime collaborative training in FL by leveraging record and replay techniques to const
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21516;&#26102;&#34920;&#31034;&#19968;&#31995;&#21015;&#29366;&#24577;&#65292;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;&#19981;&#21516;&#37327;&#23376;&#24577;&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22522;&#24577;&#30340;&#20219;&#24847;&#23616;&#37096;&#24615;&#36136;&#65292;&#26080;&#38656;&#20026;&#26032;&#30340;&#21487;&#35266;&#27979;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;</title><link>https://arxiv.org/abs/2211.16943</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#39044;&#27979;&#37327;&#23376;&#31995;&#32479;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting Properties of Quantum Systems with Conditional Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.16943
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21516;&#26102;&#34920;&#31034;&#19968;&#31995;&#21015;&#29366;&#24577;&#65292;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;&#19981;&#21516;&#37327;&#23376;&#24577;&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22522;&#24577;&#30340;&#20219;&#24847;&#23616;&#37096;&#24615;&#36136;&#65292;&#26080;&#38656;&#20026;&#26032;&#30340;&#21487;&#35266;&#27979;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26368;&#36817;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#65292;&#29992;&#20110;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;&#23545;&#20110;&#35768;&#22810;&#38388;&#38553;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#65292;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20174;&#21333;&#20010;&#37327;&#23376;&#24577;&#30340;&#27979;&#37327;&#20013;&#23398;&#20064;&#65292;&#31934;&#30830;&#37325;&#26500;&#20986;&#29366;&#24577;&#65292;&#20174;&#32780;&#39044;&#27979;&#23616;&#37096;&#21487;&#35266;&#27979;&#37327;&#12290;&#21478;&#22806;&#65292;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#20294;&#30456;&#20851;&#29366;&#24577;&#30340;&#27979;&#37327;&#26469;&#39044;&#27979;&#23616;&#37096;&#21487;&#35266;&#27979;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26469;&#21516;&#26102;&#34920;&#31034;&#19968;&#31995;&#21015;&#29366;&#24577;&#65292;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;&#19981;&#21516;&#37327;&#23376;&#24577;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#22522;&#24577;&#30340;&#20219;&#24847;&#23616;&#37096;&#24615;&#36136;&#65292;&#29978;&#33267;&#23545;&#20110;&#26410;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#65292;&#20063;&#26080;&#38656;&#20026;&#26032;&#30340;&#21487;&#35266;&#27979;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#20108;&#32500;&#38543;&#26426;&#28023;&#26862;&#22561;&#27169;&#22411;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.16943v2 Announce Type: replace-cross  Abstract: Machine learning has emerged recently as a powerful tool for predicting properties of quantum many-body systems. For many ground states of gapped Hamiltonians, generative models can learn from measurements of a single quantum state to reconstruct the state accurately enough to predict local observables. Alternatively, classification and regression models can predict local observables by learning from measurements on different but related states. In this work, we combine the benefits of both approaches and propose the use of conditional generative models to simultaneously represent a family of states, learning shared structures of different quantum states from measurements. The trained model enables us to predict arbitrary local properties of ground states, even for states not included in the training data, without necessitating further training for new observables. We first numerically validate our approach on 2D random Heisenb
&lt;/p&gt;</description></item><item><title>Centaur&#25552;&#20986;&#20102;&#38754;&#21521;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#21644;&#22522;&#20110;&#20998;&#21306;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#38480;&#21046;&#35774;&#22791;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#21442;&#19982;&#65292;&#30456;&#27604;&#26412;&#22320;&#35757;&#32451;&#33021;&#33719;&#24471;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#33410;&#32422;&#33021;&#37327;&#12290;</title><link>https://arxiv.org/abs/2211.04175</link><description>&lt;p&gt;
Centaur: &#38754;&#21521;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Centaur: Federated Learning for Constrained Edge Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04175
&lt;/p&gt;
&lt;p&gt;
Centaur&#25552;&#20986;&#20102;&#38754;&#21521;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#21644;&#22522;&#20110;&#20998;&#21306;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#38480;&#21046;&#35774;&#22791;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#21442;&#19982;&#65292;&#30456;&#27604;&#26412;&#22320;&#35757;&#32451;&#33021;&#33719;&#24471;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#33410;&#32422;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20419;&#36827;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#26032;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21487;&#31359;&#25140;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#36825;&#20123;&#35774;&#22791;&#25429;&#33719;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20869;&#23384;&#12289;&#35745;&#31639;&#12289;&#21151;&#32791;&#21644;&#36830;&#25509;&#24615;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#38459;&#30861;&#20102;&#23427;&#20204;&#21442;&#19982;FL&#12290;&#25105;&#20204;&#25552;&#20986;Centaur&#65292;&#19968;&#20010;&#22810;&#23618;FL&#26694;&#26550;&#65292;&#20351;&#36229;&#38480;&#21046;&#30340;&#35774;&#22791;&#33021;&#22815;&#39640;&#25928;&#22320;&#21442;&#19982;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;FL&#12290;Centaur&#32467;&#21512;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#24819;&#27861;&#65306;&#65288;i&#65289;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#36873;&#25321;&#19968;&#37096;&#20998;&#26679;&#26412;&#21152;&#36895;&#23398;&#20064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#20010;&#22522;&#20110;&#20998;&#21306;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#25972;&#21512;&#21516;&#19968;&#29992;&#25143;&#25317;&#26377;&#30340;&#21463;&#38480;&#21644;&#24378;&#22823;&#35774;&#22791;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#21644;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;Centaur&#30456;&#27604;&#20110;&#22312;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#35757;&#32451;&#65292;&#33021;&#22815;&#33719;&#24471;&#32422;10\%&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24179;&#22343;&#33021;&#33410;&#32422;&#32422;58\%&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#34920;&#26126;&#20102;Centaur&#22312;&#22788;&#29702;&#26102;&#30340;&#21331;&#36234;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.04175v3 Announce Type: replace  Abstract: Federated learning (FL) facilitates new applications at the edge, especially for wearable and Internet-of-Thing devices. Such devices capture a large and diverse amount of data, but they have memory, compute, power, and connectivity constraints which hinder their participation in FL. We propose Centaur, a multitier FL framework, enabling ultra-constrained devices to efficiently participate in FL on large neural nets. Centaur combines two major ideas: (i) a data selection scheme to choose a portion of samples that accelerates the learning, and (ii) a partition-based training algorithm that integrates both constrained and powerful devices owned by the same user. Evaluations, on four benchmark neural nets and three datasets, show that Centaur gains ~10\% higher accuracy than local training on constrained devices with ~58\% energy saving on average. Our experimental results also demonstrate the superior efficiency of Centaur when dealing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Group-Nonlinear-Lasso&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#35745;&#28151;&#21512;&#29289;&#20013;&#30340;&#32447;&#24615;&#31995;&#25968;&#21644;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#35777;&#26126;&#20989;&#25968;&#23545;&#39044;&#27979;&#35823;&#24046;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2210.16311</link><description>&lt;p&gt;
&#26469;&#33258;&#36830;&#32493;&#23383;&#20856;&#30340;&#28151;&#21512;&#29289;&#30340;&#31163;&#25955;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simultaneous off-the-grid learning of mixtures issued from a continuous dictionary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.16311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Group-Nonlinear-Lasso&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#35745;&#28151;&#21512;&#29289;&#20013;&#30340;&#32447;&#24615;&#31995;&#25968;&#21644;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#35777;&#26126;&#20989;&#25968;&#23545;&#39044;&#27979;&#35823;&#24046;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#20102;&#19968;&#32452;&#20449;&#21495;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#36830;&#32493;&#20449;&#21495;&#65292;&#21463;&#21040;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#27599;&#20010;&#20449;&#21495;&#26159;&#30001;&#19968;&#20010;&#26410;&#30693;&#25968;&#37327;&#30340;&#29305;&#24449;&#28151;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#29305;&#24449;&#23646;&#20110;&#19968;&#20010;&#36830;&#32493;&#23383;&#20856;&#12290;&#36830;&#32493;&#23383;&#20856;&#30001;&#19968;&#20010;&#23454;&#38750;&#32447;&#24615;&#21442;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#20449;&#21495;&#20849;&#20139;&#19968;&#20010;&#22522;&#26412;&#32467;&#26500;&#65292;&#20551;&#23450;&#27599;&#20010;&#20449;&#21495;&#30340;&#27963;&#36291;&#29305;&#24449;&#21253;&#21547;&#22312;&#19968;&#20010;&#26377;&#38480;&#31232;&#30095;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#20272;&#35745;&#28151;&#21512;&#29289;&#20013;&#30340;&#32447;&#24615;&#31995;&#25968;&#21644;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#12290;&#20248;&#21270;&#38382;&#39064;&#30001;&#25968;&#25454;&#20445;&#30495;&#24230;&#39033;&#21644;$(\ell_1,L^p)$-&#24809;&#32602;&#39033;&#32452;&#25104;&#12290;&#25105;&#20204;&#31216;&#20854;&#35299;&#20026;Group-Nonlinear-Lasso&#65292;&#24182;&#20351;&#29992;&#35777;&#26126;&#20989;&#25968;&#23545;&#39044;&#27979;&#35823;&#24046;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#20511;&#37492;&#26368;&#36817;&#20851;&#20110;&#31163;&#25955;&#23398;&#20064;&#26041;&#27861;&#20960;&#20309;&#24615;&#36136;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#34920;&#26126;&#21482;&#35201;&#29305;&#23450;&#21442;&#25968;&#28385;&#36275;&#26465;&#20214;&#65292;&#23601;&#21487;&#20197;&#26500;&#36896;&#36825;&#26679;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.16311v2 Announce Type: replace-cross  Abstract: In this paper we observe a set, possibly a continuum, of signals corrupted by noise. Each signal is a finite mixture of an unknown number of features belonging to a continuous dictionary. The continuous dictionary is parametrized by a real non-linear parameter. We shall assume that the signals share an underlying structure by assuming that each signal has its active features included in a finite and sparse set. We formulate regularized optimization problem to estimate simultaneously the linear coefficients in the mixtures and the non-linear parameters of the features. The optimization problem is composed of a data fidelity term and a $(\ell_1,L^p)$-penalty. We call its solution the Group-Nonlinear-Lasso and provide high probability bounds on the prediction error using certificate functions. Following recent works on the geometry of off-the-grid methods, we show that such functions can be constructed provided the parameters of t
&lt;/p&gt;</description></item><item><title>DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2210.12596</link><description>&lt;p&gt;
DMODE: &#26080;&#38656;&#29305;&#23450;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.12596
&lt;/p&gt;
&lt;p&gt;
DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21333;&#20010;&#25668;&#20687;&#22836;&#27979;&#37327;&#29289;&#20307;&#36317;&#31163;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#31435;&#20307;&#35270;&#35273;&#21644;&#28608;&#20809;&#38647;&#36798;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25506;&#35752;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#22312;&#32570;&#20047;&#36825;&#20123;&#24773;&#22659;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#29289;&#20307;&#29305;&#23450;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#32034;&#21487;&#33021;&#20250;&#35823;&#23548;&#19982;&#33539;&#22260;&#24191;&#27867;&#21464;&#21270;&#25110;&#23545;&#25239;&#24773;&#20917;&#19979;&#30340;&#23545;&#35937;&#65292;&#36825;&#26159;&#38754;&#21521;&#23545;&#35937;&#19981;&#21487;&#30693;&#36317;&#31163;&#20272;&#35745;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DMODE&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#30340;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#31867;&#21035;&#19981;&#21487;&#30693;&#26041;&#27861;&#12290;DMODE&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22823;&#23567;&#27874;&#21160;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#26410;&#30693;&#29289;&#20307;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.12596v2 Announce Type: replace-cross  Abstract: Utilizing a single camera for measuring object distances is a cost-effective alternative to stereo-vision and LiDAR. Although monocular distance estimation has been explored in the literature, most existing techniques rely on object class knowledge to achieve high performance. Without this contextual data, monocular distance estimation becomes more challenging, lacking reference points and object-specific cues. However, these cues can be misleading for objects with wide-range variation or adversarial situations, which is a challenging aspect of object-agnostic distance estimation. In this paper, we propose DMODE, a class-agnostic method for monocular distance estimation that does not require object class knowledge. DMODE estimates an object's distance by fusing its fluctuation in size over time with the camera's motion, making it adaptable to various object detectors and unknown objects, thus addressing these challenges. We eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#35757;&#32451;&#21160;&#24577;&#26469;&#35780;&#20272;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21253;&#21547;&#22823;&#37096;&#20998;&#34920;&#26684;&#21270;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#30456;&#36739;&#20110;&#33258;&#20449;&#23398;&#20064;&#12289;&#30452;&#25509;&#21551;&#21457;&#24335;&#21644;&#20581;&#22766;&#25552;&#21319;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2210.11327</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#35757;&#32451;&#21160;&#24577;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.11327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#35757;&#32451;&#21160;&#24577;&#26469;&#35780;&#20272;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21253;&#21547;&#22823;&#37096;&#20998;&#34920;&#26684;&#21270;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#30456;&#36739;&#20110;&#33258;&#20449;&#23398;&#20064;&#12289;&#30452;&#25509;&#21551;&#21457;&#24335;&#21644;&#20581;&#22766;&#25552;&#21319;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#24120;&#24120;&#21253;&#21547;&#26377;&#38169;&#35823;&#26631;&#35760;&#30340;&#23454;&#20363;&#65292;&#36825;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#27867;&#21270;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#26102;&#12290;&#21516;&#26102;&#65292;&#27599;&#20010;&#31034;&#20363;&#23545;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#36825;&#20419;&#20351;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#23454;&#20363;&#22312;&#27169;&#22411;&#20013;&#23545;&#22909;&#25351;&#26631;&#30340;&#36129;&#29486;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDTs&#65289;&#35757;&#32451;&#21160;&#24577;&#35745;&#31639;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21253;&#21547;&#22823;&#37096;&#20998;&#34920;&#26684;&#21270;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#65292;&#20915;&#31574;&#26641;&#38598;&#25104;&#22312;&#24615;&#33021;&#26041;&#38754;&#20173;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#19982;&#33258;&#20449;&#23398;&#20064;&#12289;&#30452;&#25509;&#21551;&#21457;&#24335;&#21644;&#20581;&#22766;&#25552;&#21319;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25972;&#20307;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26816;&#27979;&#22024;&#26434;&#26631;&#31614;&#20197;&#28165;&#29702;&#25968;&#25454;&#38598;&#12289;&#25913;&#36827;&#27169;&#22411;&#25351;&#26631;&#26041;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.11327v2 Announce Type: replace  Abstract: Real world datasets contain incorrectly labeled instances that hamper the performance of the model and, in particular, the ability to generalize out of distribution. Also, each example might have different contribution towards learning. This motivates studies to better understanding of the role of data instances with respect to their contribution in good metrics in models. In this paper we propose a method based on metrics computed from training dynamics of Gradient Boosting Decision Trees (GBDTs) to assess the behavior of each training example. We focus on datasets containing mostly tabular or structured data, for which the use of Decision Trees ensembles are still the state-of-the-art in terms of performance. Our methods achieved the best results overall when compared with confident learning, direct heuristics and a robust boosting algorithm. We show results on detecting noisy labels in order clean datasets, improving models' metri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2210.06891</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#26088;&#22312;&#32553;&#30701;&#37319;&#38598;&#26102;&#38388;&#12289;&#38477;&#20302;&#25104;&#26412;&#12289;&#21152;&#36895;&#25104;&#20687;&#35774;&#22791;&#30340;&#37096;&#32626;&#12290;&#24403;&#21069;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#19978;&#65292;&#24182;&#35201;&#27714;&#23545;&#29305;&#23450;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#65292;&#32780;&#22312;&#25104;&#20687;&#39046;&#22495;&#65292;&#20854;&#20182;&#20219;&#21153;&#21487;&#33021;&#39537;&#21160;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25104;&#20687;&#24212;&#29992;&#20013;&#23548;&#33268;&#38590;&#20197;&#27714;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#21516;&#26102;&#20248;&#21270;&#35774;&#35745;&#65288;&#22270;&#20687;&#36890;&#36947;&#38598;&#65289;&#24182;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#31354;&#38388;&#19978;&#23494;&#38598;&#37319;&#26679;&#25968;&#25454;&#65288;&#35768;&#22810;&#22270;&#20687;&#36890;&#36947;&#65289;&#36827;&#34892;&#20102;&#23569;&#37327;&#37319;&#38598;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#39044;&#20808;&#25351;&#23450;&#23610;&#23544;&#30340;&#26368;&#20339;&#25903;&#25345;&#20219;&#21153;&#30340;&#36890;&#36947;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06891v3 Announce Type: replace-cross  Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experime
&lt;/p&gt;</description></item><item><title>&#24178;&#39044;&#25968;&#25454;&#26377;&#21161;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#24178;&#39044;&#25968;&#25454;&#20013;&#28508;&#22312;&#22240;&#32032;&#25903;&#25345;&#30340;&#20960;&#20309;&#29305;&#24449;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2209.11924</link><description>&lt;p&gt;
&#24178;&#39044;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interventional Causal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.11924
&lt;/p&gt;
&lt;p&gt;
&#24178;&#39044;&#25968;&#25454;&#26377;&#21161;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#24178;&#39044;&#25968;&#25454;&#20013;&#28508;&#22312;&#22240;&#32032;&#25903;&#25345;&#30340;&#20960;&#20309;&#29305;&#24449;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#20302;&#32423;&#24863;&#23448;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#32423;&#28508;&#22312;&#22240;&#32032;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#27979;&#25968;&#25454;&#21644;&#32467;&#26500;&#20551;&#35774;&#65288;&#22914;&#26465;&#20214;&#29420;&#31435;&#24615;&#65289;&#26469;&#35782;&#21035;&#28508;&#22312;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#24178;&#39044;&#25968;&#25454;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#24178;&#39044;&#25968;&#25454;&#33021;&#21542;&#20419;&#36827;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#24178;&#39044;&#25968;&#25454;&#36890;&#24120;&#25658;&#24102;&#28508;&#22312;&#22240;&#32032;&#25903;&#25345;&#30340;&#20960;&#20309;&#29305;&#24449;&#65288;&#21363;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#21487;&#33021;&#37319;&#21462;&#30340;&#20540;&#65289;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#24403;&#28508;&#22312;&#22240;&#32032;&#23384;&#22312;&#22240;&#26524;&#32852;&#31995;&#26102;&#65292;&#24178;&#39044;&#21487;&#20197;&#25171;&#30772;&#24178;&#39044;&#28508;&#22312;&#22240;&#32032;&#25903;&#25345;&#21644;&#23427;&#20204;&#31062;&#20808;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#21033;&#29992;&#36825;&#19968;&#20107;&#23454;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#33719;&#24471;&#23436;&#32654;$do$&#24178;&#39044;&#25968;&#25454;&#21518;&#65292;&#21487;&#20197;&#30830;&#23450;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#32032;&#65292;&#32780;&#19988;&#33021;&#22815;&#23454;&#29616;&#21306;&#22359;&#20223;&#23556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.11924v4 Announce Type: replace-cross  Abstract: Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors' support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents' support and their ancestors'. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect $do$ interventions. Moreover, we can achieve block affine identific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;GNNInterpreter&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#32423;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#29983;&#25104;&#22270;&#20998;&#24067;&#26469;&#25581;&#31034;GNN&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2209.07924</link><description>&lt;p&gt;
GNNInterpreter&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#27169;&#22411;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.07924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GNNInterpreter&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#32423;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#29983;&#25104;&#22270;&#20998;&#24067;&#26469;&#25581;&#31034;GNN&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#22270;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#25216;&#26415;&#31361;&#30772;&#20351;&#20154;&#20204;&#20135;&#29983;&#20102;&#30097;&#38382;&#65306;GNN&#26159;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#30340;&#65292;&#25105;&#20204;&#33021;&#21542;&#39640;&#24230;&#20449;&#20219;&#20854;&#39044;&#27979;&#65311;&#22312;&#19968;&#20123;&#20851;&#38190;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#65292;&#20570;&#20986;&#38169;&#35823;&#20915;&#31574;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#27492;&#22312;&#24212;&#29992;&#20043;&#21069;&#35299;&#37322;GNN&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#19981;&#21516;GNN&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#27169;&#22411;&#32423;&#35299;&#37322;&#26041;&#27861;GNNInterpreter&#65292;&#26469;&#35299;&#37322;GNN&#27169;&#22411;&#30340;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GNNInterpreter&#36890;&#36807;&#20248;&#21270;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20135;&#29983;GNN&#22312;&#20570;&#20986;&#26576;&#20010;&#39044;&#27979;&#26102;&#35797;&#22270;&#26816;&#27979;&#21040;&#30340;&#26368;&#20855;&#36776;&#35782;&#24615;&#22270;&#27169;&#24335;&#30340;&#27010;&#29575;&#29983;&#25104;&#22270;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.07924v4 Announce Type: replace-cross  Abstract: Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#22909;&#22788;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#36873;&#25321;&#23614;&#25968;&#21644;&#25351;&#25968;&#20301;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;FP8&#26684;&#24335;&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#30340;&#34920;&#29616;&#26356;&#22909;&#65292;&#23588;&#20854;&#23545;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#26469;&#35828;&#20248;&#20110;INT8&#26684;&#24335;&#12290;</title><link>https://arxiv.org/abs/2208.09225</link><description>&lt;p&gt;
FP8&#37327;&#21270;&#65306;&#25351;&#25968;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
FP8 Quantization: The Power of the Exponent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.09225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#22909;&#22788;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#36873;&#25321;&#23614;&#25968;&#21644;&#25351;&#25968;&#20301;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;FP8&#26684;&#24335;&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#30340;&#34920;&#29616;&#26356;&#22909;&#65292;&#23588;&#20854;&#23545;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#26469;&#35828;&#20248;&#20110;INT8&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#20197;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#26102;&#65292;&#20302;&#27604;&#29305;&#25972;&#25968;&#26159;&#25928;&#29575;&#30340;&#39318;&#36873;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;&#20302;&#27604;&#29305;&#28014;&#28857;&#25968;&#20855;&#26377;&#39069;&#22806;&#30340;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#23558;&#19968;&#20123;&#27604;&#29305;&#20998;&#37197;&#21040;&#25351;&#25968;&#21051;&#24230;&#19978;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#28014;&#28857;&#26684;&#24335;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#36825;&#31181;&#22909;&#22788;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;FP8&#26684;&#24335;&#30340;&#36873;&#25321;&#65292;&#21253;&#25324;&#23614;&#25968;&#21644;&#25351;&#25968;&#20301;&#25968;&#30340;&#37325;&#35201;&#36873;&#25321;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#21738;&#20123;&#35774;&#32622;&#20013;&#36825;&#20123;&#36873;&#25321;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21457;&#29616;&#22914;&#20309;&#36716;&#21270;&#20026;&#30495;&#23454;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;FP8&#27169;&#25311;&#30340;&#39640;&#25928;&#23454;&#29616;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;FP8&#26684;&#24335;&#20013;&#30340;&#27604;&#20363;&#21442;&#25968;&#21644;&#25351;&#25968;&#20301;&#25968;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#35770;&#26159;&#65292;&#22312;&#23545;&#21508;&#31181;&#32593;&#32476;&#36827;&#34892;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#65292;FP8&#26684;&#24335;&#20248;&#20110;INT8&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.09225v2 Announce Type: replace  Abstract: When quantizing neural networks for efficient inference, low-bit integers are the go-to format for efficiency. However, low-bit floating point numbers have an extra degree of freedom, assigning some bits to work on an exponential scale instead. This paper in-depth investigates this benefit of the floating point format for neural network inference. We detail the choices that can be made for the FP8 format, including the important choice of the number of bits for the mantissa and exponent, and show analytically in which settings these choices give better performance. Then we show how these findings translate to real networks, provide an efficient implementation for FP8 simulation, and a new algorithm that enables the learning of both the scale parameters and the number of exponent bits in the FP8 format. Our chief conclusion is that when doing post-training quantization for a wide range of networks, the FP8 format is better than INT8 i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;&#19979;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;RKHS&#36317;&#31163;&#34913;&#37327;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#22788;&#29702;&#36801;&#31227;&#65292;&#19968;&#31181;&#38656;&#35201;&#24050;&#30693;&#27491;&#28304;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#32858;&#21512;&#25216;&#26415;&#23454;&#29616;&#26080;&#28304;&#20449;&#24687;&#30340;&#31283;&#20581;&#20256;&#36755;&#12290;&#21516;&#26102;&#24314;&#31435;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2206.04277</link><description>&lt;p&gt;
&#20851;&#20110;&#20989;&#25968;&#32447;&#24615;&#27169;&#22411;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Hypothesis Transfer Learning of Functional Linear Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;&#19979;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;RKHS&#36317;&#31163;&#34913;&#37327;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#22788;&#29702;&#36801;&#31227;&#65292;&#19968;&#31181;&#38656;&#35201;&#24050;&#30693;&#27491;&#28304;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#32858;&#21512;&#25216;&#26415;&#23454;&#29616;&#26080;&#28304;&#20449;&#24687;&#30340;&#31283;&#20581;&#20256;&#36755;&#12290;&#21516;&#26102;&#24314;&#31435;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#26694;&#26550;&#19979;&#30340;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;&#65288;FLR&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#65292;&#35266;&#23519;&#21040;&#29616;&#26377;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;TL&#25216;&#26415;&#19982;&#22522;&#20110;&#25130;&#26029;&#30340;FLR&#26041;&#27861;&#19981;&#20860;&#23481;&#65292;&#22240;&#20026;&#20989;&#25968;&#25968;&#25454;&#22312;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#32500;&#30340;&#65292;&#24182;&#30001;&#24179;&#28369;&#30340;&#22522;&#30784;&#36807;&#31243;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;RKHS&#36317;&#31163;&#26469;&#34913;&#37327;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20801;&#35768;&#20256;&#36755;&#30340;&#20449;&#24687;&#31867;&#22411;&#19982;&#25152;&#26045;&#21152;&#30340;RKHS&#30340;&#23646;&#24615;&#30456;&#20851;&#32852;&#12290;&#22522;&#20110;&#20551;&#35774;&#20559;&#31227;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22312;&#24050;&#30693;&#27491;&#28304;&#26102;&#36827;&#34892;&#20256;&#36755;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#32858;&#21512;&#25216;&#26415;&#23454;&#29616;&#26080;&#38656;&#20808;&#39564;&#20449;&#24687;&#30340;&#31283;&#20581;&#20256;&#36755;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#24314;&#31435;&#20102;&#19979;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20139;&#26377;&#21305;&#37197;&#30340;&#28176;&#36817;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04277v4 Announce Type: replace-cross  Abstract: We study the transfer learning (TL) for the functional linear regression (FLR) under the Reproducing Kernel Hilbert Space (RKHS) framework, observing the TL techniques in existing high-dimensional linear regression is not compatible with the truncation-based FLR methods as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. We measure the similarity across tasks using RKHS distance, allowing the type of information being transferred tied to the properties of the imposed RKHS. Building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. We establish lower bounds for this learning problem and show the proposed algorithms enjoy a matching asymptotic upper bound. These analyses
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;KGE&#27169;&#22411;&#65292;&#21517;&#20026;HypH&#65292;&#21033;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#23884;&#20837;&#20998;&#23618;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#20013;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2204.13704</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#20998;&#23618;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20302;&#32500;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction in Low Dimensions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.13704
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;KGE&#27169;&#22411;&#65292;&#21517;&#20026;HypH&#65292;&#21033;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#23884;&#20837;&#20998;&#23618;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#20013;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#24050;&#34987;&#35777;&#23454;&#26159;&#25512;&#26029;&#30693;&#35782;&#22270;&#65288;KGs&#65289;&#20013;&#32570;&#22833;&#38142;&#25509;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#24120;&#23558;&#23454;&#20307;&#26144;&#23556;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#23558;&#20851;&#31995;&#35270;&#20026;&#23454;&#20307;&#30340;&#36716;&#25442;&#12290; &#26368;&#36817;&#65292;&#19968;&#20123;&#27431;&#20960;&#37324;&#24471;KGE&#26041;&#27861;&#24050;&#32463;&#22686;&#24378;&#65292;&#20197;&#24314;&#27169;KGs&#20013;&#24120;&#35265;&#30340;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#65292;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#24615;&#33021;&#12290; &#20026;&#20102;&#23884;&#20837;&#20998;&#23618;&#25968;&#25454;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#24050;&#32463;&#25104;&#20026;&#20256;&#32479;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#24230;&#20445;&#30495;&#24230;&#21644;&#36739;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290; &#19982;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19981;&#21516;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#25552;&#20379;&#20102;&#26080;&#25968;&#21487;&#20379;&#36873;&#25321;&#30340;&#26354;&#29575;&#12290; &#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#36229;&#20960;&#20309;KGE&#26041;&#27861;&#38590;&#20197;&#25163;&#21160;&#33719;&#21462;&#26368;&#20339;&#26354;&#29575;&#35774;&#32622;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#22320;&#23545;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#12290; &#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textbf{Hyp}$erbolic $\textbf{H}$ierarchical $\textb
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.13704v2 Announce Type: replace-cross  Abstract: Knowledge graph embeddings (KGE) have been validated as powerful methods for inferring missing links in knowledge graphs (KGs) that they typically map entities into Euclidean space and treat relations as transformations of entities. Recently, some Euclidean KGE methods have been enhanced to model semantic hierarchies commonly found in KGs, improving the performance of link prediction. To embed hierarchical data, hyperbolic space has emerged as a promising alternative to traditional Euclidean space, offering high fidelity and lower memory consumption. Unlike Euclidean, hyperbolic space provides countless curvatures to choose from. However, it is difficult for existing hyperbolic KGE methods to obtain the optimal curvature settings manually, thereby limiting their ability to effectively model semantic hierarchies. To address this limitation, we propose a novel KGE model called $\textbf{Hyp}$erbolic $\textbf{H}$ierarchical $\textb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32593;&#32476;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#31751;&#20195;&#25968;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#31751;&#20195;&#25968;&#30340;&#20132;&#25442;&#22270;&#20013;&#23384;&#22312;&#19968;&#31181;&#20248;&#38597;&#30340;&#23545;&#31216;&#24615;&#65292;&#21487;&#25104;&#21151;&#20351;&#29992;&#31181;&#23376;&#25968;&#25454;&#23545;&#31751;&#20195;&#25968;&#36827;&#34892;&#20998;&#31867;&#65292;&#20934;&#30830;&#24615;&#36229;&#36807;0.9&#12290;</title><link>https://arxiv.org/abs/2203.13847</link><description>&lt;p&gt;
&#31751;&#20195;&#25968;&#65306;&#32593;&#32476;&#31185;&#23398;&#19982;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cluster Algebras: Network Science and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.13847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32593;&#32476;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#31751;&#20195;&#25968;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#31751;&#20195;&#25968;&#30340;&#20132;&#25442;&#22270;&#20013;&#23384;&#22312;&#19968;&#31181;&#20248;&#38597;&#30340;&#23545;&#31216;&#24615;&#65292;&#21487;&#25104;&#21151;&#20351;&#29992;&#31181;&#23376;&#25968;&#25454;&#23545;&#31751;&#20195;&#25968;&#36827;&#34892;&#20998;&#31867;&#65292;&#20934;&#30830;&#24615;&#36229;&#36807;0.9&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31751;&#20195;&#25968;&#26368;&#36817;&#22312;&#25968;&#23398;&#21644;&#29289;&#29702;&#23398;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20855;&#20307;&#37319;&#29992;&#20102;&#32593;&#32476;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#23558;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#31361;&#21464;&#31867;&#22411;&#30340;&#31751;&#20195;&#25968;&#30340;&#20132;&#25442;&#22270;&#20013;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20197;&#19981;&#36890;&#36807;&#35782;&#21035;&#31751;&#20043;&#38388;&#30340;&#32622;&#25442;&#31561;&#20215;&#24615;&#26469;&#34920;&#31034;&#22270;&#26102;&#65292;&#22312;&#20957;&#32858;&#22270;&#23884;&#20837;&#20013;&#20250;&#20986;&#29616;&#20248;&#38597;&#30340;&#23545;&#31216;&#24615;&#12290;&#23545;&#20110;&#31209;&#19981;&#36229;&#36807;5&#30340;&#26377;&#38480;Dynkin&#31867;&#22411;&#20195;&#25968;&#65292;&#35745;&#31639;&#20102;&#19982;&#35813;&#23545;&#31216;&#24615;&#30456;&#20851;&#30340;&#31181;&#23376;&#25968;&#37327;&#21644;&#20851;&#32852;&#30340;&#20934;&#32463;&#31283;&#22270;&#30340;&#27604;&#29575;&#65292;&#24182;&#25512;&#27979;&#20102;&#26356;&#39640;&#31561;&#32423;&#30340;&#24773;&#20917;&#12290;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#20351;&#29992;&#31181;&#23376;&#25968;&#25454;&#23545;&#31751;&#20195;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#23398;&#20064;&#24615;&#33021;&#22312;&#30456;&#21516;&#31361;&#21464;&#31867;&#22411;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#20195;&#25968;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#19978;&#36229;&#36807;&#20102;0.9&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.13847v2 Announce Type: replace-cross  Abstract: Cluster algebras have recently become an important player in mathematics and physics. In this work, we investigate them through the lens of modern data science, specifically with techniques from network science and machine learning. Network analysis methods are applied to the exchange graphs for cluster algebras of varying mutation types. The analysis indicates that when the graphs are represented without identifying by permutation equivalence between clusters an elegant symmetry emerges in the quiver exchange graph embedding. The ratio between number of seeds and number of quivers associated to this symmetry is computed for finite Dynkin type algebras up to rank 5, and conjectured for higher ranks. Simple machine learning techniques successfully learn to classify cluster algebras using the data of seeds. The learning performance exceeds 0.9 accuracies between algebras of the same mutation type and between types, as well as rel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20271;&#24681;&#26031;&#22359;&#27969;&#21464;&#20998;&#25512;&#26029;&#65288;BF-VI&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#36924;&#36817;&#22797;&#26434;&#30340;&#22810;&#20803;&#21518;&#39564;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;VI&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.05650</link><description>&lt;p&gt;
&#21464;&#20998;&#36125;&#21494;&#26031;&#20013;&#30340;&#26580;&#24615;&#21518;&#39564;&#30340;&#20271;&#24681;&#26031;&#22359;&#27969;
&lt;/p&gt;
&lt;p&gt;
Bernstein Flows for Flexible Posteriors in Variational Bayes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.05650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20271;&#24681;&#26031;&#22359;&#27969;&#21464;&#20998;&#25512;&#26029;&#65288;BF-VI&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#36924;&#36817;&#22797;&#26434;&#30340;&#22810;&#20803;&#21518;&#39564;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;VI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#26469;&#36817;&#20284;&#38590;&#20197;&#35745;&#31639;&#21518;&#39564;&#30340;&#25216;&#26415;&#12290;&#19982;MCMC&#30456;&#27604;&#65292;VI&#21487;&#20197;&#25193;&#23637;&#21040;&#35768;&#22810;&#35266;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21518;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;VI&#26041;&#27861;&#36890;&#24120;&#20135;&#29983;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#21518;&#39564;&#36817;&#20284;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20271;&#24681;&#26031;&#22359;&#27969;&#21464;&#20998;&#25512;&#26029;&#65288;BF-VI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31283;&#20581;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#36275;&#22815;&#28789;&#27963;&#20197;&#36924;&#36817;&#22797;&#26434;&#30340;&#22810;&#20803;&#21518;&#39564;&#12290;BF-VI&#32467;&#21512;&#20102;&#24402;&#19968;&#21270;&#27969;&#21644;&#22522;&#20110;&#20271;&#24681;&#26031;&#22810;&#39033;&#24335;&#30340;&#36716;&#25442;&#27169;&#22411;&#30340;&#24605;&#24819;&#12290;&#22312;&#22522;&#20934;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;BF-VI&#35299;&#19982;&#20934;&#30830;&#30340;&#21518;&#39564;&#12289;MCMC&#35299;&#20197;&#21450;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;VI&#31561;&#29616;&#26377;VI&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20302;&#32500;&#27169;&#22411;&#20013;&#65292;BF-VI&#21487;&#20197;&#20934;&#30830;&#36924;&#36817;&#30495;&#23454;&#21518;&#39564;&#65307;&#32780;&#22312;&#39640;&#32500;&#27169;&#22411;&#20013;&#65292;BF-VI&#20248;&#20110;&#20854;&#20182;VI&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;BF-VI&#38024;&#23545;&#21322;&#32467;&#26500;&#21270;Mela&#24320;&#21457;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.05650v2 Announce Type: replace-cross  Abstract: Variational inference (VI) is a technique to approximate difficult to compute posteriors by optimization. In contrast to MCMC, VI scales to many observations. In the case of complex posteriors, however, state-of-the-art VI approaches often yield unsatisfactory posterior approximations. This paper presents Bernstein flow variational inference (BF-VI), a robust and easy-to-use method, flexible enough to approximate complex multivariate posteriors. BF-VI combines ideas from normalizing flows and Bernstein polynomial-based transformation models. In benchmark experiments, we compare BF-VI solutions with exact posteriors, MCMC solutions, and state-of-the-art VI methods including normalizing flow based VI. We show for low-dimensional models that BF-VI accurately approximates the true posterior; in higher-dimensional models, BF-VI outperforms other VI methods. Further, we develop with BF-VI a Bayesian model for the semi-structured Mela
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;State-Dependent Causal Inference&#65288;SDCI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#25104;&#21151;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2110.06257</link><description>&lt;p&gt;
&#20174;&#26377;&#26465;&#20214;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Conditionally Stationary Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.06257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;State-Dependent Causal Inference&#65288;SDCI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#25104;&#21151;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65292;&#21363;&#20174;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#28508;&#22312;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;AI&#31995;&#32479;&#20855;&#26377;&#26497;&#22823;&#25361;&#25112;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#32972;&#26223;&#19979;&#65292;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#32771;&#34385;&#20855;&#26377;&#23436;&#20840;&#35266;&#27979;&#21464;&#37327;&#21644;/&#25110;&#26469;&#33258;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#30340;&#21463;&#38480;&#22330;&#26223;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#26469;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#21363;&#22312;&#26465;&#20214;&#19978;&#26159;&#24179;&#31283;&#30340;&#26465;&#20214;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#38750;&#24179;&#31283;&#34892;&#20026;&#34987;&#24314;&#27169;&#20026;&#22312;&#19968;&#32452;&#65288;&#21487;&#33021;&#26159;&#38544;&#34255;&#30340;&#65289;&#29366;&#24577;&#21464;&#37327;&#19978;&#30340;&#24179;&#31283;&#24615;&#12290;&#21629;&#21517;&#20026;State-Dependent Causal Inference&#65288;SDCI&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#21487;&#35777;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#65292;&#35777;&#26126;&#22312;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#19979;&#65292;&#24182;&#22312;&#23384;&#22312;&#38544;&#34255;&#29366;&#24577;&#26102;&#32463;&#39564;&#24615;&#22320;&#23454;&#29616;&#12290;&#21518;&#32773;&#36890;&#36807;&#23545;&#21512;&#25104;&#32447;&#24615;&#31995;&#32479;&#21644;&#38750;&#32447;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#30340;&#23454;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;SDCI&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#32447;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.06257v2 Announce Type: replace  Abstract: Causal discovery, i.e., inferring underlying causal relationships from observational data, has been shown to be highly challenging for AI systems. In time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. We develop a causal discovery approach to handle a wide class of non-stationary time-series that are conditionally stationary, where the non-stationary behaviour is modeled as stationarity conditioned on a set of (possibly hidden) state variables. Named State-Dependent Causal Inference (SDCI), our approach is able to recover the underlying causal dependencies, provably with fully-observed states and empirically with hidden states. The latter is confirmed by experiments on synthetic linear system and nonlinear particle interaction data, where SDCI achieves superior performance over baseline causal discovery methods
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#24615;&#27169;&#25311;&#38544;&#34255;&#23618;&#36755;&#20986;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2105.08620</link><description>&lt;p&gt;
&#20855;&#26377;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples Detection with Bayesian Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.08620
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#24615;&#27169;&#25311;&#38544;&#34255;&#23618;&#36755;&#20986;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#65292;&#20854;&#28789;&#24863;&#26469;&#28304;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#38543;&#26426;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#22120;&#30340;&#24179;&#28369;&#24615;&#65292;&#20351;&#24471;&#26356;&#23481;&#26131;&#27169;&#25311;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#31616;&#31216;&#20026;BATer&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#28982;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#38543;&#26426;&#24615;&#26469;&#27169;&#25311;&#38544;&#34255;&#23618;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20998;&#24067;&#30340;&#31163;&#25955;&#24615;&#26469;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.08620v3 Announce Type: replace-cross  Abstract: In this paper, we propose a new framework to detect adversarial examples motivated by the observations that random components can improve the smoothness of predictors and make it easier to simulate the output distribution of a deep neural network. With these observations, we propose a novel Bayesian adversarial example detector, short for BATer, to improve the performance of adversarial example detection. Specifically, we study the distributional difference of hidden layer output between natural and adversarial examples, and propose to use the randomness of the Bayesian neural network to simulate hidden layer output distribution and leverage the distribution dispersion to detect adversarial examples. The advantage of a Bayesian neural network is that the output is stochastic while a deep neural network without random components does not have such characteristics. Empirical results on several benchmark datasets against popular a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#19981;&#20934;&#30830;&#26799;&#24230;&#30340; Langevin Monte Carlo &#31639;&#27861;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#22312;Wasserstein-2&#36317;&#31163;&#20013;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/1710.00095</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#20934;&#30830;&#26799;&#24230;&#30340; Langevin Monte Carlo &#30340;&#29992;&#25143;&#21451;&#22909;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1710.00095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#19981;&#20934;&#30830;&#26799;&#24230;&#30340; Langevin Monte Carlo &#31639;&#27861;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#22312;Wasserstein-2&#36317;&#31163;&#20013;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#24050;&#30693;&#20809;&#28369;&#19988;&#24378;&#23545;&#25968;&#20985;&#20989;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#20110;(&#39640;&#24230;&#36807;&#38459;&#23612;) Langevin &#25193;&#25955;&#30340;&#31163;&#25955;&#21270;&#30340;&#36817;&#20284;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#22312;Wasserstein-2&#36317;&#31163;&#20013;&#27979;&#37327;&#30340;&#35823;&#24046;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#26041;&#21521;&#19978;&#25913;&#36827;&#25110;&#25193;&#23637;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#20248;&#21270;&#30340;&#19981;&#23450;&#27493;&#38271;&#19968;&#38454; Langevin Monte Carlo(LMC)&#31639;&#27861;&#30340;&#35823;&#24046;&#32473;&#20986;&#20102;&#19978;&#30028;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;&#20248;&#28857;&#26159;&#19981;&#21463;&#26102;&#38388;&#38480;&#21046;(&#25105;&#20204;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#30446;&#26631;&#31934;&#24230;)&#65292;&#24182;&#19988;&#22312;&#23545;&#24212;&#30340;&#24120;&#25968;&#27493;&#38271;&#32467;&#26524;&#22522;&#30784;&#19978;&#25552;&#21319;&#20102;&#23545;&#25968;&#22240;&#23376;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#65292;&#20294;&#21487;&#20197;&#33719;&#24471;&#21069;&#36848;&#26799;&#24230;&#30340;&#36817;&#20284;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1710.00095v4 Announce Type: replace-cross  Abstract: In this paper, we study the problem of sampling from a given probability density function that is known to be smooth and strongly log-concave. We analyze several methods of approximate sampling based on discretizations of the (highly overdamped) Langevin diffusion and establish guarantees on its error measured in the Wasserstein-2 distance. Our guarantees improve or extend the state-of-the-art results in three directions. First, we provide an upper bound on the error of the first-order Langevin Monte Carlo (LMC) algorithm with optimized varying step-size. This result has the advantage of being horizon free (we do not need to know in advance the target precision) and to improve by a logarithmic factor the corresponding result for the constant step-size. Second, we study the case where accurate evaluations of the gradient of the log-density are unavailable, but one can have access to approximations of the aforementioned gradient.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#36229;&#20302;&#21151;&#32791;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#36827;&#34892;&#39640;&#25928;&#30340;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#23558;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;&#33258;&#36866;&#24212;&#20998;&#31867;&#27169;&#22359;&#32467;&#21512;&#20351;&#29992;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.15236</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#36229;&#20302;&#21151;&#32791;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#30340;&#39640;&#25928;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones. (arXiv:2401.15236v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#36229;&#20302;&#21151;&#32791;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#36827;&#34892;&#39640;&#25928;&#30340;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#23558;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;&#33258;&#36866;&#24212;&#20998;&#31867;&#27169;&#22359;&#32467;&#21512;&#20351;&#29992;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#20110;10&#21400;&#31859;&#30452;&#24452;&#30340;&#32435;&#31859;&#26080;&#20154;&#26426;&#30001;&#20110;&#20854;&#36866;&#29992;&#20110;&#36739;&#22823;&#30340;&#39134;&#34892;&#26080;&#20154;&#26426;&#26080;&#27861;&#21040;&#36798;&#30340;&#29421;&#31364;&#29615;&#22659;&#21644;&#20154;&#31867;&#38468;&#36817;&#30340;&#29305;&#28857;&#65292;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20854;&#24494;&#23567;&#30340;&#22806;&#24418;&#20063;&#24102;&#26469;&#20102;&#19968;&#20010;&#20027;&#35201;&#30340;&#32570;&#28857;&#65306;&#36229;&#38480;&#30340;&#20869;&#23384;&#21644;&#22788;&#29702;&#22120;&#29992;&#20110;&#20854;&#24863;&#30693;&#27969;&#31243;&#30340;&#26426;&#36733;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24378;&#35843;&#35745;&#31639;&#25928;&#29575;&#21644;&#33410;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20915;&#23450;&#19968;&#20010;&#23436;&#20840;&#24037;&#20316;&#30340;&#38381;&#29615;&#31995;&#32479;&#21644;&#19968;&#20010;&#22833;&#36133;&#30340;&#38381;&#29615;&#31995;&#32479;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#26497;&#20854;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#25191;&#34892;&#22522;&#20110;&#35270;&#35273;&#30340;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#22238;&#24402;&#24615;&#33021;&#19982;&#35745;&#31639;&#25104;&#26412;&#25240;&#34935;&#30340;&#26368;&#26032;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;CNN&#19982;&#19968;&#20010;&#33258;&#36866;&#24212;&#20998;&#31867;&#27169;&#22359;&#32452;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sub-10cm diameter nano-drones are gaining momentum thanks to their applicability in scenarios prevented to bigger flying drones, such as in narrow environments and close to humans. However, their tiny form factor also brings their major drawback: ultra-constrained memory and processors for the onboard execution of their perception pipelines. Therefore, lightweight deep learning-based approaches are becoming increasingly popular, stressing how computational efficiency and energy-saving are paramount as they can make the difference between a fully working closed-loop system and a failing one. In this work, to maximize the exploitation of the ultra-limited resources aboard nano-drones, we present a novel adaptive deep learning-based mechanism for the efficient execution of a vision-based human pose estimation task. We leverage two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different regression performance vs. computational costs trade-offs. By combining these CNNs wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#22270;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29983;&#25104;&#26679;&#26412;&#26102;&#21033;&#29992;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14340</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#22270;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29983;&#25104;&#26679;&#26412;&#26102;&#21033;&#29992;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#30340;&#39640;&#26031;&#22270;&#27169;&#22411;&#65292;&#24182;&#19988;&#32467;&#21512;&#20102;&#20851;&#20110;&#24213;&#23618;&#22270;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#28857;&#20272;&#35745;&#26041;&#27861;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#25110;&#26368;&#22823;&#21518;&#39564;&#20934;&#21017;&#65292;&#24182;&#20351;&#29992;&#65288;&#31616;&#21333;&#30340;&#65289;&#31934;&#24230;&#30697;&#38453;&#20808;&#39564;&#26469;&#25552;&#20379;&#28857;&#20272;&#35745;&#12290;&#25105;&#20204;&#32771;&#34385;&#23545;&#22270;&#36827;&#34892;&#20808;&#39564;&#65292;&#24182;&#20381;&#36182;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#30001;&#20110;&#26391;&#26684;&#32500;&#33021;&#37319;&#26679;&#22120;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#22270;&#20808;&#39564;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#20174;&#22270;&#25968;&#25454;&#38598;&#65288;&#20107;&#20808;&#21487;&#29992;&#25110;&#20174;&#24050;&#30693;&#20998;&#24067;&#29983;&#25104;&#65289;&#20272;&#35745;&#24471;&#20998;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12686</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65306;&#19968;&#31181;&#28151;&#21512;&#22270;&#24418;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach. (arXiv:2401.12686v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22823;&#35268;&#27169;&#20195;&#29702;&#32676;&#20307;&#30340;&#34892;&#20026;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34429;&#28982;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#22312;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#20195;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#19988;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;MFGs&#65289;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20195;&#29702;&#20043;&#38388;&#30340;&#32593;&#32476;&#32467;&#26500;&#30340;&#22270;&#24418;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;GMFGs&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#35832;&#22810;&#20248;&#28857;&#65292;&#20294;GMFGs&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21463;&#21040;&#22270;&#24418;&#21482;&#33021;&#25429;&#25417;&#23494;&#38598;&#22270;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#23454;&#39564;&#35777;&#26126;&#30340;&#32593;&#32476;&#26174;&#31034;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#31232;&#30095;&#24615;&#65292;&#20363;&#22914;&#24130;&#24459;&#22270;&#65292;&#22240;&#27492;GMFG&#26694;&#26550;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#23545;&#23616;&#21338;&#24328;&#65288;GXMFGs&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#24314;&#31435;&#22312;&#22270;&#35770;&#27010;&#24565;&#22270;&#24418;&#25193;&#23637;&#65288;graphexes&#65289;&#22522;&#30784;&#19978;&#12290;&#22270;&#24418;&#25193;&#23637;&#26159;&#31232;&#30095;&#22270;&#24207;&#21015;&#30340;&#26497;&#38480;&#23545;&#35937;&#65292;&#36824;&#20855;&#26377;&#20854;&#20182;&#19968;&#20123;&#29702;&#24819;&#29305;&#24615;&#65292;&#22914;sma
&lt;/p&gt;
&lt;p&gt;
Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the sma
&lt;/p&gt;</description></item><item><title>GNNShap&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;GNNShap&#36890;&#36807;&#25277;&#26679;&#12289;&#24182;&#34892;&#21270;&#35745;&#31639;&#31561;&#25216;&#26415;&#25552;&#39640;&#20102;&#35299;&#37322;&#36895;&#24230;&#21644;&#31934;&#32454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04829</link><description>&lt;p&gt;
GNNShap: &#20351;&#29992;Shapley&#20540;&#24555;&#36895;&#32780;&#20934;&#30830;&#35299;&#37322;GNN&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
GNNShap: Fast and Accurate GNN Explanations using Shapley Values. (arXiv:2401.04829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04829
&lt;/p&gt;
&lt;p&gt;
GNNShap&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;GNNShap&#36890;&#36807;&#25277;&#26679;&#12289;&#24182;&#34892;&#21270;&#35745;&#31639;&#31561;&#25216;&#26415;&#25552;&#39640;&#20102;&#35299;&#37322;&#36895;&#24230;&#21644;&#31934;&#32454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#19968;&#31181;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;GNN&#34987;&#35748;&#20026;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#12290;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;Shapley&#20540;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#20013;&#30740;&#31350;&#36739;&#23569;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;Shapley&#20540;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#65292;&#28982;&#32780;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#26469;&#36817;&#20284;Shapley&#20540;&#65307;&#26377;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23567;&#21644;&#22823;&#30340;&#32852;&#30431;&#22823;&#23567;&#65292;&#24182;&#19988;&#23427;&#20204;&#27604;&#20854;&#20182;&#35299;&#37322;&#26041;&#27861;&#24930;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNShap&#65292;&#23427;&#25552;&#20379;&#36793;&#30340;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#22270;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#21644;&#31934;&#32454;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25152;&#26377;&#32852;&#30431;&#22823;&#23567;&#36827;&#34892;&#25277;&#26679;&#12289;&#22312;GPU&#19978;&#24182;&#34892;&#25277;&#26679;&#21644;&#21152;&#36895;&#27169;&#22411;&#31561;&#26041;&#38754;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theory-based Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value-based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04385</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#25200;&#21160;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#28041;&#21450;&#21040;&#25764;&#38144;&#25968;&#25454;&#35760;&#24405;&#21644;&#20943;&#23567;&#35813;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30446;&#26631;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#21442;&#25968;&#25200;&#21160;&#30340;&#26435;&#37325;&#21435;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#28041;&#21450;&#21040;&#20840;&#23616;&#20462;&#25913;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;Top-K&#21644;Random-k&#21442;&#25968;&#25200;&#21160;&#19981;&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#38544;&#31169;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25511;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#35780;&#20272;&#26426;&#22120;&#21435;&#23398;&#20064;&#25928;&#26524;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22312;&#21435;&#23398;&#20064;&#21644;&#21097;&#20313;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#21363;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#26080;&#27861;&#23545;&#21435;&#23398;&#20064;&#31243;&#24230;&#36827;&#34892;&#20934;&#30830;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02602</link><description>&lt;p&gt;
&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29702;&#35299;&#19990;&#30028;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#23558;&#20449;&#24687;&#21387;&#32553;&#25104;&#25277;&#35937;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#20004;&#20010;&#26631;&#24535;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#20027;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#32479;&#31216;&#20026;&#22240;&#26524;&#25277;&#35937;&#29702;&#35770;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#22312;&#30495;&#23454;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;&#25277;&#35937;&#29702;&#35770;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30495;&#23454;&#26426;&#21046;&#26159;&#26410;&#30693;&#30340;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21464;&#37327;&#21450;&#20854;&#22495;&#36827;&#34892;&#32858;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25277;&#35937;&#23478;&#26063;&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#21644;&#27010;&#25324;&#20102;&#20043;&#21069;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#24341;&#21457;&#30340;&#20010;&#20307;&#22240;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#65288;Xia&#31561;&#65292;2021&#65289;&#21487;&#20197;&#23398;&#24471;&#36825;&#26679;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20174;&#32780;&#33021;&#22815;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abilities of humans to understand the world in terms of cause and effect relationships, as well as to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem in the literature under the rubric of causal abstractions theory. In practice, it remains an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true mechanisms are unknown and only limited data is available. In this paper, we develop a new family of causal abstractions by clustering variables and their domains. This approach refines and generalizes previous notions of abstractions to better accommodate individual causal distributions that are spawned by Pearl's causal hierarchy. We show that such abstractions are learnable in practical settings through Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning toolkit to solve various challenging causal inference tasks -- iden
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16119</link><description>&lt;p&gt;
Alquist 5.0&#65306;&#23545;&#35805;&#26641;&#19982;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#22686;&#24378;SocialBot&#23545;&#35805;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16119
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;SocialBot- Alquist 5.0-&#65292;&#35813;&#31995;&#32479;&#26159;&#20026;Alexa Prize SocialBot&#22823;&#25361;&#25112;5&#24320;&#21457;&#30340;&#12290;&#22312;&#25105;&#20204;&#31995;&#32479;&#30340;&#21069;&#20960;&#20010;&#29256;&#26412;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NRG Barista&#65292;&#24182;&#27010;&#36848;&#20102;&#23558;Barista&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;SocialBot&#20013;&#30340;&#20960;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25972;&#20307;&#30340;&#23545;&#35805;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;SocialBot&#20197;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;Alquist 5.0&#24320;&#21457;&#30340;&#35265;&#35299;&#65292;&#35813;&#31995;&#32479;&#22312;&#28385;&#36275;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#26399;&#26395;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#23545;&#21508;&#31181;&#20027;&#39064;&#30340;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize SocialBot Grand Challenge~5. Building upon previous versions of our system, we introduce the NRG Barista and outline several innovative approaches for integrating Barista into our SocialBot, improving the overall conversational experience. Additionally, we extend our SocialBot to support multimodal devices. This paper offers insights into the development of Alquist~5.0, which meets evolving user expectations while maintaining empathetic and knowledgeable conversational abilities across diverse topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13164</link><description>&lt;p&gt;
&#20960;&#20046;&#31561;&#21464;&#24615;&#36890;&#36807;&#26446;&#20195;&#25968;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#30456;&#23545;&#20110;&#32676;&#20316;&#29992;&#30340;&#31561;&#21464;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#36171;&#20104;&#19968;&#20010;&#26550;&#26500;&#20855;&#20307;&#30340;&#32676;&#31561;&#21464;&#24615;&#23545;&#27169;&#22411;&#25152;&#26399;&#26395;&#30475;&#21040;&#30340;&#25968;&#25454;&#21464;&#25442;&#31867;&#22411;&#26045;&#21152;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#12290;&#20005;&#26684;&#31561;&#21464;&#27169;&#22411;&#24378;&#21046;&#25191;&#34892;&#23545;&#31216;&#24615;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#36825;&#26679;&#30340;&#20005;&#26684;&#31561;&#21464;&#24615;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25110;&#20165;&#32534;&#30721;&#20102;&#36817;&#20284;&#25110;&#37096;&#20998;&#23545;&#31216;&#24615;&#30340;&#28508;&#22312;&#29289;&#29702;&#23450;&#24459;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20005;&#26684;&#31561;&#21464;&#24615;&#30340;&#20808;&#39564;&#23454;&#38469;&#19978;&#21487;&#33021;&#36807;&#20110;&#24378;&#22823;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#21363;&#20960;&#20046;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#29616;&#26377;&#23450;&#20041;&#19981;&#21516;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
&lt;/p&gt;</description></item><item><title>ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.07446</link><description>&lt;p&gt;
ProbTS&#65306;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07446
&lt;/p&gt;
&lt;p&gt;
ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#36825;&#20010;&#39046;&#22495;&#20998;&#21270;&#25104;&#20102;&#20004;&#20010;&#26174;&#33879;&#30340;&#20998;&#25903;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20026;&#26102;&#38388;&#24207;&#21015;&#23450;&#21046;&#29305;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20998;&#25903;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25968;&#25454;&#24773;&#26223;&#12289;&#26041;&#27861;&#35770;&#28966;&#28857;&#21644;&#35299;&#30721;&#26041;&#26696;&#19978;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#28145;&#20837;&#32780;&#26410;&#34987;&#25506;&#32034;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProbTS&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#21327;&#21516;&#21644;&#27604;&#36739;&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#25903;&#12290;ProbTS&#20855;&#22791;&#32479;&#19968;&#30340;&#25968;&#25454;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#22120;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23457;&#35270;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20004;&#20010;&#20998;&#25903;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#36890;&#36807;ProbTS&#30340;&#23457;&#26597;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#12289;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CFDBench&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#22235;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#12289;&#27969;&#20307;&#29289;&#29702;&#29305;&#24615;&#21644;&#22495;&#20960;&#20309;&#30340;&#25968;&#25454;&#65292;&#33021;&#24110;&#21161;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05963</link><description>&lt;p&gt;
CFDBench&#65306;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics. (arXiv:2310.05963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CFDBench&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#22235;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#12289;&#27969;&#20307;&#29289;&#29702;&#29305;&#24615;&#21644;&#22495;&#20960;&#20309;&#30340;&#25968;&#25454;&#65292;&#33021;&#24110;&#21161;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#33021;&#23398;&#20064;&#35299;&#20915;&#25972;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#31639;&#23376;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#22312;&#31616;&#21333;&#30340;&#27969;&#21160;&#26041;&#31243;&#65288;&#22914;Burger&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19988;&#20165;&#32771;&#34385;&#20102;&#23545;&#19981;&#21516;&#21021;&#22987;&#26465;&#20214;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;CFDBench&#65292;&#19968;&#20010;&#38024;&#23545;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#20013;&#22235;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#39537;&#21160;&#33108;&#27969;&#21160;&#12289;&#22278;&#31649;&#20013;&#30340;&#23618;&#27969;&#36793;&#30028;&#23618;&#27969;&#21160;&#12289;&#36890;&#36807;&#21488;&#38454;&#30340;&#22365;&#27969;&#21160;&#21644;&#21608;&#26399;&#24615;&#30340;&#21345;&#38376;&#28065;&#34903;&#27969;&#21160;&#12290;&#27599;&#20010;&#27969;&#21160;&#38382;&#39064;&#37117;&#21253;&#25324;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#12289;&#27969;&#20307;&#29289;&#29702;&#29305;&#24615;&#21644;&#22495;&#20960;&#20309;&#30340;&#25968;&#25454;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;CFDBench&#20855;&#26377;&#20197;&#19979;&#20248;&#21183;&#65306;&#65288;1&#65289;&#32508;&#21512;&#12290;&#23427;&#21253;&#21547;&#24120;&#29992;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#22914;&#36895;&#24230;&#12289;&#21387;&#21147;&#21644;&#33108;&#20307;&#27604;&#20363;&#12290;&#65288;2&#65289;&#30495;&#23454;&#12290;&#38750;&#24120;&#36866;&#21512;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solu
&lt;/p&gt;</description></item><item><title>LARA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05668</link><description>&lt;p&gt;
LARA&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05668
&lt;/p&gt;
&lt;p&gt;
LARA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#37096;&#20998;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#37117;&#20551;&#35774;&#27491;&#24120;&#27169;&#24335;&#22987;&#32456;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;Web&#26381;&#21153;&#30340;&#27491;&#24120;&#27169;&#24335;&#32463;&#24120;&#21457;&#29983;&#21095;&#28872;&#21464;&#21270;&#12290;&#22312;&#36825;&#31181;&#21464;&#21270;&#20043;&#21518;&#65292;&#20351;&#29992;&#26087;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#32463;&#36807;&#26102;&#12290;&#27599;&#27425;&#37117;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#26159;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#27491;&#24120;&#27169;&#24335;&#21464;&#21270;&#24320;&#22987;&#26102;&#65292;&#26032;&#20998;&#24067;&#30340;&#35266;&#23519;&#25968;&#25454;&#19981;&#36275;&#12290;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#20877;&#35757;&#32451;&#26041;&#27861;&#65288;LARA&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;VAEs&#65289;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#25552;&#20986;&#19977;&#20010;&#26032;&#39062;&#30340;&#36129;&#29486;&#65306;1&#65289;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20197;&#24555;&#36895;&#25910;&#25947;&#20197;&#21450;&#38450;&#27490;&#36807;&#25311;&#21512;&#65307;2&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#65292;&#21487;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#32780;&#26080;&#38656;&#20648;&#23384;&#23427;&#20204;&#65307;3&#65289;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>DataInf&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00902</link><description>&lt;p&gt;
DataInf&#65306;&#22312;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#20013;&#39640;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#21147;
&lt;/p&gt;
&lt;p&gt;
DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models. (arXiv:2310.00902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00902
&lt;/p&gt;
&lt;p&gt;
DataInf&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#21147;&#23545;&#20110;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#25552;&#39640;AI&#31649;&#36947;&#30340;&#36879;&#26126;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#24433;&#21709;&#20989;&#25968;&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#27969;&#34892;&#30340;&#25968;&#25454;&#24402;&#23646;&#26041;&#27861;&#65292;&#20294;&#20854;&#35745;&#31639;&#25104;&#26412;&#20351;&#20854;&#38590;&#20197;&#20351;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#26356;&#21152;&#31361;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DataInf&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26131;&#20110;&#35745;&#31639;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;DataInf&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24433;&#21709;&#35745;&#31639;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;DataInf&#29305;&#21035;&#36866;&#29992;&#20110;&#35832;&#22914;LoRA&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DataInf&#33021;&#22815;&#20934;&#30830;&#22320;&#36817;&#20284;&#24433;&#21709;&#20998;&#25968;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;&#30340;&#26032;&#22411;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32431;&#31929;&#20351;&#29992;&#26680;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#21644;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#35774;&#35745;&#65292;&#36798;&#21040;&#20102;&#22312;MNIST&#12289;CIFAR-10&#21644;CIFAR-100&#19978;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.09814</link><description>&lt;p&gt;
&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Convolutional Deep Kernel Machines. (arXiv:2309.09814v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;&#30340;&#26032;&#22411;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32431;&#31929;&#20351;&#29992;&#26680;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#21644;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#35774;&#35745;&#65292;&#36798;&#21040;&#20102;&#22312;MNIST&#12289;CIFAR-10&#21644;CIFAR-100&#19978;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#26426;&#22120;(DKMs)&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#20855;&#26377;&#20854;&#20182;&#28145;&#24230;&#27169;&#22411;&#28789;&#27963;&#24615;&#30340;&#26680;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#12290;DKMs&#32431;&#31929;&#20351;&#29992;&#26680;&#65292;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#22240;&#27492;&#19982;&#20854;&#20182;&#26041;&#27861;&#65288;&#20174;&#31070;&#32463;&#32593;&#32476;&#21040;&#28145;&#24230;&#26680;&#23398;&#20064;&#29978;&#33267;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65289;&#19981;&#21516;&#65292;&#21518;&#32773;&#37117;&#20351;&#29992;&#29305;&#24449;&#20316;&#20026;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21367;&#31215;DKMs&#65292;&#24182;&#37197;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#24182;&#23454;&#39564;&#35780;&#20272;&#20102;&#35768;&#22810;&#27169;&#22411;&#21464;&#20307;&#65292;&#21253;&#25324;9&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20026;&#21367;&#31215;DKMs&#35774;&#35745;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#20004;&#31181;&#20284;&#28982;&#20989;&#25968;&#21644;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39030;&#23618;&#12290;&#23613;&#31649;&#21482;&#22312;&#32422;28&#20010;GPU&#23567;&#26102;&#20869;&#35757;&#32451;&#65288;&#27604;&#23436;&#20840;&#30340;NNGP / NTK / Myrtle kernel&#24555;1-2&#20010;&#25968;&#37327;&#32423;&#65289;&#65292;&#20294;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;MNIST&#19978;&#23454;&#29616;&#20102;&#32422;99&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#22312;CIFAR-10&#19978;&#20026;92&#65285;&#65292;&#22312;CIFAR-100&#19978;&#20026;71&#65285;&#65292;&#21516;&#26102;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#20998;&#21106;&#21644;&#32858;&#31867;&#31639;&#27861;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2308.16759</link><description>&lt;p&gt;
&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#26500;&#24314;&#23460;&#20869;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Constructing Indoor Region-based Radio Map without Location Labels. (arXiv:2308.16759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#20998;&#21106;&#21644;&#32858;&#31867;&#31639;&#27861;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#22320;&#22270;&#30340;&#26500;&#24314;&#38656;&#35201;&#22823;&#37327;&#24102;&#26377;&#20301;&#32622;&#26631;&#31614;&#30340;&#26080;&#32447;&#30005;&#27979;&#37327;&#25968;&#25454;&#65292;&#36825;&#32473;&#37096;&#32626;&#25104;&#26412;&#24102;&#26469;&#20102;&#24456;&#39640;&#30340;&#21387;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#27979;&#37327;&#25968;&#25454;&#32780;&#26080;&#38656;&#20301;&#32622;&#26631;&#31614;&#12290;&#26500;&#24314;&#36807;&#31243;&#22522;&#20110;&#20174;&#19968;&#20010;&#35774;&#22791;&#19978;&#30450;&#30446;&#25910;&#38598;&#21040;&#30340;RSS&#27979;&#37327;&#25968;&#25454;&#65292;&#35813;&#35774;&#22791;&#22312;&#23460;&#20869;&#21306;&#22495;&#20013;&#30340;&#21508;&#20010;&#21306;&#22495;&#20013;&#24688;&#22909;&#35775;&#38382;&#19968;&#27425;&#65292;&#20294;&#27809;&#26377;&#35760;&#24405;&#33050;&#21360;&#21644;&#26102;&#38388;&#25139;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#23558;RSS&#25968;&#25454;&#32858;&#31867;&#65292;&#24182;&#23558;&#32858;&#31867;&#19982;&#29289;&#29702;&#21306;&#22495;&#36827;&#34892;&#21305;&#37197;&#12290;&#30001;&#20110;&#22810;&#24452;&#21644;&#22122;&#22768;&#30340;&#23384;&#22312;&#65292;&#20256;&#32479;&#30340;&#32858;&#31867;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;RSS&#25968;&#25454;&#65292;&#22240;&#20026;RSS&#25968;&#25454;&#33258;&#28982;&#32780;&#28982;&#22320;&#21576;&#29616;&#20026;&#38750;&#32858;&#31867;&#30340;&#24418;&#24335;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#39034;&#24207;&#20808;&#39564;&#30340;&#20449;&#21495;&#23376;&#31354;&#38388;&#27169;&#22411;&#29992;&#20110;&#22788;&#29702;RSS&#25968;&#25454;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#32508;&#21512;&#20998;&#21106;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#35777;&#26126;&#33021;&#22815;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#23558;&#32858;&#31867;&#25968;&#25454;&#19982;&#29289;&#29702;&#21306;&#22495;&#36827;&#34892;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio map construction requires a large amount of radio measurement data with location labels, which imposes a high deployment cost. This paper develops a region-based radio map from received signal strength (RSS) measurements without location labels. The construction is based on a set of blindly collected RSS measurement data from a device that visits each region in an indoor area exactly once, where the footprints and timestamps are not recorded. The main challenge is to cluster the RSS data and match clusters with the physical regions. Classical clustering algorithms fail to work as the RSS data naturally appears as non-clustered due to multipaths and noise. In this paper, a signal subspace model with a sequential prior is constructed for the RSS data, and an integrated segmentation and clustering algorithm is developed, which is shown to find the globally optimal solution in a special case. Furthermore, the clustered data is matched with the physical regions using a graph-based app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12215</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#19968;&#20010;&#38024;&#23545;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#19978;&#23398;&#26415;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#20013;270&#31687;&#24191;&#21463;&#24341;&#29992;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#30340;&#25991;&#29486;&#31995;&#32479;&#21270;&#65292;&#24182;&#23545;&#23376;&#38598;&#20013;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12289;&#35774;&#35745;&#22833;&#35823;&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#23545;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#19982;&#22312;&#32447;&#26381;&#21153;&#30495;&#27491;&#38754;&#20020;&#30340;&#25361;&#25112;&#26377;&#26412;&#36136;&#19978;&#30340;&#21306;&#21035;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#36890;&#24120;&#19981;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#32780;&#19988;&#35780;&#20272;&#24448;&#24448;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#24456;&#24046;&#12290;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#22823;&#23567;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#65292;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#38656;&#35201;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#65292;&#24182;&#19988;&#20026;&#20102;&#33719;&#24471;&#26356;&#20302;&#30340;&#35757;&#32451;&#35823;&#24046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#25353;&#29031;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.06338</link><description>&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#22823;&#23567;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Size Lowerbounds for Deep Operator Networks. (arXiv:2308.06338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#22823;&#23567;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#65292;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#38656;&#35201;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#65292;&#24182;&#19988;&#20026;&#20102;&#33719;&#24471;&#26356;&#20302;&#30340;&#35757;&#32451;&#35823;&#24046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#25353;&#29031;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26159;&#19968;&#31181;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#21644;&#19968;&#27425;&#35299;&#20915;&#19968;&#31867;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#39318;&#27425;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#22823;&#23567;&#19979;&#30028;&#65292;&#20197;&#20415;&#33021;&#22815;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#20943;&#23567;&#32463;&#39564;&#35823;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#33719;&#24471;&#20302;&#35757;&#32451;&#35823;&#24046;&#65292;&#38656;&#35201;&#23558;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;n&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#12290;&#36825;&#21551;&#21457;&#20102;&#25105;&#20204;&#22312;&#35299;&#20915;&#23545;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#23545;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22266;&#23450;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#36825;&#31181;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#21487;&#20197;&#21333;&#35843;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20043;&#21576;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#65288;SSNL&#65289;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#27169;&#25311;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;SSNL&#36890;&#36807;&#25311;&#21512;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01054</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference using surjective sequential neural likelihood estimation. (arXiv:2308.01054v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01054
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#65288;SSNL&#65289;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#27169;&#25311;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;SSNL&#36890;&#36807;&#25311;&#21512;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#65288;SSNL&#65289;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#21487;&#20197;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#26102;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;SSNL&#25311;&#21512;&#19968;&#20010;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#21253;&#25324;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#25110;&#21464;&#20998;&#25512;&#26029;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;SSNL&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#26102;&#36935;&#21040;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#21253;&#21547;&#26080;&#20449;&#24687;&#25968;&#25454;&#32500;&#24230;&#25110;&#20301;&#20110;&#36739;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;SSNL&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#23427;&#36890;&#24120;&#20248;&#20110;&#22312;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#29616;&#20195;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#19968;&#39033;&#26469;&#33258;&#22825;&#20307;&#29289;&#29702;&#23398;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20363;&#23376;&#19978;&#23545;&#30913;&#22330;&#27169;&#22411;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.11792</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32463;&#20856;&#25968;&#25454;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data. (arXiv:2307.11792v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#20855;&#26377;&#24322;&#24120;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#65292;&#20960;&#20046;&#27809;&#26377;&#38169;&#35823;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#25215;&#35834;&#20043;&#19979;&#65292;&#23545;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#22810;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;MNIST&#12289;Fashion MNIST&#21644;Iris&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#29992;&#20110;&#36827;&#34892;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#36229;&#36234;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMT&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#26816;&#27979;&#24182;&#31227;&#38500;&#35757;&#32451;&#29992;&#20110;&#20174;&#36755;&#20837;&#20013;&#25552;&#21462;&#21518;&#38376;&#20449;&#24687;&#30340;&#21518;&#38376;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#26377;&#25928;&#38450;&#27490;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.11565</link><description>&lt;p&gt;
FMT: &#36890;&#36807;&#29305;&#24449;&#22270;&#27979;&#35797;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#31227;&#38500;&#21518;&#38376;&#29305;&#24449;&#22270;
&lt;/p&gt;
&lt;p&gt;
FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks. (arXiv:2307.11565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11565
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMT&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#26816;&#27979;&#24182;&#31227;&#38500;&#35757;&#32451;&#29992;&#20110;&#20174;&#36755;&#20837;&#20013;&#25552;&#21462;&#21518;&#38376;&#20449;&#24687;&#30340;&#21518;&#38376;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#26377;&#25928;&#38450;&#27490;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#20851;&#38190;&#24212;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#21518;&#38376;&#25915;&#20987;&#26159;&#36890;&#36807;&#21521;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#20154;&#24037;&#27169;&#24335;&#23454;&#29616;&#30340;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#36870;&#21521;&#24037;&#31243;&#26469;&#22797;&#29616;&#25915;&#20987;&#32773;&#29983;&#25104;&#30340;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#36890;&#36807;&#23558;&#35302;&#21457;&#22120;&#28155;&#21152;&#21040;&#36755;&#20837;&#20013;&#24182;&#20351;&#29992;&#30495;&#23454;&#26631;&#31614;&#24494;&#35843;&#27169;&#22411;&#26469;&#20462;&#22797;DNN&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#25915;&#20987;&#32773;&#29983;&#25104;&#30340;&#35302;&#21457;&#22120;&#22797;&#26434;&#32780;&#19988;&#19981;&#21487;&#35265;&#65292;&#38450;&#24481;&#32773;&#23601;&#26080;&#27861;&#25104;&#21151;&#22797;&#29616;&#35302;&#21457;&#22120;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#27809;&#26377;&#34987;&#26377;&#25928;&#21435;&#38500;&#65292;DNN&#27169;&#22411;&#23558;&#26080;&#27861;&#34987;&#20462;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#22270;&#27979;&#35797;&#65288;FMT&#65289;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#19981;&#21516;&#65292;&#23427;&#20204;&#19987;&#27880;&#20110;&#22797;&#29616;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;FMT&#35797;&#22270;&#26816;&#27979;&#35757;&#32451;&#29992;&#20110;&#20174;&#36755;&#20837;&#20013;&#25552;&#21462;&#21518;&#38376;&#20449;&#24687;&#30340;&#21518;&#38376;&#29305;&#24449;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground-truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender can not successfully reproduce the trigger. Consequently, the DNN model will not be repaired since the trigger is not effectively removed.  In this work, we propose Feature Map Testing~(FMT). Different from existing defense strategies, which focus on reproducing backdoor triggers, FMT tries to detect the backdoor feature maps, which are trained to extract backdoor information from the inputs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22330;&#22343;&#25511;&#21046;&#27169;&#22411;&#65288;Dec-POMFC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#25955;&#21270;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#25361;&#25112;&#12290;&#35813;&#27169;&#22411;&#21487;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#21487;&#35299;&#20915;&#30340;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#38598;&#20307;&#34892;&#20026;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.06175</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#22330;&#22343;&#25511;&#21046;&#26469;&#23454;&#29616;&#20154;&#24037;&#38598;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. (arXiv:2307.06175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22330;&#22343;&#25511;&#21046;&#27169;&#22411;&#65288;Dec-POMFC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#25955;&#21270;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#25361;&#25112;&#12290;&#35813;&#27169;&#22411;&#21487;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#21487;&#35299;&#20915;&#30340;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#38598;&#20307;&#34892;&#20026;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#20998;&#25955;&#21270;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#20197;&#21450;&#38754;&#23545;&#20247;&#22810;&#26234;&#33021;&#20307;&#26102;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38598;&#20307;&#34892;&#20026;&#35201;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#26368;&#21069;&#27839;&#30340;&#24212;&#29992;&#65292;&#22914;&#27963;&#21160;&#29289;&#36136;&#29289;&#29702;&#12289;&#33258;&#32452;&#32455;&#31995;&#32479;&#12289;&#33286;&#35770;&#21160;&#24577;&#20197;&#21450;&#29983;&#29289;&#25110;&#26426;&#22120;&#20154;&#32676;&#20307;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#22330;&#22343;&#25511;&#21046;&#65288;Dec-POMFC&#65289;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#22312;&#37096;&#20998;&#20449;&#24687;&#19979;&#30340;&#20998;&#25955;&#34892;&#20026;&#65292;&#36825;&#26159;&#19968;&#31867;&#20801;&#35768;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#21487;&#35299;&#20915;&#30340;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#25490;&#21015;&#19981;&#21464;&#20195;&#29702;&#30340;&#24191;&#27867;&#38382;&#39064;&#65292;&#20197;&#21450;&#21333;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13292</link><description>&lt;p&gt;
&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#25913;&#36827;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variance-Covariance Regularization Improves Representation Learning. (arXiv:2306.13292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#20174;&#19968;&#20010;&#39046;&#22495;&#33719;&#24471;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20851;&#20110;&#36825;&#20123;&#21518;&#32493;&#20219;&#21153;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24378;&#26377;&#21147;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#35201;&#27714;&#22312;&#21021;&#22987;&#39044;&#35757;&#32451;&#38454;&#27573;&#25429;&#33719;&#21508;&#31181;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#36275;&#22815;&#30340;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#38598;&#20013;&#20110;&#20027;&#35201;&#20943;&#23569;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#36235;&#21183;&#21487;&#33021;&#23548;&#33268;&#19981;&#20805;&#20998;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#25439;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#65288;VCR&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#12290;&#20511;&#37492;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#34920;&#29616;&#20986;&#39640;&#26041;&#24046;&#21644;&#39640;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has emerged as a key approach in the machine learning domain, enabling the application of knowledge derived from one domain to improve performance on subsequent tasks. Given the often limited information about these subsequent tasks, a strong transfer learning approach calls for the model to capture a diverse range of features during the initial pretraining stage. However, recent research suggests that, without sufficient regularization, the network tends to concentrate on features that primarily reduce the pretraining loss function. This tendency can result in inadequate feature learning and impaired generalization capability for target tasks. To address this issue, we propose Variance-Covariance Regularization (VCR), a regularization technique aimed at fostering diversity in the learned network features. Drawing inspiration from recent advancements in the self-supervised learning approach, our approach promotes learned representations that exhibit high variance and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#22312;&#25439;&#22833;&#20989;&#25968;&#36873;&#25321;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;&#28385;&#36275;&#29305;&#23450;&#20844;&#29702;&#30340;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#65292;&#20854;&#20013;&#24179;&#26041;L2&#35823;&#24046;&#26159;&#23454;&#36341;&#20013;&#21807;&#19968;&#21487;&#25509;&#21463;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24314;&#35758;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#32487;&#32493;&#20351;&#29992;&#23427;&#12290;</title><link>http://arxiv.org/abs/2306.04778</link><description>&lt;p&gt;
&#34892;&#20026;&#21338;&#24328;&#35770;&#30340;&#25439;&#22833;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Loss Functions for Behavioral Game Theory. (arXiv:2306.04778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#22312;&#25439;&#22833;&#20989;&#25968;&#36873;&#25321;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;&#28385;&#36275;&#29305;&#23450;&#20844;&#29702;&#30340;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#65292;&#20854;&#20013;&#24179;&#26041;L2&#35823;&#24046;&#26159;&#23454;&#36341;&#20013;&#21807;&#19968;&#21487;&#25509;&#21463;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24314;&#35758;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#32487;&#32493;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#20204;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#35780;&#20272;&#20154;&#31867;&#34892;&#20026;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#26159;&#20182;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#19978;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#38169;&#35823;&#29575;&#12289;&#36127;&#23545;&#25968;&#20284;&#28982;&#12289;&#20132;&#21449;&#29109;&#12289;Brier&#24471;&#20998;&#21644;L2&#35823;&#24046;&#37117;&#26159;&#24120;&#35265;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#35797;&#22270;&#25552;&#20379;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#31572;&#26696;&#65292;&#35299;&#20915;&#21738;&#20123;&#25439;&#22833;&#20989;&#25968;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#35268;&#33539;&#21270;&#20102;&#35748;&#20026;&#25439;&#22833;&#20989;&#25968;&#24212;&#35813;&#28385;&#36275;&#30340;&#20934;&#21017;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#32452;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#8220;&#23545;&#35282;&#32447;&#26377;&#30028;Bregman&#25955;&#24230;&#8221;&#65292;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#20844;&#29702;&#65292;&#24182;&#21253;&#25324;&#24179;&#26041;L2&#35823;&#24046;&#12290;&#23454;&#38469;&#19978;&#65292;&#24179;&#26041;L2&#35823;&#24046;&#26159;&#30456;&#23545;&#24120;&#29992;&#30340;&#21807;&#19968;&#21487;&#25509;&#21463;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#32487;&#32493;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral game theorists all use experimental data to evaluate predictive models of human behavior. However, they differ greatly in their choice of loss function for these evaluations, with error rate, negative log-likelihood, cross-entropy, Brier score, and L2 error all being common choices. We attempt to offer a principled answer to the question of which loss functions make sense for this task, formalizing desiderata that we argue loss functions should satisfy. We construct a family of loss functions, which we dub "diagonal bounded Bregman divergences", that satisfy all of these axioms and includes the squared L2 error. In fact, the squared L2 error is the only acceptable loss that is relatively commonly used in practice; we thus recommend its continued use to behavioral game theorists.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04695</link><description>&lt;p&gt;
ConceptBed: &#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#27010;&#24565;&#24182;&#20174;&#22270;&#20687;&#20013;&#22797;&#21046;&#21644;&#32452;&#21512;&#36825;&#20123;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#12290;&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#23398;&#20064;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25551;&#36848;&#26469;&#29983;&#25104;&#39640;&#28165;&#26224;&#24230;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#36136;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;T2I&#27169;&#22411;&#30340;&#37325;&#28857;&#22312;&#20110;&#29031;&#29255;&#33324;&#30340;&#30495;&#23454;&#24863;&#21644;&#26377;&#38480;&#30340;&#35270;&#35273;&#29702;&#35299;&#23450;&#24615;&#37327;&#24230;&#12290;&#20026;&#20102;&#37327;&#21270;T2I&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#21512;&#25104;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ConceptBed&#65292;&#19968;&#20010;&#21253;&#21547;284&#20010;&#29420;&#29305;&#35270;&#35273;&#27010;&#24565;&#12289;5K&#20010;&#29420;&#29305;&#27010;&#24565;&#32452;&#21512;&#21644;33K&#20010;&#32452;&#21512;&#25991;&#26412;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;Concept Confidence Deviation&#65288;CCD&#65289;&#65292;&#23427;&#21033;&#29992;oracle&#27010;&#24565;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#26469;&#34913;&#37327;T2I&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#27010;&#24565;&#19982;&#22320;&#38754;&#30495;&#23454;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#27010;&#24565;&#20043;&#38388;&#30340;&#23545;&#40784;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#23545;&#35937;&#25110;&#32773;...
&lt;/p&gt;
&lt;p&gt;
The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#24182;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#38598;&#26102;&#20351;&#29992;&#35813;&#39044;&#27979;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.03828</link><description>&lt;p&gt;
Quick-Tune&#65306;&#24555;&#36895;&#23398;&#20064;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#24494;&#35843;&#23427;
&lt;/p&gt;
&lt;p&gt;
Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How. (arXiv:2306.03828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#24182;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#25968;&#25454;&#38598;&#26102;&#20351;&#29992;&#35813;&#39044;&#27979;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#19981;&#26029;&#38754;&#20020;&#19968;&#20010;&#38382;&#39064;&#65306;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#35813;&#22914;&#20309;&#24494;&#35843;&#23427;&#20197;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32852;&#21512;&#25628;&#32034;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#36229;&#36807;20k&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#22312;87&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;24&#20010;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#26469;&#29983;&#25104;&#22823;&#35268;&#27169;&#20803;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20854;&#23398;&#20064;&#26354;&#32447;&#19978;&#20803;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#20197;&#29992;&#20110;&#24555;&#36895;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#36873;&#25321;&#19968;&#20010;&#20934;&#30830;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#25214;&#21040;&#23427;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a multi-fidelity performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00074</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#26657;&#20934;&#29992;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#23427;&#36890;&#24120;&#25552;&#20379;&#26631;&#31614;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20540;&#12290;&#28982;&#21518;&#65292;&#20915;&#31574;&#32773;&#24212;&#20351;&#29992;&#32622;&#20449;&#24230;&#20540;&#26469;&#26657;&#20934;&#23545;&#39044;&#27979;&#30340;&#20449;&#20219;&#31243;&#24230;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#32463;&#24120;&#35748;&#20026;&#32622;&#20449;&#24230;&#20540;&#24212;&#23545;&#39044;&#27979;&#26631;&#31614;&#19982;&#23454;&#38469;&#26631;&#31614;&#21305;&#37197;&#30340;&#27010;&#29575;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22810;&#26465;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20915;&#31574;&#32773;&#38590;&#20197;&#20351;&#29992;&#36825;&#20123;&#32622;&#20449;&#24230;&#20540;&#24456;&#22909;&#22320;&#30830;&#23450;&#20309;&#26102;&#20449;&#20219;&#39044;&#27979;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#39318;&#20808;&#26159;&#29702;&#35299;&#20026;&#20160;&#20040;&#65292;&#28982;&#21518;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#26356;&#26377;&#29992;&#30340;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#35748;&#20026;&#65292;&#22312;&#24191;&#27867;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20013;&#65292;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#29702;&#24615;&#20915;&#31574;&#32773;&#36890;&#24120;&#38590;&#20197;&#20351;&#29992;&#20197;&#19978;&#32622;&#20449;&#24230;&#20540;&#21457;&#29616;&#26368;&#20339;&#20915;&#31574;&#25919;&#31574;&#8212;&#8212;&#26368;&#20339;&#30340;&#20915;&#31574;&#32773;&#38656;&#35201;&#20154;&#31867;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20182;&#20204;&#22312;&#25152;&#38754;&#20020;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#19978;&#30340;&#20010;&#20154;&#20559;&#22909;&#30340;&#26032;&#26041;&#27861;&#26469;&#26500;&#36896;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#32622;&#20449;&#24230;&#20540;&#27604;&#20351;&#29992;&#26631;&#20934;&#32622;&#20449;&#24230;&#24230;&#37327;&#23548;&#33268;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09651</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25351;&#23548;&#26377;&#21161;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#33021;&#21147;&#36229;&#32676;&#30340;&#25945;&#24072;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#35753;&#23398;&#29983;&#27700;&#24179;&#24471;&#21040;&#25552;&#21319;&#65292;&#36825;&#20984;&#26174;&#20102;&#24403;&#21069;&#25945;&#24072;&#22521;&#35757;&#23454;&#36341;&#21644;&#26377;&#25928;&#30693;&#35782;&#20256;&#25480;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#25945;&#24072;&#22521;&#35757;&#36807;&#31243;&#30340;&#25351;&#23548;&#25928;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#33976;&#39311;&#25928;&#24212;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#23545;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#22909;&#25945;&#24072;&#24456;&#37325;&#35201;&#65288;LGTM&#65289;&#30340;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#23558;&#33976;&#39311;&#25928;&#24212;&#32435;&#20837;&#25945;&#24072;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#21487;&#33021;&#25552;&#21319;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;LGTM&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#32593;&#32476;&#32531;&#20914;&#21306;&#31649;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07164</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Online Packet Scheduling with Deadlines. (arXiv:2305.07164v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#32593;&#32476;&#32531;&#20914;&#21306;&#31649;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#20248;&#20808;&#22788;&#29702;&#20851;&#38190;&#27969;&#37327;&#24182;&#26377;&#25928;&#22320;&#31649;&#29702;&#27969;&#37327;&#12290;&#36825;&#38656;&#35201;&#36866;&#24403;&#30340;&#32531;&#20914;&#21306;&#31649;&#29702;&#20197;&#38450;&#27490;&#20002;&#22833;&#37325;&#35201;&#27969;&#37327;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#38750;&#20851;&#38190;&#27969;&#37327;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#25511;&#21046;&#27599;&#27493;&#35201;&#20256;&#36755;&#21738;&#20123;&#25968;&#25454;&#21253;&#12289;&#21738;&#20123;&#25968;&#25454;&#21253;&#35201;&#20002;&#24323;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#25968;&#25454;&#21253;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#24212;&#23545;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#24403;&#39044;&#27979;&#35823;&#24046;&#24456;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20250;&#25552;&#39640;&#31454;&#20105;&#27604;&#29575;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#26377;&#30028;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modern network aims to prioritize critical traffic over non-critical traffic and effectively manage traffic flow. This necessitates proper buffer management to prevent the loss of crucial traffic while minimizing the impact on non-critical traffic. Therefore, the algorithm's objective is to control which packets to transmit and which to discard at each step. In this study, we initiate the learning-augmented online packet scheduling with deadlines and provide a novel algorithmic framework to cope with the prediction. We show that when the prediction error is small, our algorithm improves the competitive ratio while still maintaining a bounded competitive ratio, regardless of the prediction error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;</title><link>http://arxiv.org/abs/2305.05448</link><description>&lt;p&gt;
&#20511;&#21161;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#40065;&#26834;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#33021;&#26377;&#35768;&#22810;&#25554;&#20540;&#35299;; &#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#25351;&#29305;&#23450;&#20248;&#21270;&#26041;&#27861;&#23545;&#20247;&#22810;&#25554;&#20540;&#35299;&#20043;&#19968;&#30340;&#38544;&#21547;&#21916;&#22909;&#12290;&#24050;&#32463;&#24314;&#31435;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#22312;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#26102;&#20542;&#21521;&#20110;&#20855;&#26377;&#20302;&#31209;&#21644;/&#25110;&#31232;&#30095;&#35299;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#20174;&#26576;&#31181;&#31243;&#24230;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24179;&#26041;&#25439;&#22833;&#30446;&#26631;&#29702;&#35770;&#36890;&#24120;&#38656;&#35201;&#21487;&#35757;&#32451;&#26435;&#37325;&#30340;&#38750;&#24120;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#20013;&#20026;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#32780;&#21021;&#22987;&#21270;&#30340;&#26356;&#22823;&#35268;&#27169;&#30340;&#26435;&#37325;&#30683;&#30462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32435;&#20837;&#24182;&#20998;&#26512;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#26435;&#37325;&#21521;&#37327;&#20197;&#26497;&#22352;&#26631;&#21442;&#25968;&#21270;&#65292;&#23548;&#33268;&#33258;&#28982;&#30340;&#26435;&#37325;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#26435;&#37325;&#21521;&#37327;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;&#20559;&#24046;&#19982;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#32463;&#39564;&#33539;&#25968;&#27491;&#21017;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;CECL&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#30340;&#26377;&#29992;&#20449;&#24687;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04203</link><description>&lt;p&gt;
&#24320;&#25918;&#38598;&#21512;&#23398;&#20064;&#30340;&#26032;&#35270;&#35282;&#65306;&#35299;&#38145;&#24320;&#25918;&#38598;&#21512;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning. (arXiv:2305.04203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;CECL&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#30340;&#26377;&#29992;&#20449;&#24687;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22024;&#26434;&#30340;&#25968;&#25454;&#19968;&#30452;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#22312;&#23553;&#38381;&#38598;&#30340;&#26631;&#31614;&#22122;&#22768;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#21516;&#26102;&#23384;&#22312;&#24320;&#25918;&#38598;&#21512;&#21644;&#23553;&#38381;&#38598;&#21512;&#30340;&#22122;&#22768;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20026;&#27599;&#31181;&#31867;&#22411;&#35774;&#35745;&#29305;&#23450;&#30340;&#31574;&#30053;&#26469;&#21306;&#20998;&#21644;&#22788;&#29702;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#38598;&#24050;&#32463;&#20005;&#37325;&#25439;&#22351;&#26102;&#12290;&#26412;&#25991;&#23545;&#27169;&#22411;&#38754;&#23545;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#26102;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#36880;&#28176;&#34701;&#20837;&#26576;&#20123;&#24050;&#30693;&#31867;&#21035;&#65292;&#36825;&#26377;&#21033;&#20110;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31163;&#12290;&#22312;&#36825;&#31181;&#29616;&#35937;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;CECL&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#30340;&#26377;&#29992;&#20449;&#24687;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#23558;&#19968;&#20123;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#20316;&#20026;&#36127;&#20363;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#27169;&#22411;&#23545;&#24320;&#25918;&#38598;&#21512;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22788;&#29702;&#24320;&#25918;&#38598;&#21512;&#21644;&#23553;&#38381;&#38598;&#21512;&#26631;&#31614;&#22122;&#22768;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the seperation among known classes. Motivated by the phenomenon, in this paper, we propose a novel two-step contrastive learning method called CECL, which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some ope
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03954</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#20197;&#36827;&#34892;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Action Embeddings for Off-Policy Evaluation. (arXiv:2305.03954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#35780;&#20272;&#65288;OPE&#65289;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#30001;&#19981;&#21516;&#31574;&#30053;&#25910;&#38598;&#30340;&#35760;&#24405;&#25968;&#25454;&#26469;&#35745;&#31639;&#31574;&#30053;&#30340;&#39044;&#26399;&#22870;&#21169;&#12290; OPE&#26159;&#36816;&#34892;&#26114;&#36149;&#30340;&#22312;&#32447;A / B&#27979;&#35797;&#30340;&#21487;&#34892;&#36873;&#25321;&#65306;&#23427;&#21487;&#20197;&#21152;&#24555;&#26032;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#24182;&#38477;&#20302;&#21521;&#23458;&#25143;&#26292;&#38706;&#27425;&#20248;&#27835;&#30103;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#24403;&#21160;&#20316;&#25968;&#37327;&#24456;&#22823;&#25110;&#35760;&#24405;&#31574;&#30053;&#26410;&#20805;&#20998;&#25506;&#32034;&#26576;&#20123;&#25805;&#20316;&#26102;&#65292;&#22522;&#20110;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#30340;&#29616;&#26377;&#20272;&#35745;&#22120;&#21487;&#33021;&#20855;&#26377;&#39640;&#29978;&#33267;&#26080;&#38480;&#26041;&#24046;&#12290;Saito&#21644;Joachims&#25552;&#20986;&#20351;&#29992;&#21160;&#20316;&#23884;&#20837;&#30340;&#36793;&#38469;IPS&#65288;MIPS&#65289;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#38477;&#20302;IPS&#30340;&#26041;&#24046;&#12290; MIPS&#20551;&#35774;&#20174;&#19994;&#32773;&#21487;&#20197;&#23450;&#20041;&#33391;&#22909;&#30340;&#21160;&#20316;&#23884;&#20837;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#20013;&#38388;&#36755;&#20986;&#26469;&#23450;&#20041;&#21160;&#20316;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;MIPS&#20272;&#35745;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to defin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03237</link><description>&lt;p&gt;
&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#23545;&#20110;&#23454;&#29992;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#36718;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#65288;Caro&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36981;&#24490;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#20174;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#29942;&#39048;&#25439;&#22833;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;Caro&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#26469;&#20174;&#36825;&#20123;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25366;&#25496;OOD&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#29992;&#36825;&#20123;OOD&#26679;&#26412;&#26469;&#35757;&#32451;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Caro&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#21333;&#36718;&#19978;&#19979;&#25991;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08177</link><description>&lt;p&gt;
&#20013;&#25991;LLaMA&#21644;Alpaca&#30340;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#26412;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08177
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#24182;&#26174;&#31034;&#20986;&#26397;&#30528;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;LLM&#30340;&#39640;&#25104;&#26412;&#23545;&#36879;&#26126;&#12289;&#21487;&#35775;&#38382;&#30340;&#23398;&#26415;&#30740;&#31350;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#25991;&#25991;&#26412;&#21450;&#20854;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05949</link><description>&lt;p&gt;
CMOS + &#38543;&#26426;&#32435;&#31859;&#30913;&#20307;&#65306;&#27010;&#29575;&#25512;&#29702;&#19982;&#23398;&#20064;&#24322;&#26500;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#25918;&#32531;&#65292;&#21033;&#29992;&#26032;&#20852;&#30340;&#32435;&#31859;&#25216;&#26415;&#65288;X&#65289;&#22686;&#24378;&#20114;&#34917;&#37329;&#23646;&#27687;&#21270;&#29289;&#21322;&#23548;&#20307;&#65288;CMOS&#65289;&#26230;&#20307;&#31649;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#12290;&#23613;&#31649;sMTJs&#35774;&#22791;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#24322;&#26500;&#35745;&#31639;&#26426;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;&#20351;&#29992;CMOS&#39044;&#27979;&#27969;&#31243;&#35774;&#35745;&#22871;&#20214;&#65288;PDK&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25968;&#23383;CMOS-based p-bits&#27169;&#25311;&#39640;&#36136;&#37327;&#38543;&#26426;&#24615;&#38656;&#35201;&#36229;&#36807;10,000&#20010;&#26230;&#20307;&#31649;&#65292;&#27599;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;&#25968;&#30340;&#33021;&#37327;&#27604;&#20351;&#29992;&#21482;&#28040;&#32791;2fJ&#30340;sMTJ-based p-bits&#39640;&#32422;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32553;&#25918;&#21644;&#38598;&#25104;&#29256;&#26412;&#21487;&#20197;&#26174;&#30528;&#25512;&#36827;&#27010;&#29575;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16749</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#28508;&#21147;&#26159;&#26368;&#36817;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Language Feedback&#65288;ILF&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;ILF&#22312;&#35757;&#32451;&#26399;&#38388;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#21453;&#39304;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#30456;&#21516;&#30340;&#21453;&#39304;&#65292;&#22240;&#27492;&#20351;&#29992;&#36215;&#26469;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ILF&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#23567;&#21270;&#19982;&#22522;&#20934;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;ILF&#22312;Mostly Basic Python Problems(MBPP)&#22522;&#20934;&#27979;&#35797;&#19978;&#23558;Codegen-Mono 6.1B&#27169;&#22411;&#30340;pass @ 1&#35206;&#30422;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;38%&#65288;&#32477;&#23545;&#25552;&#39640;&#20102;10%&#65289;&#65292;&#32988;&#36807;&#20102;&#22312;MBPP&#19978;&#24494;&#35843;&#21644;&#22312;&#20154;&#31867;&#20462;&#22797;&#30340;&#31243;&#24207;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15103</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#30456;&#20284;&#24615;&#22270;&#35889;&#19978;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#36816;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#12290;&#21033;&#29992;&#36825;&#31181;&#31561;&#20215;&#24615;&#20316;&#20026;&#22522;&#30707;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#24182;&#20005;&#26684;&#25551;&#36848;&#22810;&#27169;&#24577;&#23545;&#35937;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#19968;&#36215;&#12290;&#22312;&#29702;&#35770;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#28151;&#21512;&#25439;&#22833;&#65292;&#32467;&#21512;&#26032;&#39062;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
&lt;/p&gt;</description></item><item><title>EdgeServe &#26159;&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#23558;&#25968;&#25454;&#36335;&#30001;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#22312;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#31561;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;EdgeServe &#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08028</link><description>&lt;p&gt;
EdgeServe:&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#25191;&#34892;&#23618;
&lt;/p&gt;
&lt;p&gt;
EdgeServe: An Execution Layer for Decentralized Prediction. (arXiv:2303.08028v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08028
&lt;/p&gt;
&lt;p&gt;
EdgeServe &#26159;&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#23558;&#25968;&#25454;&#36335;&#30001;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#22312;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#31561;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;EdgeServe &#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#30456;&#20851;&#29305;&#24449;&#21487;&#33021;&#26469;&#33258;&#20110;&#32593;&#32476;&#20013;&#19981;&#21516;&#33410;&#28857;&#25910;&#38598;&#30340;&#25968;&#25454;&#28304;&#12290;&#36825;&#31181;&#38382;&#39064;&#34987;&#31216;&#20043;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#65292;&#24182;&#22312;&#25968;&#25454;&#36335;&#30001;&#12289;&#35745;&#31639;&#24067;&#23616;&#21644;&#26102;&#38388;&#21516;&#27493;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#26377;&#36259;&#30340;&#31995;&#32479;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EdgeServe&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#20197;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#25552;&#20379;&#26381;&#21153;&#12290; EdgeServe &#20381;&#36182;&#20110;&#19968;&#20010;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#32593;&#32476;&#36335;&#30001;&#25968;&#25454;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;EdgeServe &#20381;&#36182;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;EdgeServe&#65306;&#65288;1&#65289;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#65288;2&#65289;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#65288;3&#65289;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relevant features for a machine learning task may be aggregated from data sources collected on different nodes in a network. This problem, which we call decentralized prediction, creates a number of interesting systems challenges in managing data routing, placing computation, and time-synchronization. This paper presents EdgeServe, a machine learning system that can serve decentralized predictions. EdgeServe relies on a low-latency message broker to route data through a network to nodes that can serve predictions. EdgeServe relies on a series of novel optimizations that can tradeoff computation, communication, and accuracy. We evaluate EdgeServe on three decentralized prediction tasks: (1) multi-camera object tracking, (2) network intrusion detection, and (3) human activity recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053; AA ++&#65292;&#33021;&#22815;&#22312;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2301.13748</link><description>&lt;p&gt;
&#21407;&#22411;&#20998;&#26512;++&#65306;&#37325;&#26032;&#24605;&#32771;&#21021;&#22987;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Archetypal Analysis++: Rethinking the Initialization Strategy. (arXiv:2301.13748v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053; AA ++&#65292;&#33021;&#22815;&#22312;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20998;&#26512;&#26159;&#19968;&#31181;&#24102;&#26377;&#20984;&#24615;&#32422;&#26463;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#12290;&#30001;&#20110;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#22909;&#30340;&#21021;&#22987;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#32463;&#24120;&#20351;&#29992;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#35201;&#20040;&#20135;&#29983;&#27425;&#20248;&#30340;&#36215;&#22987;&#28857;&#65292;&#35201;&#20040;&#23481;&#26131;&#38519;&#20837;&#19981;&#33391;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22411;&#20998;&#26512;++&#65288;AA ++&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#23427;&#26681;&#25454;&#28857;&#23545;&#30446;&#26631;&#30340;&#24433;&#21709;&#39034;&#24207;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#31867;&#20284;&#20110;$k$-means++&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;$k$-means++&#24050;&#36817;&#36924;&#36817;&#20102;&#25152;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;$k$-means++&#30340;&#39640;&#25928;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26041;&#27861;&#24212;&#29992;&#20110;AA++&#12290;&#22312;&#23545;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#24182;&#32771;&#34385;&#20004;&#20010;&#39044;&#22788;&#29702;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;AA++&#20960;&#20046;&#24635;&#26159;&#20248;&#20110;&#25152;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones.
&lt;/p&gt;</description></item><item><title>&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12292</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12292
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#22312;&#32447;&#33829;&#38144;&#31561;&#39046;&#22495;&#65292;&#39044;&#27979;&#19981;&#21516;&#24178;&#39044;&#25514;&#26045;&#23545;&#29305;&#23450;&#20010;&#20307;&#30340;&#22240;&#26524;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#29616;&#26377;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#26377;&#35768;&#22810;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#25509;&#21463;&#36807;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#20307;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#65306;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CaML&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#39044;&#27979;&#25928;&#26524;&#20316;&#20026;&#19968;&#20010;&#20219;&#21153;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;CaML&#22312;&#25968;&#21315;&#20010;&#20219;&#21153;&#20013;&#35757;&#32451;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#25277;&#26679;&#29983;&#25104;&#19968;&#20010;&#24178;&#39044;&#25514;&#26045;&#21450;&#20854;&#25509;&#25910;&#32773;&#21644;&#38750;&#25509;&#25910;&#32773;&#26469;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#24178;&#39044;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#30340;&#23646;&#24615;&#65289;&#21644;&#20010;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#29305;&#23450;&#20010;&#20307;&#30340;&#21307;&#30103;&#35760;&#24405;&#65289;&#65292;CaML&#23398;&#20064;&#22914;&#20309;&#23558;&#24050;&#35266;&#23519;&#21040;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#20256;&#36755;&#32473;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#24178;&#39044;&#25514;&#26045;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#22411;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#35757;&#32451;&#26399;&#38388;&#27745;&#26579;&#20840;&#23616;&#27169;&#22411;&#23454;&#29616;&#38544;&#34109;&#36890;&#20449;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.10561</link><description>&lt;p&gt;
&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#34109;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Covert Channel Attack to Federated Learning Systems. (arXiv:2104.10561v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.10561
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#22411;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#35757;&#32451;&#26399;&#38388;&#27745;&#26579;&#20840;&#23616;&#27169;&#22411;&#23454;&#29616;&#38544;&#34109;&#36890;&#20449;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#22312;&#20247;&#22810;&#36793;&#32536;&#23458;&#25143;&#31471;&#20043;&#38388;&#20998;&#24067;&#27169;&#22411;&#35757;&#32451;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;&#36825;&#20123;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#27844;&#38706;&#20182;&#20204;&#30340;&#26412;&#22320;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20043;&#38388;&#20849;&#20139;&#65292;&#29992;&#20110;&#26412;&#22320;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#36716;&#21270;&#20026;&#38544;&#34109;&#20449;&#36947;&#65292;&#23454;&#29616;&#38544;&#31192;&#30340;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65292;&#22312;&#32852;&#37030;&#35757;&#32451;&#26399;&#38388;&#65292;&#24694;&#24847;&#21457;&#36865;&#32773;&#21487;&#20197;&#36890;&#36807;&#25552;&#20132;&#19987;&#38376;&#26500;&#36896;&#30340;&#26679;&#26412;&#26469;&#27745;&#26579;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;&#27169;&#22411;&#27745;&#26579;&#23545;&#20854;&#20182;&#21442;&#19982;&#32773;&#24433;&#21709;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#20063;&#19981;&#20250;&#25913;&#21464;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#23427;&#21487;&#20197;&#34987;&#24694;&#24847;&#25509;&#25910;&#32773;&#35266;&#23519;&#21040;&#65292;&#24182;&#29992;&#20110;&#20256;&#36755;&#19968;&#20010;&#27604;&#29305;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) goes beyond traditional, centralized machine learning by distributing model training among a large collection of edge clients. These clients cooperatively train a global, e.g., cloud-hosted, model without disclosing their local, private training data. The global model is then shared among all the participants which use it for local predictions. In this paper, we put forward a novel attacker model aiming at turning FL systems into covert channels to implement a stealth communication infrastructure. The main intuition is that, during federated training, a malicious sender can poison the global model by submitting purposely crafted examples. Although the effect of the model poisoning is negligible to other participants, and does not alter the overall model performance, it can be observed by a malicious receiver and used to transmit a single bit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#22122;&#22768;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#38382;&#39064;&#65292;&#24182;&#23548;&#20986;&#20102;&#22312;&#36825;&#20010;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#24418;&#24335;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#20855;&#26377;&#29109;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2102.04363</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#19982;&#26377;&#22122;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Efficient Data-Driven Optimization with Noisy Data. (arXiv:2102.04363v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#22122;&#22768;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#38382;&#39064;&#65292;&#24182;&#23548;&#20986;&#20102;&#22312;&#36825;&#20010;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#24418;&#24335;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#20855;&#26377;&#29109;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#32773;&#38754;&#20020;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21487;&#29992;&#20110;&#20915;&#31574;&#30340;&#25968;&#25454;&#37117;&#21463;&#21040;&#19968;&#23450;&#31243;&#24230;&#30340;&#27979;&#37327;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#22122;&#22768;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#38382;&#39064;&#65292;&#24182;&#23548;&#20986;&#20102;&#22312;&#36825;&#20010;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#25968;&#25454;&#39537;&#21160;&#24418;&#24335;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#20855;&#26377;&#29109;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;Strassen&#30340;&#32463;&#20856;&#34920;&#31034;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26377;&#25928;&#30340;&#40065;&#26834;&#24418;&#24335;&#22312;&#20960;&#31181;&#26377;&#36259;&#30340;&#35774;&#32622;&#19979;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical Kullback-Leibler or entropic distances are known to enjoy certain desirable statistical properties in the context of decision-making with noiseless data. However, in most practical situations the data available to a decision maker is subject to a certain amount of measurement noise. We hence study here data-driven prescription problems in which the data is corrupted by a known noise source. We derive efficient data-driven formulations in this noisy regime and indicate that they enjoy an entropic optimal transport interpretation. Finally, we show that these efficient robust formulations are tractable in several interesting settings by exploiting a classical representation result by Strassen.
&lt;/p&gt;</description></item></channel></rss>