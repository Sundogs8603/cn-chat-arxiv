<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#12289;&#26631;&#35760;&#25968;&#25454;&#38598;&#38656;&#27714;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.16485</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#30340;&#20840;&#38754;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP. (arXiv:2310.16485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#12289;&#26631;&#35760;&#25968;&#25454;&#38598;&#38656;&#27714;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#22320;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#23545;&#20110;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12289;&#26816;&#27979;&#24322;&#24120;&#21644;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#20107;&#20214;&#26816;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20173;&#26377;&#25913;&#36827;&#21644;&#21019;&#26032;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#12290;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22235;&#20010;&#29420;&#29305;&#30340;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#23427;&#22522;&#20110;&#22238;&#24402;&#32780;&#19981;&#26159;&#20108;&#20803;&#20998;&#31867;&#12290;&#20854;&#27425;&#65292;&#23427;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#28857;&#37117;&#26377;&#26631;&#31614;&#65307;&#30456;&#21453;&#65292;&#23427;&#21482;&#38656;&#35201;&#23450;&#20041;&#20026;&#26102;&#38388;&#28857;&#25110;&#26102;&#38388;&#38388;&#38548;&#30340;&#21442;&#32771;&#20107;&#20214;&#12290;&#31532;&#19977;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#35774;&#35745;&#24471;&#21040;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event detection in time series data is crucial in various domains, including finance, healthcare, cybersecurity, and science. Accurately identifying events in time series data is vital for making informed decisions, detecting anomalies, and predicting future trends. Despite extensive research exploring diverse methods for event detection in time series, with deep learning approaches being among the most advanced, there is still room for improvement and innovation in this field. In this paper, we present a new deep learning supervised method for detecting events in multivariate time series data. Our method combines four distinct novelties compared to existing deep-learning supervised methods. Firstly, it is based on regression instead of binary classification. Secondly, it does not require labeled datasets where each point is labeled; instead, it only requires reference events defined as time points or intervals of time. Thirdly, it is designed to be robust by using a stacked ensemble l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedGMIR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16407</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#24322;&#26500;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#30340;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedGMIR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36793;&#32536;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#37096;&#32626;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#34987;&#31216;&#20026;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#12290;&#22312;FEEL&#20013;&#65292;&#31227;&#21160;&#35774;&#22791;&#36890;&#36807;&#22122;&#22768;&#36890;&#36947;&#20256;&#36755;&#27169;&#22411;&#21442;&#25968;&#21644;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#36825;&#32473;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#35774;&#22791;&#38388;&#36890;&#20449;&#36827;&#34892;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65292;&#32780;&#36830;&#25509;&#35774;&#22791;&#30340;&#36890;&#20449;&#25299;&#25169;&#20063;&#24433;&#21709;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#22312;&#24320;&#23637;&#27867;&#21270;&#20998;&#26512;&#26102;&#24573;&#35270;&#20102;&#25152;&#26377;&#36825;&#20123;&#25928;&#24212;&#30340;&#32435;&#20837;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;FEEL&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#22122;&#22768;&#36890;&#36947;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#20840;&#23616;&#20114;&#20449;&#24687;&#20943;&#23569;&#65288;FedGMIR&#65289;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of edge intelligence, the deployment of federated learning (FL) over wireless networks has garnered increasing attention, which is called Federated Edge Learning (FEEL). In FEEL, both mobile devices transmitting model parameters over noisy channels and collecting data in diverse environments pose challenges to the generalization of trained models. Moreover, devices can engage in decentralized FL via Device-to-Device communication while the communication topology of connected devices also impacts the generalization of models. Most recent theoretical studies overlook the incorporation of all these effects into FEEL when developing generalization analyses. In contrast, our work presents an information-theoretic generalization analysis for topology-aware FEEL in the presence of data heterogeneity and noisy channels. Additionally, we propose a novel regularization method called Federated Global Mutual Information Reduction (FedGMIR) to enhance the performance of models
&lt;/p&gt;</description></item><item><title>HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15318</link><description>&lt;p&gt;
HetGPT: &#21033;&#29992;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15318
&lt;/p&gt;
&lt;p&gt;
HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20026;&#34920;&#31034;&#21644;&#20998;&#26512;Web&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#20351;&#24471;&#22312;&#32447;&#39029;&#38754;&#20998;&#31867;&#21644;&#31038;&#20132;&#25512;&#33616;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#23384;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#8220;&#36127;&#36716;&#31227;&#8221;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#20852;&#36215;&#34920;&#26126;&#20102;&#23558;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#8221;&#33539;&#24335;&#24212;&#29992;&#20110;&#22270;&#24418;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;&#25552;&#31034;&#25216;&#26415;&#38024;&#23545;&#30340;&#26159;&#21516;&#36136;&#22270;&#65292;&#24573;&#35270;&#20102;Web&#22270;&#30340;&#20869;&#22312;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetGPT&#65292;
&lt;/p&gt;
&lt;p&gt;
Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
&lt;/p&gt;</description></item><item><title>ReMax&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;PPO&#65292;ReMax&#31616;&#21270;&#20102;&#23454;&#29616;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;fine-tuning&#26102;&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10505</link><description>&lt;p&gt;
ReMax:&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. (arXiv:2310.10505v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10505
&lt;/p&gt;
&lt;p&gt;
ReMax&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;PPO&#65292;ReMax&#31616;&#21270;&#20102;&#23454;&#29616;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;fine-tuning&#26102;&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#35201;&#31574;&#30053;&#26159;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;PPO&#26159;&#20107;&#23454;&#19978;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;PPO&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#26159;&#26412;&#35770;&#25991;&#35797;&#22270;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;RLHF&#20219;&#21153;&#20013;&#30830;&#23450;&#20102;&#19977;&#20010;&#37325;&#35201;&#29305;&#24615;&#65306;&#24555;&#36895;&#27169;&#25311;&#12289;&#30830;&#23450;&#24615;&#36716;&#25442;&#21644;&#36712;&#36857;&#32423;&#22870;&#21169;&#65292;&#36825;&#20123;&#29305;&#24615;&#22312;PPO&#20013;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;RLHF&#30340;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;ReMax&#12290;ReMax&#30340;&#31639;&#27861;&#35774;&#35745;&#26159;&#22522;&#20110;&#19968;&#31181;&#24191;&#20026;&#20351;&#29992;&#30340;&#31639;&#27861;REINFORCE&#65292;&#20294;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;PPO&#20855;&#26377;&#19977;&#37325;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;ReMax&#23454;&#29616;&#31616;&#21333;&#65292;&#28040;&#38500;&#20102;PPO&#20013;&#30340;&#35768;&#22810;&#19982;&#35268;&#27169;&#30456;&#20851;&#19988;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;ReMax&#21407;&#21017;&#19978;&#21487;&#20197;&#33410;&#32422;&#32422;50%&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;&#32467;&#26524;&#23548;&#33268;PPO&#22312;&#36827;&#34892;fine-tuning&#26102;&#20986;&#29616;&#20869;&#23384;&#28322;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alignment is of critical importance for training large language models (LLMs). The predominant strategy to address this is through Reinforcement Learning from Human Feedback (RLHF), where PPO serves as the de-facto algorithm. Yet, PPO is known to suffer from computational inefficiency, which is a challenge that this paper aims to address. We identify three important properties in RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on such observations, we develop a new algorithm tailored for RLHF, called ReMax. The algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is equipped with a new variance-reduction technique.  Our method has three-fold advantages over PPO: first, ReMax is simple to implement and removes many hyper-parameters in PPO, which are scale-sensitive and laborious to tune. Second, ReMax saves about 50% memory usage in principle. As a result, PPO runs out-of-memory when fine-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20307;&#39038;&#23458;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65288;CLV&#65289;&#26469;&#23450;&#20301;&#26368;&#26377;&#20215;&#20540;&#30340;&#39038;&#23458;&#65292;&#24182;&#37319;&#29992;&#39044;&#27979;&#21644;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#35299;&#20915;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;12&#20010;&#27969;&#22833;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#22343;&#21033;&#28070;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.07047</link><description>&lt;p&gt;
&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A predict-and-optimize approach to profit-driven churn prevention. (arXiv:2310.07047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20307;&#39038;&#23458;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65288;CLV&#65289;&#26469;&#23450;&#20301;&#26368;&#26377;&#20215;&#20540;&#30340;&#39038;&#23458;&#65292;&#24182;&#37319;&#29992;&#39044;&#27979;&#21644;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#35299;&#20915;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;12&#20010;&#27969;&#22833;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#22343;&#21033;&#28070;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#28070;&#39537;&#21160;&#27969;&#22833;&#38450;&#27490;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#23450;&#20301;&#39038;&#23458;&#29992;&#20110;&#30041;&#23384;&#27963;&#21160;&#30340;&#20219;&#21153;&#26500;&#24314;&#20026;&#19968;&#20010;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#20010;&#20307;&#39038;&#23458;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65288;CLV&#65289;&#30830;&#20445;&#21482;&#26377;&#26368;&#26377;&#20215;&#20540;&#30340;&#39038;&#23458;&#34987;&#23450;&#20301;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#35768;&#22810;&#21033;&#28070;&#39537;&#21160;&#31574;&#30053;&#20851;&#27880;&#27969;&#22833;&#27010;&#29575;&#21516;&#26102;&#32771;&#34385;&#24179;&#22343;CLV&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#24102;&#26469;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31526;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#65288;PnO&#65289;&#26694;&#26550;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#39640;&#25928;&#35299;&#20915;&#12290;&#26469;&#33258;12&#20010;&#27969;&#22833;&#39044;&#27979;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#24179;&#22343;&#21033;&#28070;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel predict-and-optimize method for profit-driven churn prevention. We frame the task of targeting customers for a retention campaign as a regret minimization problem. The main objective is to leverage individual customer lifetime values (CLVs) to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs. This often results in significant information loss due to data aggregation. Our proposed model aligns with the guidelines of Predict-and-Optimize (PnO) frameworks and can be efficiently solved using stochastic gradient descent methods. Results from 12 churn prediction datasets underscore the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05161</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20197;&#23481;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#20351;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LMs&#24182;&#19981;&#25551;&#36848;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#65292;&#32780;&#26159;&#23450;&#20041;&#20102;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RNN LMs&#21487;&#20197;&#34920;&#31034;&#21738;&#20123;&#31867;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#38472;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;RNN&#31561;&#20215;&#20110;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22240;&#27492;&#21482;&#33021;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;RNNs&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;LMs&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#20026;&#20102;&#34920;&#31034;&#19968;&#20010;&#20219;&#24847;&#30830;&#23450;&#30340;&#26377;&#38480;&#29366;&#24577;LMs&#65292;&#20854;&#20013;&#26377;$N$&#20010;&#29366;&#24577;&#19988;&#23383;&#31526;&#38598;&#20026;$\Sigma$&#30340;RNN requir
&lt;/p&gt;
&lt;p&gt;
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#28508;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#20851;&#38190;&#36830;&#25509;&#21644;&#34917;&#20805;&#32570;&#22833;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04314</link><description>&lt;p&gt;
&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#28508;&#22312;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Inference with Limited Supervision. (arXiv:2310.04314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#28508;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#20851;&#38190;&#36830;&#25509;&#21644;&#34917;&#20805;&#32570;&#22833;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22270;&#25512;&#29702; (LGI) &#30340;&#30446;&#26631;&#26159;&#20174;&#25968;&#25454;&#29305;&#24449;&#20013;&#21516;&#26102;&#23398;&#20064;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LGI&#26041;&#27861;&#36890;&#24120;&#36973;&#21463;&#30563;&#23548;&#21294;&#20047;&#38382;&#39064;&#65292;&#23548;&#33268;&#22823;&#37327;&#30340;&#36793;&#26435;&#37325;&#22312;&#27809;&#26377;&#35821;&#20041;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#19981;&#33021;&#23545;&#35757;&#32451;&#25439;&#22833;&#20570;&#20986;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32570;&#20047;&#30417;&#30563;&#30340;&#26435;&#37325;&#21487;&#33021;&#20915;&#23450;&#27979;&#35797;&#26679;&#26412;&#30340;&#39044;&#27979;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#26159;&#35821;&#20041;&#19978;&#26368;&#20248;&#30340;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#36825;&#20010;&#38382;&#39064;&#23454;&#38469;&#19978;&#26159;&#30001;&#20110;&#22270;&#31232;&#30095;&#21270;&#25805;&#20316;&#23548;&#33268;&#30340;&#65292;&#20005;&#37325;&#30772;&#22351;&#20102;&#20851;&#38190;&#33410;&#28857;&#21644;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#30340;&#37325;&#35201;&#36830;&#25509;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24674;&#22797;&#21463;&#25439;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20026;&#26356;&#22909;&#30340;LGI&#34917;&#20805;&#32570;&#22833;&#30340;&#30417;&#30563;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#24182;&#24674;&#22797;&#25439;&#22351;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20851;&#38190;&#33410;&#28857;&#23450;&#20041;&#20026;$k$-hop&#39269;&#39295;&#33410;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#39269;&#39295;&#29366;&#20917;&#35745;&#31639;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as $k$-hop starved nodes, which can be id
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>PharmacoNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#34394;&#25311;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#30340;&#19977;&#32500;&#33647;&#29289;&#35889;&#25490;&#21015;&#26469;&#21152;&#36895;&#22823;&#35268;&#27169;&#34394;&#25311;&#31579;&#36873;&#36807;&#31243;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#20934;&#30830;&#24615;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.00681</link><description>&lt;p&gt;
PharmacoNet: &#21033;&#29992;&#28145;&#24230;&#33647;&#29289;&#35889;&#24314;&#27169;&#21152;&#36895;&#22823;&#35268;&#27169;&#34394;&#25311;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling. (arXiv:2310.00681v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00681
&lt;/p&gt;
&lt;p&gt;
PharmacoNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#34394;&#25311;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#30340;&#19977;&#32500;&#33647;&#29289;&#35889;&#25490;&#21015;&#26469;&#21152;&#36895;&#22823;&#35268;&#27169;&#34394;&#25311;&#31579;&#36873;&#36807;&#31243;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#20934;&#30830;&#24615;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#35775;&#38382;&#21270;&#21512;&#29289;&#24211;&#30340;&#35268;&#27169;&#25193;&#22823;&#21040;&#36229;&#36807;100&#20159;&#20010;&#65292;&#23545;&#26356;&#39640;&#25928;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#34394;&#25311;&#31579;&#36873;&#26041;&#27861;&#30340;&#38656;&#27714;&#27491;&#22312;&#20986;&#29616;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#19981;&#21516;&#30340;&#39044;&#31579;&#36873;&#26041;&#27861;&#26469;&#24555;&#36895;&#31579;&#36873;&#24211;&#65292;&#20294;&#36866;&#29992;&#20110;&#36890;&#29992;&#34507;&#30333;&#36136;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#65306;&#25361;&#25112;&#26159;&#22312;&#26497;&#30701;&#30340;&#26102;&#38388;&#20869;&#39044;&#27979;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#32467;&#21512;&#20301;&#23039;&#24182;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#24341;&#20837;PharmacoNet&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20174;&#32467;&#21512;&#20301;&#28857;&#29983;&#25104;&#30340;&#26368;&#20339;&#19977;&#32500;&#33647;&#29289;&#35889;&#25490;&#21015;&#26469;&#35782;&#21035;&#37197;&#20307;&#24212;&#35813;&#20855;&#26377;&#30340;&#31283;&#23450;&#32467;&#21512;&#35201;&#27714;&#12290;&#36890;&#36807;&#31895;&#31890;&#21270;&#22270;&#21305;&#37197;&#65292;&#25105;&#20204;&#22312;&#19968;&#27493;&#20013;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26114;&#36149;&#30340;&#32467;&#21512;&#20301;&#23039;&#37319;&#26679;&#21644;&#35780;&#20998;&#36807;&#31243;&#12290;PharmacoNet&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#35201;&#24555;&#24471;&#22810;&#65292;&#20294;&#20173;&#20855;&#26377;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#20877;&#21033;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of accessible compound libraries expands to over 10 billion, the need for more efficient structure-based virtual screening methods is emerging. Different pre-screening methods have been developed to rapidly screen the library, but the structure-based methods applicable to general proteins are still lacking: the challenge is to predict the binding pose between proteins and ligands and perform scoring in an extremely short time. We introduce PharmacoNet, a deep learning framework that identifies the optimal 3D pharmacophore arrangement which a ligand should have for stable binding from the binding site. By coarse-grained graph matching between ligands and the generated pharmacophore arrangement, we solve the expensive binding pose sampling and scoring procedures of existing methods in a single step. PharmacoNet is significantly faster than state-of-the-art structure-based approaches, yet reasonably accurate with a simple scoring function. Furthermore, we show the promising re
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#21457;&#29616;&#65292;&#21363;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#35757;&#32451;&#22312;&#22810;&#20010;&#27169;&#24577;&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12458</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Multimodal Learning. (arXiv:2309.12458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12458
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#21457;&#29616;&#65292;&#21363;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#35757;&#32451;&#22312;&#22810;&#20010;&#27169;&#24577;&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23545;&#32463;&#39564;&#19990;&#30028;&#30340;&#24863;&#30693;&#28041;&#21450;&#21040;&#35782;&#21035;&#22522;&#30784;&#29289;&#20307;&#30340;&#21508;&#31181;&#22806;&#35266;&#25110;&#8220;&#27169;&#24577;&#8221;&#12290;&#23613;&#31649;&#21746;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#32771;&#34385;&#36825;&#19968;&#35266;&#28857;&#65292;&#20294;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23545;&#22810;&#27169;&#24577;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#32463;&#39564;&#23454;&#36341;&#65292;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#65292;&#21482;&#26377;&#21551;&#21457;&#24335;&#35770;&#35777;&#12290;&#22810;&#27169;&#24577;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#21457;&#29616;&#26159;&#65292;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#35757;&#32451;&#22312;&#22810;&#20010;&#27169;&#24577;&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#23398;&#20064;&#20855;&#26377;&#26356;&#20248;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#39640;&#36798;$O(\sqrt{n})$&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#26679;&#26412;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human perception of the empirical world involves recognizing the diverse appearances, or 'modalities', of underlying objects. Despite the longstanding consideration of this perspective in philosophy and cognitive science, the study of multimodality remains relatively under-explored within the field of machine learning. Nevertheless, current studies of multimodal machine learning are limited to empirical practices, lacking theoretical foundations beyond heuristic arguments. An intriguing finding from the practice of multimodal learning is that a model trained on multiple modalities can outperform a finely-tuned unimodal model, even on unimodal tasks. This paper provides a theoretical framework that explains this phenomenon, by studying generalization properties of multimodal learning algorithms. We demonstrate that multimodal learning allows for a superior generalization bound compared to unimodal learning, up to a factor of $O(\sqrt{n})$, where $n$ represents the sample size. Such adva
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#25506;&#32034;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#25104;&#23545;&#26102;&#23646;&#24615;&#25913;&#21464;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#20915;&#31574;&#19982;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#32422;&#26463;&#38590;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11343</link><description>&lt;p&gt;
&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#29702;&#35299;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Using Property Elicitation to Understand the Impacts of Fairness Constraints. (arXiv:2309.11343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#25506;&#32034;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#25104;&#23545;&#26102;&#23646;&#24615;&#25913;&#21464;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#20915;&#31574;&#19982;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#32422;&#26463;&#38590;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#26576;&#20010;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#28155;&#21152;&#27491;&#21017;&#21270;&#20989;&#25968;&#26469;&#26045;&#21152;&#36829;&#21453;&#32422;&#26463;&#30340;&#24809;&#32602;&#12290;&#39044;&#26399;&#22320;&#65292;&#28155;&#21152;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#20989;&#25968;&#21487;&#20197;&#25913;&#21464;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#20540;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#21738;&#20123;&#27491;&#21017;&#21270;&#20989;&#25968;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#20540;&#65292;&#20197;&#21450;&#24403;&#26368;&#23567;&#21270;&#20540;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#23427;&#20250;&#22914;&#20309;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#21021;&#27493;&#20102;&#35299;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#19982;&#32473;&#23450;&#38382;&#39064;&#23454;&#20363;&#30340;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#32852;&#21512;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#25104;&#23545;&#26102;&#65292;&#23646;&#24615;&#25913;&#21464;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20123;&#28385;&#36275;&#36825;&#20010;&#26465;&#20214;&#30340;&#27491;&#21017;&#21270;&#20989;&#25968;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#20915;&#31574;&#22914;&#20309;&#38543;&#30528;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#21644;&#32422;&#26463;&#30340;&#38590;&#24230;&#32780;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#39640;&#32500;&#38750;&#20984;&#25104;&#26412;&#20989;&#25968;&#20248;&#21270;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SGD&#22312;&#24674;&#22797;&#39640;&#32500;&#38750;&#32447;&#24615;&#21152;&#23494;&#20449;&#21495;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.04788</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#39640;&#32500;&#20449;&#21495;&#24674;&#22797;&#30340;&#29627;&#29827;&#33021;&#37327;&#26223;&#35266;&#20013;&#34920;&#29616;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape. (arXiv:2309.04788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#39640;&#32500;&#38750;&#20984;&#25104;&#26412;&#20989;&#25968;&#20248;&#21270;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SGD&#22312;&#24674;&#22797;&#39640;&#32500;&#38750;&#32447;&#24615;&#21152;&#23494;&#20449;&#21495;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#19968;&#31181;&#38750;&#24179;&#34913;&#31639;&#27861;&#65292;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;SGD&#22312;&#36825;&#39033;&#25216;&#26415;&#30340;&#25104;&#21151;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#31243;&#24230;&#20197;&#21450;&#30456;&#23545;&#20110;&#20854;&#20182;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#26799;&#24230;&#19979;&#38477;&#65289;&#22312;&#20248;&#21270;&#39640;&#32500;&#38750;&#20984;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#30340;&#25928;&#26524;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#22312;&#39640;&#32500;&#26497;&#38480;&#20013;&#20934;&#30830;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#24674;&#22797;&#38544;&#34255;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21152;&#23494;&#20449;&#21495;&#38382;&#39064;&#65292;&#21363;&#19968;&#20010;&#20856;&#22411;&#30340;&#39640;&#32500;&#38750;&#20984;&#30340;&#38590;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;SGD&#21644;GD&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#26126;SGD&#22823;&#22823;&#20248;&#20110;GD&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24347;&#35947;&#26102;&#38388;&#36827;&#34892;&#24130;&#24459;&#25311;&#21512;&#34920;&#26126;&#65292;SGD&#22312;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#30340;&#24674;&#22797;&#38408;&#20540;&#23567;&#20110;GD&#30340;&#23545;&#24212;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used extensively to train artificial neural networks. However very little is known on to what extent SGD is crucial for to the success of this technology and, in particular, how much it is effective in optimizing high-dimensional non-convex cost functions as compared to other optimization algorithms such as Gradient Descent (GD). In this work we leverage dynamical mean field theory to analyze exactly its performances in the high-dimensional limit. We consider the problem of recovering a hidden high-dimensional non-linearly encrypted signal, a prototype high-dimensional non-convex hard optimization problem. We compare the performances of SGD to GD and we show that SGD largely outperforms GD. In particular, a power law fit of the relaxation time of these algorithms shows that the recovery threshold for SGD with small batch size is smaller than the corresponding one of GD.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.13838</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#23545;&#32852;&#21512;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#21487;&#20197;&#24418;&#25104;&#20856;&#22411;&#30340;&#20080;&#26041;&#24066;&#22330;&#65292;&#20854;&#20013;PS/&#20080;&#23478;&#25968;&#37327;&#36828;&#36828;&#23569;&#20110;&#23458;&#25143;&#31471;/&#21334;&#23478;&#25968;&#37327;&#12290;&#20026;&#20102;&#25913;&#21892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#20026;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30456;&#21516;&#30340;&#26381;&#21153;&#23450;&#20215;&#12290;&#20215;&#26684;&#24046;&#24322;&#21270;&#22522;&#20110;&#23545;&#32852;&#21512;&#23398;&#20064;&#24102;&#26469;&#30340;&#24615;&#33021;&#25913;&#36827;&#21644;&#35745;&#31639;&#36890;&#20449;&#33021;&#21147;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#20840;&#38754;&#35299;&#20915;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#30446;&#26631;&#26435;&#34913;&#12289;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#28608;&#21169;&#26426;&#21046;&#12290;&#30001;&#20110;PDG&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#21322;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#26469;&#20272;&#35745;RIS-&#21551;&#29992;&#30340;&#27627;&#31859;&#27874;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#29992;&#25143;&#35774;&#22791;&#21040;RIS&#21644;RIS&#21040;&#22522;&#31449;&#30340;&#36890;&#36947;&#65292;&#20943;&#23569;&#20102;&#20449;&#21495;&#22797;&#26434;&#24615;&#21644;&#30636;&#26102;&#20449;&#36947;&#30340;&#39640;&#20449;&#20196;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2308.13616</link><description>&lt;p&gt;
RIS-&#21551;&#29992;&#30340;&#27627;&#31859;&#27874;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#65306;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational Inference Approach. (arXiv:2308.13616v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#26469;&#20272;&#35745;RIS-&#21551;&#29992;&#30340;&#27627;&#31859;&#27874;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#29992;&#25143;&#35774;&#22791;&#21040;RIS&#21644;RIS&#21040;&#22522;&#31449;&#30340;&#36890;&#36947;&#65292;&#20943;&#23569;&#20102;&#20449;&#21495;&#22797;&#26434;&#24615;&#21644;&#30636;&#26102;&#20449;&#36947;&#30340;&#39640;&#20449;&#20196;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23436;&#20840;&#34987;&#21160;&#30340;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;(RIS)&#36741;&#21161;&#30340;&#27627;&#31859;&#27874;&#21333;&#29992;&#25143;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;(SIMO)&#36890;&#20449;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#25512;&#26029;(VI)&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VI&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#34892;&#35757;&#32451;&#20449;&#21495;&#26469;&#20272;&#35745;&#29992;&#25143;&#35774;&#22791;(UE)&#21040;RIS(UE-RIS)&#21644;RIS&#21040;&#22522;&#31449;(RIS-BS)&#20449;&#36947;&#22312;&#34987;&#21160;RIS&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30636;&#26102;CSI(I-CSI)&#26356;&#26032;&#30456;&#31227;&#20250;&#23548;&#33268;&#39640;&#20449;&#20196;&#24320;&#38144;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;UE-RIS&#20449;&#36947;&#30340;&#30701;&#30456;&#24178;&#22359;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20943;&#23569;&#20449;&#20196;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;RIS-BS&#20449;&#36947;&#20197;&#21450;&#23545;&#20110;&#27604;&#30636;&#26102;UE-RIS&#20449;&#36947;&#26356;&#38271;&#26102;&#38388;&#20445;&#25345;&#20934;&#38745;&#24577;&#30340;UE-RIS&#20449;&#36947;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#22312;VI&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#26041;&#20415;&#30340;&#20998;&#24067;&#20989;&#25968;&#26469;&#36817;&#20284;&#20449;&#36947;&#22686;&#30410;/&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a variational inference (VI)-based channel state information (CSI) estimation approach in a fully-passive reconfigurable intelligent surface (RIS)-aided mmWave single-user single-input multiple-output (SIMO) communication system. Specifically, we first propose a VI-based joint channel estimation method to estimate the user-equipment (UE) to RIS (UE-RIS) and RIS to base station (RIS-BS) channels using uplink training signals in a passive RIS setup. However, updating the phase-shifts based on the instantaneous CSI (I-CSI) leads to a high signaling overhead especially due to the short coherence block of the UE-RIS channel. Therefore, to reduce the signaling complexity, we propose a VI-based method to estimate the RIS-BS channel along with the covariance matrix of the UE-RIS channel that remains quasi-static for a longer period than the instantaneous UE-RIS channel. In the VI framework, we approximate the posterior of the channel gains/covariance matrix with convenient distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10997</link><description>&lt;p&gt;
SPEGTI: &#32467;&#26500;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26082;&#36924;&#30495;&#21448;&#19982;&#25991;&#26412;&#25552;&#31034;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36136;&#37327;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#25512;&#26029;&#65292;&#24182;&#20351;&#29992;&#22823;&#27169;&#22411;&#12290;&#36825;&#31181;&#36845;&#20195;&#36807;&#31243;&#26159;&#20026;&#20102;&#30830;&#20445;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#19981;&#20165;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#65292;&#36824;&#19982;&#20854;&#20182;&#21306;&#22495;&#30456;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;Muse&#27169;&#22411;&#37197;&#21512;&#20351;&#29992;&#12290;MRF&#32534;&#30721;&#20102;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#22270;&#20687;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#23481;&#24615;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;Muse&#39044;&#27979;&#27493;&#39588;&#12290;&#20351;&#29992;MRF&#30340;&#25512;&#26029;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#24555;&#36895;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#36890;&#36807;&#23545;MRF&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2308.09720</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#24819;&#19981;&#21040;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09720
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#23637;&#31034;&#20986;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#65288;&#39044;&#27979;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#65289;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#30340;&#24615;&#36136;&#21450;&#20854;&#19982;&#20854;&#20182;&#24050;&#30693;&#38388;&#25509;&#36807;&#31243;&#30340;&#20851;&#31995;&#12290;&#25991;&#31456;&#20027;&#24352;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#30340;&#19968;&#20010;&#37325;&#35201;&#21103;&#20316;&#29992;&#26159;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24320;&#21457;&#30340;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#21487;&#39044;&#27979;&#30340;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#31616;&#35201;&#35752;&#35770;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#33719;&#24471;&#30340;&#35748;&#30693;&#25216;&#33021;&#19982;&#20154;&#31867;&#35748;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.09437</link><description>&lt;p&gt;
&#20174;&#26399;&#26395;&#21040;&#23433;&#20840;&#65306;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#27491;&#30830;&#30340;&#21407;&#22240;&#26469;&#28040;&#38500;&#28145;&#24230;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space. (arXiv:2308.09437v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#28508;&#34255;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#36825;&#22312;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#65288;&#22914;&#21307;&#23398;&#24212;&#29992;&#65289;&#26102;&#23384;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#30340;&#21518;&#22788;&#29702;&#27169;&#22411;&#26657;&#27491;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36755;&#20837;&#32423;&#21035;&#30340;&#27880;&#37322;&#65292;&#36825;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#21270;&#20559;&#35265;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#20805;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#24076;&#26395;&#33021;&#23454;&#29616;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#24403;&#36890;&#36807;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#24314;&#27169;&#20559;&#35265;&#26102;&#65292;&#25105;&#20204;&#24378;&#35843;&#36873;&#25321;&#31283;&#20581;&#30340;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#65288;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24448;&#24448;&#20250;&#23548;&#33268;&#21457;&#25955;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#20351;&#29992;VGG&#12289;ResNet&#21644;EfficientN&#22312;ISIC&#12289;&#39592;&#40836;&#12289;ImageNet&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#22312;&#21463;&#25511;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#20943;&#36731;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25391;&#33633;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#21033;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#26102;&#38388;&#29305;&#24615;&#26469;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08989</link><description>&lt;p&gt;
&#31070;&#32463;&#25391;&#33633;&#22120;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Neural oscillators for generalization of physics-informed machine learning. (arXiv:2308.08989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25391;&#33633;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#21033;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#26102;&#38388;&#29305;&#24615;&#26469;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20854;&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#34920;&#31034;&#30340;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#12290;&#26412;&#25991;&#26088;&#22312;&#22686;&#24378;PIML&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#22312;&#26410;&#25506;&#32034;&#21306;&#22495;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;PDE&#35299;&#30340;&#22266;&#26377;&#22240;&#26524;&#20851;&#31995;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24615;&#65292;&#23558;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#24490;&#29615;&#31070;&#32463;&#26550;&#26500;&#19982;PIML&#27169;&#22411;&#34701;&#21512;&#65292;&#31216;&#20043;&#20026;&#31070;&#32463;&#25391;&#33633;&#22120;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#32531;&#35299;&#26799;&#24230;&#29190;&#28856;&#21644;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#31070;&#32463;&#25391;&#33633;&#22120;&#20419;&#36827;&#20102;PIML&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#28041;&#21450;&#26102;&#38388;&#20381;&#36182;&#30340;&#38750;&#32447;&#24615;PDE&#21644;&#21452;&#35843;&#21644;&#26753;&#26041;&#31243;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary challenge of physics-informed machine learning (PIML) is its generalization beyond the training domain, especially when dealing with complex physical problems represented by partial differential equations (PDEs). This paper aims to enhance the generalization capabilities of PIML, facilitating practical, real-world applications where accurate predictions in unexplored regions are crucial. We leverage the inherent causality and temporal sequential characteristics of PDE solutions to fuse PIML models with recurrent neural architectures based on systems of ordinary differential equations, referred to as neural oscillators. Through effectively capturing long-time dependencies and mitigating the exploding and vanishing gradient problem, neural oscillators foster improved generalization in PIML tasks. Extensive experimentation involving time-dependent nonlinear PDEs and biharmonic beam equations demonstrates the efficacy of the proposed approach. Incorporating neural oscillators out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2308.08536</link><description>&lt;p&gt;
Transformers&#33021;&#21542;&#23398;&#20064;&#29992;&#20110;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20248;&#28388;&#27874;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#36807;&#21435;&#30340;&#25152;&#26377;&#36755;&#20986;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20808;&#39564;&#20998;&#24067;&#30340;&#21508;&#31181;&#31995;&#32479;&#26469;&#35757;&#32451;transformer&#65292;&#28982;&#21518;&#22312;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#30456;&#21516;&#20998;&#24067;&#30340;&#31995;&#32479;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;transformer&#23601;&#20687;&#19968;&#20010;&#39044;&#27979;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24182;&#24555;&#36895;&#36866;&#24212;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#31995;&#32479;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;MOP&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07728</link><description>&lt;p&gt;
&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65306;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#24050;&#20855;&#22791;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#21457;&#29983;&#30072;&#21464;&#12290;&#22312;&#36866;&#24212;&#26032;&#30446;&#26631;&#39046;&#22495;&#26102;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#23545;&#22836;&#23618;&#36827;&#34892;&#23545;&#40784;&#22788;&#29702;&#21487;&#20197;&#22788;&#29702;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25209;&#24402;&#19968;&#21270;&#23618;&#30340;&#22788;&#29702;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#12289;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#25913;&#26469;&#26377;&#25928;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06148</link><description>&lt;p&gt;
NetGPT: &#36229;&#36234;&#25552;&#20379;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06148
&lt;/p&gt;
&lt;p&gt;
NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20449;&#24687;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;LLMs&#30340;&#20010;&#24615;&#21270;&#21487;&#33021;&#36827;&#19968;&#27493;&#20419;&#36827;&#23427;&#20204;&#22312;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;&#38024;&#23545;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#21548;&#36215;&#26469;&#24456;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#26377;&#25928;&#21327;&#35843;&#24322;&#26500;&#20998;&#24067;&#24335;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#31181;&#20505;&#36873;&#30340;&#20113;&#36793;&#21327;&#20316;&#25216;&#26415;&#30340;&#21033;&#24330;&#65292;&#25552;&#20986;&#20102;NetGPT&#65292;&#26681;&#25454;&#20854;&#35745;&#31639;&#33021;&#21147;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;LLMs&#12290;&#27492;&#22806;&#65292;&#36793;&#32536;LLMs&#21487;&#20197;&#39640;&#25928;&#21033;&#29992;&#22522;&#20110;&#20301;&#32622;&#30340;&#20449;&#24687;&#36827;&#34892;&#20010;&#24615;&#21270;&#25552;&#31034;&#23436;&#25104;&#65292;&#20174;&#32780;&#26377;&#30410;&#20110;&#19982;&#20113;&#31471;LLMs&#30340;&#20114;&#21160;&#12290;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;GPT-2-base&#21644;LLaMA&#27169;&#22411;&#65289;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NetGPT&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
&lt;/p&gt;</description></item><item><title>PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;</title><link>http://arxiv.org/abs/2307.05845</link><description>&lt;p&gt;
PIGEON: &#39044;&#27979;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05845
&lt;/p&gt;
&lt;p&gt;
PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;PIGEON&#65292;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#22312;&#22806;&#37096;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#26631;&#31614;&#24179;&#28369;&#65292;&#23545;&#20855;&#26377;&#22320;&#29702;&#20449;&#24687;&#30340;&#22270;&#20687;&#36827;&#34892;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;ProtoNets&#22312;&#20505;&#36873;&#22320;&#29702;&#21333;&#20803;&#38598;&#21512;&#20013;&#25913;&#36827;&#20301;&#32622;&#39044;&#27979;&#12290;PIGEON&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#30340;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#21019;&#24314;&#21644;&#20998;&#21106;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22320;&#29702;&#21333;&#20803;&#20869;&#37096;&#31934;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;StreetCLIP&#65292;&#20844;&#24320;&#25552;&#20379;&#65292;&#21487;&#29992;&#20110;&#19982;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#22478;&#24066;&#20065;&#26449;&#22330;&#26223;&#29702;&#35299;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.02507</link><description>&lt;p&gt;
STS-CCL&#65306;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#37319;&#29992;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#29992;&#20110;&#26102;&#31354;&#22270;&#25968;&#25454;&#30340;&#22522;&#26412;&#21644;&#24378;&#22823;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25200;&#21160;&#20102;&#22270;&#32467;&#26500;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21033;&#29992;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#36827;&#34892;&#33258;&#36866;&#24212;&#22686;&#24378;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#31354;&#21516;&#27493;&#23545;&#27604;&#27169;&#22359;&#65288;STS-CM&#65289;&#65292;&#20197;&#21516;&#26102;&#25429;&#25417;&#33391;&#22909;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#24182;&#23454;&#29616;&#22270;&#32423;&#23545;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21306;&#20998;&#36127;&#31579;&#36873;&#20013;&#30340;&#33410;&#28857;&#20010;&#20307;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#29305;&#24449;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01504</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#24050;&#25104;&#20026;&#35768;&#22810;&#22270;&#20219;&#21153;&#30340;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21033;&#29992;&#36890;&#29992;&#30340;&#22270;&#30693;&#35782;&#26469;&#32531;&#35299;&#27599;&#20010;&#24212;&#29992;&#20013;&#32570;&#20047;&#22270;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#30340;&#22270;&#20219;&#21153;&#24046;&#24322;&#24456;&#22823;&#65292;&#23548;&#33268;&#39044;&#35757;&#32451;&#39044;&#25991;&#26412;&#36890;&#24120;&#19982;&#36825;&#20123;&#22810;&#20219;&#21153;&#19981;&#20860;&#23481;&#12290;&#36825;&#31181;&#24046;&#36317;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#8220;&#36127;&#36801;&#31227;&#8221;&#65292;&#20174;&#32780;&#23548;&#33268;&#32467;&#26524;&#19981;&#20339;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21033;&#29992;&#20808;&#21069;&#30693;&#35782;&#24050;&#32463;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22635;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#24046;&#36317;&#30340;&#25552;&#31034;&#20027;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;&#20196;&#29260;&#12289;&#20196;&#29260;&#32467;&#26500;&#21644;&#25554;&#20837;&#27169;&#24335;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In
&lt;/p&gt;</description></item><item><title>AVSegFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#26469;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#35270;&#35273;&#29305;&#24449;&#65292;&#36824;&#20351;&#29992;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01146</link><description>&lt;p&gt;
AVSegFormer: &#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01146
&lt;/p&gt;
&lt;p&gt;
AVSegFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#26469;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#35270;&#35273;&#29305;&#24449;&#65292;&#36824;&#20351;&#29992;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#32467;&#21512;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#22810;&#27169;&#24577;&#39046;&#22495;&#30340;&#19968;&#20010;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#65288;AVS&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#23450;&#20301;&#21644;&#20998;&#21106;&#32473;&#23450;&#35270;&#39057;&#20013;&#30340;&#26377;&#22768;&#23545;&#35937;&#12290;&#36825;&#20010;&#20219;&#21153;&#39318;&#27425;&#35201;&#27714;&#22312;&#20687;&#32032;&#32423;&#21035;&#23545;&#38899;&#39057;&#39537;&#21160;&#30340;&#22330;&#26223;&#36827;&#34892;&#29702;&#35299;&#65292;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AVSegFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;AVS&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#24863;&#20852;&#36259;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30456;&#20851;&#30340;&#31354;&#38388;&#36890;&#36947;&#21644;&#25233;&#21046;&#26080;&#20851;&#30340;&#31354;&#38388;&#36890;&#36947;&#26469;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#65292;&#20197;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#65292;&#40723;&#21169;&#32593;&#32476;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20013;&#38388;&#39044;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.13803</link><description>&lt;p&gt;
&#22823;&#35937;&#19982;&#31639;&#27861;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#22823;&#35937;&#30417;&#27979;&#20013;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#20316;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20026;&#22686;&#36827;&#23545;&#21160;&#29289;&#34892;&#20026;&#21644;&#20445;&#25252;&#31574;&#30053;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#26426;&#20250;&#12290;&#20197;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#20026;&#28966;&#28857;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#23427;&#20204;&#20445;&#25252;&#20013;&#30340;&#20316;&#29992;&#12290;&#32473;&#23450;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#65288;&#22914;&#25668;&#20687;&#22836;&#12289;&#40614;&#20811;&#39118;&#12289;&#22320;&#38663;&#20202;&#12289;&#26080;&#20154;&#26426;&#21644;&#21355;&#26143;&#65289;&#25910;&#38598;&#21040;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#65292;&#25361;&#25112;&#22312;&#20110;&#31649;&#29702;&#21644;&#35299;&#35835;&#36825;&#20123;&#24222;&#22823;&#30340;&#25968;&#25454;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24110;&#21161;&#25105;&#20204;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;AI&#39537;&#21160;&#30417;&#27979;&#26041;&#27861;&#21450;&#20854;&#22312;&#25913;&#21892;&#22823;&#35937;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21033;&#29992;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#20197;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#30340;&#20851;&#38190;&#25152;&#22312;&#65292;&#20026;&#35768;&#22810;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20102;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12212</link><description>&lt;p&gt;
MimiC&#65306;&#27169;&#20223;&#20013;&#24515;&#26356;&#26032;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#20219;&#21153;&#20998;&#21457;&#32473;&#23458;&#25143;&#31471;&#65292;&#21482;&#38656;&#35201;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#25910;&#38598;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#37096;&#32626;&#26102;&#65292;&#23458;&#25143;&#31471;&#65288;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21487;&#33021;&#20250;&#26080;&#39044;&#35686;&#22320;&#36864;&#20986;&#20219;&#20309;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#36825;&#20250;&#38459;&#30861;&#32852;&#37030;&#23398;&#20064;&#36798;&#21040;&#25910;&#25947;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#21517;&#20026; MimiC &#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20013;&#24515;&#26381;&#21153;&#22120;&#20462;&#25913;&#20854;&#26356;&#26032;&#20197;&#27169;&#20223;&#32570;&#22833;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MimiC &#30456;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#32858;&#28966;&#20110;&#24102;&#24335;&#34880;&#21387;&#30417;&#27979;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#30001;&#20110;&#27979;&#37327;&#21644;&#35774;&#22791;&#35823;&#24046;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20307;&#22411;&#24046;&#24322;&#31561;&#22240;&#32032;&#23548;&#33268;&#30340;&#34880;&#21387;&#27979;&#37327;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#30740;&#21457;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#32416;&#27491;&#35823;&#24046;&#30340;&#26032;&#19968;&#20195;&#24102;&#24335;&#34880;&#21387;&#35774;&#22791;&#26159;&#37325;&#28857;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.08451</link><description>&lt;p&gt;
&#34880;&#21387;&#27979;&#37327;&#25216;&#26415;&#32508;&#36848;&#65306;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias. (arXiv:2306.08451v2 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08451
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#32858;&#28966;&#20110;&#24102;&#24335;&#34880;&#21387;&#30417;&#27979;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#30001;&#20110;&#27979;&#37327;&#21644;&#35774;&#22791;&#35823;&#24046;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20307;&#22411;&#24046;&#24322;&#31561;&#22240;&#32032;&#23548;&#33268;&#30340;&#34880;&#21387;&#27979;&#37327;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#30740;&#21457;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#32416;&#27491;&#35823;&#24046;&#30340;&#26032;&#19968;&#20195;&#24102;&#24335;&#34880;&#21387;&#35774;&#22791;&#26159;&#37325;&#28857;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#21644;&#27969;&#21160;&#22330;&#26223;&#20013;&#30340;&#23450;&#26399;&#34880;&#21387;&#65288;BP&#65289;&#30417;&#27979;&#22312;&#39044;&#38450;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#24515;&#34880;&#31649;&#30142;&#30149;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#27969;&#21160;&#24335;BP&#27979;&#37327;&#35774;&#22791;&#30340;&#24191;&#27867;&#37319;&#29992;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#34880;&#21387;&#30340;&#26222;&#36941;&#22686;&#21152;&#20197;&#21450;&#20854;&#30456;&#20851;&#39118;&#38505;&#21644;&#20020;&#24202;&#29366;&#20917;&#12290;&#26368;&#36817;&#30340;&#25351;&#21335;&#24314;&#35758;&#23558;&#23450;&#26399;BP&#30417;&#27979;&#20316;&#20026;&#24120;&#35268;&#20020;&#24202;&#35775;&#35270;&#29978;&#33267;&#22312;&#23478;&#37324;&#36827;&#34892;&#12290; &#36825;&#31181;&#22686;&#21152;&#30340;BP&#27979;&#37327;&#25216;&#26415;&#21033;&#29992;&#24102;&#24335;&#27979;&#21387;&#26041;&#27861;&#20063;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#65292;&#28041;&#21450;&#21508;&#31181;&#35774;&#32622;&#19979;&#25253;&#21578;&#30340;BP&#20540;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#24102;&#24335;&#27979;&#21387;&#25216;&#26415;&#30340;BP&#27979;&#37327;&#26041;&#27861;&#22914;&#20309;&#30001;&#20110;&#27979;&#37327;&#21644;&#35774;&#22791;&#35823;&#24046;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20307;&#22411;&#24046;&#24322;&#31561;&#22240;&#32032;&#23548;&#33268;&#26174;&#33879;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#22312;&#36825;&#20123;&#22266;&#26377;&#30340;&#20559;&#24046;&#24773;&#20917;&#19979;&#65292;&#21457;&#23637;&#19968;&#31181;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#24102;&#24335;&#27979;&#21387;&#35774;&#22791;&#65292;&#20854;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#32416;&#27491;&#35823;&#24046;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular blood pressure (BP) monitoring in clinical and ambulatory settings plays a crucial role in the prevention, diagnosis, treatment, and management of cardiovascular diseases. Recently, the widespread adoption of ambulatory BP measurement devices has been driven predominantly by the increased prevalence of hypertension and its associated risks and clinical conditions. Recent guidelines advocate for regular BP monitoring as part of regular clinical visits or even at home. This increased utilization of BP measurement technologies has brought up significant concerns, regarding the accuracy of reported BP values across settings.  In this survey, focusing mainly on cuff-based BP monitoring technologies, we highlight how BP measurements can demonstrate substantial biases and variances due to factors such as measurement and device errors, demographics, and body habitus. With these inherent biases, the development of a new generation of cuff-based BP devices which use artificial-intelligen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#39044;&#27979;&#38598;&#30340;&#26399;&#26395;&#22823;&#23567;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#37327;&#21270;&#26041;&#27861;&#20197;&#21450;&#28857;&#20272;&#35745;&#21644;&#39640;&#27010;&#29575;&#21306;&#38388;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07254</link><description>&lt;p&gt;
&#20851;&#20110;&#36866;&#24212;&#24615;&#39044;&#27979;&#38598;&#26399;&#26395;&#22823;&#23567;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expected Size of Conformal Prediction Sets. (arXiv:2306.07254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#39044;&#27979;&#38598;&#30340;&#26399;&#26395;&#22823;&#23567;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#37327;&#21270;&#26041;&#27861;&#20197;&#21450;&#28857;&#20272;&#35745;&#21644;&#39640;&#27010;&#29575;&#21306;&#38388;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36866;&#24212;&#24615;&#39044;&#27979;&#22120;&#22312;&#35823;&#24046;&#39057;&#29575;&#26041;&#38754;&#20855;&#26377;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#20294;&#20854;&#39044;&#27979;&#38598;&#22823;&#23567;&#23545;&#20854;&#23454;&#38469;&#25928;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#21644;&#39044;&#27979;&#38598;&#22823;&#23567;&#30340;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20998;&#35010;&#36866;&#24212;&#24615;&#39044;&#27979;&#26694;&#26550;&#19979;&#29702;&#35770;&#37327;&#21270;&#39044;&#27979;&#38598;&#30340;&#26399;&#26395;&#22823;&#23567;&#12290;&#22240;&#20026;&#36825;&#31181;&#31934;&#30830;&#30340;&#35745;&#31639;&#36890;&#24120;&#26080;&#27861;&#30452;&#25509;&#35745;&#31639;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20986;&#21487;&#36731;&#26494;&#35745;&#31639;&#30340;&#28857;&#20272;&#35745;&#21644;&#39640;&#27010;&#29575;&#21306;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25551;&#36848;&#27979;&#35797;&#21644;&#26657;&#20934;&#25968;&#25454;&#19981;&#21516;&#21487;&#33021;&#23454;&#29616;&#30340;&#26399;&#26395;&#39044;&#27979;&#38598;&#22823;&#23567;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While conformal predictors reap the benefits of rigorous statistical guarantees for their error frequency, the size of their corresponding prediction sets is critical to their practical utility. Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes. To address this shortfall, we theoretically quantify the expected size of the prediction set under the split conformal prediction framework. As this precise formulation cannot usually be calculated directly, we further derive point estimates and high probability intervals that can be easily computed, providing a practical method for characterizing the expected prediction set size across different possible realizations of the test and calibration data. Additionally, we corroborate the efficacy of our results with experiments on real-world datasets, for both regression and classification problems.
&lt;/p&gt;</description></item><item><title>Mesogeos&#26159;&#19968;&#20010;&#22320;&#20013;&#28023;&#22320;&#21306;&#30340;&#22823;&#35268;&#27169;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#37326;&#28779;&#24314;&#27169;&#12290;&#23427;&#38598;&#25104;&#20102;&#21382;&#21490;&#37326;&#28779;&#35760;&#24405;&#21644;&#37326;&#28779;&#39537;&#21160;&#22240;&#32032;&#65292;&#20855;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#30701;&#26399;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#21644;&#26368;&#32456;&#28903;&#27585;&#21306;&#22495;&#20272;&#35745;&#20004;&#20010;&#21487;&#29992;&#20110;&#28436;&#31034;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.05144</link><description>&lt;p&gt;
Mesogeos: &#22320;&#20013;&#28023;&#21306;&#22495;&#25968;&#25454;&#39537;&#21160;&#37326;&#28779;&#24314;&#27169;&#30340;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean. (arXiv:2306.05144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05144
&lt;/p&gt;
&lt;p&gt;
Mesogeos&#26159;&#19968;&#20010;&#22320;&#20013;&#28023;&#22320;&#21306;&#30340;&#22823;&#35268;&#27169;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#37326;&#28779;&#24314;&#27169;&#12290;&#23427;&#38598;&#25104;&#20102;&#21382;&#21490;&#37326;&#28779;&#35760;&#24405;&#21644;&#37326;&#28779;&#39537;&#21160;&#22240;&#32032;&#65292;&#20855;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#30701;&#26399;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#21644;&#26368;&#32456;&#28903;&#27585;&#21306;&#22495;&#20272;&#35745;&#20004;&#20010;&#21487;&#29992;&#20110;&#28436;&#31034;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Mesogeos&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22320;&#20013;&#28023;&#22320;&#21306;&#30340;&#37326;&#28779;&#24314;&#27169;&#12290;Mesogeos&#38598;&#25104;&#20102;&#20195;&#34920;&#37326;&#28779;&#39537;&#21160;&#22240;&#32032;&#65288;&#27668;&#35937;&#12289;&#26893;&#34987;&#12289;&#20154;&#31867;&#27963;&#21160;&#65289;&#21644;17&#24180;&#65288;2006-2022&#24180;&#65289;&#37326;&#28779;&#28857;&#29123;&#21644;&#28903;&#27585;&#21306;&#22495;&#30340;&#21382;&#21490;&#35760;&#24405;&#30340;&#21464;&#37327;&#12290;&#23427;&#34987;&#35774;&#35745;&#20026;&#20113;&#21451;&#22909;&#22411;&#26102;&#31354;&#25968;&#25454;&#38598;&#65288;&#21363;&#25968;&#25454;&#31435;&#26041;&#20307;&#65289;&#65292;&#22312;1km x 1km x 1&#22825;&#30340;&#20998;&#36776;&#29575;&#19979;&#23545;&#25152;&#26377;&#21464;&#37327;&#36827;&#34892;&#20102;&#21327;&#35843;&#12290;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#32467;&#26500;&#20026;&#21508;&#31181;&#37326;&#28779;&#24314;&#27169;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#20351;&#29992;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#21462;&#20986;&#20004;&#20010;ML&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#23637;&#31034;&#36825;&#20010;&#28508;&#21147;&#65306;(1)&#30701;&#26399;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#21644;(2)&#22312;&#28857;&#28779;&#20301;&#32622;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26368;&#32456;&#28903;&#27585;&#21306;&#22495;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#25351;&#26631;&#21644;&#22522;&#32447;&#26469;&#35780;&#20272;&#27599;&#20010;&#36319;&#36394;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21457;&#24067;&#25968;&#25454;&#31435;&#26041;&#20307;&#65292;&#20197;&#21450;&#29992;&#20110;&#21019;&#24314;ML&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#25105;&#20204;&#40723;&#21169;&#31038;&#21306;&#20419;&#36827;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#21333;&#20010;&#28508;&#22312;&#21464;&#37327;&#36830;&#25509;&#21040;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25513;&#34109;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#27169;&#24577;&#38388;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04445</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28508;&#22312;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Latent Diffusion. (arXiv:2306.04445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#21333;&#20010;&#28508;&#22312;&#21464;&#37327;&#36830;&#25509;&#21040;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25513;&#34109;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#27169;&#24577;&#38388;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26080;&#22788;&#19981;&#22312;&#65292;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#19981;&#21516;&#27169;&#24577;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#36830;&#36143;&#24615; - &#36136;&#37327;&#25240;&#34935;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#29983;&#25104;&#36136;&#37327;&#30340;&#27169;&#22411;&#32570;&#20047;&#27169;&#24577;&#38388;&#30340;&#29983;&#25104;&#36830;&#36143;&#24615;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#35752;&#35770;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#35828;&#26126;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#21333;&#20010;&#28508;&#22312;&#21464;&#37327;&#36830;&#25509;&#21040;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#21040;&#25513;&#34109;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#23454;&#29616;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#26465;&#20214;&#24471;&#20998;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#37117;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal data-sets are ubiquitous in modern applications, and multi-modal Variational Autoencoders are a popular family of models that aim to learn a joint representation of the different modalities. However, existing approaches suffer from a coherence-quality tradeoff, where models with good generation quality lack generative coherence across modalities, and vice versa. We discuss the limitations underlying the unsatisfactory performance of existing methods, to motivate the need for a different approach. We propose a novel method that uses a set of independently trained, uni-modal, deterministic autoencoders. Individual latent variables are concatenated into a common latent space, which is fed to a masked diffusion model to enable generative modeling. We also introduce a new multi-time training method to learn the conditional score network for multi-modal diffusion. Our methodology substantially outperforms competitors in both generation quality and coherence, as shown through an e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20808;&#21069;&#24037;&#20316;&#24369;&#19968;&#31867;&#38382;&#39064;&#36827;&#34892;&#25512;&#24191;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#28151;&#21512;&#19979;&#30340;&#24178;&#39044;&#20013;&#23398;&#20064;&#32447;&#24615;&#22240;&#26524;&#34920;&#31034;&#30340;&#24378;&#21487;&#35782;&#21035;&#24615;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02235</link><description>&lt;p&gt;
&#20174;&#38750;&#32447;&#24615;&#28151;&#21512;&#19979;&#30340;&#24178;&#39044;&#20013;&#23398;&#20064;&#32447;&#24615;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Linear Causal Representations from Interventions under General Nonlinear Mixing. (arXiv:2306.02235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20808;&#21069;&#24037;&#20316;&#24369;&#19968;&#31867;&#38382;&#39064;&#36827;&#34892;&#25512;&#24191;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#28151;&#21512;&#19979;&#30340;&#24178;&#39044;&#20013;&#23398;&#20064;&#32447;&#24615;&#22240;&#26524;&#34920;&#31034;&#30340;&#24378;&#21487;&#35782;&#21035;&#24615;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#28151;&#21512;&#20989;&#25968;&#23436;&#20840;&#36890;&#29992;&#30340;&#19968;&#33324;&#35774;&#32622;&#19979;&#65292;&#20174;&#26410;&#30693;&#30340;&#28508;&#22312;&#24178;&#39044;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28508;&#22312;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290; &#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21333;&#33410;&#28857;&#26410;&#30693;&#24178;&#39044;&#65288;&#21363;&#27809;&#26377;&#24178;&#39044;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65289;&#32473;&#20986;&#24378;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;&#36825;&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#30528;&#37325;&#20110;&#26356;&#24369;&#30340;&#31867;&#21035;&#65292;&#20363;&#22914;&#32447;&#24615;&#26144;&#23556;&#25110;&#25104;&#23545;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;&#36825;&#20063;&#26159;&#39318;&#27425;&#20174;&#38750;&#37197;&#23545;&#24178;&#39044;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20013;&#33719;&#24471;&#22240;&#26524;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#20180;&#32454;&#25581;&#31034;&#32463;&#36807;&#38750;&#32447;&#24615;&#23494;&#24230;&#36716;&#25442;&#21518;&#25968;&#25454;&#20998;&#24067;&#20013;&#23384;&#22312;&#30340;&#39640;&#32500;&#20960;&#20309;&#32467;&#26500;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#28508;&#22312;&#20998;&#24067;&#30340;&#31934;&#24230;&#30697;&#38453;&#30340;&#20108;&#27425;&#24418;&#24335;&#26469;&#25429;&#25417;&#36825;&#31181;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#31639;&#27861;&#26469;&#23454;&#38469;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning causal representations from unknown, latent interventions in a general setting, where the latent distribution is Gaussian but the mixing function is completely general. We prove strong identifiability results given unknown single-node interventions, i.e., without having access to the intervention targets. This generalizes prior works which have focused on weaker classes, such as linear maps or paired counterfactual data. This is also the first instance of causal identifiability from non-paired interventions for deep neural network embeddings. Our proof relies on carefully uncovering the high-dimensional geometric structure present in the data distribution after a non-linear density transformation, which we capture by analyzing quadratic forms of precision matrices of the latent distributions. Finally, we propose a contrastive algorithm to identify the latent variables in practice and evaluate its performance on various tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;NeSy&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#23450;&#20041;&#20026;&#23398;&#20064;&#30446;&#26631;&#30340;&#24847;&#22806;&#26368;&#20248;&#35299;&#65292;&#24182;&#30830;&#23450;&#20854;&#21457;&#29983;&#30340;&#22235;&#20010;&#20851;&#38190;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#38590;&#20197;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.19951</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#37117;&#26159;&#24179;&#31561;&#30340;&#65306; &#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts. (arXiv:2305.19951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;NeSy&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#23450;&#20041;&#20026;&#23398;&#20064;&#30446;&#26631;&#30340;&#24847;&#22806;&#26368;&#20248;&#35299;&#65292;&#24182;&#30830;&#23450;&#20854;&#21457;&#29983;&#30340;&#22235;&#20010;&#20851;&#38190;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#38590;&#20197;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#65288;NeSy&#65289;&#39044;&#27979;&#27169;&#22411;&#25215;&#35834;&#20855;&#26377;&#25913;&#36827;&#30340;&#32422;&#26463;&#36981;&#20174;&#24615;&#65292;&#31995;&#32479;&#21270;&#27867;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#36890;&#36807;&#23545;&#20174;&#23376;&#31526;&#21495;&#36755;&#20837;&#20013;&#25552;&#21462;&#20986;&#30340;&#39640;&#32423;&#27010;&#24565;&#36827;&#34892;&#25512;&#29702;&#26469;&#25512;&#26029;&#19982;&#26576;&#20123;&#20808;&#39564;&#30693;&#35782;&#19968;&#33268;&#30340;&#26631;&#31614;&#12290;&#26368;&#36817;&#26174;&#31034;NeSy&#39044;&#27979;&#22120;&#21463;&#21040;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#30340;&#24433;&#21709;&#65306;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#24847;&#22806;&#35821;&#20041;&#30340;&#27010;&#24565;&#36798;&#21040;&#39640;&#31934;&#24230;&#65292;&#20174;&#32780;&#30701;&#20110;&#20854;&#25215;&#35834;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#32570;&#23569;&#23545;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#21450;&#20854;&#28508;&#22312;&#32531;&#35299;&#31574;&#30053;&#30340;&#31995;&#32479;&#25551;&#36848;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20854;&#34920;&#24449;&#20026;&#23398;&#20064;&#30446;&#26631;&#30340;&#24847;&#22806;&#26368;&#20248;&#35299;&#65292;&#24182;&#30830;&#23450;&#20854;&#21457;&#29983;&#32972;&#21518;&#30340;&#22235;&#20010;&#20851;&#38190;&#26465;&#20214;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20960;&#31181;&#33258;&#28982;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#20998;&#26512;&#23427;&#20204;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#24456;&#38590;&#22788;&#29702;&#65292;&#36825;&#23545;&#20110;&#20449;&#20219;&#23427;&#20204;&#30340;&#21512;&#29702;&#24615;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic (NeSy) predictive models hold the promise of improved compliance with given constraints, systematic generalization, and interpretability, as they allow to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts extracted from sub-symbolic inputs. It was recently shown that NeSy predictors are affected by reasoning shortcuts: they can attain high accuracy but by leveraging concepts with unintended semantics, thus coming short of their promised advantages. Yet, a systematic characterization of reasoning shortcuts and of potential mitigation strategies is missing. This work fills this gap by characterizing them as unintended optima of the learning objective and identifying four key conditions behind their occurrence. Based on this, we derive several natural mitigation strategies, and analyze their efficacy both theoretically and empirically. Our analysis shows reasoning shortcuts are difficult to deal with, casting doubts on the trus
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24191;&#20041;&#22343;&#34913;&#8221;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20197;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22914;&#26524;&#27809;&#26377;&#32431;&#32435;&#20160;&#22343;&#34913;&#65292;&#21017;&#19968;&#21517;&#29609;&#23478;&#21487;&#20197;&#20174;&#19981;&#21516;&#31574;&#30053;&#20013;&#21463;&#30410;&#65292;&#32467;&#26524;&#25429;&#33719;&#20102;Stackelberg&#22343;&#34913;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.19496</link><description>&lt;p&gt;
&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#26159;&#21542;&#23545;&#23398;&#20064;&#32773;&#26377;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Learning in Games Good for the Learners?. (arXiv:2305.19496v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19496
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24191;&#20041;&#22343;&#34913;&#8221;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20197;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22914;&#26524;&#27809;&#26377;&#32431;&#32435;&#20160;&#22343;&#34913;&#65292;&#21017;&#19968;&#21517;&#29609;&#23478;&#21487;&#20197;&#20174;&#19981;&#21516;&#31574;&#30053;&#20013;&#21463;&#30410;&#65292;&#32467;&#26524;&#25429;&#33719;&#20102;Stackelberg&#22343;&#34913;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19982;&#22870;&#21169;&#21644;&#21518;&#24724;&#22312;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#37325;&#22797;&#29609;&#28216;&#25103;&#30456;&#20851;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#19981;&#23545;&#31216;&#30340;&#21518;&#24724;&#32422;&#26463;&#65292;&#24182;&#20026;&#27599;&#20010;&#20195;&#29702;&#21644;&#19968;&#23545;&#21518;&#24724;&#32422;&#26463;&#27966;&#29983;&#21487;&#34892;&#20540;&#30340;&#22810;&#38754;&#20307;&#12290;&#20316;&#20026;&#26680;&#24515;&#26696;&#20363;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#26041;&#26159;&#31105;&#27490;&#20132;&#25442;&#30340;&#65292;&#21478;&#19968;&#26041;&#30340;&#21518;&#24724;&#27809;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#23427;&#25429;&#33719;&#20102;&#19982;Stackelberg&#22343;&#34913;&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#21487;&#21305;&#37197;&#26368;&#20248;&#20540;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#22823;&#31867;&#28216;&#25103;&#65292;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#65292;&#19968;&#21517;&#29609;&#23478;&#21487;&#20197;&#36890;&#36807;&#20174;&#31105;&#27490;&#20132;&#25442;&#30340;&#21518;&#24724;&#31639;&#27861;&#20013;&#20559;&#31163;&#65292;&#26174;&#33879;&#22686;&#21152;&#33258;&#24049;&#30340;&#25928;&#29992;&#65288;&#23454;&#38469;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#27809;&#26377;&#32431;&#32435;&#20160;&#22343;&#34913;&#30340;&#28216;&#25103;&#37117;&#26159;&#36825;&#31181;&#24418;&#24335;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this, we introduce a notion of {\it generalized equilibrium} which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of {\it Stackelberg} equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21450;&#20854;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#21160;&#24577;&#19979;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#25214;&#21040;&#20102;&#22810;&#20010;&#21644;&#21333;&#19968;&#26041;&#21521;&#30340;&#26368;&#20339;&#25209;&#37327;&#22823;&#23567;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#21644;&#26041;&#21521;&#30340;&#19987;&#19994;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18270</link><description>&lt;p&gt;
&#23398;&#20064;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#27425;(&#24040;&#22823;)&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Two-Layer Neural Networks, One (Giant) Step at a Time. (arXiv:2305.18270v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21450;&#20854;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#21160;&#24577;&#19979;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#25214;&#21040;&#20102;&#22810;&#20010;&#21644;&#21333;&#19968;&#26041;&#21521;&#30340;&#26368;&#20339;&#25209;&#37327;&#22823;&#23567;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#21644;&#26041;&#21521;&#30340;&#19987;&#19994;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#30740;&#31350;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#26377;&#21161;&#20110;&#22312;&#26680;&#24515;&#33539;&#22260;&#20043;&#22806;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25209;&#37327;&#22823;&#23567;&#21644;&#22810;&#20010;(&#20294;&#26377;&#38480;&#30340;)&#27493;&#39588;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#27493;&#39588;&#36807;&#31243;&#65292;&#21457;&#29616;&#25209;&#37327;&#22823;&#23567;&#20026;$n=O(d)$&#21487;&#20197;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#20294;&#21482;&#36866;&#21512;&#23398;&#20064;&#21333;&#19968;&#26041;&#21521;&#25110;&#21333;&#32034;&#24341;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;$n=O(d^2)$&#23545;&#20110;&#23398;&#20064;&#22810;&#20010;&#26041;&#21521;&#21644;&#19987;&#19994;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#8220;&#30828;&#8221;&#26041;&#21521;&#32570;&#20047;&#21069;$\ell$&#20010;Hermite&#31995;&#25968;&#65292;&#20173;&#26410;&#34987;&#21457;&#29616;&#65292;&#24182;&#19988;&#38656;&#35201;&#25209;&#37327;&#22823;&#23567;&#20026;$n=O(d^\ell)$&#25165;&#33021;&#34987;&#26799;&#24230;&#19979;&#38477;&#25429;&#33719;&#12290;&#32463;&#36807;&#20960;&#27425;&#36845;&#20195;&#65292;&#24773;&#20917;&#21457;&#29983;&#21464;&#21270;&#65306;&#25209;&#37327;&#22823;&#23567;&#20026;$n=O(d)$&#36275;&#20197;&#23398;&#20064;&#26032;&#30340;&#30446;&#26631;&#26041;&#21521;&#65292;&#36825;&#20123;&#26041;&#21521;&#22312;Hermite&#22522;&#30784;&#19978;&#32447;&#24615;&#36830;&#25509;&#21040;&#20043;&#21069;&#23398;&#20064;&#30340;&#26041;&#21521;&#25152;&#28085;&#30422;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the training dynamics of shallow neural networks, investigating the conditions under which a limited number of large batch gradient descent steps can facilitate feature learning beyond the kernel regime. We compare the influence of batch size and that of multiple (but finitely many) steps. Our analysis of a single-step process reveals that while a batch size of $n = O(d)$ enables feature learning, it is only adequate for learning a single direction, or a single-index model. In contrast, $n = O(d^2)$ is essential for learning multiple directions and specialization. Moreover, we demonstrate that ``hard'' directions, which lack the first $\ell$ Hermite coefficients, remain unobserved and require a batch size of $n = O(d^\ell)$ for being captured by gradient descent. Upon iterating a few steps, the scenario changes: a batch-size of $n = O(d)$ is enough to learn new target directions spanning the subspace linearly connected in the Hermite basis to the previously learned directions,
&lt;/p&gt;</description></item><item><title>GLOBE-CE&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22240;&#26524;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36229;&#36234;&#23616;&#37096;&#35299;&#37322;&#65292;&#25552;&#20379;&#26356;&#26377;&#25928;&#21644;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.17021</link><description>&lt;p&gt;
GLOBE-CE&#65306;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22240;&#26524;&#35299;&#37322;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. (arXiv:2305.17021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17021
&lt;/p&gt;
&lt;p&gt;
GLOBE-CE&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22240;&#26524;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36229;&#36234;&#23616;&#37096;&#35299;&#37322;&#65292;&#25552;&#20379;&#26356;&#26377;&#25928;&#21644;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35299;&#37322;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20844;&#24179;&#24615;&#12289;&#36861;&#32034;&#26435;&#21644;&#27169;&#22411;&#29702;&#35299;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26368;&#22823;&#30340;&#32570;&#28857;&#26159;&#26080;&#27861;&#25552;&#20379;&#36229;&#36234;&#23616;&#37096;&#25110;&#23454;&#20363;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#35768;&#22810;&#20316;&#21697;&#28041;&#21450;&#20840;&#23616;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#36890;&#24120;&#24314;&#35758;&#32858;&#21512;&#22823;&#37327;&#23616;&#37096;&#35299;&#37322;&#20197;&#30830;&#23450;&#20840;&#23616;&#23646;&#24615;&#65292;&#20294;&#24456;&#23569;&#25552;&#20379;&#21487;&#38752;&#19988;&#35745;&#31639;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#23454;&#36341;&#32773;&#38656;&#35201;&#26356;&#26377;&#25928;&#21644;&#20132;&#20114;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#12290;&#25105;&#20204;&#20511;&#27492;&#26426;&#20250;&#25552;&#20986;&#20102;&#20840;&#23616;&#19988;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26694;&#26550;(GLOBE-CE)&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26694;&#26550;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#21644;&#36830;&#32493;&#29305;&#24449;&#23384;&#22312;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global &amp; Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathemati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;UnionSNN&#65292;&#27880;&#20837;&#20102;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#23376;&#22270;&#26469;&#32534;&#30721;&#39640;&#38454;&#36830;&#25509;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;1-WL&#21644;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15747</link><description>&lt;p&gt;
Union Subgraph&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Union Subgraph Neural Networks. (arXiv:2305.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;UnionSNN&#65292;&#27880;&#20837;&#20102;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#23376;&#22270;&#26469;&#32534;&#30721;&#39640;&#38454;&#36830;&#25509;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;1-WL&#21644;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#30001;&#20110;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#20256;&#36882;&#28040;&#24687;&#26469;&#22788;&#29702;&#26377;&#26681;&#23376;&#26641;&#65292;&#22240;&#27492;&#26222;&#36890;&#30340;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#19978;&#38480;&#20026;1&#32500;Weisfeiler-Leman(1-WL)&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#20174;&#26032;&#31867;&#22411;&#30340;&#23376;&#32467;&#26500;&#20013;&#25552;&#21462;&#30340;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#26469;&#22686;&#24378;GNNs&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#23616;&#37096;&#37051;&#22495;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#36830;&#25509;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31216;&#20026;&#32852;&#21512;&#23376;&#22270;&#30340;&#23376;&#32467;&#26500;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21040;&#19968;&#26465;&#36793;&#30340;1-&#36339;&#37051;&#23621;&#30340;&#23436;&#25972;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#30701;&#36335;&#24452;&#30340;&#23376;&#32467;&#26500;&#25551;&#36848;&#31526;&#65292;&#20855;&#26377;&#19977;&#20010;&#33391;&#22909;&#30340;&#24615;&#36136;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#32852;&#21512;&#23376;&#22270;&#20013;&#30340;&#39640;&#38454;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#27880;&#20837;&#32534;&#30721;&#37051;&#23621;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21363;Union Subgraph&#31070;&#32463;&#32593;&#32476;(UnionSNN)&#65292;&#23427;&#34987;&#35777;&#26126;&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#26041;&#38754;&#27604;1-WL&#20005;&#26684;&#26356;&#24378;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;UnionSNN&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are widely used for graph representation learning in many application domains. The expressiveness of vanilla GNNs is upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on rooted subtrees through iterative message passing. In this paper, we empower GNNs by injecting neighbor-connectivity information extracted from a new type of substructure. We first investigate different kinds of connectivities existing in a local neighborhood and identify a substructure called union subgraph, which is able to capture the complete picture of the 1-hop neighborhood of an edge. We then design a shortest-path-based substructure descriptor that possesses three nice properties and can effectively encode the high-order connectivities in union subgraphs. By infusing the encoded neighbor connectivities, we propose a novel model, namely Union Subgraph Neural Network (UnionSNN), which is proven to be strictly more powerful than 1-WL in distinguishing non-isom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14196</link><description>&lt;p&gt;
ZeroSCROLLS&#65306;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14196
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ZeroSCROLLS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#20165;&#21253;&#21547;&#27979;&#35797;&#38598;&#32780;&#27809;&#26377;&#35757;&#32451;&#25110;&#24320;&#21457;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;SCROLLS&#22522;&#20934;&#27979;&#35797;&#20013;&#36866;&#24212;&#20102;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20449;&#24687;&#34701;&#21512;&#20219;&#21153;&#65292;&#20363;&#22914;&#32858;&#21512;&#27491;&#38754;&#35780;&#20215;&#30340;&#30334;&#20998;&#27604;&#12290;&#20351;&#29992;ZeroSCROLLS&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;Claude&#20248;&#20110;ChatGPT&#65292;&#24182;&#19988;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;ZeroSCROLLS&#30340;&#22810;&#20010;&#24320;&#25918;&#25361;&#25112;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#20219;&#21153;&#65289;&#65292;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#22240;&#20026;&#27169;&#22411;&#24456;&#38590;&#36890;&#36807;&#26420;&#32032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#30001;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36824;&#22312;&#19981;&#26029;&#26356;&#26032;&#65292;&#25105;&#20204;&#36992;&#35831;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#26102;&#30340;ZeroSCROLLS&#25490;&#34892;&#27036;&#19978;&#35780;&#20272;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31232;&#30095;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26159;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2305.10391</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#30340;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31232;&#30095;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26159;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29305;&#24449;&#35013;&#39280;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#22312;&#31232;&#30095;&#35774;&#32622;&#19979;&#65292;&#21363;&#33410;&#28857;&#30340;&#39044;&#26399;&#24230;&#25968;&#20026;&#33410;&#28857;&#25968;&#30340;O(1)&#26102;&#12290;&#36825;&#26679;&#30340;&#22270;&#36890;&#24120;&#34987;&#31216;&#20026;&#26412;&#22320;&#26641;&#29366;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21483;&#20570;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#24182;&#26681;&#25454;&#36825;&#20010;&#26631;&#20934;&#35745;&#31639;&#20102;&#20855;&#26377;&#20219;&#24847;&#33410;&#28857;&#29305;&#24449;&#21644;&#36793;&#36830;&#25509;&#20998;&#24067;&#30340;&#30456;&#24403;&#19968;&#33324;&#30340;&#32479;&#35745;&#25968;&#25454;&#27169;&#22411;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#35813;&#26368;&#20248;&#20998;&#31867;&#22120;&#21487;&#20197;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23454;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#20102;&#35813;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;&#19968;&#20010;&#24050;&#32463;&#30740;&#31350;&#20805;&#20998;&#30340;&#32479;&#35745;&#27169;&#22411;&#19978;&#20174;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20302;&#22270;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20339;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#25554;&#20540;&#20110;&#26631;&#20934;MLP&#21644;&#19968;&#31181;&#20856;&#22411;&#30340;c&#26550;&#26500;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02041</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#20302;&#22797;&#26434;&#24230;&#30340;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#23545;&#20989;&#25968;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#21464;&#20307;&#19981;&#21516;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992; carefully chosen &#30340;&#23376;&#31354;&#38388;&#65292;&#20351;&#24471;&#26356;&#26032;&#21487;&#20197;&#20889;&#25104;&#36845;&#20195;&#30340; Cholesky &#22240;&#23376;&#21644;&#19968;&#20010;&#31232;&#30095;&#30697;&#38453;&#30340;&#20056;&#31215;&#24418;&#24335;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26356;&#26032;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#30697;&#38453;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#25351;&#25968;&#21644;&#23494;&#38598;&#30697;&#38453;&#20056;&#27861;&#65292;&#36825;&#20123;&#25805;&#20316;&#36890;&#24120;&#22312;&#20960;&#20046;&#25152;&#26377;&#20854;&#20182; Riemannian &#20248;&#21270;&#31639;&#27861;&#20013;&#37117;&#26159;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#30340;&#28151;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#24230;&#37327;&#20219;&#21153;&#31995;&#32479;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;$O(\ell^2)$&#30340;&#31454;&#20105;&#27604;&#65292;&#21487;&#20197;&#20351;&#31639;&#27861;&#36319;&#38543;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#65292;&#23545;&#38480;&#21046;&#20999;&#25442;&#27425;&#25968;&#30340;&#24773;&#20917;&#21487;&#20197;&#33719;&#24471;$(1+\epsilon)$-&#31454;&#20105;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01781</link><description>&lt;p&gt;
&#22312;&#32447;&#25351;&#26631;&#31639;&#27861;&#30340;&#28151;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mixing predictions for online metric algorithms. (arXiv:2304.01781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#30340;&#28151;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#24230;&#37327;&#20219;&#21153;&#31995;&#32479;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;$O(\ell^2)$&#30340;&#31454;&#20105;&#27604;&#65292;&#21487;&#20197;&#20351;&#31639;&#27861;&#36319;&#38543;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#65292;&#23545;&#38480;&#21046;&#20999;&#25442;&#27425;&#25968;&#30340;&#24773;&#20917;&#21487;&#20197;&#33719;&#24471;$(1+\epsilon)$-&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;-&#22686;&#24378;&#30340;&#22312;&#32447;&#31639;&#27861;&#20013;&#65292;&#20027;&#35201;&#25216;&#26415;&#20043;&#19968;&#26159;&#32452;&#21512;&#22810;&#20010;&#31639;&#27861;&#25110;&#39044;&#27979;&#22120;&#12290;&#30001;&#20110;&#27599;&#20010;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#22240;&#27492;&#24076;&#26395;&#20351;&#29992;&#21160;&#24577;&#32452;&#21512;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#26102;&#38388;&#36319;&#38543;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21333;&#20010;&#26368;&#20339;&#39044;&#27979;&#22120;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#32452;&#21512;&#39044;&#27979;&#24182;&#38024;&#23545;&#24191;&#27867;&#30340;&#22312;&#32447;&#38382;&#39064;&#31867;&#21035;&#65288;&#21363;&#24230;&#37327;&#20219;&#21153;&#31995;&#32479;&#65289;&#19982;&#36825;&#26679;&#30340;&#21160;&#24577;&#32452;&#21512;&#36827;&#34892;&#31454;&#20105;&#12290;&#38024;&#23545;&#26368;&#20339;&#65288;&#20107;&#21518;&#65289;&#26080;&#32422;&#26463;&#32452;&#21512;&#30340;$\ell$&#20010;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;$O(\ell^2)$&#30340;&#31454;&#20105;&#27604;&#65292;&#24182;&#35777;&#26126;&#36825;&#26159;&#26368;&#20248;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20010;&#32422;&#26463;&#22312;&#19981;&#21516;&#39044;&#27979;&#22120;&#20043;&#38388;&#20999;&#25442;&#27425;&#25968;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;$(1+\epsilon)$-&#31454;&#20105;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#31867;&#20284;&#20110;&#36172;&#21338;&#26426;&#24335;&#30340;&#35775;&#38382;&#39044;&#27979;&#22120;&#30340;&#26041;&#24335;&#65292;&#27599;&#27425;&#26597;&#35810;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#20854;&#20013;&#19968;&#26465;&#19979;&#30028;&#30340;&#19968;&#20010;&#24847;&#22806;&#25512;&#35770;&#26159;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major technique in learning-augmented online algorithms is combining multiple algorithms or predictors. Since the performance of each predictor may vary over time, it is desirable to use not the single best predictor as a benchmark, but rather a dynamic combination which follows different predictors at different times. We design algorithms that combine predictions and are competitive against such dynamic combinations for a wide class of online problems, namely, metrical task systems. Against the best (in hindsight) unconstrained combination of $\ell$ predictors, we obtain a competitive ratio of $O(\ell^2)$, and show that this is best possible. However, for a benchmark with slightly constrained number of switches between different predictors, we can get a $(1+\epsilon)$-competitive algorithm. Moreover, our algorithms can be adapted to access predictors in a bandit-like fashion, querying only one predictor at a time. An unexpected implication of one of our lower bounds is a new structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01168</link><description>&lt;p&gt;
DeepAccident&#65306;V2X&#33258;&#21160;&#39550;&#39542;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#39318;&#35201;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25903;&#25345;&#33258;&#21160;&#39550;&#39542;&#30340;&#30452;&#25509;&#21644;&#21487;&#35299;&#37322;&#30340;&#23433;&#20840;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DeepAccident&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#29616;&#23454;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32463;&#24120;&#22312;&#29616;&#23454;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#21508;&#31181;&#20107;&#25925;&#22330;&#26223;&#12290;DeepAccident &#25968;&#25454;&#38598;&#21253;&#21547; 57k &#20010;&#24102;&#27880;&#37322;&#24103;&#21644; 285k &#20010;&#24102;&#27880;&#37322;&#30340;&#26679;&#26412;&#65292;&#36825;&#20960;&#20046;&#26159;&#22823;&#35268;&#27169; nuScenes &#25968;&#25454;&#38598;&#30340; 7 &#20493;&#65292;&#20854;&#26679;&#26412;&#25968;&#20026; 40k&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#31181;&#22330;&#26223;&#65292;&#25105;&#20204;&#35774;&#32622;&#20102;&#22235;&#36742;&#36710;&#21644;&#19968;&#20010;&#22522;&#30784;&#35774;&#26045;&#26469;&#35760;&#24405;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#20107;&#25925;&#22330;&#26223;&#25552;&#20379;&#20102;&#22810;&#31181;&#35270;&#35282;&#65292;&#24182;&#20351; V2X&#65288;&#36710;&#36742;&#23545;&#19968;&#20999;&#65289;&#24863;&#30693;&#21644;&#39044;&#27979;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset contains 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, based on the proposed dataset, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#22270;&#29983;&#25104;&#27169;&#22411;FairGen&#65292;&#36890;&#36807;&#26631;&#31614;&#25439;&#22833;&#21644;&#20844;&#24179;&#25439;&#22833;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#38597;&#21487;&#27604;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#36924;&#30495;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17743</link><description>&lt;p&gt;
FairGen: &#36808;&#21521;&#20844;&#24179;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FairGen: Towards Fair Graph Generation. (arXiv:2303.17743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#22270;&#29983;&#25104;&#27169;&#22411;FairGen&#65292;&#36890;&#36807;&#26631;&#31614;&#25439;&#22833;&#21644;&#20844;&#24179;&#25439;&#22833;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#38597;&#21487;&#27604;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#24418;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#35745;&#31639;&#26426;&#32593;&#32476;&#65292;&#20174;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21040;&#22312;&#32447;&#20132;&#26131;&#32593;&#32476;&#12290;&#23613;&#31649;&#36825;&#20123;&#24037;&#20316;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#32477;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#27809;&#26377;&#30417;&#30563;&#24615;&#36136;&#65292;&#36890;&#24120;&#26159;&#22312;&#35757;&#32451;&#26102;&#26368;&#23567;&#21270;&#39044;&#26399;&#30340;&#22270;&#24418;&#37325;&#26500;&#25439;&#22833;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#29983;&#25104;&#30340;&#22270;&#24418;&#20013;&#20986;&#29616;&#34920;&#31034;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;(&#36890;&#24120;&#26159;&#23569;&#25968;&#32676;&#20307;)&#23545;&#30446;&#26631;&#36129;&#29486;&#26356;&#23569;&#65292;&#22240;&#27492;&#20250;&#36973;&#21463;&#31995;&#32479;&#24615;&#26356;&#39640;&#30340;&#35823;&#24046;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#21644;&#29992;&#25143;&#39318;&#36873;&#30340;&#24179;&#31561;&#32422;&#26463;&#65292;&#23558;&#22270;&#29983;&#25104;&#35843;&#25972;&#20026;&#19979;&#28216;&#25366;&#25496;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#25506;&#31350;&#22270;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairGen&#30340;&#20844;&#24179;&#24863;&#30693;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32852;&#21512;&#20102;&#26631;&#31614;&#25439;&#22833;&#21644;&#20844;&#24179;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#38597;&#21487;&#27604;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#24471;&#21040;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been tremendous efforts over the past decades dedicated to the generation of realistic graphs in a variety of domains, ranging from social networks to computer networks, from gene regulatory networks to online transaction networks. Despite the remarkable success, the vast majority of these works are unsupervised in nature and are typically trained to minimize the expected graph reconstruction loss, which would result in the representation disparity issue in the generated graphs, i.e., the protected groups (often minorities) contribute less to the objective and thus suffer from systematically higher errors. In this paper, we aim to tailor graph generation to downstream mining tasks by leveraging label information and user-preferred parity constraint. In particular, we start from the investigation of representation disparity in the context of graph generative models. To mitigate the disparity, we propose a fairness-aware graph generative model named FairGen. Our model jointly 
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#20102;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#65288;FDPs&#65289;&#65292;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#25193;&#23637;&#65292;FDPs&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26500;&#24314;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#26102;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#25152;&#38656;&#21442;&#25968;&#25968;&#37327;&#27604;&#29616;&#26377;&#27169;&#22411;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2303.00800</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00800
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#20102;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#65288;FDPs&#65289;&#65292;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#25193;&#23637;&#65292;FDPs&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26500;&#24314;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#26102;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#25152;&#38656;&#21442;&#25968;&#25968;&#37327;&#27604;&#29616;&#26377;&#27169;&#22411;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#65288;FDPs&#65289;&#65292;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290; FDPs&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#26469;&#25551;&#36848;&#21069;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#24182;&#36827;&#34892;&#22810;&#20010;&#25193;&#23637;&#20197;&#24471;&#20986;&#23454;&#38469;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#20123;&#25193;&#23637;&#21253;&#25324;Girsanov&#23450;&#29702;&#30340;&#26080;&#38480;&#32500;&#29256;&#26412;&#65292;&#20197;&#20415;&#33021;&#22815;&#35745;&#31639;ELBO&#65292;&#20197;&#21450;&#37319;&#26679;&#23450;&#29702;&#30340;&#26080;&#38480;&#32500;&#29256;&#26412;&#65292;&#20197;&#30830;&#20445;&#21487;&#25968;&#20010;&#28857;&#19978;&#30340;&#20989;&#25968;&#35780;&#20272;&#31561;&#20215;&#20110;&#26080;&#38480;&#32500;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;FDPs&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#19987;&#38376;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#36830;&#32493;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#32467;&#26500;&#65292;FDPs&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.
&lt;/p&gt;</description></item><item><title>CrystalBox&#26159;&#19968;&#31181;&#35299;&#37322;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#26410;&#26469;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#24433;&#21709;&#26469;&#29983;&#25104;&#31616;&#26126;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2302.13483</link><description>&lt;p&gt;
CrystalBox&#65306;&#22522;&#20110;&#26410;&#26469;&#30340;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrystalBox: Future-Based Explanations for DRL Network Controllers. (arXiv:2302.13483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13483
&lt;/p&gt;
&lt;p&gt;
CrystalBox&#26159;&#19968;&#31181;&#35299;&#37322;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#26410;&#26469;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#24433;&#21709;&#26469;&#29983;&#25104;&#31616;&#26126;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#19981;&#36275;&#26159;&#38480;&#21046;&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25511;&#21046;&#22120;&#23454;&#38469;&#37319;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32593;&#32476;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#36804;&#20170;&#20351;&#29992;&#24341;&#20154;&#27880;&#30446;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#24120;&#65292;&#36816;&#33829;&#21830;&#26377;&#20852;&#36259;&#20102;&#35299;&#25511;&#21046;&#22120;&#23545;&#26410;&#26469;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32780;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#25429;&#25417;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrystalBox&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#26410;&#26469;&#24433;&#21709;&#26469;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#12290;CrystalBox&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#31616;&#27905;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#22870;&#21169;&#32452;&#20214;&#20316;&#20026;&#35299;&#37322;&#30340;&#22522;&#30784;&#65292;&#36825;&#26159;&#36816;&#33829;&#21830;&#26377;&#24847;&#20041;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#12290;CrystalBox&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of explainability is a key factor limiting the practical adoption of high-performant Deep Reinforcement Learning (DRL) controllers. Explainable RL for networking hitherto used salient input features to interpret a controller's behavior. However, these feature-based solutions do not completely explain the controller's decision-making process. Often, operators are interested in understanding the impact of a controller's actions on performance in the future, which feature-based solutions cannot capture.  In this paper, we present CrystalBox, a framework that explains a controller's behavior in terms of the future impact on key network performance metrics. CrystalBox employs a novel learning-based approach to generate succinct and expressive explanations. We use reward components of the DRL network controller, which are key performance metrics meaningful to operators, as the basis for explanations. CrystalBox is generalizable and can work across both discrete and continuous control en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07317</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#25928;&#29575;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#25152;&#38656;&#30340;&#26631;&#35760;&#31034;&#20363;&#25968;&#37327;&#65292;&#20294;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#20013;&#65292;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#35777;&#24615;&#33021;&#21487;&#33021;&#20250;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;&#20107;&#20808;&#24456;&#38590;&#30693;&#36947;&#21738;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#33391;&#22909;&#25110;&#26368;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;&#12290;&#23545;&#20110;&#20219;&#20309;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;(meta)&#31639;&#27861;TAILOR (Thompson ActIve Learning algORithm selection)&#36845;&#20195;&#22320;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19968;&#32452;&#20505;&#36873;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#12290;TAILOR&#20351;&#29992;&#26088;&#22312;&#25910;&#38598;&#31867;&#24179;&#34913;&#31034;&#20363;&#30340;&#26032;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;TAILOR&#22312;&#23454;&#29616;&#19982;&#20505;&#36873;&#31639;&#27861;&#20013;&#26368;&#20339;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#65288;DirectUQ&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30452;&#25509;&#36755;&#20986;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#39118;&#38505;&#36793;&#30028;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2302.02420</link><description>&lt;p&gt;
&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Uncertainty Quantification. (arXiv:2302.02420v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#65288;DirectUQ&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30452;&#25509;&#36755;&#20986;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#39118;&#38505;&#36793;&#30028;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26131;&#20110;&#35757;&#32451;&#65292;&#20294;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65307;&#32780;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#37327;&#21270;&#65292;&#20294;&#20248;&#21270;&#23427;&#20204;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#8220;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#8221;&#65288;DirectUQ&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#36755;&#20986;&#26368;&#21518;&#19968;&#23618;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#12290;DirectUQ&#21487;&#20197;&#23548;&#20986;&#20026;&#19968;&#20010;&#26367;&#20195;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#22240;&#27492;&#20174;&#33853;&#21333;&#21464;&#20998;&#25512;&#29702;&#20013;&#33719;&#30410;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;&#38750;&#27010;&#29575;&#27169;&#22411;&#19968;&#26679;&#65292;DirectUQ&#20855;&#26377;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;Rademacher&#22797;&#26434;&#24615;&#20026;&#27169;&#22411;&#25552;&#20379;&#39118;&#38505;&#36793;&#30028;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DirectUQ&#21644;DirectUQ&#38598;&#25104;&#25552;&#20379;&#20102;&#26102;&#38388;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#30340;&#33391;&#22909;&#24179;&#34913;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional neural networks are simple to train but they produce overconfident predictions, while Bayesian neural networks provide good uncertainty quantification but optimizing them is time consuming. This paper introduces a new approach, direct uncertainty quantification (DirectUQ), that combines their advantages where the neural network directly outputs the mean and variance of the last layer. DirectUQ can be derived as an alternative variational lower bound, and hence benefits from collapsed variational inference that provides improved regularizers. On the other hand, like non-probabilistic models, DirectUQ enjoys simple training and one can use Rademacher complexity to provide risk bounds for the model. Experiments show that DirectUQ and ensembles of DirectUQ provide a good tradeoff in terms of run time and uncertainty quantification, especially for out of distribution data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.10343</link><description>&lt;p&gt;
ClimaX:&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10343
&lt;/p&gt;
&lt;p&gt;
ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#22825;&#27668;&#21644;&#27668;&#20505;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#20540;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#36825;&#26679;&#30340;&#25968;&#20540;&#27169;&#22411;&#22312;&#27169;&#25311;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#22823;&#27668;&#29616;&#35937;&#26102;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#33021;&#26144;&#23556;&#26469;&#30452;&#25509;&#35299;&#20915;&#19979;&#28216;&#39044;&#27979;&#25110;&#25237;&#23556;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#26159;&#20351;&#29992;&#20026;&#29305;&#23450;&#26102;&#31354;&#20219;&#21153;&#31574;&#21010;&#21644;&#21516;&#36136;&#21270;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#25968;&#20540;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#28436;&#31034;&#20102;ClimaX&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;</title><link>http://arxiv.org/abs/2212.01071</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#39578;&#25200;&#34892;&#20026;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#36825;&#23548;&#33268;&#20102;&#34394;&#20551;&#26816;&#27979;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#25968;&#25454;&#30340;&#22270;&#24418;&#29305;&#24615;&#20197;&#21450;&#22823;&#37327;&#33410;&#28857;&#23548;&#33268;&#20102;&#35768;&#22810;&#38556;&#30861;&#65292;&#21253;&#25324;&#30697;&#38453;&#20013;&#22823;&#37327;&#26080;&#20851;&#29305;&#24449;&#30340;&#39640;&#31163;&#25955;&#24230;&#21644;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21363;SGAN&#12290;&#26412;&#25991;&#23558;&#23569;&#37327;&#26631;&#31614;&#24212;&#29992;&#20110;SGAN&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#34394;&#20551;&#36134;&#25143;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08012</link><description>&lt;p&gt;
&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#30340;&#32463;&#39564;&#21487;&#20197;&#20998;&#35299;&#20026;&#22522;&#26412;&#30340;&#21407;&#23376;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#21442;&#19982;&#26032;&#39062;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#23398;&#20064;&#20197;&#32452;&#21512;&#26041;&#24335;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#30340;&#34892;&#20026;&#31216;&#20026;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#65288;CLBs&#65289;&#12290;&#23398;&#20064;CLBs&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#65288;BP&#65289;&#12290;&#23613;&#31649;&#36825;&#26159;&#20154;&#31867;&#36731;&#26494;&#23436;&#25104;&#30340;&#26234;&#33021;&#22766;&#20030;&#65292;&#20294;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#20195;&#29702;&#21830;&#36890;&#36807;&#35299;&#20915;BP&#30340;&#39046;&#22495;&#26080;&#20851;&#29256;&#26412;&#26469;&#23637;&#31034;CLBs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21463;&#21040;&#25351;&#20195;&#28216;&#25103;&#30340;&#35821;&#35328;&#28044;&#29616;&#21644;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#25193;&#23637;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20197;&#21450;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#24213;&#23618;&#20132;&#20114;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.06950</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#21644;&#25552;&#21319;&#26041;&#27861;&#25311;&#21512;&#20302;&#38454;&#20989;&#25968;ANOVA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20197;&#21450;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#24213;&#23618;&#20132;&#20114;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#38454;&#20989;&#25968;ANOVA&#27169;&#22411;&#24050;&#32463;&#34987;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37325;&#26032;&#21457;&#29616;&#65292;&#24182;&#31216;&#20043;&#20026;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#31867;&#20284;&#20110;EBM&#65292;&#20294;&#20855;&#26377;&#19968;&#20123;&#36235;&#21521;&#26356;&#22909;&#24615;&#33021;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26641;&#65292;&#24182;&#34701;&#20837;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#24213;&#23618;&#20132;&#20114;&#30340;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36845;&#20195;&#35757;&#32451;&#26041;&#27861;&#25910;&#25947;&#20110;&#20855;&#26377;&#26356;&#22909;&#39044;&#27979;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#24182;&#30830;&#20445;&#30456;&#20114;&#20316;&#29992;&#22312;&#20998;&#23618;&#24847;&#20041;&#19978;&#27491;&#20132;&#20110;&#20027;&#25928;&#24212;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#24191;&#27867;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#23454;&#29616;&#24555;&#36895;&#39640;&#25928;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#12289;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#12289;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.12841</link><description>&lt;p&gt;
&#29992;&#20154;&#36896;&#40644;&#27833;&#20174;&#21518;&#39564;&#26679;&#26412;&#20013;&#21435;&#38500;&#33026;&#32938;
&lt;/p&gt;
&lt;p&gt;
Removing the fat from your posterior samples with margarine. (arXiv:2205.12841v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#12289;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#12289;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#23431;&#23449;&#23398;&#39046;&#22495;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#65292;&#21253;&#25324;&#24341;&#21147;&#27874;&#30740;&#31350;&#12289;&#23431;&#23449;&#24494;&#27874;&#32972;&#26223;&#21644;&#23431;&#23449;&#40654;&#26126;&#26102;&#26399;&#30340;21&#21400;&#31859;&#20449;&#21495;&#31561;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#27169;&#22411;&#19982;&#25551;&#36848;&#20851;&#38190;&#23431;&#23449;&#23398;&#21644;&#22825;&#20307;&#29289;&#29702;&#20449;&#21495;&#20197;&#21450;&#21508;&#31181;&#27745;&#26579;&#20449;&#21495;&#21644;&#20202;&#22120;&#25928;&#24212;&#30340;'&#24178;&#25200;&#21442;&#25968;'&#25311;&#21512;&#21040;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36793;&#38469;&#25110;&#8220;&#26080;&#24178;&#25200;&#8221;&#30340;&#21518;&#39564;&#20998;&#24067;&#21450;&#20854;&#30456;&#20851;&#30340;&#20284;&#28982;&#20989;&#25968;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#20197;&#21069;&#38590;&#20197;&#22788;&#29702;&#30340;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#21644;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#65292;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#29609;&#20855;&#20363;&#23376;&#21644;&#23454;&#38469;&#26696;&#20363;&#20998;&#21035;&#23637;&#31034;&#20102;&#27599;&#20010;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian analysis has become an indispensable tool across many different cosmological fields including the study of gravitational waves, the Cosmic Microwave Background and the 21-cm signal from the Cosmic Dawn among other phenomena. The method provides a way to fit complex models to data describing key cosmological and astrophysical signals and a whole host of contaminating signals and instrumental effects modelled with 'nuisance parameters'. In this paper, we summarise a method that uses Masked Autoregressive Flows and Kernel Density Estimators to learn marginal posterior densities corresponding to core science parameters. We find that the marginal or 'nuisance-free' posteriors and the associated likelihoods have an abundance of applications including; the calculation of previously intractable marginal Kullback-Leibler divergences and marginal Bayesian Model Dimensionalities, likelihood emulation and prior emulation. We demonstrate each application using toy examples, examples from t
&lt;/p&gt;</description></item><item><title>AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.</title><link>http://arxiv.org/abs/2205.12787</link><description>&lt;p&gt;
&#20844;&#27491;&#28216;&#25103;&#65306;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12787
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;AlphaZero&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26827;&#30424;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20844;&#27491;&#28216;&#25103;&#20013;&#21364;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20123;&#28216;&#25103;&#20013;&#29609;&#23478;&#20849;&#20139;&#26827;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#28216;&#25103;&#20363;&#23376;&#65292;&#21363;&#23567;&#23401;&#20204;&#29609;&#30340;&#23612;&#22982;&#28216;&#25103;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#20844;&#27491;&#28216;&#25103;&#65292;&#36825;&#20123;&#28216;&#25103;&#20284;&#20046;&#25104;&#20026;AlphaZero&#21644;&#31867;&#20284;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32458;&#33050;&#30707;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;AlphaZero-style&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#21644;&#25932;&#23545;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26174;&#31034;&#20102;&#22312;&#25152;&#26377;&#21512;&#27861;&#29366;&#24577;&#19979;&#23398;&#20064;&#25484;&#25569;&#36825;&#20123;&#28216;&#25103;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#23612;&#22982;&#28216;&#25103;&#22312;&#23567;&#22411;&#26827;&#30424;&#19978;&#21487;&#20197;&#23398;&#20064;&#65292;&#20294;&#24403;&#26827;&#30424;&#23610;&#23544;&#22686;&#22823;&#26102;&#65292;AlphaZero-style&#31639;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#30452;&#35266;&#19978;&#65292;&#23612;&#22982;&#31561;&#20844;&#27491;&#28216;&#25103;&#19982;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#20826;&#27966;&#28216;&#25103;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#65292;&#22914;&#26524;&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#24494;&#23567;&#30340;&#22122;&#38899;&#65288;&#20363;&#22914;&#65292;&#26827;&#30424;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35206;&#30422;&#65289;&#65292;&#23545;&#20110;&#20844;&#27491;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65288;DeepFS&#65289;&#65292;&#21487;&#20197;&#20811;&#26381;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#24182;&#23545;&#36229;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;</title><link>http://arxiv.org/abs/2204.01682</link><description>&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#39640;&#32500;&#25968;&#25454;&#30340;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks. (arXiv:2204.01682v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65288;DeepFS&#65289;&#65292;&#21487;&#20197;&#20811;&#26381;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#24182;&#23545;&#36229;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome the challenges of high-dimensional, low-sample-size data and identify significant features with high precision for ultra high-dimensional, low-sample-size data.
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32479;&#35745;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#22914;&#36807;&#25311;&#21512;&#12289;&#32500;&#25968;&#28798;&#38590;&#12289;&#35745;&#31639;&#19981;&#21487;&#34892;&#21644;&#24378;&#27169;&#22411;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65288;DeepFS&#65289;&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23545;&#36229;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#22522;&#20110;Deb&#21644;Sen&#65288;2021&#65289;&#26368;&#36817;&#24320;&#21457;&#30340;&#22810;&#20803;&#31209;&#36317;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#24449;&#31579;&#36873;&#30340;&#20248;&#28857;&#65292;&#38500;&#20102;&#22788;&#29702;&#20855;&#26377;&#23569;&#37327;&#26679;&#26412;&#30340;&#36229;&#39640;&#32500;&#25968;&#25454;&#30340;&#33021;&#21147;&#22806;&#65292;&#36824;&#20855;&#26377;&#20197;&#19979;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65306;&#65288;1&#65289;&#23427;&#26159;&#27169;&#22411;&#33258;&#30001;&#21644;&#20998;&#24067;&#33258;&#30001;&#30340;&#65307;&#65288;2&#65289;&#23427;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
The applications of traditional statistical feature selection methods to high-dimension, low sample-size data often struggle and encounter challenging problems, such as overfitting, curse of dimensionality, computational infeasibility, and strong model assumption. In this paper, we propose a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome these problems and identify significant features with high precision for ultra high-dimensional, low-sample-size data. This approach first extracts a low-dimensional representation of input data and then applies feature screening based on multivariate rank distance correlation recently developed by Deb and Sen (2021). This approach combines the strengths of both deep neural networks and feature screening, and thereby has the following appealing features in addition to its ability of handling ultra high-dimensional data with small number of samples: (1) it is model free and distribution free; (2) it can be
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861; FedGCN&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451; GCN &#27169;&#22411;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#23454;&#29616;&#25910;&#25947;&#24555;&#65292;&#36890;&#20449;&#37327;&#23567;&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2201.12433</link><description>&lt;p&gt;
FedGCN&#65306;&#32852;&#37030;&#35757;&#32451;&#20013;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#25910;&#25947;&#19982;&#36890;&#20449;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12433
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861; FedGCN&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451; GCN &#27169;&#22411;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#23454;&#29616;&#25910;&#25947;&#24555;&#65292;&#36890;&#20449;&#37327;&#23567;&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#20998;&#24067;&#20110;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#22240;&#20854;&#22270;&#30340;&#35268;&#27169;&#21644;&#25968;&#25454;&#20445;&#30041;&#35268;&#23450;&#30340;&#21407;&#22240;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36830;&#25509;&#22270;&#33410;&#28857;&#30340;&#36328;&#23458;&#25143;&#31471;&#36793;&#32536;&#65292;&#21333;&#20010;&#36830;&#25509;&#22270;&#19981;&#33021;&#34987;&#20998;&#21035;&#20998;&#38548;&#21040;&#22810;&#20010;&#23458;&#25143;&#31471;&#12290;&#22240;&#27492;&#65292;&#22312;&#21333;&#20010;&#22270;&#19978;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#22411;&#20250;&#23548;&#33268;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#24040;&#22823;&#25110;&#35757;&#32451;&#20013;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FedGCN&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;GCN&#27169;&#22411;&#65292;&#32780;&#19988;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#32780;&#19988;&#36890;&#20449;&#37327;&#36739;&#23567;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#22312;&#27599;&#20010;&#35757;&#32451;&#36718;&#27425;&#20013;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;FedGCN&#23458;&#25143;&#31471;&#20165;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#27493;&#39588;&#20013;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#26469;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedGCN&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#36890;&#20449;&#37327;&#26126;&#26174;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for training models on graphs distributed across multiple clients have recently grown in popularity, due to the size of these graphs as well as regulations on keeping data where it is generated. However, a single connected graph cannot be disjointly partitioned onto multiple clients due to the cross-client edges connecting graph nodes. Thus, distributed methods for training a model on a single graph incur either significant communication overhead between clients or a loss of available information to the training. We introduce the Federated Graph Convolutional Network (FedGCN) algorithm, which uses federated learning to train GCN models for semi-supervised node classification with fast convergence and little communication. Compared to prior methods that require communication among clients at each training round, FedGCN clients only communicate with the central server in one pre-training step, greatly reducing communication costs and allowing the use of homomorphic encryption to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38745;&#24687;&#29366;&#24577;&#19979;&#20154;&#31867;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#32500;&#24230;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2201.00087</link><description>&lt;p&gt;
&#38745;&#24687;&#26102;&#20154;&#31867;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#25345;&#32493;&#21516;&#35843;&#29366;&#24577;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest. (arXiv:2201.00087v3 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38745;&#24687;&#29366;&#24577;&#19979;&#20154;&#31867;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#32500;&#24230;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21160;&#24577;&#21464;&#21270;&#30340;&#20154;&#31867;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#36827;&#34892;&#24809;&#32602;&#65292;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#25299;&#25169;&#19978;&#19981;&#21516;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#32593;&#32476;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20272;&#35745;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#30830;&#23450;&#21160;&#24577;&#21464;&#21270;&#30340;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#29983;&#23376;&#30740;&#31350;&#35774;&#35745;&#25506;&#35752;&#20102;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#26159;&#21542;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new data driven topological data analysis (TDA) approach for estimating state spaces in dynamically changing human functional brain networks of human. Our approach penalizes the topological distance between networks and clusters dynamically changing brain networks into topologically distinct states. Our method takes into account the temporal dimension of the data through the Wasserstein distance between networks. Our method is shown to outperform the widely used k-means clustering often used in estimating the state space in brain networks. The method is applied to more accurately determine the state spaces of dynamically changing functional brain networks. Subsequently, we address the question of whether the overall topology of brain networks is a heritable feature using the twin study design.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#26469;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#24182;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.05216</link><description>&lt;p&gt;
&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#30340;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#26469;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#24182;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#39640;&#38454;&#32479;&#35745;&#20449;&#24687;&#65292;&#24418;&#25104;&#24352;&#37327;&#25551;&#36848;&#31526;&#12290;&#24352;&#37327;&#25551;&#36848;&#31526;&#35201;&#27714;&#20855;&#22791;&#40065;&#26834;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20197;&#24212;&#23545;&#32858;&#21512;&#21521;&#37327;&#25968;&#37327;&#36739;&#23569;&#21644;&#29190;&#21457;&#29616;&#35937;&#65292;&#21363;&#26576;&#20123;&#29305;&#24449;&#20986;&#29616;&#30340;&#39057;&#29575;&#39640;&#20110;&#25110;&#20302;&#20110;&#32479;&#35745;&#39044;&#26399;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23558;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#19978;&#30340;&#28909;&#25193;&#25955;&#36807;&#31243;&#19982;&#21327;&#26041;&#24046;/&#33258;&#30456;&#20851;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#20854;&#36870;&#24418;&#25104;&#20102;&#19968;&#20010;&#29615;&#29366;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#28909;&#25193;&#25955;&#36807;&#31243;&#19982;EPN&#20855;&#26377;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#21363;&#22686;&#24378;&#25110;&#20943;&#24369;&#29305;&#24449;&#20540;&#35889;&#30340;&#24133;&#24230;&#65292;&#20174;&#32780;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#39640;&#38454;&#24352;&#37327;&#37197;&#22791;&#20102;EPN&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#39640;&#38454;&#20986;&#29616;&#30340;&#35889;&#26816;&#27979;&#22120;&#65292;&#20197;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#20010;&#30001;d&#32500;&#29305;&#24449;&#25551;&#36848;&#31526;&#26500;&#24314;&#30340;&#38454;&#25968;&#20026;r&#30340;&#24352;&#37327;&#65292;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#32473;&#20986;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#39640;&#38454;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim at capturing high-order statistics of feature vectors formed by a neural network, and propose end-to-end second- and higher-order pooling to form a tensor descriptor. Tensor descriptors require a robust similarity measure due to low numbers of aggregated vectors and the burstiness phenomenon, when a given feature appears more/less frequently than statistically expected. The Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix, whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum thus preventing the burstiness. We equip higher-order tensors with EPN which acts as a spectral detector of higher-order occurrences to prevent burstiness. We also prove that for a tensor of order r built from d dimensional feature descriptors, such a detector gives the likelihood if at least one high
&lt;/p&gt;</description></item></channel></rss>