<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17010</link><description>&lt;p&gt;
Calib3D&#65306;&#26657;&#20934;&#27169;&#22411;&#20559;&#22909;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;3D&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17010
&lt;/p&gt;
&lt;p&gt;
Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#38656;&#35201;&#30340;&#19981;&#20165;&#20165;&#26159;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36824;&#38656;&#35201;&#26469;&#33258;3D&#24863;&#30693;&#27169;&#22411;&#30340;&#33258;&#20449;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25512;&#20986;&#20102;Calib3D&#65292;&#36825;&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#22522;&#20934;&#21644;&#23457;&#26597;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;28&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;10&#20010;&#19981;&#21516;&#30340;3D&#25968;&#25454;&#38598;&#19978;&#65292;&#25581;&#31034;&#20102;&#33021;&#22815;&#22788;&#29702;3D&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#35823;&#24046;&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#35265;&#22320;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24230;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745; -- &#36825;&#20010;&#20851;&#38190;&#30340;&#32570;&#38519;&#20005;&#37325;&#25439;&#23475;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#25935;&#24863;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#32593;&#32476;&#23481;&#37327;&#12289;LiDAR&#34920;&#31034;&#12289;&#20809;&#26629;&#20998;&#36776;&#29575;&#21644;3D&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#30452;&#25509;&#23558;&#36825;&#20123;&#26041;&#38754;&#19982;&#27169;&#22411;&#26657;&#20934;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17010v1 Announce Type: cross  Abstract: Safety-critical 3D scene understanding tasks necessitate not only accurate but also confident predictions from 3D perception models. This study introduces Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D scene understanding models from an uncertainty estimation viewpoint. We comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D datasets, uncovering insightful phenomena that cope with both the aleatoric and epistemic uncertainties in 3D scene understanding. We discover that despite achieving impressive levels of accuracy, existing models frequently fail to provide reliable uncertainty estimates -- a pitfall that critically undermines their applicability in safety-sensitive contexts. Through extensive analysis of key factors such as network capacity, LiDAR representations, rasterization resolutions, and 3D data augmentation techniques, we correlate these aspects directly with the model cal
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16995</link><description>&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#65306;&#36890;&#36807;&#27010;&#29575;&#27969;&#25512;&#21160;&#25193;&#25955;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16995
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#25511;&#21046;&#21477;&#23376;&#23646;&#24615;&#65288;&#20363;&#22914;&#24773;&#24863;&#65289;&#21644;&#32467;&#26500;&#65288;&#20363;&#22914;&#21477;&#27861;&#32467;&#26500;&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#19968;&#20010;&#25512;&#21160;&#39640;&#36136;&#37327;&#26679;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36845;&#20195;&#21435;&#22122;&#25968;&#21315;&#27493;&#12290;&#23613;&#31649;&#26377;&#30410;&#65292;&#20294;&#20174;&#22122;&#22768;&#24320;&#22987;&#30340;&#22797;&#26434;&#24615;&#21644;&#23398;&#20064;&#27493;&#39588;&#38480;&#21046;&#20102;&#20854;&#22312;&#35768;&#22810;NLP&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Language Rectified Flow&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#37325;&#26500;&#12290;&#35821;&#35328;&#26657;&#27491;&#27969;&#23398;&#20064;&#65288;&#31070;&#32463;&#65289;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#21644;&#22495;&#36716;&#31227;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20174;&#28304;&#20998;&#24067;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#26657;&#27491;&#27969;&#20135;&#29983;&#24555;&#36895;&#20223;&#30495;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#26377;&#30028;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#27880;&#24847;&#21147;&#23618;&#28151;&#21512;&#19981;&#21516;&#20027;&#20307;&#35270;&#35273;&#29305;&#24449;&#23548;&#33268;&#30340;&#35821;&#20041;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16990</link><description>&lt;p&gt;
&#20570;&#33258;&#24049;&#65306;&#22810;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#26377;&#30028;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16990
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#26377;&#30028;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#27880;&#24847;&#21147;&#23618;&#28151;&#21512;&#19981;&#21516;&#20027;&#20307;&#35270;&#35273;&#29305;&#24449;&#23548;&#33268;&#30340;&#35821;&#20041;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16990v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21069;&#25152;&#26410;&#26377;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#38590;&#20197;&#24544;&#23454;&#22320;&#25429;&#25417;&#21253;&#21547;&#22810;&#20010;&#20027;&#39064;&#30340;&#22797;&#26434;&#36755;&#20837;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#25193;&#23637;&#24050;&#34987;&#24341;&#20837;&#20197;&#25552;&#39640;&#29992;&#25143;&#25511;&#21046;&#65292;&#26088;&#22312;&#23450;&#20301;&#30001;&#29305;&#23450;&#20196;&#29260;&#34920;&#31034;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#35821;&#20041;&#19981;&#20934;&#30830;&#30340;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22810;&#20010;&#22312;&#35821;&#20041;&#19978;&#25110;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#20027;&#39064;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#38480;&#21046;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#25581;&#31034;&#20102;&#36825;&#20010;&#20027;&#35201;&#38382;&#39064;&#36215;&#28304;&#20110;&#21435;&#22122;&#36807;&#31243;&#20013;&#20027;&#39064;&#20043;&#38388;&#30340;&#26080;&#24847;&#20041;&#35821;&#20041;&#27844;&#28431;&#12290;&#36825;&#31181;&#27844;&#28431;&#24402;&#22240;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#23618;&#65292;&#36825;&#20123;&#23618;&#20542;&#21521;&#20110;&#28151;&#21512;&#19981;&#21516;&#20027;&#20307;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#30028;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16990v1 Announce Type: cross  Abstract: Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.16986</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#21160;&#24577;&#30456;&#23545;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relative Representations for Goal-Oriented Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;6G&#26080;&#32447;&#32593;&#32476;&#20013;&#65292;&#36890;&#20449;&#30340;&#35821;&#20041;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#23558;&#21457;&#25381;&#22522;&#30784;&#20316;&#29992;&#65292;&#23558;&#21547;&#20041;&#21644;&#30456;&#20851;&#24615;&#32435;&#20837;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#36923;&#36753;&#25110;&#20869;&#37096;&#34920;&#31034;&#26102;&#65292;&#20250;&#20986;&#29616;&#38556;&#30861;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#21487;&#33021;&#21361;&#21450;&#29702;&#35299;&#12290;&#22312;&#28508;&#22312;&#31354;&#38388;&#36890;&#20449;&#20013;&#65292;&#36825;&#19968;&#25361;&#25112;&#34920;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#26102;&#39640;&#32500;&#34920;&#31034;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26469;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20248;&#21270;&#31574;&#30053;&#65292;&#20197;&#35843;&#25972;&#30456;&#23545;&#34920;&#31034;&#12289;&#36890;&#20449;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32531;&#35299;&#20013;&#36215;&#20316;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16986v1 Announce Type: cross  Abstract: In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#28145;&#24230;&#23637;&#24320;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#33258;&#32534;&#30721;&#22120;&#20174;&#32473;&#23450;&#27979;&#37327;&#20013;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.16974</link><description>&lt;p&gt;
&#33258;&#25105;STORM&#65306;&#29992;&#20110;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#30340;&#28145;&#24230;&#23637;&#24320;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16974
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#28145;&#24230;&#23637;&#24320;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#33258;&#32534;&#30721;&#22120;&#20174;&#32473;&#23450;&#27979;&#37327;&#20013;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33639;&#20809;&#20998;&#23376;&#21019;&#24314;&#19968;&#38271;&#24207;&#21015;&#20302;&#23494;&#24230;&#12289;&#34893;&#23556;&#38480;&#21046;&#22270;&#20687;&#65292;&#33021;&#23454;&#29616;&#39640;&#31934;&#24230;&#20998;&#23376;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#28459;&#38271;&#30340;&#25104;&#20687;&#26102;&#38388;&#65292;&#38480;&#21046;&#20102;&#22312;&#30701;&#26102;&#38388;&#23610;&#24230;&#19978;&#35266;&#23519;&#27963;&#32454;&#32990;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;&#35768;&#22810;&#25216;&#26415;&#34987;&#24320;&#21457;&#29992;&#26469;&#20943;&#23569;&#23450;&#20301;&#25152;&#38656;&#24103;&#25968;&#65292;&#20174;&#32463;&#20856;&#36845;&#20195;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#29305;&#21035;&#22320;&#65292;&#28145;&#24230;&#31639;&#27861;&#23637;&#24320;&#32467;&#21512;&#20102;&#36845;&#20195;&#31232;&#30095;&#24674;&#22797;&#31639;&#27861;&#30340;&#32467;&#26500;&#21644;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#26377;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23637;&#24320;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24207;&#21015;&#29305;&#23450;&#12289;&#22522;&#20110;&#27169;&#22411;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#20165;&#20174;&#32473;&#23450;&#30340;&#27979;&#37327;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#36825;&#31181;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16974v1 Announce Type: cross  Abstract: The use of fluorescent molecules to create long sequences of low-density, diffraction-limited images enables highly-precise molecule localization. However, this methodology requires lengthy imaging times, which limits the ability to view dynamic interactions of live cells on short time scales. Many techniques have been developed to reduce the number of frames needed for localization, from classic iterative optimization to deep neural networks. Particularly, deep algorithm unrolling utilizes both the structure of iterative sparse recovery algorithms and the performance gains of supervised deep learning. However, the robustness of this approach is highly dependant on having sufficient training data. In this paper we introduce deep unrolled self-supervised learning, which alleviates the need for such data by training a sequence-specific, model-based autoencoder that learns only from given measurements. Our proposed method exceeds the perf
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16967</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#23450;&#28857;&#26426;&#22120;&#20154;&#36816;&#21160;&#25805;&#20316;&#30340;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visual Whole-Body Control for Legged Loco-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#37197;&#22791;&#25163;&#33218;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#21363;&#33151;&#24335;&#23450;&#28857;&#25805;&#20316;&#12290;&#23613;&#31649;&#26426;&#22120;&#20154;&#30340;&#33151;&#36890;&#24120;&#29992;&#20110;&#31227;&#21160;&#65292;&#20294;&#36890;&#36807;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#65292;&#21487;&#20197;&#25193;&#22823;&#20854;&#25805;&#20316;&#33021;&#21147;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#20854;&#24037;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#33258;&#20027;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;\ourFull~(\our)&#65292;&#30001;&#19968;&#20010;&#20302;&#32423;&#31574;&#30053;&#21644;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#32452;&#25104;&#12290;&#20302;&#32423;&#31574;&#30053;&#20351;&#29992;&#25152;&#26377;&#33258;&#30001;&#24230;&#26469;&#36319;&#36394;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#20301;&#32622;&#65292;&#39640;&#32423;&#31574;&#30053;&#26681;&#25454;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#20174;Sim&#21040;&#23454;&#29289;&#30340;&#36716;&#31227;&#20197;&#36827;&#34892;&#23454;&#38469;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#65288;&#39640;&#24230;&#12289;&#65289;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#65292;&#30456;&#23545;&#22522;&#32447;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.16952</link><description>&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65306;&#36890;&#36807;&#39044;&#27979;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#26469;&#20248;&#21270;&#25968;&#25454;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21253;&#25324;&#22810;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#23398;&#26415;&#35770;&#25991;&#12289;&#20195;&#30721;&#65289;&#65292;&#20854;&#28151;&#21512;&#27604;&#20363;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#23450;&#24615;&#31574;&#30053;&#26469;&#35843;&#25972;&#27604;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23450;&#37327;&#21487;&#39044;&#27979;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#12290;&#22312;&#26679;&#26412;&#28151;&#21512;&#19978;&#25311;&#21512;&#36825;&#31181;&#20989;&#25968;&#25581;&#31034;&#20102;&#26410;&#35265;&#28151;&#21512;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#24341;&#23548;&#36873;&#25321;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#27493;&#39588;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25105;&#20204;&#30340;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#30340;&#32553;&#25918;&#35268;&#24459;&#30340;&#23884;&#22871;&#20351;&#29992;&#65292;&#20197;&#20351;&#24471;&#20165;&#36890;&#36807;&#23567;&#35268;&#27169;&#35757;&#32451;&#23601;&#33021;&#22815;&#39044;&#27979;&#22312;&#21508;&#31181;&#28151;&#21512;&#25968;&#25454;&#19979;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20248;&#21270;&#20102;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16952v1 Announce Type: cross  Abstract: Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.16933</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#22823;&#33041;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through space, time, and the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#23545;&#35299;&#20915;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26469;&#35843;&#25972;&#21333;&#20010;&#31361;&#35302;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#29289;&#29702;&#31070;&#32463;&#31995;&#32479;&#37117;&#21463;&#21040;&#26102;&#31354;&#23616;&#38480;&#12290;&#36825;&#26679;&#30340;&#32593;&#32476;&#22914;&#20309;&#25191;&#34892;&#39640;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38169;&#35823;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#20960;&#20046;&#26222;&#36941;&#34987;&#31354;&#38388;&#65288;BP&#65289;&#21644;&#26102;&#38388;&#65288;BPTT&#65289;&#20004;&#31181;&#26041;&#24335;&#32473;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;BP(TT)&#34987;&#24191;&#27867;&#35748;&#20026;&#20381;&#36182;&#20110;&#19981;&#20855;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#20551;&#35774;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#26102;&#31354;&#23616;&#38480;&#24615;&#65292;&#32780;&#27491;&#21521;&#20256;&#25773;&#27169;&#22411;&#65292;&#22914;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#21017;&#21463;&#21040;&#20869;&#23384;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#28508;&#22312;&#24179;&#34913;&#65288;GLE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31070;&#32463;&#20803;&#29289;&#29702;&#21160;&#24577;&#32593;&#32476;&#23436;&#20840;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
&lt;/p&gt;</description></item><item><title>FLIGAN&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16930</link><description>&lt;p&gt;
FLIGAN: &#20351;&#29992; GAN &#25913;&#36827;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16930
&lt;/p&gt;
&lt;p&gt;
FLIGAN&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#32593;&#32476;&#21270;&#35774;&#22791;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#12289;&#29289;&#32852;&#32593;&#36793;&#32536;&#33410;&#28857;&#65289;&#19978;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#23427;&#36890;&#36807;&#22312;&#32593;&#32476;&#19978;&#19981;&#20849;&#20139;&#23454;&#38469;&#25968;&#25454;&#26469;&#23454;&#29616;&#36793;&#32536;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20391;&#37325;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#36890;&#29992;&#26041;&#38754;&#21644;&#23458;&#25143;&#31995;&#32479;&#29305;&#24449;&#30340;&#24322;&#36136;&#24615;&#65292;&#20294;&#20182;&#20204;&#24120;&#24120;&#24573;&#35270;&#27169;&#22411;&#24320;&#21457;&#20013;&#19981;&#36275;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26469;&#33258;&#36793;&#32536;&#33410;&#28857;&#19978;&#19981;&#22343;&#21248;&#30340;&#31867;&#21035;&#26631;&#31614;&#20998;&#24067;&#65292;&#20197;&#21450;&#39640;&#24230;&#21487;&#21464;&#30340;&#25968;&#25454;&#23481;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLIGAN&#30340;&#21019;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#25935;&#38160;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#19982;&#30495;&#23454;&#25968;&#25454;&#32039;&#23494;&#30456;&#20284;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16930v1 Announce Type: new  Abstract: Federated Learning (FL) provides a privacy-preserving mechanism for distributed training of machine learning models on networked devices (e.g., mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the edge by creating models without sharing the actual data across the network. Existing research works typically focus on generic aspects of non-IID data and heterogeneity in client's system characteristics, but they often neglect the issue of insufficient data for model development, which can arise from uneven class label distribution and highly variable data volumes across edge nodes. In this work, we propose FLIGAN, a novel approach to address the issue of data incompleteness in FL. First, we leverage Generative Adversarial Networks (GANs) to adeptly capture complex data distributions and generate synthetic data that closely resemble the real-world data. Then, we use synthetic data to enhance the robustness and comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SCOD&#38382;&#39064;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#21644;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#22120;&#65292;&#20197;&#35299;&#20915;&#19981;&#30830;&#23450;&#25110;&#31163;&#32676;&#26679;&#26412;&#39044;&#27979;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16916</link><description>&lt;p&gt;
&#20174;&#21551;&#21457;&#24335;&#21040;&#29702;&#35770;&#65306;SCOD
&lt;/p&gt;
&lt;p&gt;
SCOD: From Heuristics to Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SCOD&#38382;&#39064;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#21644;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#22120;&#65292;&#20197;&#35299;&#20915;&#19981;&#30830;&#23450;&#25110;&#31163;&#32676;&#26679;&#26412;&#39044;&#27979;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#35774;&#35745;&#21487;&#38752;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24403;&#38754;&#23545;&#19981;&#30830;&#23450;&#25110;&#31163;&#32676;&#26679;&#26412;&#26102;&#36991;&#20813;&#39044;&#27979;&#30340;&#38382;&#39064; - &#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#34987;&#31216;&#20026;Selective Classification in the presence of Out-of-Distribution data (SCOD)&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;SCOD&#20570;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20248;&#30340;SCOD&#31574;&#30053;&#28041;&#21450;&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#36827;&#34892;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#21644;&#22312;2D&#31354;&#38388;&#20013;&#34920;&#31034;&#20026;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#22120;&#65292;&#20351;&#29992;ID&#20998;&#31867;&#22120;&#30340;&#26465;&#20214;&#39118;&#38505;&#21644;ID&#19982;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#30340;&#20284;&#28982;&#27604;&#20316;&#20026;&#36755;&#20837;&#12290;&#36825;&#19982;&#24403;&#21069;OOD&#26816;&#27979;&#26041;&#27861;&#21644;&#19987;&#20026;SCOD&#24320;&#21457;&#30340;Softmax Information Retaining Combination (SIRC)&#30340;&#27425;&#20248;&#31574;&#30053;&#24418;&#25104;&#23545;&#27604;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22312;&#19968;&#20010;&#26080;&#20998;&#24067;&#35774;&#32622;&#20013;&#65292;&#24403;&#20165;&#20381;&#36182;&#20110;&#26465;&#20214;&#20998;&#24067;&#21644;IID&#26679;&#26412;&#30340;&#36924;&#36817;&#21487;&#33021;&#24615;&#26102;&#65292;SCOD&#38382;&#39064;&#19981;&#21487;&#33021;&#34987;&#27491;&#30830;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16916v1 Announce Type: new  Abstract: This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#23558;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25972;&#21512;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20026;&#25511;&#21046;&#29702;&#35770;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.16899</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#20010;&#25511;&#21046;&#29702;&#35770;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
State Space Models as Foundation Models: A Control Theoretic Overview
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16899
&lt;/p&gt;
&lt;p&gt;
&#23558;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25972;&#21512;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20026;&#25511;&#21046;&#29702;&#35770;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#25972;&#21512;&#21040;&#22522;&#30784;&#27169;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;Mamba&#30340;&#25104;&#21151;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#26088;&#22312;&#23558;&#24207;&#21015;&#25968;&#25454;&#32534;&#30721;&#20026;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#25511;&#21046;&#29702;&#35770;&#23478;&#20351;&#29992;SSM&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#36861;&#27714;&#30456;&#21516;&#30340;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;SSM&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#28145;&#24230;&#24207;&#21015;&#24314;&#27169;&#30456;&#36830;&#25509;&#65292;&#25552;&#20379;&#20102;&#22312;&#30456;&#24212;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#21019;&#36896;&#21327;&#21516;&#20316;&#29992;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#26088;&#22312;&#21521;&#25511;&#21046;&#29702;&#35770;&#23478;&#31616;&#35201;&#20171;&#32461;&#22522;&#20110;SSM&#30340;&#26550;&#26500;&#65292;&#24182;&#24635;&#32467;&#26368;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#23427;&#31995;&#32479;&#22238;&#39038;&#20102;&#26368;&#25104;&#21151;&#30340;SSM&#25552;&#35758;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16899v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their m
&lt;/p&gt;</description></item><item><title>GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16883</link><description>&lt;p&gt;
&#24102;&#25193;&#25955;&#26725;&#30340;&#31163;&#25955;&#28508;&#22312;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Latent Graph Generative Modeling with Diffusion Bridges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16883
&lt;/p&gt;
&lt;p&gt;
GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#65292;&#36804;&#20170;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#20047;&#21892;&#21487;&#38472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GLAD&#65292;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;GLAD&#22312;&#20445;&#30041;&#22270;&#32467;&#26500;&#30340;&#31163;&#25955;&#24615;&#36136;&#26041;&#38754;&#36816;&#34892;&#65292;&#26080;&#38656;&#36827;&#34892;&#35832;&#22914;&#28508;&#22312;&#31354;&#38388;&#36830;&#32493;&#24615;&#31561;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25193;&#25955;&#26725;&#35843;&#25972;&#21040;&#20854;&#32467;&#26500;&#65292;&#26469;&#23398;&#20064;&#25105;&#20204;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#12290;&#36890;&#36807;&#22312;&#36866;&#24403;&#26500;&#24314;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20381;&#36182;&#20110;&#24120;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#25805;&#20316;&#30340;&#27169;&#22411;&#20013;&#30340;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26126;&#26174;&#23637;&#31034;&#20102;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#29983;&#25104;&#24615;&#33021;&#65292;&#20351;GLA
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.16877</link><description>&lt;p&gt;
&#24863;&#30693;&#21147;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#21271;&#26041;&#26862;&#26519;&#30340;&#22320;&#24418;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proprioception Is All You Need: Terrain Classification for Boreal Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16877
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#39046;&#22495;&#26426;&#22120;&#20154;&#23398;&#30740;&#31350;&#24378;&#35843;&#20102;&#25269;&#24481;&#19981;&#21516;&#31867;&#22411;&#22320;&#24418;&#30340;&#37325;&#35201;&#24615;&#12290;&#21271;&#26041;&#26862;&#26519;&#29305;&#21035;&#21463;&#21040;&#35768;&#22810;&#38480;&#21046;&#26426;&#21160;&#24615;&#30340;&#22320;&#24418;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22320;&#24418;&#24212;&#35813;&#22312;&#36234;&#37326;&#33258;&#20027;&#23548;&#33322;&#20013;&#21152;&#20197;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#22320;&#29699;&#19978;&#26368;&#22823;&#30340;&#38470;&#22320;&#29983;&#29289;&#32676;&#33853;&#20043;&#19968;&#65292;&#21271;&#26041;&#26862;&#26519;&#26159;&#39044;&#35745;&#33258;&#20027;&#36710;&#36742;&#23558;&#26085;&#30410;&#26222;&#21450;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BorealTC&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#24863;&#30693;&#21147;&#30340;&#22320;&#24418;&#20998;&#31867;&#65288;TC&#65289;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;Husky A200&#30340;116&#20998;&#38047;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#12289;&#30005;&#26426;&#30005;&#27969;&#21644;&#36718;&#32974;&#37324;&#31243;&#25968;&#25454;&#65292;&#37325;&#28857;&#20851;&#27880;&#20856;&#22411;&#30340;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#65292;&#29305;&#21035;&#26159;&#38634;&#12289;&#20912;&#21644;&#28132;&#27877;&#22756;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19982;&#21478;&#19968;&#20010;&#26469;&#33258;&#26368;&#26032;&#25216;&#26415;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;TC t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16877v1 Announce Type: cross  Abstract: Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16871</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Prediction for Multi-Agent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;OPP&#65289;&#65292;&#21363;&#20165;&#20351;&#29992;&#22312;&#19968;&#20010;&#27491;&#24120;&#65288;&#34892;&#20026;&#65289;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#30446;&#26631;&#31574;&#30053;&#30340;&#32467;&#26524;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20998;&#26512;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#37096;&#32626;&#26032;&#31574;&#30053;&#21487;&#33021;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#20449;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#65292;&#26368;&#36817;&#20851;&#20110;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;COPP&#65289;&#30340;&#24037;&#20316;&#21033;&#29992;&#19968;&#33268;&#39044;&#27979;&#26694;&#26550;&#26469;&#22312;&#30446;&#26631;&#36807;&#31243;&#19979;&#25512;&#23548;&#24102;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;COPP&#26041;&#27861;&#21487;&#20197;&#32771;&#34385;&#30001;&#31574;&#30053;&#20999;&#25442;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#20165;&#38480;&#20110;&#21333;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#26631;&#37327;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;&#22870;&#21169;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;OPP&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#8220;&#33258;&#25105;&#8221;&#26234;&#33021;&#20307;&#25913;&#21464;&#31574;&#30053;&#26102;&#20026;&#25152;&#26377;&#26234;&#33021;&#20307;&#36712;&#36857;&#25512;&#23548;&#32852;&#21512;&#39044;&#27979;&#21306;&#22495;&#12290;&#19982;&#21333;&#26234;&#33021;&#20307;&#22330;&#26223;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16871v1 Announce Type: cross  Abstract: Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more "ego" agents change their policies. Unlike the single-agent scenario, this se
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#21644;&#20445;&#30041;&#32454;&#33268;&#20960;&#20309;&#32454;&#33410;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16862</link><description>&lt;p&gt;
INPC&#65306;&#29992;&#20110;&#36752;&#23556;&#22330;&#28210;&#26579;&#30340;&#38544;&#24335;&#31070;&#32463;&#28857;&#20113;
&lt;/p&gt;
&lt;p&gt;
INPC: Implicit Neural Point Clouds for Radiance Field Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#21644;&#20445;&#30041;&#32454;&#33268;&#20960;&#20309;&#32454;&#33410;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#21644;&#21512;&#25104;&#26080;&#36793;&#30028;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#20307;&#31215;&#22330;&#12289;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#25110;&#31163;&#25955;&#28857;&#20113;&#20195;&#29702;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22330;&#26223;&#34920;&#31034;&#65292;&#23427;&#22312;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#20013;&#38544;&#21547;&#22320;&#32534;&#30721;&#28857;&#20113;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#21183;&#65292;&#20445;&#30041;&#20102;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#21033;&#30340;&#34892;&#20026;&#65306;&#25105;&#20204;&#30340;&#26032;&#39062;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#21644;&#21487;&#24494;&#30340;&#21452;&#32447;&#24615;&#20809;&#26629;&#21270;&#22120;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#32454;&#24494;&#30340;&#20960;&#20309;&#32454;&#33410;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#20687;&#32467;&#26500;&#36816;&#21160;&#28857;&#20113;&#36825;&#26679;&#30340;&#21021;&#22987;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24555;&#36895;&#25512;&#29702;&#65292;&#21487;&#20132;&#20114;&#24103;&#36895;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#21462;&#26174;&#24335;&#28857;&#20113;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16862v1 Announce Type: cross  Abstract: We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid. In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on several common benchmark datasets. Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance.
&lt;/p&gt;</description></item><item><title>DISL&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;514,506&#20010;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#20027;&#32593;&#19978;&#30340;&#29420;&#29305;Solidity&#25991;&#20214;&#65292;&#25104;&#20026;&#20102;&#24320;&#21457;&#26234;&#33021;&#21512;&#32422;&#35774;&#35745;&#24037;&#20855;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.16861</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;Solidity&#26234;&#33021;&#21512;&#32422;&#25968;&#25454;&#38598;&#25512;&#21160;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16861
&lt;/p&gt;
&lt;p&gt;
DISL&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;514,506&#20010;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#20027;&#32593;&#19978;&#30340;&#29420;&#29305;Solidity&#25991;&#20214;&#65292;&#25104;&#20026;&#20102;&#24320;&#21457;&#26234;&#33021;&#21512;&#32422;&#35774;&#35745;&#24037;&#20855;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DISL&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;514,506&#20010;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#20027;&#32593;&#19978;&#30340;&#29420;&#29305;Solidity&#25991;&#20214;&#65292;&#28385;&#36275;&#20102;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26234;&#33021;&#21512;&#32422;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;DISL&#25104;&#20026;&#20102;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#20026;&#26234;&#33021;&#21512;&#32422;&#35774;&#35745;&#30340;&#36719;&#20214;&#24037;&#31243;&#24037;&#20855;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#25910;&#38598;&#25130;&#33267;2024&#24180;1&#26376;15&#26085;&#22312;Etherscan&#19978;&#39564;&#35777;&#30340;&#27599;&#20010;&#26234;&#33021;&#21512;&#32422;&#65292;DISL&#22312;&#35268;&#27169;&#21644;&#26102;&#25928;&#24615;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16861v1 Announce Type: cross  Abstract: The DISL dataset features a collection of $514,506$ unique Solidity files that have been deployed to Ethereum mainnet. It caters to the need for a large and diverse dataset of real-world smart contracts. DISL serves as a resource for developing machine learning systems and for benchmarking software engineering tools designed for smart contracts. By aggregating every verified smart contract from Etherscan up to January 15, 2024, DISL surpasses existing datasets in size and recency.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#35821;&#20041;&#24847;&#35782;&#30340;&#36828;&#31243;&#22810;&#39532;&#23572;&#21487;&#22827;&#28304;&#20272;&#35745;&#30340;&#26368;&#20248;&#35843;&#24230;&#31574;&#30053;&#20197;&#21450;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861; Insec-RVI&#12290;</title><link>https://arxiv.org/abs/2403.16855</link><description>&lt;p&gt;
&#20855;&#26377;&#35821;&#20041;&#24847;&#35782;&#30340;&#36828;&#31243;&#22810;&#39532;&#23572;&#21487;&#22827;&#28304;&#20272;&#35745;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16855
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#35821;&#20041;&#24847;&#35782;&#30340;&#36828;&#31243;&#22810;&#39532;&#23572;&#21487;&#22827;&#28304;&#20272;&#35745;&#30340;&#26368;&#20248;&#35843;&#24230;&#31574;&#30053;&#20197;&#21450;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861; Insec-RVI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20002;&#22833;&#21644;&#21463;&#36895;&#29575;&#38480;&#21046;&#30340;&#36890;&#36947;&#19978;&#23545;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#28304;&#36827;&#34892;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#30340;&#36828;&#31243;&#20272;&#35745;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#23558;&#25152;&#26377;&#28304;&#29366;&#24577;&#35270;&#20026;&#30456;&#31561;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#30340;&#35821;&#20041;&#24182;&#32771;&#34385;&#36828;&#31243;&#25191;&#34892;&#22120;&#23545;&#19981;&#21516;&#29366;&#24577;&#30340;&#20272;&#35745;&#35823;&#24046;&#20855;&#26377;&#19981;&#21516;&#30340;&#23481;&#24525;&#24230;&#12290;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#22312;&#20256;&#36755;&#39057;&#29575;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#20272;&#35745;&#35823;&#24046;&#30340;&#38271;&#26399;&#29366;&#24577;&#30456;&#20851;&#25104;&#26412;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#36890;&#36807;&#21033;&#29992;&#24179;&#22343;&#25104;&#26412;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#29702;&#35770;&#21644;Lagrange&#21160;&#24577;&#35268;&#21010;&#23637;&#31034;&#20102;&#26368;&#20248;&#31574;&#30053;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#32467;&#26500;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20132;&#21449;&#25628;&#32034;&#21152;&#30456;&#23545;&#20540;&#36845;&#20195;&#65288;Insec-RVI&#65289;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#23569;&#37327;&#36845;&#20195;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#36991;&#20813;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16855v1 Announce Type: cross  Abstract: This paper studies semantic-aware communication for remote estimation of multiple Markov sources over a lossy and rate-constrained channel. Unlike most existing studies that treat all source states equally, we exploit the semantics of information and consider that the remote actuator has different tolerances for the estimation errors of different states. We aim to find an optimal scheduling policy that minimizes the long-term state-dependent costs of estimation errors under a transmission frequency constraint. We theoretically show the structure of the optimal policy by leveraging the average-cost Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic programming. By exploiting the optimal structural results, we develop a novel policy search algorithm, termed intersection search plus relative value iteration (Insec-RVI), that can find the optimal policy using only a few iterations. To avoid the ``curse of dimensio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.16851</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT predict article retraction based on Twitter mentions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#26377;&#38382;&#39064;&#30340;&#30740;&#31350;&#25991;&#31456;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26681;&#25454;&#34987;&#25764;&#22238;&#25991;&#31456;&#22312;Twitter&#19978;&#30340;&#25552;&#21450;&#26159;&#21542;&#33021;&#22815;&#22312;&#25991;&#31456;&#34987;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#20998;&#26512;&#20102;&#21253;&#25324;3,505&#31687;&#24050;&#25764;&#22238;&#25991;&#31456;&#21450;&#20854;&#30456;&#20851;Twitter&#25552;&#21450;&#22312;&#20869;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20351;&#29992;&#31895;&#31961;&#31934;&#30830;&#21305;&#37197;&#26041;&#27861;&#33719;&#21462;&#30340;&#20855;&#26377;&#31867;&#20284;&#29305;&#24449;&#30340;3,505&#31687;&#26410;&#25764;&#22238;&#25991;&#31456;&#12290;&#36890;&#36807;&#22235;&#31181;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#20102;Twitter&#25552;&#21450;&#22312;&#39044;&#27979;&#25991;&#31456;&#25764;&#22238;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25163;&#21160;&#26631;&#27880;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;ChatGPT&#12290;&#25163;&#21160;&#26631;&#27880;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30340;&#30830;&#26377;&#34987;&#25764;&#22238;&#30340;&#25991;&#31456;&#65292;&#20854;Twitter&#25552;&#21450;&#21253;&#21547;&#22312;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#30340;&#21487;&#35782;&#21035;&#35777;&#25454;&#65292;&#23613;&#31649;&#23427;&#20204;&#21482;&#21344;&#25152;&#26377;&#34987;&#25764;&#22238;&#25991;&#31456;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16851v1 Announce Type: cross  Abstract: Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and ChatGPT. Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#21644;CoDy&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoDy&#22312;&#23547;&#25214;&#37325;&#35201;&#21453;&#20107;&#23454;&#36755;&#20837;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;59%&#12290;</title><link>https://arxiv.org/abs/2403.16846</link><description>&lt;p&gt;
GreeDy&#21644;CoDy&#65306;&#21160;&#24577;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#21644;CoDy&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoDy&#22312;&#23547;&#25214;&#37325;&#35201;&#21453;&#20107;&#23454;&#36755;&#20837;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;59%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNNs&#65289;&#23545;&#20110;&#24314;&#27169;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#20132;&#20114;&#30340;&#21160;&#24577;&#22270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#30740;&#31350;&#36755;&#20837;&#22270;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340; TGNNs &#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#65288;&#21160;&#24577;&#22270;&#30340;&#36138;&#24515;&#35299;&#37322;&#22120;&#65289;&#21644; CoDy&#65288;&#21160;&#24577;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65289;&#12290;&#23427;&#20204;&#23558;&#35299;&#37322;&#35270;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65292;&#23547;&#25214;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#30340;&#36755;&#20837;&#22270;&#20462;&#25913;&#12290;GreeDy &#20351;&#29992;&#31616;&#21333;&#30340;&#36138;&#24515;&#26041;&#27861;&#65292;&#32780; CoDy &#20351;&#29992;&#22797;&#26434;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#26377;&#25928;&#29983;&#25104;&#28165;&#26224;&#30340;&#35299;&#37322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CoDy &#30340;&#24615;&#33021;&#20248;&#20110; GreeDy &#21644;&#29616;&#26377;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#23547;&#25214;&#21040;&#37325;&#35201;&#30340;&#21453;&#20107;&#23454;&#36755;&#20837;&#30340;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;&#39640;&#36798; 59\%&#12290;&#36825;&#31361;&#20986;&#20102; CoDy &#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16846v1 Announce Type: cross  Abstract: Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs with time-varying interactions, face a significant challenge in explainability due to their complex model structure. Counterfactual explanations, crucial for understanding model decisions, examine how input graph changes affect outcomes. This paper introduces two novel counterfactual explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer for Dynamic Graphs). They treat explanations as a search problem, seeking input graph alterations that alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher success rate in finding significant counterfactual inputs. This highlights CoDy's p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16843</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#26159;&#21542;&#20250;&#24863;&#21040;&#21518;&#24724;&#65311;&#22312;&#32447;&#23398;&#20064;&#21644;&#28216;&#25103;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLM Agents Have Regret? A Case Study in Online Learning and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16843
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;(&#20132;&#20114;&#24335;)&#20915;&#31574;&#21046;&#23450;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#19981;&#26029;&#30340;&#25104;&#21151;&#65292;&#20294;LLM&#20195;&#29702;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#36827;&#34892;&#20805;&#20998;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#26102;&#30340;&#22810;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20856;&#22411;&#22330;&#26223;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLM&#20195;&#29702;&#22312;&#36825;&#20123;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#23427;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;\emph{&#21518;&#24724;}&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32463;&#20856;(&#38750;&#24179;&#31283;)&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;LLMs&#30340;&#26080;&#21518;&#24724;&#34892;&#20026;&#65292;&#20197;&#21450;&#24403;LLM&#20195;&#29702;&#36890;&#36807;&#36827;&#34892;&#37325;&#22797;&#28216;&#25103;&#36827;&#34892;&#20132;&#20114;&#26102;&#22343;&#34913;&#30340;&#20986;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#26080;&#21518;&#24724;&#34892;&#20026;&#25552;&#20379;&#19968;&#20123;&#29702;&#35770;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.16829</link><description>&lt;p&gt;
&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#32452;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#24674;&#22797;&#19968;&#20010;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#22870;&#21169;&#65292;&#37319;&#29992;&#38543;&#26426;&#36719;&#31574;&#30053;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#20351;&#29992;$\mathcal{O}(1/\varepsilon^{2})$&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26679;&#26412;&#24674;&#22797;&#20986;&#19968;&#20010;&#20351;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;$\mathcal{O}(1/\varepsilon^{4})$&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#24674;&#22797;&#22870;&#21169;&#23545;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#19978;&#19982;&#19987;&#23478;&#31574;&#30053;$\varepsilon$-&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#31070;&#32463;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#38544;&#34255;&#21333;&#20803;&#21644;&#35757;&#32451;&#27493;&#25968;&#30340;&#25968;&#37327;$\rightarrow \infty$&#26102;&#65292;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#25910;&#25947;&#20110;&#38543;&#26426;ODE&#65292;&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#21644;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#35777;&#26126;&#27169;&#22411;&#26356;&#26032;&#27874;&#21160;&#28040;&#22833;&#65292;&#28436;&#21592;&#31070;&#32463;&#32593;&#32476;&#21644;&#35780;&#35770;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;ODE&#31995;&#32479;&#30340;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16825</link><description>&lt;p&gt;
&#22312;&#32447;&#31070;&#32463;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#30340;&#24369;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Weak Convergence Analysis of Online Neural Actor-Critic Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16825
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31070;&#32463;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#38544;&#34255;&#21333;&#20803;&#21644;&#35757;&#32451;&#27493;&#25968;&#30340;&#25968;&#37327;$\rightarrow \infty$&#26102;&#65292;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#25910;&#25947;&#20110;&#38543;&#26426;ODE&#65292;&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#21644;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#35777;&#26126;&#27169;&#22411;&#26356;&#26032;&#27874;&#21160;&#28040;&#22833;&#65292;&#28436;&#21592;&#31070;&#32463;&#32593;&#32476;&#21644;&#35780;&#35770;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;ODE&#31995;&#32479;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#22312;&#32447;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#35757;&#32451;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#38544;&#34255;&#21333;&#20803;&#21644;&#35757;&#32451;&#27493;&#25968;&#30340;&#25968;&#37327;$\rightarrow \infty$&#26102;&#65292;&#25910;&#25947;&#20110;&#19968;&#20010;&#38543;&#26426;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#22312;&#32447;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#20013;&#65292;&#38543;&#30528;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#24067;&#20250;&#21160;&#24577;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#20219;&#20309;&#25910;&#25947;&#20998;&#26512;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22266;&#23450;&#28436;&#21592;&#31574;&#30053;&#19979;&#24314;&#31435;&#20102;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#30001;&#20110;&#38543;&#26426;&#21040;&#36798;&#30340;&#25968;&#25454;&#26679;&#26412;&#24102;&#26469;&#30340;&#27169;&#22411;&#26356;&#26032;&#27874;&#21160;&#20250;&#38543;&#30528;&#21442;&#25968;&#26356;&#26032;&#27425;&#25968;&#30340;&#22686;&#21152;$\rightarrow \infty$&#32780;&#28040;&#22833;&#12290;&#21033;&#29992;&#27850;&#26494;&#26041;&#31243;&#21644;&#24369;&#25910;&#25947;&#25216;&#26415;&#65292;&#25105;&#20204;&#35777;&#26126;&#28436;&#21592;&#31070;&#32463;&#32593;&#32476;&#21644;&#35780;&#35770;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;ODE&#31995;&#32479;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16825v1 Announce Type: new  Abstract: We prove that a single-layer neural network trained with the online actor critic algorithm converges in distribution to a random ordinary differential equation (ODE) as the number of hidden units and the number of training steps $\rightarrow \infty$. In the online actor-critic algorithm, the distribution of the data samples dynamically changes as the model is updated, which is a key challenge for any convergence analysis. We establish the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the model updates around the limit distribution due to the randomly-arriving data samples vanish as the number of parameter updates $\rightarrow \infty$. Using the Poisson equation and weak convergence techniques, we prove that the actor neural network and critic neural network converge to the solutions of a system of ODEs with random initial conditions. Analysis of the 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#28151;&#21512;LiFi&#21644;WiFi&#32593;&#32476;&#20013;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#32593;&#32476;&#20013;&#24515;&#21270;&#26041;&#27861;&#22312;&#22788;&#29702;&#31227;&#21160;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16823</link><description>&lt;p&gt;
&#28151;&#21512;LiFi&#21644;WiFi&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#21644;&#31227;&#21160;&#31649;&#29702;&#65306;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A User-Centric Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16823
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#28151;&#21512;LiFi&#21644;WiFi&#32593;&#32476;&#20013;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#32593;&#32476;&#20013;&#24515;&#21270;&#26041;&#27861;&#22312;&#22788;&#29702;&#31227;&#21160;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20809;&#36890;&#20449;&#65288;LiFi&#65289;&#21644;&#26080;&#32447;&#23616;&#22495;&#32593;&#65288;WiFi&#65289;&#32593;&#32476;&#65288;HLWNets&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#23460;&#20869;&#26080;&#32447;&#36890;&#20449;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;LiFi&#30340;&#23485;&#23637;&#20809;&#35889;&#20248;&#21183;&#21644;WiFi&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#35206;&#30422;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36127;&#36733;&#24179;&#34913;&#65288;LB&#65289;&#25104;&#20026;&#36825;&#31181;&#28151;&#21512;&#32593;&#32476;&#36164;&#28304;&#31649;&#29702;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#22823;&#22810;&#26159;&#32593;&#32476;&#20013;&#24515;&#21270;&#30340;&#65292;&#20381;&#36182;&#20013;&#22830;&#21333;&#20803;&#19968;&#27425;&#24615;&#20026;&#25152;&#26377;&#29992;&#25143;&#21046;&#23450;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#19981;&#35770;&#29992;&#25143;&#30340;&#31227;&#21160;&#29366;&#24577;&#22914;&#20309;&#65292;&#35299;&#20915;&#26041;&#26696;&#37117;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#25152;&#26377;&#29992;&#25143;&#12290;&#36825;&#20250;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#30340;&#20004;&#20010;&#26041;&#38754;&#65306;i&#65289;&#24403;&#26356;&#26032;&#39057;&#29575;&#36739;&#20302;&#26102;&#65292;&#20250;&#24433;&#21709;&#24555;&#36895;&#31227;&#21160;&#29992;&#25143;&#30340;&#36830;&#25509;&#24615;&#65307;ii&#65289;&#24403;&#26356;&#26032;&#39057;&#29575;&#36739;&#39640;&#26102;&#65292;&#20250;&#23548;&#33268;&#23545;&#20110;&#24930;&#36895;&#31227;&#21160;&#29992;&#25143;&#19981;&#24517;&#35201;&#30340;&#20999;&#25442;&#20197;&#21450;&#24040;&#22823;&#30340;&#21453;&#39304;&#25104;&#26412;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20801;&#35768;&#29992;&#25143;&#26356;&#26032;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16823v1 Announce Type: cross  Abstract: Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets) are an emerging indoor wireless communication paradigm, which combines the advantages of the capacious optical spectra of LiFi and ubiquitous coverage of WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource management for such hybrid networks. The existing LB methods are mostly network-centric, relying on a central unit to make a solution for the users all at once. Consequently, the solution needs to be updated for all users at the same pace, regardless of their moving status. This would affect the network performance in two aspects: i) when the update frequency is low, it would compromise the connectivity of fast-moving users; ii) when the update frequency is high, it would cause unnecessary handovers as well as hefty feedback costs for slow-moving users. Motivated by this, we investigate user-centric LB which allows users to update their 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;BOSouL&#27169;&#25311;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#24555;&#29031;&#35266;&#27979;&#20013;&#23454;&#29616;&#22810;&#28304;&#23450;&#20301;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#20449;&#24687;&#26377;&#38480;&#12289;&#28304;&#20132;&#20114;&#20316;&#29992;&#21644;&#25193;&#25955;&#27169;&#22411;&#20381;&#36182;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16818</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#36125;&#21494;&#26031;&#20248;&#21270;&#20174;&#21333;&#24555;&#29031;&#35266;&#27979;&#23454;&#29616;&#22810;&#28304;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16818
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;BOSouL&#27169;&#25311;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#24555;&#29031;&#35266;&#27979;&#20013;&#23454;&#29616;&#22810;&#28304;&#23450;&#20301;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#20449;&#24687;&#26377;&#38480;&#12289;&#28304;&#20132;&#20114;&#20316;&#29992;&#21644;&#25193;&#25955;&#27169;&#22411;&#20381;&#36182;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#28304;&#23450;&#20301;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#25104;&#20026;&#24212;&#23545;&#25193;&#25955;&#21361;&#23475;&#30340;&#26368;&#37325;&#35201;&#25163;&#27573;&#20043;&#19968;&#12290;&#20174;&#21333;&#20010;&#24555;&#29031;&#35266;&#27979;&#20013;&#23454;&#29616;&#22810;&#28304;&#23450;&#20301;&#23588;&#20854;&#37325;&#35201;&#65292;&#20294;&#36825;&#19968;&#38382;&#39064;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#22914;&#20449;&#24687;&#26377;&#38480;&#12289;&#28304;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#20381;&#36182;&#65292;&#32473;&#35299;&#20915;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#21551;&#21457;&#24335;&#21644;&#36138;&#23146;&#36873;&#25321;&#65292;&#36890;&#24120;&#19982;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#32465;&#23450;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BOSouL&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26469;&#36817;&#20284;&#32467;&#26524;&#65292;&#20854;&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#23545;&#26469;&#33258;&#26377;&#38480;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#23427;&#20197;&#33410;&#28857;&#38598;&#21512;&#32780;&#38750;&#21333;&#20010;&#33410;&#28857;&#20316;&#20026;&#36755;&#20837;&#12290;BOSouL&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16818v1 Announce Type: new  Abstract: Due to the significance of its various applications, source localization has garnered considerable attention as one of the most important means to confront diffusion hazards. Multi-source localization from a single-snapshot observation is especially relevant due to its prevalence. However, the inherent complexities of this problem, such as limited information, interactions among sources, and dependence on diffusion models, pose challenges to resolution. Current methods typically utilize heuristics and greedy selection, and they are usually bonded with one diffusion model. Consequently, their effectiveness is constrained. To address these limitations, we propose a simulation-based method termed BOSouL. Bayesian optimization (BO) is adopted to approximate the results for its sample efficiency. A surrogate function models uncertainty from the limited information. It takes sets of nodes as the input instead of individual nodes. BOSouL can in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#20248;&#21270;&#20154;&#22312;&#22238;&#36335;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#36141;&#29289;&#20013;&#24515;&#20013;&#27169;&#25311;&#21508;&#31181;&#20154;&#32676;&#28909;&#37327;&#20559;&#22909;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.16809</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#20248;&#21270;&#20154;&#22312;&#22238;&#36335;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#20248;&#21270;&#20154;&#22312;&#22238;&#36335;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#36141;&#29289;&#20013;&#24515;&#20013;&#27169;&#25311;&#21508;&#31181;&#20154;&#32676;&#28909;&#37327;&#20559;&#22909;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#31995;&#32479;(CPS-IoT)&#24212;&#29992;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#26032;&#30340;&#24212;&#29992;&#27491;&#22312;&#20852;&#36215;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#23454;&#26102;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20379;&#26262;&#12289;&#36890;&#39118;&#21644;&#31354;&#35843;(HVAC)&#31995;&#32479;&#30340;&#23454;&#26102;&#25511;&#21046;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20026;&#20154;&#21592;&#33298;&#36866;&#32780;&#36816;&#34892;&#26102;&#20943;&#23569;&#20854;&#20351;&#29992;&#65292;&#20174;&#32780;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#36825;&#31181;&#20154;&#22312;&#22238;&#36335;(HITL)&#31995;&#32479;&#20013;&#20154;&#31867;&#20559;&#22909;&#30340;&#23454;&#26102;&#21453;&#39304;&#25910;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#24212;&#23545;CPS&#20248;&#21270;&#20013;&#21160;&#24577;&#29615;&#22659;&#21644;&#38590;&#20197;&#33719;&#21462;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#27169;&#20223;&#36141;&#29289;&#20013;&#24515;&#20013;&#21508;&#31181;&#20154;&#32676;&#65288;&#22914;&#24180;&#36731;&#23478;&#24237;&#12289;&#32769;&#24180;&#20154;&#65289;&#30340;&#34892;&#20026;&#21644;&#28909;&#37327;&#20559;&#22909;&#12290;&#32858;&#21512;&#30340;&#28909;&#37327;&#20559;&#22909;&#34987;&#25972;&#21512;&#21040;&#19968;&#20010;&#20195;&#29702;&#20307;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16809v1 Announce Type: cross  Abstract: The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16798</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cluster-Based Normalization Layer for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#20869;&#37096;&#21327;&#21464;&#37327;&#28418;&#31227;&#12289;&#26631;&#31614;&#28418;&#31227;&#12289;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20256;&#32479;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#22914;&#25209;&#26631;&#20934;&#21270;&#65292;&#26088;&#22312;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#38480;&#21046;&#20854;&#36866;&#24212;&#24615;&#30340;&#20551;&#35774;&#12290;&#28151;&#21512;&#35268;&#33539;&#21270;&#22312;&#22788;&#29702;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;CB-Norm&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#8212;&#8212;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;SCB-Norm&#65289;&#21644;&#26080;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;UCB-Norm&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#19968;&#27493;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;CB-Norm&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#19987;&#38376;&#35299;&#20915;&#19982;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21152;&#24615;&#22122;&#22768;&#30340;&#21508;&#21521;&#21516;&#24615;&#20316;&#20026;&#32422;&#26463;&#65292;&#25913;&#36827;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#65292;&#39564;&#35777;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#32422;&#26463;&#30340;&#25972;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16790</link><description>&lt;p&gt;
Iso-Diffusion: &#20351;&#29992;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#30340;&#21508;&#21521;&#21516;&#24615;&#25913;&#36827;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16790
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21152;&#24615;&#22122;&#22768;&#30340;&#21508;&#21521;&#21516;&#24615;&#20316;&#20026;&#32422;&#26463;&#65292;&#25913;&#36827;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#65292;&#39564;&#35777;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#32422;&#26463;&#30340;&#25972;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#23601;&#12290;&#23613;&#31649;&#23427;&#20204;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21033;&#29992;&#24378;&#21152;&#32467;&#26500;&#23436;&#25972;&#24615;&#30340;&#32479;&#35745;&#23646;&#24615;&#26469;&#25552;&#39640;&#26679;&#26412;&#20445;&#30495;&#24230;&#65292;&#22914;&#21508;&#21521;&#21516;&#24615;&#12290;&#20165;&#20943;&#23567;&#21152;&#24615;&#21644;&#39044;&#27979;&#22122;&#22768;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#24182;&#19981;&#33021;&#24378;&#21152;&#23545;&#39044;&#27979;&#22122;&#22768;&#20026;&#21508;&#21521;&#21516;&#24615;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21463;&#21040;&#21160;&#21147;&#65292;&#21033;&#29992;&#21152;&#24615;&#22122;&#22768;&#30340;&#21508;&#21521;&#21516;&#24615;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#32422;&#26463;&#26469;&#22686;&#24378;DDPMs&#30340;&#20445;&#30495;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;DDPM&#21464;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22235;&#20010;&#21512;&#25104;2D&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#20197;&#21450;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27491;&#22914;&#32467;&#26524;&#25152;&#31034;&#65292;&#36825;&#31181;&#32422;&#26463;&#30340;&#25972;&#21512;&#25913;&#21892;&#20102;2D&#25968;&#25454;&#38598;&#30340;&#20445;&#30495;&#24230;&#25351;&#26631;Precision&#21644;Density&#20197;&#21450;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16790v1 Announce Type: new  Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in the realm of generative AI. Despite their high performance, there is room for improvement, especially in terms of sample fidelity by utilizing statistical properties that impose structural integrity, such as isotropy. Minimizing the mean squared error between the additive and predicted noise alone does not impose constraints on the predicted noise to be isotropic. Thus, we were motivated to utilize the isotropy of the additive noise as a constraint on the objective function to enhance the fidelity of DDPMs. Our approach is simple and can be applied to any DDPM variant. We validate our approach by presenting experiments conducted on four synthetic 2D datasets as well as on unconditional image generation. As demonstrated by the results, the incorporation of this constraint improves the fidelity metrics, Precision and Density for the 2D datasets as well as for the un
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#21040;&#30340;&#27010;&#24565;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#26032;&#27010;&#24565;&#25110;&#20462;&#25913;&#29616;&#26377;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#31181;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#20998;&#35299;&#23545;&#25200;&#21160;&#36827;&#34892;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.16782</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#35299;&#21078;&#23398;&#65306;&#22522;&#20110;&#27010;&#24565;&#30340;XAI&#35299;&#21078;
&lt;/p&gt;
&lt;p&gt;
The Anatomy of Adversarial Attacks: Concept-based XAI Dissection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16782
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#21040;&#30340;&#27010;&#24565;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#26032;&#27010;&#24565;&#25110;&#20462;&#25913;&#29616;&#26377;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#31181;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#20998;&#35299;&#23545;&#25200;&#21160;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;(AAs)&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#34429;&#28982;&#36825;&#20123;&#25915;&#20987;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21644;&#27010;&#24565;&#30340;&#24433;&#21709;&#20173;&#28982;&#22823;&#22810;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#25216;&#26415;&#65292;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;AA&#25216;&#26415;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;AAs&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36896;&#25104;&#27010;&#24565;&#32452;&#25104;&#30340;&#23454;&#36136;&#24615;&#21464;&#21270;&#65292;&#24341;&#20837;&#26032;&#27010;&#24565;&#25110;&#20462;&#25913;&#29616;&#26377;&#27010;&#24565;&#12290;&#20854;&#27425;&#65292;&#23545;&#25239;&#24615;&#25200;&#21160;&#26412;&#36523;&#21487;&#20197;&#34987;&#32447;&#24615;&#20998;&#35299;&#20026;&#19968;&#32452;&#28508;&#22312;&#30690;&#37327;&#20998;&#37327;&#65292;&#20854;&#20013;&#37096;&#20998;&#20998;&#37327;&#36127;&#36131;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16782v1 Announce Type: cross  Abstract: Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.16776</link><description>&lt;p&gt;
Diff-Def: &#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#30340;&#24418;&#21464;&#22330;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16776
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#22270;&#35889;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#21475;&#20998;&#26512;&#12290;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#38024;&#23545;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#25110;&#30149;&#29702;&#23398;&#65289;&#23450;&#20041;&#30340;&#29305;&#23450;&#23376;&#20154;&#21475;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#24418;&#24577;&#23398;&#24046;&#24322;&#31561;&#32454;&#31890;&#24230;&#35299;&#21078;&#23398;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#37197;&#20934;&#30340;&#26041;&#27861;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#21069;&#32773;&#26080;&#27861;&#22788;&#29702;&#22823;&#30340;&#35299;&#21078;&#23398;&#21464;&#24322;&#65292;&#21518;&#32773;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#19981;&#31283;&#23450;&#21644;&#24187;&#35273;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#20010;&#24120;&#35268;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#20195;&#34920;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#12290;&#36890;&#36807;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#24182;&#23558;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#27880;&#20876;&#21040;&#19968;&#32452;&#22270;&#20687;&#38468;&#36817;&#65292;&#25105;&#20204;&#30830;&#20445;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#30452;&#25509;&#22270;&#20687;&#21512;&#25104;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16776v1 Announce Type: cross  Abstract: Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;</title><link>https://arxiv.org/abs/2403.16771</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#35821;&#35328;&#19990;&#30028;&#20013;&#30340;&#24191;&#27867;&#32593;&#32476;&#20132;&#27969;&#20026;&#22312;&#21333;&#20010;&#35805;&#35821;&#20013;&#28151;&#21512;&#22810;&#31181;&#35821;&#35328;&#65288;&#21448;&#31216;&#28151;&#21512;&#20195;&#30721;&#35821;&#35328;&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#21644;&#22122;&#38899;&#30340;&#23384;&#22312;&#65292;&#36825;&#32473;&#35745;&#31639;&#27169;&#22411;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#20013;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#32763;&#35793;&#21033;&#29992;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#20013;&#30340;&#29616;&#26377;&#25968;&#25454;&#12290;&#26412;&#25991;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#65288;&#21360;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21512;&#25104;&#24320;&#21457;&#20102;HINMIX&#19968;&#20010;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;420&#19975;&#20010;&#21477;&#23545;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RCMT&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#20581;&#25200;&#21160;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#21333;&#35789;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#65292;&#23398;&#20064;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#20013;&#30340;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RCMT&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#23545;&#23391;&#21152;&#25289;&#35821;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16771v1 Announce Type: new  Abstract: The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish t
&lt;/p&gt;</description></item><item><title>DeepKnowledge&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27867;&#21270;&#29702;&#35770;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;DNN&#30340;&#31283;&#20581;&#24615;&#24182;&#20943;&#23569;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.16768</link><description>&lt;p&gt;
DeepKnowledge: &#22522;&#20110;&#27867;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DeepKnowledge: Generalisation-Driven Deep Learning Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16768
&lt;/p&gt;
&lt;p&gt;
DeepKnowledge&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27867;&#21270;&#29702;&#35770;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;DNN&#30340;&#31283;&#20581;&#24615;&#24182;&#20943;&#23569;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#26497;&#20026;&#33030;&#24369;&#65292;&#36825;&#35201;&#27714;&#26377;&#25928;&#30340;&#27979;&#35797;&#25216;&#26415;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#31995;&#32479;&#21270;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#21644;&#36816;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;DeepKnowledge&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27867;&#21270;&#29702;&#35770;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#20943;&#23569;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#30340;&#21097;&#20313;&#39118;&#38505;&#12290;&#26681;&#25454;&#36825;&#19968;&#29702;&#35770;&#65292;DeepKnowledge&#35748;&#20026;&#26680;&#24515;&#35745;&#31639;DNN&#21333;&#20803;&#65292;&#31216;&#20026;&#36716;&#31227;&#30693;&#35782;&#31070;&#32463;&#20803;&#65292;&#22312;&#22495;&#21464;&#21270;&#19979;&#21487;&#20197;&#27867;&#21270;&#12290;DeepKnowledge&#25552;&#20379;&#20102;&#19968;&#31181;&#23458;&#35266;&#30340;&#20449;&#24515;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;DNN&#27979;&#35797;&#27963;&#21160;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#25512;&#21160;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16768v1 Announce Type: cross  Abstract: Despite their unprecedented success, DNNs are notoriously fragile to small shifts in data distribution, demanding effective testing techniques that can assess their dependability. Despite recent advances in DNN testing, there is a lack of systematic testing approaches that assess the DNN's capability to generalise and operate comparably beyond data in their training distribution. We address this gap with DeepKnowledge, a systematic testing methodology for DNN-based systems founded on the theory of knowledge generalisation, which aims to enhance DNN robustness and reduce the residual risk of 'black box' models. Conforming to this theory, DeepKnowledge posits that core computational DNN units, termed Transfer Knowledge neurons, can generalise under domain shift. DeepKnowledge provides an objective confidence measurement on testing activities of DNN given data distribution shifts and uses this information to instrument a generalisation-in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16707</link><description>&lt;p&gt;
&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Domain Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#20851;&#20110;&#29992;&#20110;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#35752;&#35770;&#20102;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65288;DIL&#65289;&#12290;&#22312;DIL&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#35266;&#23519;&#26032;&#39046;&#22495;&#19978;&#30340;&#26679;&#26412;&#12290;&#27169;&#22411;&#24517;&#39035;&#23545;&#25152;&#26377;&#39046;&#22495;&#19978;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#38656;&#35201;&#22312;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#20165;&#38388;&#27463;&#24615;&#22320;&#34987;&#35266;&#23519;&#30340;&#32422;&#26463;&#19979;&#25191;&#34892;DIL&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26497;&#31471;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#21482;&#26377;&#19968;&#20221;&#26469;&#33258;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21333;&#27425;DIL&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#29616;&#26377;&#30340;DIL&#26041;&#27861;&#22312;&#21333;&#27425;DIL&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#21508;&#31181;&#35843;&#26597;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#21333;&#27425;DIL&#30340;&#22256;&#38590;&#26159;&#30001;&#25209;&#24402;&#19968;&#21270;&#23618;&#20013;&#30340;&#32479;&#35745;&#25968;&#25454;&#24341;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16707v1 Announce Type: cross  Abstract: Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our tech
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;11&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#21069;&#21015;&#33146;&#30284;&#33258;&#21160;&#26684;&#37324;&#26862;&#20998;&#32423;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;ConvNeXt&#34920;&#29616;&#26368;&#20339;&#65292;&#26032;&#26550;&#26500;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#20294;&#22312;&#21306;&#20998;&#23494;&#20999;&#30456;&#20851;&#30340;&#26684;&#37324;&#26862;&#20998;&#32423;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#20026;&#25552;&#39640;&#26684;&#37324;&#26862;&#20998;&#32423;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.16695</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#22312;&#21069;&#21015;&#33146;&#30284;&#33258;&#21160;&#26684;&#37324;&#26862;&#20998;&#32423;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Assessing the Performance of Deep Learning for Automated Gleason Grading in Prostate Cancer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;11&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#21069;&#21015;&#33146;&#30284;&#33258;&#21160;&#26684;&#37324;&#26862;&#20998;&#32423;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;ConvNeXt&#34920;&#29616;&#26368;&#20339;&#65292;&#26032;&#26550;&#26500;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#20294;&#22312;&#21306;&#20998;&#23494;&#20999;&#30456;&#20851;&#30340;&#26684;&#37324;&#26862;&#20998;&#32423;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#20026;&#25552;&#39640;&#26684;&#37324;&#26862;&#20998;&#32423;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21015;&#33146;&#30284;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#21033;&#29992;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;11&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#33258;&#21160;&#21270;&#21069;&#21015;&#33146;&#30284;&#26684;&#37324;&#26862;&#20998;&#32423;&#20013;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#26159;&#27604;&#36739;&#20256;&#32479;&#21644;&#26368;&#36817;&#30340;&#26550;&#26500;&#12290;&#22522;&#20110;AUCMEDI&#26694;&#26550;&#30340;&#26631;&#20934;&#21270;&#22270;&#20687;&#20998;&#31867;&#31649;&#36947;&#65292;&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;34,264&#20010;&#27880;&#37322;&#30340;&#32452;&#32455;&#20999;&#29255;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#65292;&#20415;&#20110;&#36827;&#34892;&#24378;&#22823;&#30340;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#30340;&#25935;&#24863;&#24615;&#19981;&#21516;&#65292;ConvNeXt&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36739;&#26032;&#30340;&#26550;&#26500;&#22312;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38754;&#20020;&#30528;&#21306;&#20998;&#20851;&#31995;&#23494;&#20999;&#30340;&#26684;&#37324;&#26862;&#20998;&#32423;&#30340;&#25361;&#25112;&#12290;ConvNeXt&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#20026;&#25552;&#39640;&#26684;&#37324;&#26862;&#20998;&#32423;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#21487;&#33021;&#20250;&#23545;&#20020;&#24202;&#23454;&#36341;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16695v1 Announce Type: cross  Abstract: Prostate cancer is a dominant health concern calling for advanced diagnostic tools. Utilizing digital pathology and artificial intelligence, this study explores the potential of 11 deep neural network architectures for automated Gleason grading in prostate carcinoma focusing on comparing traditional and recent architectures. A standardized image classification pipeline, based on the AUCMEDI framework, facilitated robust evaluation using an in-house dataset consisting of 34,264 annotated tissue tiles. The results indicated varying sensitivity across architectures, with ConvNeXt demonstrating the strongest performance. Notably, newer architectures achieved superior performance, even though with challenges in differentiating closely related Gleason grades. The ConvNeXt model was capable of learning a balance between complexity and generalizability. Overall, this study lays the groundwork for enhanced Gleason grading systems, potentially i
&lt;/p&gt;</description></item><item><title>Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.16689</link><description>&lt;p&gt;
Synapse: &#20174;&#35270;&#35273;&#28436;&#31034;&#20013;&#23398;&#20064;&#20248;&#20808;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Synapse: Learning Preferential Concepts from Visual Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16689
&lt;/p&gt;
&lt;p&gt;
Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#23398;&#20064;&#29992;&#25143;&#29305;&#23450;&#20559;&#22909;&#65288;&#20363;&#22914;&#65292;&#8220;&#22909;&#20572;&#36710;&#20301;&#8221;&#65292;&#8220;&#26041;&#20415;&#30340;&#19979;&#36710;&#20301;&#32622;&#8221;&#65289;&#12290;&#23613;&#31649;&#19982;&#23398;&#20064;&#20107;&#23454;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#65289;&#30456;&#20284;&#65292;&#20294;&#20559;&#22909;&#23398;&#20064;&#26159;&#19968;&#20010;&#22522;&#26412;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#20027;&#35266;&#24615;&#36136;&#21644;&#20010;&#20154;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Synapse&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#12290;Synapse&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#22312;&#22270;&#20687;&#19978;&#36816;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#20013;&#30340;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30340;&#26032;&#32452;&#21512;&#26469;&#23398;&#20064;&#20195;&#34920;&#20010;&#20154;&#20559;&#22909;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;Synapse&#65292;&#21253;&#25324;&#19968;&#20010;&#20851;&#27880;&#19982;&#31227;&#21160;&#30456;&#20851;&#30340;&#29992;&#25143;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16689v1 Announce Type: cross  Abstract: This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., "good parking spot", "convenient drop-off location") from visual input. Despite its similarity to learning factual concepts (e.g., "red cube"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#26377;&#38480;&#30697;&#30340;&#27867;&#21270;&#30028;&#65292;&#24182;&#25512;&#23548;&#20102;&#39640;&#27010;&#29575;&#30340;PAC-Bayes&#30028;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23545;&#25439;&#22833;&#20989;&#25968;&#26377;&#30028;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#30028;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26399;&#26395;&#21644;&#21333;&#27425;&#25277;&#26679;PAC-Bayes&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#38024;&#23545;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#24555;&#36895;&#36895;&#29575;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16681</link><description>&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#20855;&#26377;&#26377;&#38480;&#30697;&#30340;&#25439;&#22833;&#20989;&#25968;&#27867;&#21270;&#30028;&#30340;&#27880;&#35760;
&lt;/p&gt;
&lt;p&gt;
A note on generalization bounds for losses with finite moments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#26377;&#38480;&#30697;&#30340;&#27867;&#21270;&#30028;&#65292;&#24182;&#25512;&#23548;&#20102;&#39640;&#27010;&#29575;&#30340;PAC-Bayes&#30028;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23545;&#25439;&#22833;&#20989;&#25968;&#26377;&#30028;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#30028;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26399;&#26395;&#21644;&#21333;&#27425;&#25277;&#26679;PAC-Bayes&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#38024;&#23545;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#24555;&#36895;&#36895;&#29575;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Alquier [1]&#25552;&#20986;&#30340;&#25130;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#20855;&#26377;&#37325;&#23614;&#29305;&#24615;&#30340;&#26080;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#12290;&#20551;&#35774;$p$-&#38454;&#30697;&#26377;&#30028;&#65292;&#24471;&#21040;&#30340;&#30028;&#22312;$p=2$&#26102;&#25554;&#20540;&#20026;&#32531;&#24930;&#30340;&#36895;&#29575;$1 / \sqrt{n}$&#65292;&#22312;$p \to \infty$&#19988;&#25439;&#22833;&#20989;&#25968;&#22522;&#26412;&#26377;&#30028;&#26102;&#25554;&#20540;&#20026;&#24555;&#36895;&#30340;&#36895;&#29575;$1 / n$&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23548;&#20986;&#20102;&#20855;&#26377;&#26377;&#30028;&#26041;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#12290;&#35813;&#30028;&#23545;&#32622;&#20449;&#21442;&#25968;&#21644;&#20381;&#36182;&#24230;&#37327;&#30340;&#20381;&#36182;&#20851;&#31995;&#30456;&#27604;&#25991;&#29486;&#20013;&#20808;&#21069;&#30340;&#30028;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#23558;&#25152;&#26377;&#32467;&#26524;&#25512;&#24191;&#21040;&#26399;&#26395;&#20445;&#35777;&#21644;&#21333;&#27425;&#25277;&#26679;PAC-Bayes&#20013;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#23427;&#33719;&#24471;&#20102;[2]&#20013;&#38024;&#23545;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;PAC-Bayes&#24555;&#36895;&#36895;&#29575;&#30028;&#30340;&#31867;&#20284;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16681v1 Announce Type: cross  Abstract: This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \to \infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.
&lt;/p&gt;</description></item><item><title>&#23545;&#31216;&#22522;&#30784;&#21367;&#31215;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#20998;&#31163;&#22522;&#30784;&#20989;&#25968;&#30340;&#36830;&#32493;&#21367;&#31215;&#30340;&#36890;&#29992;&#20844;&#24335;&#20316;&#20026;&#29616;&#26377;&#26041;&#27861;&#30340;&#36229;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#22823;&#37327;&#22522;&#30784;&#20989;&#25968;&#22312;&#19981;&#21516;&#27969;&#20307;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#20598;&#25968;&#21644;&#22855;&#25968;&#23545;&#31216;&#24615;&#26159;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.16680</link><description>&lt;p&gt;
&#23545;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#21147;&#23398;&#30340;&#23545;&#31216;&#22522;&#30784;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16680
&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#22522;&#30784;&#21367;&#31215;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#20998;&#31163;&#22522;&#30784;&#20989;&#25968;&#30340;&#36830;&#32493;&#21367;&#31215;&#30340;&#36890;&#29992;&#20844;&#24335;&#20316;&#20026;&#29616;&#26377;&#26041;&#27861;&#30340;&#36229;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#22823;&#37327;&#22522;&#30784;&#20989;&#25968;&#22312;&#19981;&#21516;&#27969;&#20307;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#20598;&#25968;&#21644;&#22855;&#25968;&#23545;&#31216;&#24615;&#26159;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29289;&#29702;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#35768;&#22810;&#26368;&#26032;&#30740;&#31350;&#24037;&#20316;&#30340;&#19968;&#20010;&#22522;&#26412;&#21644;&#26680;&#24515;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#22522;&#30784;&#30340;&#27969;&#20307;&#21147;&#23398;&#12290;&#32463;&#20856;&#25968;&#20540;&#27714;&#35299;&#22120;&#20256;&#32479;&#19978;&#22312;&#36870;&#38382;&#39064;&#20013;&#35745;&#31639;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#31070;&#32463;&#27714;&#35299;&#22120;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#20998;&#31163;&#22522;&#30784;&#20989;&#25968;&#30340;&#36830;&#32493;&#21367;&#31215;&#30340;&#36890;&#29992;&#20844;&#24335;&#20316;&#20026;&#29616;&#26377;&#26041;&#27861;&#30340;&#36229;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#22823;&#37327;&#22522;&#30784;&#20989;&#25968;&#22312;&#65288;a&#65289;&#21487;&#21387;&#32553;1D SPH&#27169;&#25311;&#65292;&#65288;b&#65289;&#24369;&#21487;&#21387;&#32553;2D SPH&#27169;&#25311;&#21644;&#65288;c&#65289;&#19981;&#21487;&#21387;&#32553;2D SPH&#27169;&#25311;&#20013;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#30784;&#20989;&#25968;&#20013;&#21253;&#25324;&#30340;&#20598;&#23545;&#31216;&#24615;&#21644;&#22855;&#23545;&#31216;&#24615;&#26159;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#22522;&#20110;&#20613;&#37324;&#21494;&#30340;&#36830;&#32493;&#21367;&#31215;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16680v1 Announce Type: new  Abstract: Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning. We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalizat
&lt;/p&gt;</description></item><item><title>DeepGleason&#26159;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#23545;&#21069;&#21015;&#33146;&#30284;&#36827;&#34892;&#26684;&#37324;&#26862;&#20998;&#32423;&#30340;&#31995;&#32479;&#65292;&#22312;tile-wise&#20998;&#31867;&#26041;&#27861;&#21644;ConvNeXt&#26550;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#65292;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.16678</link><description>&lt;p&gt;
DeepGleason: &#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#35780;&#20272;&#21069;&#21015;&#33146;&#30284;&#26684;&#37324;&#26862;&#20998;&#32423;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16678
&lt;/p&gt;
&lt;p&gt;
DeepGleason&#26159;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#23545;&#21069;&#21015;&#33146;&#30284;&#36827;&#34892;&#26684;&#37324;&#26862;&#20998;&#32423;&#30340;&#31995;&#32479;&#65292;&#22312;tile-wise&#20998;&#31867;&#26041;&#27861;&#21644;ConvNeXt&#26550;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#65292;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#20026;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#21644;&#22686;&#24378;&#35786;&#26029;&#24037;&#20316;&#27969;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26426;&#20250;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#21160;&#26684;&#37324;&#26862;&#20998;&#32423;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#35770;&#21644;&#27169;&#22411;&#21487;&#37325;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepGleason&#65306;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#21033;&#29992;&#21069;&#21015;&#33146;&#32452;&#32455;&#20999;&#29255;&#30340;&#20840;&#20999;&#29255;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#26684;&#37324;&#26862;&#20998;&#32423;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#37319;&#29992;&#26631;&#20934;&#21270;&#30340;AUCMEDI&#26694;&#26550;&#65292;&#37319;&#29992;&#20197;&#29943;&#30742;&#20026;&#21333;&#20301;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#31934;&#35843;&#30340;&#22270;&#20687;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;ConvNeXt&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#19982;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#19968;&#32452;&#20849;369&#24352;&#21069;&#21015;&#33146;&#30284;&#20999;&#29255;&#30340;34,264&#20010;&#24102;&#27880;&#37322;&#29943;&#30742;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DeepGle
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16678v1 Announce Type: cross  Abstract: Advances in digital pathology and artificial intelligence (AI) offer promising opportunities for clinical decision support and enhancing diagnostic workflows. Previous studies already demonstrated AI's potential for automated Gleason grading, but lack state-of-the-art methodology and model reusability. To address this issue, we propose DeepGleason: an open-source deep neural network based image classification system for automated Gleason grading using whole-slide histopathology images from prostate tissue sections. Implemented with the standardized AUCMEDI framework, our tool employs a tile-wise classification approach utilizing fine-tuned image preprocessing techniques in combination with a ConvNeXt architecture which was compared to various state-of-the-art architectures. The neural network model was trained and validated on an in-house dataset of 34,264 annotated tiles from 369 prostate carcinoma slides. We demonstrated that DeepGle
&lt;/p&gt;</description></item><item><title>FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16677</link><description>&lt;p&gt;
FOOL: &#29992;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#35299;&#20915;&#21355;&#26143;&#35745;&#31639;&#20013;&#30340;&#19979;&#34892;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16677
&lt;/p&gt;
&lt;p&gt;
FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20256;&#24863;&#22120;&#30340;&#32435;&#21355;&#26143;&#26143;&#24231;&#25429;&#33719;&#22823;&#33539;&#22260;&#22320;&#29702;&#21306;&#22495;&#65292;&#20026;&#22320;&#29699;&#35266;&#27979;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#38543;&#30528;&#26143;&#24231;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20105;&#29992;&#24418;&#25104;&#20102;&#19979;&#34892;&#29942;&#39048;&#12290;&#36712;&#36947;&#36793;&#32536;&#35745;&#31639;&#65288;OEC&#65289;&#21033;&#29992;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#36164;&#28304;&#36890;&#36807;&#22312;&#28304;&#22836;&#22788;&#29702;&#21407;&#22987;&#25429;&#33719;&#26469;&#20943;&#23569;&#20256;&#36755;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#31895;&#31961;&#30340;&#36807;&#28388;&#26041;&#27861;&#25110;&#36807;&#20998;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FOOL&#65292;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20445;&#30041;&#39044;&#27979;&#24615;&#33021;&#12290;FOOL&#23558;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#20998;&#21306;&#65292;&#20197;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#23884;&#20837;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#36739;&#20302;&#30340;&#24320;&#38144;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#12290;&#34429;&#28982;FOOL&#26159;&#19968;&#31181;&#29305;&#24449;&#21387;&#32553;&#22120;&#65292;&#20294;&#23427;&#21487;&#20197;&#22312;&#20302;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16677v1 Announce Type: new  Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at low
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.16674</link><description>&lt;p&gt;
&#29702;&#35299;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Understanding the Functional Roles of Modelling Components in Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16674
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#33041;&#31070;&#32463;&#22238;&#36335;&#21551;&#21457;&#65292;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#23454;&#29616;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#29983;&#29289;&#20445;&#30495;&#24230;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;SNNs&#30456;&#24403;&#22256;&#38590;&#65292;&#22240;&#20026;&#20854;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;&#20173;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;&#32463;&#20856;&#27169;&#22411;&#30340;&#20960;&#20010;&#21464;&#20307;&#65292;&#25105;&#20204;&#31995;&#32479;&#30740;&#31350;&#20102;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#36825;&#20123;&#20851;&#38190;&#24314;&#27169;&#32452;&#20214;&#22312;&#22522;&#20110;&#28431;&#31215;&#20998;&#25918;&#30005;&#65288;LIF&#65289;&#30340;SNNs&#20013;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#20123;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;SNNs&#30340;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#28388;&#27844;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#37325;&#32622;&#26426;&#21046;&#23545;&#20110;&#19981;&#38388;&#26029;&#30340;&#26102;&#38388;&#22788;&#29702;&#21644;&#35745;&#31639;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24490;&#29615;&#21017;&#20016;&#23500;&#20102;&#27169;&#22411;&#22797;&#26434;&#21160;&#24577;&#30340;&#33021;&#21147;&#65292;&#20294;&#20250;&#25439;&#23475;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16674v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphAug&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#29983;&#25104;&#21435;&#22122;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#21644;GNN&#26550;&#26500;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16656</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph Augmentation for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphAug&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#29983;&#25104;&#21435;&#22122;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#21644;GNN&#26550;&#26500;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph augmentation&#19982;&#23545;&#27604;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20986;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#29992;&#25143;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#29616;&#26377;&#30340;GCL&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#25512;&#33616;&#29615;&#22659;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23545;&#27604;&#23398;&#20064;&#20013;&#24573;&#30053;&#25968;&#25454;&#22122;&#22768;&#21487;&#33021;&#23548;&#33268;&#22024;&#26434;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#20174;&#32780;&#38477;&#20302;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#38750;&#33258;&#36866;&#24212;&#20449;&#24687;&#20256;&#36882;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;GraphAug&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#65292;&#29983;&#25104;&#21435;&#22122;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;GraphAug&#26694;&#26550;&#36824;&#34701;&#20837;&#20102;&#22270;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16656v1 Announce Type: new  Abstract: Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;Slide&#25439;&#22833;&#20989;&#25968;&#65288;$\ell_s$&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\ell_s$-SVM&#20998;&#31867;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20256;&#32479;SVM&#22312;&#36793;&#32536;&#27491;&#30830;&#20998;&#31867;&#26679;&#26412;&#24809;&#32602;&#31243;&#24230;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#25552;&#21319;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16654</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#20998;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
A Novel Loss Function-based Support Vector Machine for Binary Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;Slide&#25439;&#22833;&#20989;&#25968;&#65288;$\ell_s$&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\ell_s$-SVM&#20998;&#31867;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20256;&#32479;SVM&#22312;&#36793;&#32536;&#27491;&#30830;&#20998;&#31867;&#26679;&#26412;&#24809;&#32602;&#31243;&#24230;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#25552;&#21319;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21253;&#25324;0/1&#25439;&#22833;SVM&#12289;&#38128;&#38142;&#25439;&#22833;SVM&#12289;&#22369;&#24230;&#25439;&#22833;SVM&#12289;&#25130;&#26029;&#38024;&#22443;&#25439;&#22833;SVM&#31561;&#65292;&#22312;&#27491;&#30830;&#20998;&#31867;&#30340;&#26679;&#26412;&#22312;&#36793;&#38469;&#20869;&#30340;&#24809;&#32602;&#31243;&#24230;&#19978;&#23384;&#22312;&#19968;&#23450;&#30340;&#30095;&#24573;&#12290;&#36825;&#31181;&#30095;&#24573;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;SVM&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#32622;&#20449;&#36793;&#38469;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;Slide&#25439;&#22833;&#20989;&#25968;&#65288;$\ell_s$&#65289;&#26469;&#26500;&#24314;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65288;$\ell_s$-SVM&#65289;&#12290;&#36890;&#36807;&#24341;&#20837;&#36817;&#31471;&#31283;&#23450;&#28857;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;Lipschitz&#36830;&#32493;&#24615;&#30340;&#23646;&#24615;&#65292;&#25512;&#23548;&#20986;&#20102;$\ell_s$-SVM&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;$\ell_s$&#25903;&#25345;&#21521;&#37327;&#21644;$\ell_s$-SVM&#30340;&#24037;&#20316;&#38598;&#12290;&#20026;&#20102;&#39640;&#25928;&#22788;&#29702;$\ell_s$-SVM&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#24037;&#20316;&#38598;&#30340;&#24555;&#36895;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#65288;$\ell_s$-ADMM&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16654v1 Announce Type: new  Abstract: The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\ell_s$) to construct the support vector machine classifier($\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\ell_s$-SVM. Based on this, we define the $\ell_s$ support vectors and working set of $\ell_s$-SVM. To efficiently handle $\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\ell_s$-ADMM), and provide the convergence analysis. The numerical 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SIM-FSVGD&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#20808;&#39564;&#65292;&#25104;&#21151;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;&#65292;&#33021;&#22815;&#22312;&#20302;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#24615;&#33021;&#36187;&#36710;&#31995;&#32479;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16644</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Sim-to-Real Gap with Bayesian Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16644
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SIM-FSVGD&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#20808;&#39564;&#65292;&#25104;&#21151;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;&#65292;&#33021;&#22815;&#22312;&#20302;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#24615;&#33021;&#36187;&#36710;&#31995;&#32479;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SIM-FSVGD&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;SIM-FSVGD&#21033;&#29992;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#20808;&#39564;&#65292;&#22914;&#27169;&#25311;&#22120;&#30340;&#24418;&#24335;&#65292;&#26469;&#35268;&#33539;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#24050;&#32463;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#21147;&#23398;&#65292;SIM-FSVGD&#22312;&#26356;&#22810;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#33021;&#22815;&#25193;&#23637;&#21644;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23398;&#20064;&#38544;&#24335;&#29289;&#29702;&#20808;&#39564;&#23548;&#33268;&#20934;&#30830;&#30340;&#24179;&#22343;&#27169;&#22411;&#20272;&#35745;&#20197;&#21450;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SIM-FSVGD&#22312;&#39640;&#24615;&#33021;RC&#36187;&#36710;&#31995;&#32479;&#19978;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#24046;&#36317;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39640;&#24230;&#21160;&#24577;&#30340;&#20572;&#36710;&#36716;&#21521;&#21160;&#20316;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#29616;&#26377;&#25216;&#26415;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16644v1 Announce Type: cross  Abstract: We present SIM-FSVGD for learning robot dynamics from data. As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26356;&#22909;&#22320;&#25429;&#25417;&#22797;&#26434;&#30340;&#22270;&#20687;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.16640</link><description>&lt;p&gt;
&#20351;&#29992;GAN&#36827;&#34892;CT&#21435;&#22122;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Texture Loss for CT denoising with GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26356;&#22909;&#22320;&#25429;&#25417;&#22797;&#26434;&#30340;&#22270;&#20687;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21435;&#22122;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GAN&#30340;&#21435;&#22122;&#31639;&#27861;&#20173;&#28982;&#23384;&#22312;&#25429;&#25417;&#22270;&#20687;&#20869;&#22797;&#26434;&#20851;&#31995;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25484;&#25569;&#39640;&#24230;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30340;&#32441;&#29702;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#65288;GLCM&#65289;&#22266;&#26377;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#20998;&#31867;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#20551;&#35774;&#23558;&#20854;&#20449;&#24687;&#20869;&#23481;&#25972;&#21512;&#21040;GANs&#30340;&#35757;&#32451;&#20013;&#20250;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#30340;GLCM&#30340;&#21487;&#24494;&#20998;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16640v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging. However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimiza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#23884;&#20837;&#27169;&#22411;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20855;&#20307;&#25506;&#35752;&#20102;Sentence Transformers (SBERT) &#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16630</link><description>&lt;p&gt;
&#19987;&#21033;&#30456;&#20284;&#24615;&#23884;&#20837;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A comparative analysis of embedding models for patent similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#23884;&#20837;&#27169;&#22411;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20855;&#20307;&#25506;&#35752;&#20102;Sentence Transformers (SBERT) &#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#19987;&#21033;&#30456;&#20284;&#24615;&#39046;&#22495;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#29305;&#23450;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#65288;&#22914;word2vec&#21644;doc2vec&#27169;&#22411;&#65289;&#21644;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#27169;&#22411;&#65288;&#22914;&#22522;&#20110;transformers&#30340;&#27169;&#22411;&#65289;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20854;&#27425;&#65292;&#23427;&#20855;&#20307;&#27604;&#36739;&#20102;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;Sentence Transformers&#65288;SBERT&#65289;&#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20851;&#20110;&#19987;&#21033;&#24178;&#28041;&#30340;&#20449;&#24687;&#65292;&#21363;&#20004;&#20010;&#25110;&#22810;&#20010;&#19987;&#21033;&#30003;&#35831;&#20013;&#30340;&#19987;&#21033;&#35201;&#27714;&#34987;&#19987;&#21033;&#23457;&#26597;&#21592;&#35777;&#26126;&#23384;&#22312;&#37325;&#21472;&#30340;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#24178;&#28041;&#26696;&#20363;&#35270;&#20026;&#20004;&#20010;&#19987;&#21033;&#20043;&#38388;&#30340;&#26368;&#22823;&#30456;&#20284;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#29992;&#23427;&#20204;&#20316;&#20026;&#22522;&#20934;&#26469;&#35780;&#20272;&#19981;&#21516;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16630v1 Announce Type: new  Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26657;&#20934;&#36125;&#21494;&#26031;UNet++&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#21644;&#26356;&#28165;&#26224;&#30340;&#27425;&#23395;&#33410;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22914;&#22825;&#27668;&#39044;&#27979;&#21592;&#26469;&#35828;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.16612</link><description>&lt;p&gt;
&#20026;&#27425;&#23395;&#33410;&#39044;&#27979;&#26657;&#20934;&#36125;&#21494;&#26031;UNet++
&lt;/p&gt;
&lt;p&gt;
Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16612
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26657;&#20934;&#36125;&#21494;&#26031;UNet++&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#21644;&#26356;&#28165;&#26224;&#30340;&#27425;&#23395;&#33410;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22914;&#22825;&#27668;&#39044;&#27979;&#21592;&#26469;&#35828;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23395;&#33410;&#24615;&#39044;&#27979;&#22312;&#26816;&#27979;&#30001;&#27668;&#20505;&#21464;&#21270;&#24341;&#36215;&#30340;&#26497;&#31471;&#28909;&#21644;&#23506;&#20919;&#26102;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#23545;&#39044;&#27979;&#30340;&#20449;&#24515;&#24212;&#24403;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#19968;&#24180;&#20013;&#28201;&#24230;&#30340;&#24494;&#23567;&#22686;&#21152;&#23545;&#19990;&#30028;&#26377;&#30528;&#24040;&#22823;&#24433;&#21709;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26657;&#20934;&#25552;&#20379;&#20102;&#19968;&#31181;&#30830;&#20445;&#25105;&#20204;&#23545;&#39044;&#27979;&#30340;&#20449;&#24515;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#26657;&#20934;&#22238;&#24402;&#27169;&#22411;&#26159;&#19968;&#20010;&#30740;&#31350;&#19981;&#36275;&#30340;&#35805;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#32773;&#20013;&#12290;&#25105;&#20204;&#26657;&#20934;&#20102;&#22522;&#20110;UNet++&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34987;&#35777;&#26126;&#22312;&#28201;&#24230;&#24322;&#24120;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#26657;&#20934;&#35823;&#24046;&#20043;&#38388;&#30053;&#24494;&#26435;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#21644;&#26356;&#28165;&#26224;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26657;&#20934;&#24212;&#24403;&#25104;&#20026;&#35832;&#22914;&#22825;&#27668;&#39044;&#25253;&#21592;&#31561;&#23433;&#20840;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16612v1 Announce Type: new  Abstract: Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change. Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world. Calibration of the neural networks provides a way to ensure our confidence in the predictions. However, calibrating regression models is an under-researched topic, especially in forecasters. We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies. We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts. We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#20998;&#20139;&#30340;&#20998;&#24067;&#24335;&#21327;&#20316;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#26041;&#27861;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#23884;&#20837;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#23545;&#20854;&#36827;&#34892;&#32858;&#21512;&#65292;&#36890;&#36807;&#24322;&#24120;&#26292;&#38706;&#26469;&#25191;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;6.8%&#30340;AUC&#12290;</title><link>https://arxiv.org/abs/2403.16610</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#20998;&#20139;&#30340;&#20998;&#24067;&#24335;&#21327;&#20316;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distributed collaborative anomalous sound detection by embedding sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16610
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#20998;&#20139;&#30340;&#20998;&#24067;&#24335;&#21327;&#20316;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#26041;&#27861;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#23884;&#20837;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#23545;&#20854;&#36827;&#34892;&#32858;&#21512;&#65292;&#36890;&#36807;&#24322;&#24120;&#26292;&#38706;&#26469;&#25191;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;6.8%&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24320;&#21457;&#19968;&#31181;&#26426;&#22120;&#22768;&#38899;&#30417;&#27979;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#24322;&#24120;&#22768;&#38899;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22810;&#20010;&#23458;&#25143;&#31471;&#21512;&#20316;&#23398;&#20064;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#24444;&#27492;&#30340;&#21407;&#22987;&#25968;&#25454;&#31169;&#23494;&#12290;&#22312;&#24037;&#19994;&#26426;&#22120;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25317;&#26377;&#26469;&#33258;&#19981;&#21516;&#26426;&#22120;&#25110;&#19981;&#21516;&#36816;&#34892;&#29366;&#24577;&#30340;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#25110;&#20998;&#24067;&#24335;&#23398;&#20064;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#20026;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;&#32780;&#24320;&#21457;&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#23884;&#20837;&#65292;&#28982;&#21518;&#22312;&#26381;&#21153;&#22120;&#19978;&#23545;&#36825;&#20123;&#35745;&#31639;&#24471;&#21040;&#30340;&#23884;&#20837;&#36827;&#34892;&#32858;&#21512;&#65292;&#36890;&#36807;&#24322;&#24120;&#26292;&#38706;&#26469;&#25191;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;AUC&#24179;&#22343;&#25552;&#39640;&#20102;6.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16610v1 Announce Type: cross  Abstract: To develop a machine sound monitoring system, a method for detecting anomalous sound is proposed. In this paper, we explore a method for multiple clients to collaboratively learn an anomalous sound detection model while keeping their raw data private from each other. In the context of industrial machine anomalous sound detection, each client possesses data from different machines or different operational states, making it challenging to learn through federated learning or split learning. In our proposed method, each client calculates embeddings using a common pre-trained model developed for sound data classification, and these calculated embeddings are aggregated on the server to perform anomalous sound detection through outlier exposure. Experiments showed that our proposed method improves the AUC of anomalous sound detection by an average of 6.8%.
&lt;/p&gt;</description></item><item><title>Style Filter&#26159;&#19968;&#31181;&#22312;&#24037;&#19994;&#39046;&#22495;&#20013;&#37327;&#36523;&#23450;&#21046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30693;&#35782;&#36716;&#31227;&#20043;&#21069;&#36873;&#25321;&#24615;&#22320;&#36807;&#28388;&#28304;&#22495;&#25968;&#25454;&#65292;&#22312;&#20943;&#23569;&#25968;&#25454;&#37327;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#25110;&#25552;&#21319;&#20102;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16607</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#26684;&#28388;&#27874;&#22686;&#24378;&#24037;&#19994;&#36801;&#31227;&#23398;&#20064;&#65306;&#25104;&#26412;&#38477;&#20302;&#21644;&#32570;&#38519;&#32858;&#28966;
&lt;/p&gt;
&lt;p&gt;
Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16607
&lt;/p&gt;
&lt;p&gt;
Style Filter&#26159;&#19968;&#31181;&#22312;&#24037;&#19994;&#39046;&#22495;&#20013;&#37327;&#36523;&#23450;&#21046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30693;&#35782;&#36716;&#31227;&#20043;&#21069;&#36873;&#25321;&#24615;&#22320;&#36807;&#28388;&#28304;&#22495;&#25968;&#25454;&#65292;&#22312;&#20943;&#23569;&#25968;&#25454;&#37327;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#25110;&#25552;&#21319;&#20102;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#39046;&#22495;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#19979;&#65292;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#33539;&#24335;&#24212;&#36816;&#32780;&#29983;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Style Filter&#65292;&#19968;&#31181;&#20026;&#24037;&#19994;&#29615;&#22659;&#37327;&#36523;&#23450;&#21046;&#30340;&#26041;&#27861;&#35770;&#12290;&#36890;&#36807;&#22312;&#30693;&#35782;&#36716;&#31227;&#20043;&#21069;&#26377;&#36873;&#25321;&#24615;&#22320;&#36807;&#28388;&#28304;&#22495;&#25968;&#25454;&#65292;Style Filter&#20943;&#23569;&#20102;&#25968;&#25454;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25552;&#21319;&#20102;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;Style Filter&#25552;&#20379;&#26080;&#38656;&#26631;&#31614;&#25805;&#20316;&#65292;&#26368;&#23567;&#21270;&#20808;&#21069;&#30693;&#35782;&#20381;&#36182;&#65292;&#29420;&#31435;&#20110;&#29305;&#23450;&#27169;&#22411;&#65292;&#24182;&#21487;&#37325;&#22797;&#21033;&#29992;&#65292;&#26412;&#25991;&#22312;&#30495;&#23454;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#22312;&#24120;&#35268;&#36801;&#31227;&#31574;&#30053;&#20043;&#21069;&#20351;&#29992;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;Style Filter&#22312;&#30495;&#23454;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16607v1 Announce Type: new  Abstract: Addressing the challenge of data scarcity in industrial domains, transfer learning emerges as a pivotal paradigm. This work introduces Style Filter, a tailored methodology for industrial contexts. By selectively filtering source domain data before knowledge transfer, Style Filter reduces the quantity of data while maintaining or even enhancing the performance of transfer learning strategy. Offering label-free operation, minimal reliance on prior knowledge, independence from specific models, and re-utilization, Style Filter is evaluated on authentic industrial datasets, highlighting its effectiveness when employed before conventional transfer strategies in the deep learning domain. The results underscore the effectiveness of Style Filter in real-world industrial applications.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#20998;&#27495;&#24341;&#23548;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;EDUE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#22320;&#38754;&#23454;&#20917;&#27880;&#37322;&#30340;&#21464;&#24322;&#24615;&#26469;&#24341;&#23548;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#20197;&#25552;&#39640;&#26657;&#20934;&#20449;&#24515;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;55%&#21644;23%&#30340;&#30456;&#20851;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16594</link><description>&lt;p&gt;
EDUE:&#19987;&#23478;&#20998;&#27495;&#24341;&#23548;&#30340;&#19968;&#27425;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16594
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#20998;&#27495;&#24341;&#23548;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;EDUE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#22320;&#38754;&#23454;&#20917;&#27880;&#37322;&#30340;&#21464;&#24322;&#24615;&#26469;&#24341;&#23548;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#20197;&#25552;&#39640;&#26657;&#20934;&#20449;&#24515;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;55%&#21644;23%&#30340;&#30456;&#20851;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#20110;&#39044;&#27979;&#24615;&#33021;&#21644;&#20854;&#20182;&#37325;&#35201;&#22240;&#32032;&#65292;&#22914;&#20256;&#36798;&#21487;&#20449;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#26041;&#27861;&#20026;&#35780;&#20272;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#25552;&#39640;&#27169;&#22411;&#32622;&#20449;&#24230;&#26657;&#20934;&#25552;&#20379;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#23545;UE&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#22914;&#38656;&#35201;&#26126;&#30830;&#25429;&#33719;&#36866;&#24403;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39046;&#22495;&#19987;&#23478;&#20043;&#38388;&#30340;&#29616;&#23454;&#20998;&#27495;&#36827;&#34892;&#23545;&#40784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#20998;&#27495;&#24341;&#23548;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;EDUE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#35780;&#20998;&#32773;&#30340;&#22320;&#38754;&#23454;&#20917;&#27880;&#37322;&#20013;&#30340;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#23548;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#26657;&#20934;&#20449;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19982;&#22270;&#20687;&#19987;&#23478;&#20998;&#27495;&#24179;&#22343;&#30456;&#20851;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;55%&#21644;23%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16594v1 Announce Type: cross  Abstract: Deploying deep learning (DL) models in medical applications relies on predictive performance and other critical factors, such as conveying trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide potential solutions for evaluating prediction reliability and improving the model confidence calibration. Despite increasing interest in UE, challenges persist, such as the need for explicit methods to capture aleatoric uncertainty and align uncertainty estimates with real-life disagreements among domain experts. This paper proposes an Expert Disagreement-Guided Uncertainty Estimation (EDUE) for medical image segmentation. By leveraging variability in ground-truth annotations from multiple raters, we guide the model during training and incorporate random sampling-based strategies to enhance calibration confidence. Our method achieves 55% and 23% improvement in correlation on average with expert disagreements at the image
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16582</link><description>&lt;p&gt;
&#22312;&#21033;&#29992;&#20840;&#29699;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20316;&#29289;&#20998;&#31867;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20339;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16582
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20998;&#31867;&#22312;&#30740;&#31350;&#20316;&#29289;&#27169;&#24335;&#21464;&#21270;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#30899;&#22266;&#23384;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36827;&#34892;&#39044;&#27979;&#26102;&#65292;&#21033;&#29992;&#21508;&#31181;&#26102;&#38388;&#25968;&#25454;&#28304;&#26159;&#24517;&#35201;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32423;&#34920;&#31034;&#20197;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#29486;&#23545;&#22810;&#35270;&#22270;&#23398;&#20064;&#65288;MVL&#65289;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#25351;&#23548;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#25506;&#32034;&#20855;&#26377;&#29305;&#23450;&#32534;&#30721;&#22120;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#22312;&#23616;&#37096;&#22320;&#21306;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20892;&#30000;&#22303;&#22320;&#21644;&#20316;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#26102;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16582v1 Announce Type: cross  Abstract: Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20013;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29702;&#24615;&#32467;&#26500;&#21644;&#33391;&#22909;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16576</link><description>&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20013;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29702;&#24615;&#32467;&#26500;&#21644;&#33391;&#22909;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#21508;&#31181;&#39046;&#22495;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22914;&#27835;&#30103;&#21644;&#29983;&#29289;&#23398;&#65292;&#30001;&#20110;&#20854;&#38169;&#32508;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20316;&#20026;&#19968;&#20010;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#29702;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#21033;&#29992;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#25239;&#20307;&#20013;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;&#20102;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#24341;&#23548;&#29983;&#25104;&#26082;&#20855;&#26377;&#21512;&#29702;&#32467;&#26500;&#21448;&#20855;&#26377;&#26126;&#26174;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27531;&#22522;&#32423;&#20998;&#35299;&#33021;&#37327;&#20559;&#22909;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#26799;&#24230;&#25163;&#26415;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#33021;&#37327;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#20363;&#22914;&#21560;&#24341;&#21644;&#26021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16576v1 Announce Type: cross  Abstract: Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repu
&lt;/p&gt;</description></item><item><title>NSINA&#26159;&#20026;&#35299;&#20915;&#20711;&#20285;&#32599;&#35821;&#20013;LLMs&#36866;&#24212;&#24615;&#25361;&#25112;&#32780;&#24341;&#20837;&#30340;&#26368;&#22823;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20026;&#25913;&#36827;&#35813;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#21644;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.16571</link><description>&lt;p&gt;
NSINA&#65306;&#29992;&#20110;&#20711;&#20285;&#32599;&#35821;&#30340;&#26032;&#38395;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NSINA: A News Corpus for Sinhala
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16571
&lt;/p&gt;
&lt;p&gt;
NSINA&#26159;&#20026;&#35299;&#20915;&#20711;&#20285;&#32599;&#35821;&#20013;LLMs&#36866;&#24212;&#24615;&#25361;&#25112;&#32780;&#24341;&#20837;&#30340;&#26368;&#22823;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20026;&#25913;&#36827;&#35813;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#21644;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#36164;&#28304;&#12290;&#23588;&#20854;&#26159;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#20711;&#20285;&#32599;&#35821;&#65289;&#20013;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#23427;&#20204;&#38754;&#20020;&#30528;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;NSINA&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#28909;&#38376;&#20711;&#20285;&#32599;&#35821;&#26032;&#38395;&#32593;&#31449;&#30340;50&#19975;&#22810;&#31687;&#25991;&#31456;&#30340;&#20840;&#38754;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#19977;&#39033;NLP&#20219;&#21153;&#65306;&#26032;&#38395;&#23186;&#20307;&#35782;&#21035;&#12289;&#26032;&#38395;&#31867;&#21035;&#39044;&#27979;&#21644;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#12290;NSINA&#30340;&#21457;&#24067;&#26088;&#22312;&#20026;&#36866;&#24212;&#20711;&#20285;&#32599;&#35821;&#30340;LLMs&#24102;&#26469;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#21644;&#29992;&#20110;&#25913;&#36827;&#20711;&#20285;&#32599;&#35821;NLP&#30340;&#22522;&#20934;&#12290;NSINA&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#20711;&#20285;&#32599;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16571v1 Announce Type: cross  Abstract: The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#23398;&#20064;&#21644;&#23545;&#25239;&#20013;&#30340;&#28431;&#27934;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#35299;&#37322;&#25935;&#24863;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#26469;&#38480;&#21046;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.16569</link><description>&lt;p&gt;
&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#23398;&#20064;&#21644;&#23545;&#25239;&#20013;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#23398;&#20064;&#21644;&#23545;&#25239;&#20013;&#30340;&#28431;&#27934;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#35299;&#37322;&#25935;&#24863;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#26469;&#38480;&#21046;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31574;&#30053;&#22312;&#22686;&#21152;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#20250;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#36974;&#34109;&#25915;&#20987;&#21487;&#20197;&#20005;&#37325;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#65292;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#19981;&#21487;&#35265;&#30340;&#35270;&#35273;&#24037;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#32473;&#30830;&#20445;XAI&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#20026;&#20102;&#20445;&#35777;XAI&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#32479;&#35745;&#20998;&#26512;&#26469;&#31361;&#20986;&#36974;&#34109;&#25915;&#20987;&#21518;CNN&#20869;&#37096;CNN&#26435;&#37325;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#38480;&#21046;&#27492;&#31867;&#25915;&#20987;&#22312;&#35780;&#20272;&#38454;&#27573;&#30340;&#26377;&#25928;&#24615;&#65292;&#36991;&#20813;&#39069;&#22806;&#30340;&#35757;&#32451;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25269;&#24481;&#22823;&#22810;&#25968;&#29616;&#20195;&#30340;&#23545;&#35299;&#37322;&#25935;&#24863;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16569v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedFixer&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#20840;&#23616;&#27169;&#22411;&#21512;&#20316;&#26469;&#26377;&#25928;&#36873;&#25321;&#24178;&#20928;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#22120;&#21644;&#22522;&#20110;&#26679;&#26412;&#20849;&#20139;&#30340;&#21452;&#27169;&#22411;&#26356;&#26032;&#31574;&#30053;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.16561</link><description>&lt;p&gt;
FedFixer&#65306;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#32531;&#35299;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedFixer&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#20840;&#23616;&#27169;&#22411;&#21512;&#20316;&#26469;&#26377;&#25928;&#36873;&#25321;&#24178;&#20928;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#22120;&#21644;&#22522;&#20110;&#26679;&#26412;&#20849;&#20139;&#30340;&#21452;&#27169;&#22411;&#26356;&#26032;&#31574;&#30053;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#22312;&#24615;&#33021;&#19978;&#20005;&#37325;&#20381;&#36182;&#26631;&#31614;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20010;&#20307;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#20998;&#24067;&#36890;&#24120;&#21516;&#26102;&#23384;&#22312;&#22122;&#22768;&#21644;&#24322;&#26500;&#24615;&#12290;&#22312;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;&#20013;&#30001;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#24341;&#36215;&#30340;&#39640;&#25439;&#22833;&#23545;&#21306;&#20998;&#23458;&#25143;&#31471;&#29305;&#23450;&#21644;&#22024;&#26434;&#26631;&#31614;&#26679;&#26412;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#24433;&#21709;&#20102;&#29616;&#26377;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFixer&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#20840;&#23616;&#27169;&#22411;&#21512;&#20316;&#65292;&#20197;&#26377;&#25928;&#36873;&#25321;&#24178;&#20928;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#12290;&#22312;&#21452;&#27169;&#22411;&#20013;&#65292;&#20165;&#22312;&#26412;&#22320;&#32423;&#21035;&#26356;&#26032;&#20010;&#24615;&#21270;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#26679;&#26412;&#26377;&#38480;&#32780;&#23545;&#22122;&#22768;&#25968;&#25454;&#36807;&#24230;&#25311;&#21512;&#65292;&#36827;&#32780;&#24433;&#21709;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20943;&#36731;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16561v1 Announce Type: cross  Abstract: Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BHerd&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26377;&#30410;&#30340;&#23616;&#37096;&#26799;&#24230;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#25910;&#25947;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.16557</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#26377;&#30410;&#30340;&#23616;&#37096;&#26799;&#24230;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BHerd&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26377;&#30410;&#30340;&#23616;&#37096;&#26799;&#24230;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#25910;&#25947;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#22312;&#36890;&#20449;&#32593;&#32476;&#31995;&#32479;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#25968;&#25454;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25928;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#20026;&#21482;&#26377;&#36825;&#20123;&#25968;&#25454;&#26679;&#26412;&#30340;&#19968;&#20010;&#23376;&#38598;&#23545;&#27169;&#22411;&#25910;&#25947;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#23547;&#25214;&#36825;&#20010;&#23376;&#38598;&#65292;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26159;&#30830;&#23450;&#19968;&#20010;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#26469;&#23545;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BHerd&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36873;&#25321;&#26377;&#30410;&#30340;&#23616;&#37096;&#26799;&#24230;&#26469;&#21152;&#36895;FL&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#23616;&#37096;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#26144;&#23556;&#21040;&#23616;&#37096;&#26799;&#24230;&#19978;&#65292;&#24182;&#20351;&#29992;&#25506;&#32650;&#31574;&#30053;&#33719;&#21462;&#26799;&#24230;&#38598;&#21512;&#30340;&#25490;&#21015;&#65292;&#25490;&#21015;&#20013;&#36739;&#20026;&#20808;&#36827;&#30340;&#26799;&#24230;&#38752;&#36817;&#26799;&#24230;&#38598;&#21512;&#30340;&#24179;&#22343;&#20540;&#12290;&#36825;&#20123;&#25490;&#22312;&#21069;&#38754;&#30340;&#26799;&#24230;&#23558;&#34987;&#36873;&#20013;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16557v1 Announce Type: new  Abstract: Federated Learning (FL) is a distributed machine learning framework in communication network systems. However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portion of the gradients will be selected and sent to the server for global
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16542</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Federated Learning with Correlated Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#25552;&#39640;&#25928;&#29992;&#21516;&#26102;&#30830;&#20445;&#36830;&#32493;&#21457;&#24067;&#30340;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#33258;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#27969;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25200;&#21160;&#36845;&#20195;&#20998;&#26512;&#26469;&#25511;&#21046;DP&#22122;&#22768;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20934;&#24378;&#20984;&#26465;&#20214;&#19979;&#22914;&#20309;&#26377;&#25928;&#31649;&#29702;&#26469;&#33258;&#26412;&#22320;&#26356;&#26032;&#30340;&#28418;&#31227;&#35823;&#24046;&#12290;&#22312;$(\epsilon, \delta)$-DP&#39044;&#31639;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25972;&#20010;&#26102;&#38388;&#27573;&#19978;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#37327;&#21270;&#20102;&#20851;&#38190;&#21442;&#25968;&#30340;&#24433;&#21709;&#20197;&#21450;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#24378;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16542v1 Announce Type: new  Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#20174;&#35745;&#25968;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#38750;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#26681;&#39030;&#28857;$X$&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#30830;&#23450;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#12290;</title><link>https://arxiv.org/abs/2403.16523</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#21644;&#36335;&#24452;&#20998;&#26512;&#20174;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16523
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35745;&#25968;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#38750;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#26681;&#39030;&#28857;$X$&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#30830;&#23450;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#25968;&#25968;&#25454;&#22312;&#37329;&#34701;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#31561;&#39046;&#22495;&#20013;&#33258;&#28982;&#20135;&#29983;&#65292;&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#19994;&#22330;&#26223;&#20013;&#21457;&#29616;&#35745;&#25968;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#35745;&#25968;&#25968;&#25454;&#30340;&#19968;&#20010;&#26368;&#24120;&#35265;&#29305;&#24449;&#26159;&#30001;&#20108;&#39033;&#24335;&#31232;&#30095;&#36816;&#31639;&#31526;&#21644;&#29420;&#31435;&#30340;&#27850;&#26494;&#20998;&#24067;&#25551;&#36848;&#30340;&#22266;&#26377;&#20998;&#25903;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#25429;&#25417;&#20102;&#20998;&#25903;&#21644;&#22122;&#22768;&#12290;&#20363;&#22914;&#65292;&#22312;&#20154;&#21475;&#35745;&#25968;&#24773;&#26223;&#20013;&#65292;&#27515;&#20129;&#21644;&#31227;&#27665;&#23545;&#35745;&#25968;&#26377;&#36129;&#29486;&#65292;&#20854;&#20013;&#29983;&#23384;&#36981;&#24490;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;&#31227;&#27665;&#36981;&#24490;&#27850;&#26494;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#21333;&#19968;&#22240;&#26524;&#23545;&#26159;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#65292;&#21363;$X\rightarrow Y$&#21644;$Y\rightarrow X$&#22312;&#20998;&#24067;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;$X$&#26159;&#19968;&#20010;&#26681;&#39030;&#28857;&#65292;&#37027;&#20040;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16523v1 Announce Type: cross  Abstract: Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and
&lt;/p&gt;</description></item><item><title>2024&#24180;&#20154;&#31867;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35770;&#25991;&#25361;&#25112;&#23558;&#25552;&#20379;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20197;&#29702;&#35299;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#37325;&#28857;&#32771;&#34385;&#25968;&#25454;&#22788;&#29702;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.16509</link><description>&lt;p&gt;
2024&#24180;&#20154;&#31867;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35770;&#25991;&#25361;&#25112;&#8212;&#8212;&#25968;&#25454;&#38598;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Human Understanding AI Paper Challenge 2024 -- Dataset Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16509
&lt;/p&gt;
&lt;p&gt;
2024&#24180;&#20154;&#31867;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35770;&#25991;&#25361;&#25112;&#23558;&#25552;&#20379;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20197;&#29702;&#35299;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#37325;&#28857;&#32771;&#34385;&#25968;&#25454;&#22788;&#29702;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16509v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22312;2024&#24180;&#65292;&#25105;&#20204;&#23558;&#20030;&#21150;&#19968;&#22330;&#30740;&#31350;&#35770;&#25991;&#31454;&#36187;&#65288;&#31532;&#19977;&#23626;&#20154;&#31867;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35770;&#25991;&#25361;&#25112;&#36187;&#65289;&#65292;&#26088;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20197;&#29702;&#35299;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#25552;&#20379;&#32473;&#21442;&#36187;&#32773;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#25968;&#25454;&#22788;&#29702;&#21644;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16509v1 Announce Type: new  Abstract: In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life. This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development.
&lt;/p&gt;</description></item><item><title>PathoTune&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#39640;&#25928;&#22320;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.16497</link><description>&lt;p&gt;
PathoTune: &#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#33267;&#30149;&#29702;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
PathoTune: Adapting Visual Foundation Model to Pathological Specialists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16497
&lt;/p&gt;
&lt;p&gt;
PathoTune&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#39640;&#25928;&#22320;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#36208;&#21521;&#39044;&#35757;&#32451;&#24494;&#35843;&#30340;&#26102;&#20195;&#30340;&#21516;&#26102;&#65292;&#30149;&#29702;&#24433;&#20687;&#30340;&#30740;&#31350;&#20063;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#23613;&#31649;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#30149;&#29702;&#22522;&#30784;&#27169;&#22411;&#65292;&#20294;&#22914;&#20309;&#23558;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#19979;&#28216;&#35843;&#25972;&#65292;&#25105;&#20204;&#25552;&#20986;&#23384;&#22312;&#20004;&#20010;&#22495;&#24046;&#36317;&#65292;&#21363;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PathoTune&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#39640;&#25928;&#22320;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#26469;&#35782;&#21035;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#65292;&#20197;&#21450;&#23454;&#20363;&#29305;&#23450;&#30340;&#35270;&#35273;&#25552;&#31034;&#26469;&#32534;&#30721;&#21333;&#20010;&#30149;&#29702;&#22270;&#20687;&#29305;&#24449;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20197;&#34917;&#19969;&#32423;&#21035;&#21644;WSI&#32423;&#21035;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#21333;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16497v1 Announce Type: cross  Abstract: As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality
&lt;/p&gt;</description></item><item><title>LSTTN&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#21382;&#21490;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#65292;&#36890;&#36807;&#25513;&#30721;&#23376;&#24207;&#21015;Transformer&#35299;&#20915;&#20102;&#29616;&#26377;STGNNs&#27169;&#22411;&#21482;&#33021;&#21033;&#29992;&#30701;&#31243;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#27969;&#37327;&#22797;&#26434;&#36235;&#21183;&#21644;&#21608;&#26399;&#29305;&#24449;&#30340;&#20805;&#20998;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.16495</link><description>&lt;p&gt;
LSTTN&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;Transformer&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16495
&lt;/p&gt;
&lt;p&gt;
LSTTN&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#21382;&#21490;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#65292;&#36890;&#36807;&#25513;&#30721;&#23376;&#24207;&#21015;Transformer&#35299;&#20915;&#20102;&#29616;&#26377;STGNNs&#27169;&#22411;&#21482;&#33021;&#21033;&#29992;&#30701;&#31243;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#27969;&#37327;&#22797;&#26434;&#36235;&#21183;&#21644;&#21608;&#26399;&#29305;&#24449;&#30340;&#20805;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36890;&#36807;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#23398;&#20064;&#24102;&#20851;&#38190;&#20449;&#24687;&#30340;&#38271;&#31243;&#20132;&#36890;&#34920;&#31034;&#26159;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32467;&#26500;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;STGNNs&#21482;&#33021;&#21033;&#29992;&#30701;&#31243;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#65307;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#23398;&#20064;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#21644;&#21608;&#26399;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20174;&#38271;&#26399;&#21382;&#21490;&#20132;&#36890;&#31995;&#21015;&#20013;&#25552;&#21462;&#20851;&#38190;&#26102;&#38388;&#20449;&#24687;&#24182;&#33719;&#24471;&#32039;&#20945;&#34920;&#31034;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LSTTN&#65288;Long-Short Term Transformer-based Network&#65289;&#26694;&#26550;&#65292;&#20840;&#38754;&#32771;&#34385;&#21382;&#21490;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#25513;&#30721;&#23376;&#24207;&#21015;Transformer&#26469;&#25512;&#26029;&#26469;&#33258;&#23569;&#37327;&#26410;&#34987;&#25513;&#30721;&#30340;&#23376;&#24207;&#21015;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16495v1 Announce Type: cross  Abstract: Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmask
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Determined Multi-Label Learning&#8221;&#65288;DMLL&#65289;&#30340;&#26032;&#30340;&#26631;&#27880;&#35774;&#32622;&#65292;&#26088;&#22312;&#26377;&#25928;&#20943;&#23569;&#22810;&#26631;&#31614;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.16482</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#25552;&#31034;&#30340;&#30830;&#23450;&#24615;&#22810;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Determined Multi-Label Learning via Similarity-Based Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Determined Multi-Label Learning&#8221;&#65288;DMLL&#65289;&#30340;&#26032;&#30340;&#26631;&#27880;&#35774;&#32622;&#65292;&#26088;&#22312;&#26377;&#25928;&#20943;&#23569;&#22810;&#26631;&#31614;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#65292;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#21516;&#26102;&#19982;&#22810;&#20010;&#31867;&#26631;&#31614;&#30456;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#25910;&#38598;&#23436;&#20840;&#31934;&#30830;&#30340;&#31867;&#26631;&#31614;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#26469;&#35828;&#26159;&#32791;&#26102;&#19988;&#32791;&#21147;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Determined Multi-Label Learning&#8221;&#65288;DMLL&#65289;&#30340;&#26032;&#30340;&#26631;&#27880;&#35774;&#32622;&#65292;&#26088;&#22312;&#26377;&#25928;&#20943;&#23569;&#22810;&#26631;&#31614;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#26631;&#27880;&#25104;&#26412;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#26631;&#27880;&#35774;&#32622;&#20013;&#65292;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#19982;&#19968;&#20010;&#8220;&#30830;&#23450;&#24615;&#26631;&#31614;&#8221;&#65288;&#8220;&#26159;&#8221;&#25110;&#8220;&#21542;&#8221;&#65289;&#30456;&#20851;&#32852;&#65292;&#34920;&#31034;&#35813;&#35757;&#32451;&#23454;&#20363;&#26159;&#21542;&#21253;&#21547;&#25152;&#25552;&#20379;&#30340;&#31867;&#26631;&#31614;&#12290;&#25152;&#25552;&#20379;&#30340;&#31867;&#26631;&#31614;&#20174;&#25972;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#38543;&#26426;&#22343;&#21248;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#21482;&#38656;&#30830;&#23450;&#19968;&#27425;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#26631;&#27880;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16482v1 Announce Type: new  Abstract: In multi-label classification, each training instance is associated with multiple class labels simultaneously. Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications. To alleviate this problem, a novel labeling setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks. In this novel labeling setting, each training instance is associated with a \textit{determined label} (either "Yes" or "No"), which indicates whether the training instance contains the provided class label. The provided class label is randomly and uniformly selected from the whole candidate labels set. Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets. In this paper, we theoretically de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30417;&#30563;&#20449;&#24687;&#30340;&#19979;&#38477;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#25104;&#26412;</title><link>https://arxiv.org/abs/2403.16469</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20943;&#23569;&#26631;&#31614;&#30340;&#38271;&#23614;&#25968;&#25454;&#20013;
&lt;/p&gt;
&lt;p&gt;
Learning from Reduced Labels for Long-Tailed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30417;&#30563;&#20449;&#24687;&#30340;&#19979;&#38477;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#30417;&#30563;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#27880;&#37322;&#36807;&#31243;&#24322;&#24120;&#32791;&#26102;&#19988;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#26159;&#32531;&#35299;&#26631;&#31614;&#25104;&#26412;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#20805;&#20998;&#20445;&#30041;&#23614;&#37096;&#26679;&#26412;&#30340;&#30417;&#30563;&#20449;&#24687;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#12290;&#25152;&#25552;&#20986;&#30340;&#26631;&#31614;&#35774;&#32622;&#19981;&#20165;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30340;&#30417;&#30563;&#20449;&#24687;&#19979;&#38477;&#65292;&#36824;&#20943;&#23569;&#20102;&#19982;&#38271;&#23614;&#25968;&#25454;&#30456;&#20851;&#30340;&#26631;&#31614;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30452;&#35266;&#19988;&#39640;&#25928;&#30340;&#26080;&#20559;&#26694;&#26550;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21487;&#20197;&#20174;&#36825;&#20123;Reduced Labels&#20013;&#23398;&#20064;&#12290;&#22312;&#21253;&#25324;Imag&#22312;&#20869;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16469v1 Announce Type: new  Abstract: Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on benchmark datasets including Imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22686;&#24378;&#26465;&#20214;&#37492;&#21035;&#22120;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22768;&#30721;&#22120;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#37492;&#21035;&#22120;&#23545;&#22686;&#24378;&#25968;&#25454;&#30340;&#19981;&#25935;&#24863;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16464</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#26465;&#20214;&#37492;&#21035;&#22120;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22768;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22686;&#24378;&#26465;&#20214;&#37492;&#21035;&#22120;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22768;&#30721;&#22120;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#37492;&#21035;&#22120;&#23545;&#22686;&#24378;&#25968;&#25454;&#30340;&#19981;&#25935;&#24863;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22768;&#30721;&#22120;&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#24120;&#29992;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24555;&#36895;&#12289;&#36731;&#37327;&#21644;&#39640;&#36136;&#37327;&#30340;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#23548;&#33268;&#39640;&#26114;&#30340;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#35757;&#32451;GAN&#22768;&#30721;&#22120;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#37492;&#21035;&#22120;&#26159;&#26080;&#26465;&#20214;&#30340;&#65292;&#23545;&#20110;&#30001;&#25968;&#25454;&#22686;&#24378;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#19981;&#25935;&#24863;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#21518;&#30340;&#35821;&#38899;&#65288;&#21487;&#33021;&#38750;&#24120;&#20986;&#33394;&#65289;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#35821;&#38899;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26465;&#20214;&#37492;&#21035;&#22120;&#65288;AugCondD&#65289;&#65292;&#38500;&#20102;&#35821;&#38899;&#22806;&#65292;&#23427;&#36824;&#25509;&#25910;&#22686;&#24378;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#26681;&#25454;&#22686;&#24378;&#29366;&#24577;&#35780;&#20272;&#36755;&#20837;&#35821;&#38899;&#65292;&#32780;&#19981;&#25233;&#21046;&#21407;&#22987;&#30340;&#38750;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16464v1 Announce Type: cross  Abstract: A generative adversarial network (GAN)-based vocoder trained with an adversarial discriminator is commonly used for speech synthesis because of its fast, lightweight, and high-quality characteristics. However, this data-driven model requires a large amount of training data incurring high data-collection costs. This fact motivates us to train a GAN-based vocoder on limited data. A promising solution is to augment the training data to avoid overfitting. However, a standard discriminator is unconditional and insensitive to distributional changes caused by data augmentation. Thus, augmented speech (which can be extraordinary) may be considered real speech. To address this issue, we propose an augmentation-conditional discriminator (AugCondD) that receives the augmentation state as input in addition to speech, thereby assessing the input speech according to the augmentation state, without inhibiting the learning of the original non-augmente
&lt;/p&gt;</description></item><item><title>FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16460</link><description>&lt;p&gt;
FedAC&#65306;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16460
&lt;/p&gt;
&lt;p&gt;
FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedAC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#26377;&#25928;&#22320;&#23558;&#20840;&#23616;&#30693;&#35782;&#25972;&#21512;&#21040;&#31751;&#20869;&#23398;&#20064;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32500;&#30340;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65307;&#24182;&#19988;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#22797;&#26434;&#24615;&#30340;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36866;&#24212;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16460v1 Announce Type: cross  Abstract: Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16459</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the rates of convergence for learning with convolutional neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#19968;&#20010;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#26435;&#37325;&#19978;&#26377;&#19968;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#12290;&#31532;&#20108;&#20010;&#32467;&#26524;&#32473;&#20986;&#20102;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#30340;&#26032;&#20998;&#26512;&#65292;&#20854;&#20013;CNNs&#26159;&#20854;&#29305;&#20363;&#12290;&#35813;&#20998;&#26512;&#35814;&#32454;&#32771;&#34385;&#20102;&#26435;&#37325;&#30340;&#22823;&#23567;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#22909;&#30340;&#19978;&#30028;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#22522;&#20110;CNNs&#30340;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#20026;&#22522;&#20110;CNNs&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#24314;&#31435;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#38128;&#38142;&#25439;&#22833;&#21644;&#36923;&#36753;&#25439;&#22833;&#30340;CNN&#20998;&#31867;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#36824;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#26159;&#26497;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.16442</link><description>&lt;p&gt;
&#22914;&#26524;CLIP&#33021;&#35828;&#35805;: &#36890;&#36807;&#23427;&#20204;&#30340;&#39318;&#36873;&#27010;&#24565;&#25551;&#36848;&#29702;&#35299;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16442
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24120;&#24120;&#20551;&#35774;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#34920;&#31034;&#26159;&#22522;&#20110;&#24418;&#29366;&#31561;&#35270;&#35273;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;VLM&#22312;&#34920;&#31034;&#27010;&#24565;&#26102;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#20248;&#20808;&#32771;&#34385;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Extract and Explore&#65288;EX2&#65289;&#65292;&#29992;&#20110;&#21051;&#30011;VLM&#30340;&#37325;&#35201;&#25991;&#26412;&#29305;&#24449;&#12290;EX2&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;VLM&#39318;&#36873;&#39033;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;VLM&#37325;&#35201;&#29305;&#24449;&#30340;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#36825;&#20123;&#25551;&#36848;&#20197;&#30830;&#23450;&#23545;VLM&#34920;&#31034;&#26377;&#36129;&#29486;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25552;&#20379;&#20102;&#27809;&#26377;&#24110;&#21161;&#20449;&#24687;&#30340;&#34394;&#20551;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#21333;&#20987;&#25918;&#22823;&#27010;&#24565;&#30340;&#29031;&#29255;&#65289;&#65292;&#20294;&#22312;VLM&#34920;&#31034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#25551;&#36848;&#20013;&#65292;VLM&#22312;&#34920;&#31034;&#35270;&#35273;&#27010;&#24565;&#26102;&#26174;&#33879;&#20381;&#36182;&#38750;&#35270;&#35273;&#23646;&#24615;&#65288;&#22914;&#26646;&#24687;&#22320;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16442v1 Announce Type: new  Abstract: Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize differen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#22312;&#32447;&#22320;&#22270;&#20272;&#35745;&#26041;&#27861;&#20197;&#39069;&#22806;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#22312;&#32447;&#21046;&#22270;&#19982;&#36712;&#36857;&#39044;&#27979;&#38598;&#25104;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2403.16439</link><description>&lt;p&gt;
&#22312;&#36712;&#36857;&#39044;&#27979;&#20013;&#29983;&#25104;&#21644;&#21033;&#29992;&#22312;&#32447;&#22320;&#22270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Producing and Leveraging Online Map Uncertainty in Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16439
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#22312;&#32447;&#22320;&#22270;&#20272;&#35745;&#26041;&#27861;&#20197;&#39069;&#22806;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#22312;&#32447;&#21046;&#22270;&#19982;&#36712;&#36857;&#39044;&#27979;&#38598;&#25104;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#26224;&#65288;HD&#65289;&#22320;&#22270;&#22312;&#29616;&#20195;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23613;&#31649;&#20276;&#38543;&#30528;&#39640;&#26114;&#30340;&#26631;&#27880;&#21644;&#32500;&#25252;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#32447;&#20272;&#35745;HD&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#20351;AV&#33021;&#22815;&#22312;&#20808;&#21069;&#26410;&#26144;&#23556;&#30340;&#21306;&#22495;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22312;&#32447;&#22320;&#22270;&#20272;&#35745;&#26041;&#27861;&#26159;&#29420;&#31435;&#24320;&#21457;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#38598;&#25104;&#22312;AV&#31995;&#32479;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20204;&#19981;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#25110;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#22312;&#32447;&#22320;&#22270;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39069;&#22806;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#22914;&#20309;&#33021;&#22815;&#26356;&#32039;&#23494;&#22320;&#23558;&#22312;&#32447;&#22320;&#22270;&#21046;&#22270;&#19982;&#36712;&#36857;&#39044;&#27979;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#19981;&#30830;&#23450;&#24615;&#25972;&#21512;&#36827;&#21435;&#21487;&#20197;&#20351;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;50&#65285;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;nuScenes&#39550;&#39542;&#25968;&#25454;&#19978;&#65292;&#39044;&#27979;&#24615;&#33021;&#25552;&#39640;&#39640;&#36798;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16439v1 Announce Type: cross  Abstract: High-definition (HD) maps have played an integral role in the development of modern autonomous vehicle (AV) stacks, albeit with high associated labeling and maintenance costs. As a result, many recent works have proposed methods for estimating HD maps online from sensor data, enabling AVs to operate outside of previously-mapped regions. However, current online map estimation approaches are developed in isolation of their downstream tasks, complicating their integration in AV stacks. In particular, they do not produce uncertainty or confidence estimates. In this work, we extend multiple state-of-the-art online map estimation methods to additionally estimate uncertainty and show how this enables more tightly integrating online mapping with trajectory forecasting. In doing so, we find that incorporating uncertainty yields up to 50% faster training convergence and up to 15% better prediction performance on the real-world nuScenes driving d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;IMLIB&#65292;&#32467;&#21512;&#20102;SAT&#21644;MaxSAT&#26041;&#27861;&#65292;&#38480;&#21046;&#35268;&#21017;&#22823;&#23567;&#20197;&#23454;&#29616;&#24179;&#34913;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16418</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An incremental MaxSAT-based model to learn balanced rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16418
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;IMLIB&#65292;&#32467;&#21512;&#20102;SAT&#21644;MaxSAT&#26041;&#27861;&#65292;&#38480;&#21046;&#35268;&#21017;&#22823;&#23567;&#20197;&#23454;&#29616;&#24179;&#34913;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#20102;&#20247;&#22810;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#24182;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20165;&#20934;&#30830;&#24615;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#36824;&#38656;&#35201;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;MaxSAT&#30340;&#22686;&#37327;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24179;&#34913;&#30340;&#35268;&#21017;&#65292;&#31216;&#20026;IMLIB&#12290;&#36825;&#20010;&#26032;&#27169;&#22411;&#22522;&#20110;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;SAT&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;MaxSAT&#30340;&#12290;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#38480;&#21046;&#20102;&#27599;&#20010;&#29983;&#25104;&#35268;&#21017;&#30340;&#22823;&#23567;&#65292;&#20351;&#24471;&#21487;&#20197;&#24179;&#34913;&#23427;&#20204;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#19968;&#32452;&#35268;&#21017;&#27604;&#19968;&#20010;&#28151;&#21512;&#20102;&#22823;&#35268;&#21017;&#21644;&#23567;&#35268;&#21017;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#22522;&#20110;MaxSAT&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;IMLI&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16418v1 Announce Type: cross  Abstract: The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38477;&#20302;&#25915;&#20987;&#20256;&#36882;&#24615;&#26469;&#22686;&#24378;&#38598;&#25104;&#22810;&#26679;&#24615;&#65292;&#20108;&#38454;&#26799;&#24230;&#20316;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.16405</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#20998;&#25955;&#30340;&#20302;&#26354;&#29575;&#27169;&#22411;&#26469;&#36827;&#34892;&#38598;&#25104;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16405
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38477;&#20302;&#25915;&#20987;&#20256;&#36882;&#24615;&#26469;&#22686;&#24378;&#38598;&#25104;&#22810;&#26679;&#24615;&#65292;&#20108;&#38454;&#26799;&#24230;&#20316;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20197;&#22686;&#24378;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;&#23376;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#22686;&#21152;&#20102;&#27450;&#39575;&#22823;&#22810;&#25968;&#38598;&#25104;&#25152;&#38656;&#30340;&#25915;&#20987;&#25104;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#38477;&#20302;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#26469;&#22686;&#24378;&#38598;&#25104;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25551;&#36848;&#25439;&#22833;&#26354;&#29575;&#30340;&#20108;&#38454;&#26799;&#24230;&#20316;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16405v1 Announce Type: new  Abstract: The integration of an ensemble of deep learning models has been extensively explored to enhance defense against adversarial attacks. The diversity among sub-models increases the attack cost required to deceive the majority of the ensemble, thereby improving the adversarial robustness. While existing approaches mainly center on increasing diversity in feature representations or dispersion of first-order gradients with respect to input, the limited correlation between these diversity metrics and adversarial robustness constrains the performance of ensemble adversarial defense. In this work, we aim to enhance ensemble diversity by reducing attack transferability. We identify second-order gradients, which depict the loss curvature, as a key factor in adversarial robustness. Computing the Hessian matrix involved in second-order gradients is computationally expensive. To address this, we approximate the Hessian-vector product using differentia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedU2&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32479;&#19968;&#27491;&#21017;&#21270;&#22120;&#65288;FUR&#65289;&#21644;&#39640;&#25928;&#30340;&#32479;&#19968;&#32858;&#21512;&#22120;&#65288;EUA&#65289;&#65292;&#22686;&#24378;&#20102;&#22312;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;FUSL&#20013;&#29983;&#25104;&#32479;&#19968;&#21644;&#32479;&#19968;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.16398</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#32852;&#37030;&#38750;IID&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16398
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedU2&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32479;&#19968;&#27491;&#21017;&#21270;&#22120;&#65288;FUR&#65289;&#21644;&#39640;&#25928;&#30340;&#32479;&#19968;&#32858;&#21512;&#22120;&#65288;EUA&#65289;&#65292;&#22686;&#24378;&#20102;&#22312;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;FUSL&#20013;&#29983;&#25104;&#32479;&#19968;&#21644;&#32479;&#19968;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16398v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;&#22312;&#24314;&#27169;&#20998;&#24067;&#24335;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23458;&#25143;&#31471;&#25968;&#25454;&#26631;&#31614;&#19981;&#23436;&#21892;&#65292;&#36825;&#20351;&#24471;&#32852;&#37030;&#38750;IID&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65288;FUSL&#65289;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;FUSL&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#34920;&#31034;&#19981;&#36275;&#30340;&#24433;&#21709;&#65292;&#21363;&#65288;1&#65289;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#23849;&#28291;&#32416;&#32544;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23616;&#37096;&#27169;&#22411;&#20043;&#38388;&#34920;&#31034;&#31354;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#21069;&#32773;&#34920;&#31034;&#23616;&#37096;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23849;&#28291;&#23558;&#38543;&#21518;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#21644;&#20854;&#20182;&#23616;&#37096;&#27169;&#22411;&#12290;&#21518;&#32773;&#24847;&#21619;&#30528;&#30001;&#20110;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#19981;&#19968;&#33268;&#30340;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedU2&#65292;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#22312;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;FUSL&#20013;&#29983;&#25104;&#32479;&#19968;&#21644;&#32479;&#19968;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedU2&#30001;&#28789;&#27963;&#30340;&#32479;&#19968;&#27491;&#21017;&#21270;&#22120;&#65288;FUR&#65289;&#21644;&#39640;&#25928;&#30340;&#32479;&#19968;&#32858;&#21512;&#22120;&#65288;EUA&#65289;&#32452;&#25104;&#12290;&#27599;&#20010;FUR
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16398v1 Announce Type: cross  Abstract: Federated learning achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.16393</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;
&lt;/p&gt;
&lt;p&gt;
Concurrent Linguistic Error Detection (CLED) for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#37319;&#29992;&#20351;&#24471;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#38169;&#35823;&#30340;&#26816;&#27979;&#26159;&#20943;&#36731;&#20854;&#23545;&#31995;&#32479;&#24433;&#21709;&#30340;&#31532;&#19968;&#27493;&#65292;&#22240;&#27492;&#65292;LLMs&#30340;&#39640;&#25928;&#38169;&#35823;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22522;&#20110;&#23545;LLMs&#36755;&#20986;&#36827;&#34892;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#36827;&#34892;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;&#65307;&#35813;&#26041;&#26696;&#25552;&#21462;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#20123;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#36755;&#20837;&#21040;&#19968;&#20010;&#24182;&#21457;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16393v1 Announce Type: new  Abstract: The wide adoption of Large language models (LLMs) makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue. In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes. An interesting observation is that the output of LLMs in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#20272;&#35745;&#26368;&#22823;&#23433;&#20840;&#34892;&#21160;&#30340;&#38271;&#26399;&#23433;&#20840;&#27010;&#29575;&#65292;&#36991;&#20813;&#39118;&#38505;&#36807;&#24230;&#36924;&#36817;&#23548;&#33268;&#30340;&#20445;&#23432;&#34892;&#20026;</title><link>https://arxiv.org/abs/2403.16391</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26368;&#22823;&#23433;&#20840;&#27010;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-informed RL for Maximal Safety Probability Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16391
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#20272;&#35745;&#26368;&#22823;&#23433;&#20840;&#34892;&#21160;&#30340;&#38271;&#26399;&#23433;&#20840;&#27010;&#29575;&#65292;&#36991;&#20813;&#39118;&#38505;&#36807;&#24230;&#36924;&#36817;&#23548;&#33268;&#30340;&#20445;&#23432;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#39118;&#38505;&#37327;&#21270;&#21644;&#21487;&#36798;&#24615;&#20998;&#26512;&#23545;&#20110;&#23433;&#20840;&#25511;&#21046;&#21644;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20174;&#31232;&#26377;&#20107;&#20214;&#12289;&#39118;&#38505;&#29366;&#24577;&#25110;&#38271;&#26399;&#36712;&#36857;&#20013;&#37319;&#26679;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#36275;&#22815;&#35206;&#30422;&#26469;&#33258;&#39118;&#38505;&#29366;&#24577;&#21644;&#38271;&#26399;&#36712;&#36857;&#30340;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26368;&#22823;&#23433;&#20840;&#34892;&#21160;&#30340;&#38271;&#26399;&#23433;&#20840;&#27010;&#29575;&#12290;&#22312;&#25511;&#21046;&#21644;&#23398;&#20064;&#20013;&#20351;&#29992;&#26368;&#22823;&#23433;&#20840;&#27010;&#29575;&#39044;&#35745;&#21487;&#20197;&#36991;&#20813;&#30001;&#20110;&#39118;&#38505;&#36807;&#24230;&#36924;&#36817;&#32780;&#23548;&#33268;&#30340;&#20445;&#23432;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#38271;&#26399;&#23433;&#20840;&#27010;&#29575;&#21487;&#20197;&#36716;&#21270;&#20026;&#21152;&#27861;&#25104;&#26412;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#29575;&#25512;&#23548;&#20026;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#65288;PIRL&#65289;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#31232;&#30095;&#22870;&#21169;&#36827;&#34892;&#23398;&#20064;&#65292;&#22240;&#20026;&#29289;&#29702;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16391v1 Announce Type: cross  Abstract: Accurate risk quantification and reachability analysis are crucial for safe control and learning, but sampling from rare events, risky states, or long-term trajectories can be prohibitively costly. Motivated by this, we study how to estimate the long-term safety probability of maximally safe actions without sufficient coverage of samples from risky states and long-term trajectories. The use of maximal safety probability in control and learning is expected to avoid conservative behaviors due to over-approximation of risk. Here, we first show that long-term safety probability, which is multiplicative in time, can be converted into additive costs and be solved using standard reinforcement learning methods. We then derive this probability as solutions of partial differential equations (PDEs) and propose Physics-Informed Reinforcement Learning (PIRL) algorithm. The proposed method can learn using sparse rewards because the physics constrain
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#26465;&#20214;&#30417;&#27979;&#20449;&#21495;&#39044;&#27979;&#20013;&#23454;&#29616;&#34920;&#31034;&#33021;&#21147;&#21644;&#25935;&#25463;&#24615;&#30340;&#26435;&#34913;</title><link>https://arxiv.org/abs/2403.16377</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#26631;&#31614;&#24863;&#30693;&#30340;&#31070;&#32463;&#36807;&#31243;&#36827;&#34892;&#23454;&#26102;&#36866;&#24212;&#26465;&#20214;&#30417;&#27979;&#20449;&#21495;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#26465;&#20214;&#30417;&#27979;&#20449;&#21495;&#39044;&#27979;&#20013;&#23454;&#29616;&#34920;&#31034;&#33021;&#21147;&#21644;&#25935;&#25463;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#24555;&#36895;&#36866;&#24212;&#23454;&#26102;&#26465;&#20214;&#30417;&#27979;&#65288;CM&#65289;&#20449;&#21495;&#30340;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#24037;&#31243;&#31995;&#32479;/&#21333;&#20803;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#26041;&#27861;&#22312;&#34920;&#31034;&#33021;&#21147;&#21644;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#25935;&#25463;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#26435;&#34913;&#38382;&#39064;&#12290;&#23427;&#23558;CM&#20449;&#21495;&#20013;&#30340;&#21487;&#29992;&#35266;&#27979;&#32534;&#30721;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#37325;&#24314;&#20449;&#21495;&#30340;&#21382;&#21490;&#21644;&#28436;&#21464;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16377v1 Announce Type: new  Abstract: Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for predi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28176;&#36827;&#20132;&#20114;&#32593;&#32476;&#65292;&#20195;&#29702;&#22312;&#23398;&#20064;&#20013;&#36880;&#28176;&#32858;&#28966;&#20110;&#30456;&#20851;&#22320;&#22270;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#30456;&#20851;&#22320;&#22270;&#32422;&#26463;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.16374</link><description>&lt;p&gt;
&#22522;&#20110;&#28176;&#36827;&#20132;&#20114;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16374
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#20132;&#20114;&#32593;&#32476;&#65292;&#20195;&#29702;&#22312;&#23398;&#20064;&#20013;&#36880;&#28176;&#32858;&#28966;&#20110;&#30456;&#20851;&#22320;&#22270;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#30456;&#20851;&#22320;&#22270;&#32422;&#26463;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#26469;&#35828;&#65292;&#20934;&#30830;&#39044;&#27979;&#34892;&#20154;&#12289;&#39569;&#36710;&#32773;&#21644;&#20854;&#20182;&#21608;&#22260;&#36710;&#36742;&#65288;&#32479;&#31216;&#20026;&#20195;&#29702;&#65289;&#30340;&#36816;&#21160;&#36712;&#36857;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20132;&#20114;&#32593;&#32476;&#65292;&#20351;&#20195;&#29702;&#30340;&#29305;&#24449;&#33021;&#22815;&#36880;&#28176;&#32858;&#28966;&#20110;&#30456;&#20851;&#22320;&#22270;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#25429;&#33719;&#30456;&#20851;&#22320;&#22270;&#32422;&#26463;&#30340;&#20195;&#29702;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16374v1 Announce Type: new  Abstract: Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation. However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical traj
&lt;/p&gt;</description></item><item><title>signSGD-FV&#26159;&#19968;&#31181;&#20855;&#26377;&#32852;&#37030;&#25237;&#31080;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#36793;&#32536;&#35774;&#22791;&#30340;&#26435;&#37325;&#20998;&#37197;&#20197;&#20811;&#26381;SignSGD-MV&#22312;&#23567;&#25209;&#37327;&#22823;&#23567;&#19981;&#21516;&#30340;&#24037;&#20316;&#32773;&#38388;&#25910;&#25947;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16372</link><description>&lt;p&gt;
&#20855;&#26377;&#32852;&#37030;&#25237;&#31080;&#30340;SignSGD
&lt;/p&gt;
&lt;p&gt;
SignSGD with Federated Voting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16372
&lt;/p&gt;
&lt;p&gt;
signSGD-FV&#26159;&#19968;&#31181;&#20855;&#26377;&#32852;&#37030;&#25237;&#31080;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#36793;&#32536;&#35774;&#22791;&#30340;&#26435;&#37325;&#20998;&#37197;&#20197;&#20811;&#26381;SignSGD-MV&#22312;&#23567;&#25209;&#37327;&#22823;&#23567;&#19981;&#21516;&#30340;&#24037;&#20316;&#32773;&#38388;&#25910;&#25947;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#36890;&#24120;&#29992;&#20110;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#36793;&#32536;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#24037;&#20316;&#32773;&#21644;&#20013;&#22830;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#38656;&#35201;&#22823;&#37327;&#20449;&#24687;&#20132;&#25442;&#65292;&#36890;&#20449;&#24310;&#36831;&#25104;&#20026;&#29942;&#39048;&#12290;SignSGD&#19982;&#22810;&#25968;&#25237;&#31080;&#65288;signSGD-MV&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#19968;&#20301;&#37327;&#21270;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#24403;&#24037;&#20316;&#32773;&#20043;&#38388;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#19981;&#21516;&#26102;&#65292;&#23427;&#26080;&#27861;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#32852;&#37030;&#25237;&#31080;&#30340;signSGD&#20248;&#21270;&#22120;&#65288;signSGD-FV&#65289;&#12290;&#32852;&#37030;&#25237;&#31080;&#30340;&#29702;&#24565;&#22312;&#20110;&#21033;&#29992;&#21487;&#23398;&#20064;&#26435;&#37325;&#25191;&#34892;&#21152;&#26435;&#22810;&#25968;&#25237;&#31080;&#12290;&#26381;&#21153;&#22120;&#26681;&#25454;&#36793;&#32536;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#22312;&#32447;&#23398;&#20064;&#20998;&#37197;&#32473;&#36825;&#20123;&#35774;&#22791;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16372v1 Announce Type: new  Abstract: Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization. However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24341;&#23548;&#25193;&#25955;&#29983;&#25104;&#30340;&#22522;&#30784;&#26679;&#26412;&#33021;&#20135;&#29983;&#26356;&#24378;&#22823;&#30340;&#27602;&#33647;&#21644;&#21518;&#38376;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.16365</link><description>&lt;p&gt;
&#20174;&#22836;&#24320;&#22987;&#21033;&#29992;&#24341;&#23548;&#25193;&#25955;&#29983;&#25104;&#24378;&#22823;&#30340;&#27602;&#33647;&#21644;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16365
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24341;&#23548;&#25193;&#25955;&#29983;&#25104;&#30340;&#22522;&#30784;&#26679;&#26412;&#33021;&#20135;&#29983;&#26356;&#24378;&#22823;&#30340;&#27602;&#33647;&#21644;&#21518;&#38376;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#24120;&#19981;&#21463;&#20154;&#24037;&#26816;&#26597;&#32780;&#26159;&#36890;&#36807;&#32593;&#32476;&#25235;&#21462;&#24471;&#21040;&#12290;&#30001;&#20110;&#36825;&#31181;&#19981;&#23433;&#20840;&#30340;&#31579;&#36873;&#36807;&#31243;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#21521;&#20114;&#32852;&#32593;&#19978;&#20256;&#24694;&#24847;&#25968;&#25454;&#24182;&#31561;&#24453;&#21463;&#23475;&#32773;&#25235;&#21462;&#21644;&#35757;&#32451;&#23427;&#65292;&#26469;&#27745;&#26579;&#25110;&#26893;&#20837;&#21518;&#38376;&#21040;&#29983;&#25104;&#30340;&#27169;&#22411;&#20013;&#12290;&#29616;&#26377;&#30340;&#21019;&#24314;&#27602;&#33647;&#21644;&#21518;&#38376;&#30340;&#26041;&#27861;&#36890;&#24120;&#20174;&#38543;&#26426;&#25277;&#21462;&#30340;&#24178;&#20928;&#25968;&#25454;&#65288;&#31216;&#20026;&#22522;&#30784;&#26679;&#26412;&#65289;&#24320;&#22987;&#65292;&#28982;&#21518;&#20462;&#25913;&#36825;&#20123;&#26679;&#26412;&#26469;&#29983;&#25104;&#27602;&#33647;&#12290;&#20294;&#26159;&#65292;&#19968;&#20123;&#22522;&#30784;&#26679;&#26412;&#21487;&#33021;&#27604;&#20854;&#20182;&#26356;&#23481;&#26131;&#21463;&#21040;&#27602;&#23475;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#22522;&#30784;&#26679;&#26412;&#26469;&#31934;&#24515;&#21046;&#20316;&#26356;&#24378;&#22823;&#30340;&#27602;&#33647;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24341;&#23548;&#25193;&#25955;&#20174;&#22836;&#24320;&#22987;&#21512;&#25104;&#22522;&#30784;&#26679;&#26412;&#65292;&#36825;&#20123;&#22522;&#30784;&#26679;&#26412;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25915;&#20987;&#20135;&#29983;&#30340;&#26356;&#24378;&#22823;&#30340;&#27602;&#33647;&#21644;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;Guided Diffusion Poisoning (GDP)&#22522;&#30784;&#26679;&#26412;&#21487;&#20197;&#19982;&#20219;&#20309;&#19979;&#28216;&#27602;&#23475;&#25110;&#21518;&#38376;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16365v1 Announce Type: new  Abstract: Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor at
&lt;/p&gt;</description></item><item><title>ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.16354</link><description>&lt;p&gt;
ChatDBG: &#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
ChatDBG: An AI-Powered Debugging Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16354
&lt;/p&gt;
&lt;p&gt;
ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatDBG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;&#12290;ChatDBG&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20256;&#32479;&#35843;&#35797;&#22120;&#30340;&#21151;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;ChatDBG&#20801;&#35768;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#31243;&#24207;&#29366;&#24577;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#23545;&#23849;&#28291;&#25110;&#26029;&#35328;&#22833;&#36133;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#35832;&#22914;&#8220;&#20026;&#20160;&#20040;x&#20026;&#31354;&#65311;&#8221;&#20043;&#31867;&#30340;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;ChatDBG&#25480;&#20104;LLM&#33258;&#20027;&#26435;&#65292;&#36890;&#36807;&#21457;&#20986;&#21629;&#20196;&#26469;&#27983;&#35272;&#22534;&#26632;&#21644;&#26816;&#26597;&#31243;&#24207;&#29366;&#24577;&#36827;&#34892;&#35843;&#35797;&#65307;&#28982;&#21518;&#25253;&#21578;&#20854;&#21457;&#29616;&#24182;&#23558;&#25511;&#21046;&#26435;&#20132;&#36824;&#32473;&#31243;&#24207;&#21592;&#12290;&#25105;&#20204;&#30340;ChatDBG&#21407;&#22411;&#19982;&#26631;&#20934;&#35843;&#35797;&#22120;&#38598;&#25104;&#65292;&#21253;&#25324;LLDB&#12289;GDB&#21644;WinDBG&#29992;&#20110;&#26412;&#22320;&#20195;&#30721;&#20197;&#21450;&#29992;&#20110;Python&#30340;Pdb&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20195;&#30721;&#38598;&#21512;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20855;&#26377;&#24050;&#30693;&#38169;&#35823;&#30340;C/C++&#20195;&#30721;&#21644;&#19968;&#22871;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16354v1 Announce Type: cross  Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code includi
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;CID&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;ChatGPT&#24182;&#35201;&#27714;&#25552;&#20986;&#24773;&#22659;&#30456;&#20284;&#20294;&#25991;&#26412;&#26377;&#20998;&#27495;&#30340;&#38382;&#39064;&#65292;&#26469;&#33258;&#21160;&#27979;&#35797;&#21644;&#26816;&#27979;ChatGPT&#21709;&#24212;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16347</link><description>&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#35780;&#35770;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Incorrectness Detection in Software Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16347
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;CID&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;ChatGPT&#24182;&#35201;&#27714;&#25552;&#20986;&#24773;&#22659;&#30456;&#20284;&#20294;&#25991;&#26412;&#26377;&#20998;&#27495;&#30340;&#38382;&#39064;&#65292;&#26469;&#33258;&#21160;&#27979;&#35797;&#21644;&#26816;&#27979;ChatGPT&#21709;&#24212;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;135&#21517;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#20174;&#19994;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#22914;ChatGPT&#65289;&#36827;&#34892;SE&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#24076;&#26395;&#23558;ChatGPT&#29992;&#20110;&#36719;&#20214;&#24211;&#36873;&#25321;&#31561;SE&#20219;&#21153;&#65292;&#20294;&#32463;&#24120;&#25285;&#24515;ChatGPT&#21709;&#24212;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#25216;&#26415;&#21644;&#19968;&#31181;&#21517;&#20026;CID&#65288;ChatGPT&#19981;&#27491;&#30830;&#24615;&#26816;&#27979;&#22120;&#65289;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#35797;&#21644;&#26816;&#27979;ChatGPT&#21709;&#24212;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#12290;CID&#22522;&#20110;&#36890;&#36807;&#35831;&#27714;&#23545;ChatGPT&#36827;&#34892;&#36845;&#20195;&#25552;&#31034;&#26469;&#25552;&#38382;&#20855;&#26377;&#24773;&#22659;&#30456;&#20284;&#20294;&#25991;&#26412;&#26377;&#20998;&#27495;&#30340;&#38382;&#39064;&#65288;&#20351;&#29992;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#20013;&#30340;&#21464;&#24577;&#20851;&#31995;&#30340;&#26041;&#27861;&#65289;&#12290;CID&#30340;&#22522;&#26412;&#21407;&#21017;&#26159;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#21709;&#24212;&#19981;&#21516;&#65288;&#36328;&#38382;&#39064;&#30340;&#22810;&#20010;&#21270;&#36523;&#65289;&#30340;&#21709;&#24212;&#21487;&#33021;&#26159;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#12290; &#22312;&#19968;&#20010;&#20851;&#20110;&#24211;&#36873;&#25321;&#30340;&#22522;&#20934;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;CID&#33021;&#22815;&#26816;&#27979;ChatGPT&#30340;&#19981;&#27491;&#30830;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16347v1 Announce Type: cross  Abstract: We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#22810;&#29615;&#22659;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;&#65292;&#36825;&#22312;&#31070;&#32463;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#34920;&#29616;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.16336</link><description>&lt;p&gt;
&#22810;&#29615;&#22659;&#22330;&#26223;&#20013;&#30340;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Predictive Inference in Multi-environment Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#22810;&#29615;&#22659;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;&#65292;&#36825;&#22312;&#31070;&#32463;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#34920;&#29616;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#36328;&#22810;&#20010;&#29615;&#22659;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#20004;&#31181;&#35206;&#30422;&#31867;&#22411;&#65292;&#25193;&#23637;&#20102;Jackknife&#21644;&#20998;&#35010;&#19968;&#33268;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#36825;&#31181;&#38750;&#20256;&#32479;&#30340;&#23618;&#27425;&#25968;&#25454;&#29983;&#25104;&#22330;&#26223;&#20013;&#33719;&#24471;&#26080;&#20998;&#24067;&#35206;&#30422;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21253;&#25324;&#23545;&#38750;&#23454;&#20540;&#21709;&#24212;&#35774;&#32622;&#30340;&#25193;&#23637;&#65292;&#20197;&#21450;&#36825;&#20123;&#19968;&#33324;&#38382;&#39064;&#20013;&#39044;&#27979;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#29702;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#36825;&#36866;&#29992;&#20110;&#20855;&#26377;&#23618;&#27425;&#25968;&#25454;&#30340;&#39044;&#27979;&#25512;&#26029;&#30340;&#29616;&#26377;&#26041;&#27861;&#20197;&#21450;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#65307;&#36825;&#36890;&#36807;&#31070;&#32463;&#21270;&#23398;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16336v1 Announce Type: cross  Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.
&lt;/p&gt;</description></item><item><title>MEDDAP&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#20449;&#24687;&#26631;&#35760;&#26679;&#26412;&#65292;&#25552;&#21319;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;</title><link>https://arxiv.org/abs/2403.16335</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#22686;&#24378;&#31649;&#36947;&#25552;&#21319;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;&#65306;MEDDAP
&lt;/p&gt;
&lt;p&gt;
MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16335
&lt;/p&gt;
&lt;p&gt;
MEDDAP&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#20449;&#24687;&#26631;&#35760;&#26679;&#26412;&#65292;&#25552;&#21319;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#26696;&#20363;&#20013;&#65292;&#25910;&#38598;&#21644;&#27880;&#37322;&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#24403;&#20174;&#19994;&#32773;&#24050;&#34987;&#24037;&#20316;&#21344;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MEDDAP&#30340;&#26032;&#22411;&#31649;&#36947;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#20449;&#24687;&#26631;&#35760;&#26679;&#26412;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16335v1 Announce Type: cross  Abstract: The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data. However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties. Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer. To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples. Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLIDER&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#20998;&#24067;&#36716;&#31227;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#27867;&#21270;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.16334</link><description>&lt;p&gt;
&#22270;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graphs Generalization under Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLIDER&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#20998;&#24067;&#36716;&#31227;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#27867;&#21270;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#24403;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#26377;&#25152;&#20559;&#31163;&#26102;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#38024;&#23545;&#24050;&#30693;&#20998;&#24067;&#36716;&#31227;&#32780;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#38024;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26126;&#26224;&#24615;&#19988;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#22270;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#21516;&#26102;&#21457;&#29983;&#22312;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#25299;&#25169;&#19978;&#12290;&#20854;&#27425;&#65292;&#22312;&#21508;&#31181;&#20998;&#24067;&#36716;&#31227;&#20013;&#25429;&#33719;&#19981;&#21464;&#20449;&#24687;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22270;&#23398;&#20064;&#19981;&#21464;&#22495;&#29983;&#25104;&#65288;GLIDER&#65289;&#12290;&#20854;&#30446;&#26631;&#26159;(1)&#22810;&#26679;&#21270;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16334v1 Announce Type: cross  Abstract: Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution. To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process. However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges. Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology. Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge. To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER). The goal is to (1) diversify variations
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#27169;&#25311;&#21407;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24320;&#21457;&#20102;&#27169;&#25311;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#22120;&#30340;&#36924;&#30495;&#25968;&#23383;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.16331</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23545;&#27169;&#25311;&#27169;&#25311;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#22120;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Analog Dynamic Range Compressors using Deep Learning and State-space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16331
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#27169;&#25311;&#21407;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24320;&#21457;&#20102;&#27169;&#25311;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#22120;&#30340;&#36924;&#30495;&#25968;&#23383;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#24320;&#21457;&#25968;&#23383;&#38899;&#39057;&#29983;&#20135;&#20013;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#22120;&#36924;&#30495;&#25968;&#23383;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#20854;&#27169;&#25311;&#21407;&#22411;&#36827;&#34892;&#12290;&#23613;&#31649;&#36924;&#30495;&#30340;&#25968;&#23383;&#21160;&#24577;&#21387;&#32553;&#22120;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26377;&#28508;&#22312;&#29992;&#36884;&#65292;&#20294;&#30001;&#20110;&#36825;&#20123;&#21387;&#32553;&#22120;&#22312;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#38750;&#32447;&#24615;&#36816;&#34892;&#65292;&#22240;&#27492;&#35774;&#35745;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#65292;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#23454;&#26045;&#24050;&#34987;&#35777;&#26126;&#22312;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#24615;&#26041;&#38754;&#39640;&#25928;&#65292;&#24182;&#19988;&#26377;&#26395;&#29992;&#20110;&#24314;&#27169;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#22120;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;S4&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#27169;Teletronix LA-2A&#27169;&#25311;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#22120;&#12290;&#35813;&#27169;&#22411;&#26159;&#22240;&#26524;&#30340;&#65292;&#22312;&#23454;&#26102;&#25191;&#34892;&#26102;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22823;&#33268;&#30456;&#21516;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16331v1 Announce Type: cross  Abstract: We describe a novel approach for developing realistic digital models of dynamic range compressors for digital audio production by analyzing their analog prototypes. While realistic digital dynamic compressors are potentially useful for many applications, the design process is challenging because the compressors operate nonlinearly over long time scales. Our approach is based on the structured state space sequence model (S4), as implementing the state-space model (SSM) has proven to be efficient at learning long-range dependencies and is promising for modeling dynamic range compressors. We present in this paper a deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic range compressor. The model is causal, executes efficiently in real time, and achieves roughly the same quality as previous deep-learning models but with fewer parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#31070;&#32463;&#24494;&#30005;&#36335;&#30340;&#21551;&#21457;&#19979;&#65292;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;&#20316;&#20026;&#32452;&#35013;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#36991;&#20813;&#20102;&#32467;&#26500;&#19978;&#30340;&#40784;&#36136;&#21270;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#35757;&#32451;&#21644;&#23398;&#20064;&#24037;&#20855;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16327</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;&#20316;&#20026;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65306;&#27010;&#24565;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Microcircuits as Building Blocks: Concept and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#31070;&#32463;&#24494;&#30005;&#36335;&#30340;&#21551;&#21457;&#19979;&#65292;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;&#20316;&#20026;&#32452;&#35013;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#36991;&#20813;&#20102;&#32467;&#26500;&#19978;&#30340;&#40784;&#36136;&#21270;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#35757;&#32451;&#21644;&#23398;&#20064;&#24037;&#20855;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#26159;&#26368;&#24191;&#27867;&#24212;&#29992;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#35745;&#31639;&#24418;&#24335;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#23558;ANNs&#32467;&#26500;&#19978;&#20445;&#25345;&#40784;&#36136;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#32467;&#26500;&#19978;&#30340;&#40784;&#36136;&#24615;&#38656;&#35201;&#24212;&#29992;&#22797;&#26434;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#20135;&#29983;&#29305;&#23450;&#24212;&#29992;&#30340;ANNs&#65292;&#23481;&#26131;&#36935;&#21040;&#36807;&#25311;&#21512;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#31070;&#32463;&#24494;&#30005;&#36335;&#22312;&#29983;&#29289;&#23398;&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#21363;&#26377;&#26426;&#31070;&#32463;&#31995;&#32479;&#30340;&#8220;&#22522;&#26412;&#22788;&#29702;&#20803;&#20214;&#8221;&#12290;&#22914;&#20309;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;(ANMs)&#32452;&#35013;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#65292;&#26088;&#22312;&#20316;&#20026;&#29616;&#25104;&#38646;&#20214;&#65292;&#34249;&#30001;&#21033;&#29992;&#26032;&#39062;&#24615;&#25628;&#32034;&#26469;&#20135;&#29983;&#36825;&#31181;&#24494;&#30005;&#36335;&#30446;&#24405;&#30340;&#21021;&#27493;&#24037;&#20316;&#32467;&#26524;&#65307;&#25509;&#30528;&#26159;&#25193;&#23637;&#36825;&#39033;&#21021;&#27493;&#24037;&#20316;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#35752;&#35770;&#30456;&#24212;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16327v1 Announce Type: cross  Abstract: Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation. However the current trend is for ANNs to be structurally homogeneous. Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems. How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges unc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#33021;&#24110;&#21161;&#26356;&#22909;&#29702;&#35299;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#38477;&#20302;oracle&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.16317</link><description>&lt;p&gt;
&#22312;&#26356;&#32454;&#31890;&#24230;&#19978;&#30340;&#20248;&#21270;&#65306;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#33021;&#24110;&#21161;&#26356;&#22909;&#29702;&#35299;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#38477;&#20302;oracle&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#24320;&#22987;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#20551;&#35774;&#22312;&#28857;&#38468;&#36817;&#30340;&#23567;&#21306;&#22495;&#20869;&#65292;(&#27425;)&#26799;&#24230;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#29992;&#24179;&#22343;&#25110;&#26368;&#22823;&#26041;&#24335;&#27714;&#20540;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#21253;&#25324;&#20256;&#32479;&#20248;&#21270;&#20013;&#20256;&#32479;&#30740;&#31350;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#31867;&#26681;&#25454;&#30446;&#26631;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#25110;&#20854;&#26799;&#24230;&#30340;H\"{o}lder/Lipschitz&#36830;&#32493;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#20041;&#31867;&#21253;&#21547;&#37027;&#20123;&#26082;&#19981;&#26159;Lipschitz&#36830;&#32493;&#30340;&#20063;&#27809;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20989;&#25968;&#12290;&#24403;&#38480;&#21046;&#22312;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#31867;&#26102;&#65292;&#23450;&#20041;&#30740;&#31350;&#31867;&#30340;&#21442;&#25968;&#23548;&#33268;&#26356;&#21152;&#31934;&#32454;&#30340;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#24674;&#22797;&#20256;&#32479;&#30340;oracle&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20294;&#19968;&#33324;&#24773;&#20917;&#19979;&#20250;&#23548;&#33268;&#37027;&#20123;&#19981;&#26159;&#8220;&#26368;&#22351;&#24773;&#20917;&#8221;&#30340;&#20989;&#25968;&#20855;&#26377;&#36739;&#20302;&#30340;oracle &#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16317v1 Announce Type: cross  Abstract: We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;IRL&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;DNN&#35299;&#37322;&#20026;&#20915;&#31574;&#26641;&#65292;&#35299;&#20915;&#20102;DRL&#35843;&#24230;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16293</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#35843;&#24230;&#21487;&#35299;&#37322;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;IRL&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;DNN&#35299;&#37322;&#20026;&#20915;&#31574;&#26641;&#65292;&#35299;&#20915;&#20102;DRL&#35843;&#24230;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#65292;&#26368;&#36817;&#25506;&#32034;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38598;&#32676;&#35843;&#24230;&#65288;DRL&#35843;&#24230;&#65289;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#31995;&#32479;&#31649;&#29702;&#21592;&#30475;&#19981;&#25026;&#30340;&#40657;&#21283;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#20102;DRL&#35843;&#24230;&#30340;&#23454;&#38469;&#37096;&#32626;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;IRL&#65288;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;DRL&#35843;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#21033;&#29992;&#27169;&#20223;&#23398;&#20064;&#23558;DNN&#65288;&#21363;DRL&#31574;&#30053;&#65289;&#35299;&#37322;&#20026;&#20915;&#31574;&#26641;&#12290;&#19982;DNN&#19981;&#21516;&#65292;&#20915;&#31574;&#26641;&#27169;&#22411;&#26159;&#38750;&#21442;&#25968;&#21270;&#30340;&#65292;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;&#20026;&#20102;&#25552;&#21462;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#20915;&#31574;&#26641;&#65292;IRL&#32467;&#21512;&#20102;&#25968;&#25454;&#38598;&#32858;&#21512;&#65288;DAgger&#65289;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#35780;&#35770;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16293v1 Announce Type: new  Abstract: In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of crit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#33521;&#36229;&#32852;&#36187;&#30340;&#27604;&#36187;&#32467;&#26524;&#65292;&#36890;&#36807;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#21644;&#30740;&#31350;&#21508;&#31181;&#29305;&#24449;&#30340;&#24847;&#20041;&#65292;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#39044;&#27979;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.16282</link><description>&lt;p&gt;
&#36275;&#29699;&#21338;&#24425;&#30340;&#28436;&#21464;-&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#27604;&#36187;&#32467;&#26524;&#21644;&#20070;maker&#36180;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
The Evolution of Football Betting- A Machine Learning Approach to Match Outcome Forecasting and Bookmaker Odds Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#33521;&#36229;&#32852;&#36187;&#30340;&#27604;&#36187;&#32467;&#26524;&#65292;&#36890;&#36807;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#21644;&#30740;&#31350;&#21508;&#31181;&#29305;&#24449;&#30340;&#24847;&#20041;&#65292;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#39044;&#27979;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32844;&#19994;&#36275;&#29699;&#21644;&#21338;&#24425;&#34892;&#19994;&#30340;&#37325;&#35201;&#21382;&#21490;&#65292;&#36861;&#28335;&#20102;&#20854;&#20174;&#31192;&#23494;&#24320;&#22987;&#21457;&#23637;&#20026;&#19968;&#20010;&#21033;&#28070;&#20016;&#21402;&#30340;&#30334;&#19975;&#33521;&#38225;&#20225;&#19994;&#30340;&#28436;&#21464;&#36807;&#31243;&#12290;&#20174;1960&#24180;&#21512;&#27861;&#21270;&#36172;&#21338;&#24320;&#22987;&#65292;&#20877;&#21040;&#30001;Thorold Charles Reep&#24320;&#21019;&#30340;&#36275;&#29699;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30340;&#36827;&#27493;&#65292;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#25512;&#21160;&#20102;&#24555;&#36895;&#22686;&#38271;&#21644;&#21019;&#26032;&#12290;&#22312;&#36807;&#21435;&#30340;&#20845;&#21313;&#24180;&#37324;&#65292;&#36825;&#20004;&#20010;&#34892;&#19994;&#37117;&#32463;&#21382;&#20102;&#26681;&#26412;&#24615;&#30340;&#21464;&#38761;&#65292;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20174;&#22522;&#30784;&#30340;&#35760;&#31508;&#35760;&#21457;&#23637;&#21040;&#20102;&#39640;&#28165;&#25668;&#20687;&#22836;&#21644;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#20998;&#26512;&#36825;&#26679;&#30340;&#20808;&#36827;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;&#33521;&#36229;&#32852;&#36187;&#30340;&#27604;&#36187;&#32467;&#26524;&#12290;&#36890;&#36807;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#21644;&#30740;&#31350;&#21508;&#31181;&#29305;&#24449;&#30340;&#24847;&#20041;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#39044;&#27979;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16282v1 Announce Type: new  Abstract: This paper explores the significant history of professional football and the betting industry, tracing its evolution from clandestine beginnings to a lucrative multi-million-pound enterprise. Initiated by the legalization of gambling in 1960 and complemented by advancements in football data gathering pioneered by Thorold Charles Reep, the symbiotic relationship between these sectors has propelled rapid growth and innovation. Over the past six decades, both industries have undergone radical transformations, with data collection methods evolving from rudimentary notetaking to sophisticated technologies such as high-definition cameras and Artificial Intelligence (AI)-driven analytics. Therefore, the primary aim of this study is to utilize Machine Learning (ML) algorithms to forecast premier league football match outcomes. By analyzing historical data and investigating the significance of various features, the study seeks to identify the mos
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16260</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22810;&#29702;&#35299;&#38598;&#25104;&#23454;&#29616;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16260
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#36234;&#30028;&#65288;OOD&#65289;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#35268;&#27169;&#23545;&#27169;&#22411;&#22312;OOD&#26816;&#27979;&#20013;&#25928;&#26524;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#22686;&#24378;&#36825;&#19968;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#30340;&#31361;&#20986;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#26399;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#30406;/&#38556;&#30861;&#21487;&#35270;&#21270;&#21644;&#33258;&#32806;&#21512;&#25351;&#25968;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#21253;&#21547;&#21487;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#26435;&#37325;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21464;&#24615;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#29305;&#24449;&#34920;&#31034;&#20013;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#38750;&#21508;&#21521;&#21516;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#29109;&#27169;&#22411;&#22312;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#65292;&#20197;&#21306;&#20998;&#39057;&#29575;&#20869;&#23481;&#24182;&#21152;&#36895;&#29109;&#35299;&#30721;&#27493;&#39588;&#65292;&#25552;&#39640;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16258</link><description>&lt;p&gt;
&#22312;&#27169;&#31946;&#25193;&#25955;&#21512;&#25104;&#20013;&#30340;&#25289;&#26222;&#25289;&#26031;&#24341;&#23548;&#29109;&#27169;&#22411;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#20013;
&lt;/p&gt;
&lt;p&gt;
Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16258
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#38750;&#21508;&#21521;&#21516;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#29109;&#27169;&#22411;&#22312;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#65292;&#20197;&#21306;&#20998;&#39057;&#29575;&#20869;&#23481;&#24182;&#21152;&#36895;&#29109;&#35299;&#30721;&#27493;&#39588;&#65292;&#25552;&#39640;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#26031;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#20294;&#23427;&#20204;&#23545;&#22270;&#20687;&#25968;&#25454;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23454;&#29616;&#26368;&#20808;&#36827;&#24863;&#30693;&#27700;&#24179;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#35299;&#30721;&#22120;&#31471;&#37319;&#29992;&#20102;&#38750;&#21508;&#21521;&#21516;&#24615;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26045;&#21152;&#20102;&#19968;&#31181;&#26088;&#22312;&#21306;&#20998;&#39057;&#29575;&#20869;&#23481;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29109;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#31354;&#38388;&#36890;&#36947;&#30456;&#20851;&#24615;&#20934;&#30830;&#24314;&#27169;&#28508;&#22312;&#34920;&#24449;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21516;&#26102;&#21152;&#36895;&#29109;&#35299;&#30721;&#27493;&#39588;&#12290;&#36825;&#31181;&#36890;&#36947;&#32423;&#29109;&#27169;&#22411;&#21033;&#29992;&#27599;&#20010;&#36890;&#36947;&#22359;&#20869;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#31354;&#38388;&#19978;&#19979;&#25991;&#12290;&#20840;&#23616;&#31354;&#38388;&#19978;&#19979;&#25991;&#26159;&#24314;&#31435;&#22312;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16258v1 Announce Type: cross  Abstract: While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for 
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20351;&#29992;&#20803;&#21551;&#21457;&#26041;&#27861;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.16247</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#21551;&#21457;&#26041;&#27861;&#25913;&#36827;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#29992;&#20110;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16247
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20351;&#29992;&#20803;&#21551;&#21457;&#26041;&#27861;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#31038;&#20250;&#36807;&#28193;&#21040;&#20449;&#24687;&#26102;&#20195;&#65292;&#25105;&#20204;&#27880;&#24847;&#21147;&#30340;&#20943;&#23569;&#26159;&#19968;&#20010;&#24517;&#28982;&#36235;&#21183;&#65292;&#33457;&#26102;&#38388;&#38405;&#35835;&#20887;&#38271;&#26032;&#38395;&#25991;&#31456;&#30340;&#20154;&#32676;&#27491;&#22312;&#36805;&#36895;&#20943;&#23569;&#65292;&#32780;&#23545;&#31616;&#27905;&#20449;&#24687;&#30340;&#38656;&#27714;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31616;&#27905;&#22320;&#24635;&#32467;&#39030;&#32423;&#26032;&#38395;&#25991;&#31456;&#21644;&#26368;&#30452;&#35266;&#30340;&#26631;&#39064;&#65292;&#25552;&#20379;&#37325;&#35201;&#26032;&#38395;&#30340;&#24555;&#36895;&#27010;&#36848;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#31867;&#22312;&#23581;&#35797;&#36827;&#34892;&#25688;&#35201;&#26102;&#65292;&#20250;&#20174;&#26469;&#28304;&#20013;&#25552;&#21462;&#22522;&#26412;&#20449;&#24687;&#65292;&#24182;&#20174;&#21407;&#22987;&#25552;&#21462;&#20013;&#28155;&#21152;&#26377;&#29992;&#30701;&#35821;&#21644;&#35821;&#27861;&#27880;&#37322;&#12290;&#20154;&#31867;&#26377;&#21019;&#24314;&#25277;&#35937;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#25688;&#35201;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#31070;&#32463;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#65292;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#30340;&#24212;&#29992;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21019;&#26032;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#21457;&#23637;&#24403;&#21069;&#30340;seq2seq&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16247v1 Announce Type: new  Abstract: As human society transitions into the information age, reduction in our attention span is a contingency, and people who spend time reading lengthy news articles are decreasing rapidly and the need for succinct information is higher than ever before. Therefore, it is essential to provide a quick overview of important news by concisely summarizing the top news article and the most intuitive headline. When humans try to make summaries, they extract the essential information from the source and add useful phrases and grammatical annotations from the original extract. Humans have a unique ability to create abstractions. However, automatic summarization is a complicated problem to solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive text summarization has been ascending as far as prevalence. Numerous innovative strategies have been proposed to develop the current seq2seq models further, permitting them to handle diffe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#20998;&#31867;&#32593;&#32476;&#20013;&#29305;&#23450;&#31867;&#21035;&#25968;&#25454;&#30340;&#30446;&#30340;&#24615;&#28040;&#38500;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#27169;&#22411;&#23545;&#26410;&#23398;&#20064;&#25968;&#25454;&#31867;&#21035;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20854;&#20182;&#31867;&#21035;&#24615;&#33021;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16246</link><description>&lt;p&gt;
&#37096;&#20998;&#36974;&#34109;&#30340;&#36951;&#24536;: &#19968;&#31181;&#36125;&#21494;&#26031;&#35270;&#35282;&#19979;&#30340;&#28145;&#24230;&#32593;&#32476;&#31867;&#21035;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#20998;&#31867;&#32593;&#32476;&#20013;&#29305;&#23450;&#31867;&#21035;&#25968;&#25454;&#30340;&#30446;&#30340;&#24615;&#28040;&#38500;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#27169;&#22411;&#23545;&#26410;&#23398;&#20064;&#25968;&#25454;&#31867;&#21035;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20854;&#20182;&#31867;&#21035;&#24615;&#33021;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#30417;&#31649;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#26631;&#20934;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24517;&#39035;&#31995;&#32479;&#22320;&#28040;&#38500;&#20174;&#29992;&#25143;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#23450;&#23376;&#38598;&#23548;&#20986;&#30340;&#26080;&#27861;&#20877;&#21033;&#29992;&#30340;&#20449;&#24687;&#12290;&#26426;&#22120;&#36951;&#24536;&#36825;&#19968;&#26032;&#20852;&#23398;&#31185;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20419;&#36827;&#20102;&#26377;&#36873;&#25321;&#24615;&#22320;&#20002;&#24323;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25351;&#23450;&#32473;&#29305;&#23450;&#25968;&#25454;&#38598;&#25110;&#31867;&#21035;&#30340;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24517;&#39035;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#24191;&#27867;&#37325;&#26032;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#21046;&#23450;&#19968;&#31181;&#38024;&#23545;&#20174;&#39044;&#35757;&#32451;&#20998;&#31867;&#32593;&#32476;&#20013;&#29305;&#23450;&#31867;&#21035;&#30340;&#25968;&#25454;&#20013;&#30446;&#30340;&#24615;&#28040;&#38500;&#20449;&#24687;&#30340;&#26041;&#27861;&#35770;&#12290;&#36825;&#31181;&#26377;&#24847;&#30340;&#21435;&#38500;&#26088;&#22312;&#38477;&#20302;&#27169;&#22411;&#23545;&#26410;&#23398;&#20064;&#25968;&#25454;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#27169;&#22411;&#22312;&#20854;&#20182;&#31867;&#21035;&#20013;&#24615;&#33021;&#30340;&#20219;&#20309;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16246v1 Announce Type: new  Abstract: In order to adhere to regulatory standards governing individual data privacy and safety, machine learning models must systematically eliminate information derived from specific subsets of a user's training data that can no longer be utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal area of research, facilitating the process of selectively discarding information designated to specific sets or classes of data from a pre-trained model, thereby eliminating the necessity for extensive retraining from scratch. The principal aim of this study is to formulate a methodology tailored for the purposeful elimination of information linked to a specific class of data from a pre-trained classification network. This intentional removal is crafted to degrade the model's performance specifically concerning the unlearned data class while concurrently minimizing any detrimental impacts on the model's performance in other classe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#31561;&#25928;&#24615;&#12289;&#21487;&#26367;&#20195;&#24615;&#20197;&#21450;&#29983;&#25104;&#22120;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16244</link><description>&lt;p&gt;
&#20851;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#31561;&#25928;&#24615;&#12289;&#21487;&#26367;&#20195;&#24615;&#21644;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Equivalency, Substitutability, and Flexibility of Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#31561;&#25928;&#24615;&#12289;&#21487;&#26367;&#20195;&#24615;&#20197;&#21450;&#29983;&#25104;&#22120;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32463;&#39564;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#24863;&#30693;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#31038;&#21306;&#37319;&#32435;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#25193;&#23637;&#24615;&#12289;&#23436;&#32654;&#30340;&#27880;&#37322;&#21644;&#20302;&#25104;&#26412;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#26377;&#20248;&#21183;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#24378;&#35843;&#22914;&#20309;&#26377;&#25928;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#20197;&#21450;&#21512;&#25104;&#25968;&#25454;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#20943;&#23569;&#23454;&#38469;&#25968;&#25454;&#25910;&#38598;&#30340;&#24037;&#20316;&#37327;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#20960;&#20010;&#26377;&#36259;&#23646;&#24615; -- &#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#31561;&#25928;&#24615;&#65292;&#21512;&#25104;&#25968;&#25454;&#23545;&#23454;&#38469;&#25968;&#25454;&#30340;&#26367;&#20195;&#24615;&#65292;&#20197;&#21450;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#28789;&#27963;&#24615;&#26469;&#22635;&#34917;&#39046;&#22495;&#24046;&#36317;&#12290;&#21033;&#29992;M3Act&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#22312;DanceTrack&#21644;MOT17&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#25104;&#25968;&#25454;&#19981;&#20165;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16244v1 Announce Type: new  Abstract: We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios. Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs. Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection. To answer the questions, we systematically investigate several interesting properties of synthetic data -- the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps. Leveraging the M3Act synthetic data generator, we conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic data not only enhances model per
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#22312;&#21463;&#22122;&#22768;&#24433;&#21709;&#30340;&#30142;&#30149;&#20256;&#25773;&#27169;&#22411;&#19978;&#30340;&#26089;&#26399;&#39044;&#35686;&#25351;&#26631;&#65292;&#20197;&#25552;&#20379;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#26377;&#25928;&#39044;&#35686;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2403.16233</link><description>&lt;p&gt;
&#35757;&#32451;&#22312;&#20855;&#26377;&#19981;&#21516;&#22122;&#22768;&#30340;&#38543;&#26426;&#30142;&#30149;&#20256;&#25773;&#27169;&#22411;&#19978;&#30340;&#39044;&#35686;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
An early warning indicator trained on stochastic disease-spreading models with different noises
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16233
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#22312;&#21463;&#22122;&#22768;&#24433;&#21709;&#30340;&#30142;&#30149;&#20256;&#25773;&#27169;&#22411;&#19978;&#30340;&#26089;&#26399;&#39044;&#35686;&#25351;&#26631;&#65292;&#20197;&#25552;&#20379;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#26377;&#25928;&#39044;&#35686;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#36890;&#36807;&#21487;&#38752;&#30340;&#26089;&#26399;&#39044;&#35686;&#20449;&#21495;&#65288;EWSs&#65289;&#26816;&#27979;&#30142;&#30149;&#29190;&#21457;&#23545;&#20110;&#26377;&#25928;&#30340;&#20844;&#20849;&#21355;&#29983;&#32531;&#35299;&#25112;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30142;&#30149;&#20256;&#25773;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24120;&#24120;&#21463;&#21040;&#22810;&#31181;&#22122;&#22768;&#28304;&#21644;&#26089;&#26399;&#29190;&#21457;&#38454;&#27573;&#26377;&#38480;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#30340;EWSs&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#29616;&#26377;&#25351;&#26631;&#30340;&#24615;&#33021;&#22240;&#22806;&#37096;&#21644;&#20869;&#22312;&#22122;&#22768;&#30340;&#21464;&#21270;&#32780;&#19981;&#21516;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24403;&#27979;&#37327;&#34987;&#21152;&#24615;&#30333;&#22122;&#22768;&#12289;&#20056;&#24615;&#29615;&#22659;&#22122;&#22768;&#21644;&#20154;&#21475;&#22122;&#22768;&#27745;&#26579;&#26102;&#24314;&#27169;&#30142;&#30149;&#30340;&#25361;&#25112;&#65292;&#23558;&#20854;&#32435;&#20837;&#26631;&#20934;&#20256;&#26579;&#30149;&#25968;&#23398;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#22122;&#22768;&#28304;&#24341;&#20837;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#24863;&#26579;&#30149;&#29190;&#21457;&#20013;&#21463;&#22122;&#22768;&#24433;&#21709;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#20379;EWS&#12290;&#36890;&#36807;&#24212;&#29992;&#65292;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16233v1 Announce Type: new  Abstract: The timely detection of disease outbreaks through reliable early warning signals (EWSs) is indispensable for effective public health mitigation strategies. Nevertheless, the intricate dynamics of real-world disease spread, often influenced by diverse sources of noise and limited data in the early stages of outbreaks, pose a significant challenge in developing reliable EWSs, as the performance of existing indicators varies with extrinsic and intrinsic noises. Here, we address the challenge of modeling disease when the measurements are corrupted by additive white noise, multiplicative environmental noise, and demographic noise into a standard epidemic mathematical model. To navigate the complexities introduced by these noise sources, we employ a deep learning algorithm that provides EWS in infectious disease outbreak by training on noise-induced disease-spreading models. The indicator's effectiveness is demonstrated through its application
&lt;/p&gt;</description></item><item><title>CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.16218</link><description>&lt;p&gt;
CoverUp&#65306;&#22522;&#20110;&#35206;&#30422;&#29575;&#24341;&#23548;&#30340;LLM&#27979;&#35797;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoverUp: Coverage-Guided LLM-Based Test Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16218
&lt;/p&gt;
&lt;p&gt;
CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CoverUp&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32467;&#21512;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#12290;CoverUp&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#35206;&#30422;&#29575;&#65292;&#23558;&#35206;&#30422;&#29575;&#20998;&#26512;&#19982;LLM&#23545;&#35805;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20415;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#23578;&#26410;&#28085;&#30422;&#30340;&#20195;&#30721;&#34892;&#21644;&#20998;&#25903;&#19978;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#22871;&#20214;&#30456;&#27604;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#26174;&#33879;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#65306;&#19982;CodaMosa&#30456;&#27604;&#65292;&#19968;&#31181;&#28151;&#21512;LLM / &#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#31995;&#32479;&#65292;CoverUp&#22312;&#21508;&#26041;&#38754;&#37117;&#22823;&#24133;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;&#20197;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;CoverUp&#23454;&#29616;&#20102;81%&#30340;&#20013;&#20301;&#32447;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;62%&#65289;&#12289;53%&#30340;&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;35%&#65289;&#21644;78%&#30340;&#32447;+&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;55%&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoverUp&#30340;&#36845;&#20195;&#12289;&#35206;&#30422;&#29575;&#24341;&#23548;&#26041;&#27861;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25104;&#21151;&#30340;&#36817;&#19968;&#21322;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#32447;&#24615;&#23450;&#24120;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26799;&#24230;&#31639;&#27861;&#65292;&#30452;&#25509;&#20174;&#32473;&#23450;&#30340;&#31995;&#32479;&#20013;&#35745;&#31639;&#31232;&#30095;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#65292;&#20855;&#26377;&#27700;&#24179;&#38544;&#34255;&#23618;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33539;&#24335;</title><link>https://arxiv.org/abs/2403.16215</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#21270;&#26500;&#24314;&#29992;&#20110;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Systematic construction of continuous-time neural networks for linear dynamical systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#32447;&#24615;&#23450;&#24120;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26799;&#24230;&#31639;&#27861;&#65292;&#30452;&#25509;&#20174;&#32473;&#23450;&#30340;&#31995;&#32479;&#20013;&#35745;&#31639;&#31232;&#30095;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#65292;&#20855;&#26377;&#27700;&#24179;&#38544;&#34255;&#23618;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16215v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21457;&#29616;&#36866;&#21512;&#27169;&#25311;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24448;&#24448;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#30340;&#35797;&#38169;&#21644;&#22312;&#39640;&#32500;&#36229;&#21442;&#25968;&#31354;&#38388;&#20013;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#19968;&#31867;&#21160;&#21147;&#31995;&#32479;&#65292;&#21363;&#32447;&#24615;&#23450;&#24120;(LTI)&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20986;&#36830;&#32493;&#22320;&#28436;&#21270;&#20026;&#19968;&#38454;&#25110;&#20108;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#26799;&#24230;&#31639;&#27861;&#65292;&#30452;&#25509;&#20174;&#32473;&#23450;&#30340;LTI&#31995;&#32479;&#20013;&#35745;&#31639;&#31232;&#30095;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#32593;&#32476;&#21442;&#25968;&#65292;&#21033;&#29992;&#20854;&#29305;&#24615;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#27700;&#24179;&#38544;&#34255;&#23618;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33539;&#24335;&#65292;&#24182;&#38416;&#26126;&#20102;&#20026;&#20160;&#20040;&#20351;&#29992;con
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16215v1 Announce Type: new  Abstract: Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space. In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems. We use a variant of continuous-time neural networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE). Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties. We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing con
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;MRI&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#21019;&#26032;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#22411;&#26500;&#24314;&#27493;&#39588;&#65292;&#35782;&#21035;&#19981;&#21516;&#38454;&#27573;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;</title><link>https://arxiv.org/abs/2403.16212</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;Xception&#26550;&#26500;&#25552;&#39640;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;MRI&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;MRI&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#21019;&#26032;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#22411;&#26500;&#24314;&#27493;&#39588;&#65292;&#35782;&#21035;&#19981;&#21516;&#38454;&#27573;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20026;&#35266;&#23519;&#21644;&#35786;&#26029;&#22797;&#26434;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#25552;&#20379;&#20102;&#29420;&#29305;&#35270;&#35282;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21644;Xception&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#29616;&#22312;&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#20998;&#26512;&#21644;&#20998;&#31867;&#22823;&#37327;MRI&#25968;&#25454;&#12290;&#36825;&#39033;&#25216;&#26415;&#30340;&#36827;&#23637;&#19981;&#20165;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#33041;&#32467;&#26500;&#21464;&#21270;&#30340;&#29702;&#35299;&#65292;&#36824;&#20026;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#25163;&#27573;&#30417;&#27979;&#30142;&#30149;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#21516;&#26102;&#20063;&#26377;&#26395;&#22312;&#30142;&#30149;&#26089;&#26399;&#38454;&#27573;&#23454;&#29616;&#31934;&#30830;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16212v1 Announce Type: cross  Abstract: Exploring the application of deep learning technologies in the field of medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique perspective for observing and diagnosing complex neurodegenerative diseases such as Alzheimer Disease (AD). With advancements in deep learning, particularly in Convolutional Neural Networks (CNNs) and the Xception network architecture, we are now able to analyze and classify vast amounts of MRI data with unprecedented accuracy. The progress of this technology not only enhances our understanding of brain structural changes but also opens up new avenues for monitoring disease progression through non-invasive means and potentially allows for precise diagnosis in the early stages of the disease.   This study aims to classify MRI images using deep learning models to identify different stages of Alzheimer Disease through a series of innovative data processing and model construction steps. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;OT-Flow&#36825;&#19968;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24314;&#31435;&#19968;&#20123;&#25910;&#25947;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;&#27491;&#21017;&#21270;&#39033;&#21442;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;OT-Flow&#19982;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#25910;&#25947;&#20851;&#31995;&#65292;&#20197;&#21450;&#38543;&#30528;&#26679;&#26412;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#31163;&#25955;&#25439;&#22833;&#20989;&#25968;&#19982;&#36830;&#32493;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#25910;&#25947;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.16208</link><description>&lt;p&gt;
OT-Flow&#26679;&#26412;&#29983;&#25104;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence analysis of OT-Flow for sample generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;OT-Flow&#36825;&#19968;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24314;&#31435;&#19968;&#20123;&#25910;&#25947;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;&#27491;&#21017;&#21270;&#39033;&#21442;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;OT-Flow&#19982;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#25910;&#25947;&#20851;&#31995;&#65292;&#20197;&#21450;&#38543;&#30528;&#26679;&#26412;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#31163;&#25955;&#25439;&#22833;&#20989;&#25968;&#19982;&#36830;&#32493;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#25910;&#25947;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#24182;&#29983;&#25104;&#26032;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#23454;&#36341;&#20013;&#30340;&#39640;&#36136;&#37327;&#29983;&#25104;&#24615;&#33021;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#25910;&#25947;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;OT-Flow&#36825;&#19968;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#25910;&#25947;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;OT-Flow&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27491;&#21017;&#21270;&#39033;&#21442;&#25968;$\alpha$&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;OT-Flow&#27169;&#22411;&#30340;&#34920;&#36848;&#19982;&#23545;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#38382;&#39064;&#20043;&#38388;&#30340;$\Gamma$-&#25910;&#25947;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#20013;&#23558;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36924;&#36817;&#65292;&#24403;&#26679;&#26412;&#25968;$N$&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#20102;&#31163;&#25955;&#25439;&#22833;&#20989;&#25968;&#19982;&#36830;&#32493;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#25910;&#25947;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#20026;&#31163;&#25955;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16208v1 Announce Type: cross  Abstract: Deep generative models aim to learn the underlying distribution of data and generate new ones. Despite the diversity of generative models and their high-quality generation performance in practice, most of them lack rigorous theoretical convergence proofs. In this work, we aim to establish some convergence results for OT-Flow, one of the deep generative models. First, by reformulating the framework of OT-Flow model, we establish the $\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal transport (OT) problem as the regularization term parameter $\alpha$ goes to infinity. Second, since the loss function will be approximated by Monte Carlo method in training, we established the convergence between the discrete loss function and the continuous one when the sample number $N$ goes to infinity as well. Meanwhile, the approximation capability of the neural network provides an upper bound for the discrete loss function
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#25935;&#24863;&#23646;&#24615;&#30340;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#39118;&#26684;&#30340;&#30446;&#26631;&#20989;&#25968;&#23398;&#20064;&#20844;&#24179;&#19988;&#26377;&#21033;&#20110;&#32858;&#31867;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.16201</link><description>&lt;p&gt;
&#20174;&#31163;&#25955;&#21040;&#36830;&#32493;: &#20855;&#26377;&#21487;&#36801;&#31227;&#34920;&#31034;&#30340;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Discrete to Continuous: Deep Fair Clustering With Transferable Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#25935;&#24863;&#23646;&#24615;&#30340;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#39118;&#26684;&#30340;&#30446;&#26631;&#20989;&#25968;&#23398;&#20064;&#20844;&#24179;&#19988;&#26377;&#21033;&#20110;&#32858;&#31867;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#30340;&#34920;&#31034;&#23558;&#25968;&#25454;&#20998;&#25104;&#31751;&#65292;&#21516;&#26102;&#38544;&#34255;&#25935;&#24863;&#25968;&#25454;&#23646;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#20026;&#20102;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#32676;&#20307;&#20844;&#24179;&#24615;&#26631;&#20934;&#30340;&#21508;&#31181;&#20844;&#24179;&#24615;&#30456;&#20851;&#30446;&#26631;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#20551;&#23450;&#25935;&#24863;&#23646;&#24615;&#26159;&#31163;&#25955;&#30340;&#65292;&#23545;&#20110;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#65288;&#22914;&#21306;&#22495;&#20013;&#22899;&#24615;&#20154;&#21475;&#27604;&#20363;&#65289;&#19981;&#36215;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#24037;&#20316;&#24573;&#30053;&#20102;&#20174;&#32858;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#20854;&#20182;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#37492;&#20110;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#25935;&#24863;&#23646;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20449;&#24687;&#29942;&#39048;&#39118;&#26684;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23398;&#20064;&#20844;&#24179;&#19988;&#26377;&#21033;&#20110;&#32858;&#31867;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16201v1 Announce Type: new  Abstract: We consider the problem of deep fair clustering, which partitions data into clusters via the representations extracted by deep neural networks while hiding sensitive data attributes. To achieve fairness, existing methods present a variety of fairness-related objective functions based on the group fairness criterion. However, these works typically assume that the sensitive attributes are discrete and do not work for continuous sensitive variables, such as the proportion of the female population in an area. Besides, the potential of the representations learned from clustering tasks to improve performance on other tasks is ignored by existing works. In light of these limitations, we propose a flexible deep fair clustering method that can handle discrete and continuous sensitive attributes simultaneously. Specifically, we design an information bottleneck style objective function to learn fair and clustering-friendly representations. Furtherm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#20855;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#32447;&#24615;SVC&#65292;&#25552;&#20379;&#20102;&#27491;&#30830;&#24615;&#21644;&#26497;&#23567;&#24615;&#20445;&#35777;&#30340;&#35299;&#37322;&#65292;&#30456;&#27604;&#21551;&#21457;&#24335;&#31639;&#27861;Anchors&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#30701;&#30340;&#35299;&#37322;</title><link>https://arxiv.org/abs/2403.16190</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#25298;&#32477;&#36873;&#39033;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Logic-based Explanations for Linear Support Vector Classifiers with Reject Option
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16190
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#20855;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#32447;&#24615;SVC&#65292;&#25552;&#20379;&#20102;&#27491;&#30830;&#24615;&#21644;&#26497;&#23567;&#24615;&#20445;&#35777;&#30340;&#35299;&#37322;&#65292;&#30456;&#27604;&#21551;&#21457;&#24335;&#31639;&#27861;Anchors&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#30701;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#65288;SVC&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#19982;&#25298;&#32477;&#36873;&#39033;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#65292;&#25298;&#32477;&#38590;&#20197;&#27491;&#30830;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24182;&#23558;&#23427;&#20204;&#22996;&#25176;&#32473;&#19987;&#23478;&#12290;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20449;&#24515;&#12290;&#22240;&#27492;&#65292;&#33719;&#24471;&#25298;&#32477;&#21407;&#22240;&#30340;&#35299;&#37322;&#23545;&#20110;&#19981;&#30450;&#30446;&#20449;&#20219;&#25152;&#33719;&#32467;&#26524;&#26159;&#37325;&#35201;&#30340;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#30456;&#20851;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#36825;&#31181;&#35299;&#37322;&#30340;&#25163;&#27573;&#65292;&#20294;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#26377;&#20154;&#20026;&#23384;&#22312;&#25298;&#32477;&#36873;&#39033;&#26102;&#25552;&#20379;&#36825;&#31181;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#23545;&#20855;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#32447;&#24615;SVC&#30340;&#35299;&#37322;&#36827;&#34892;&#27491;&#30830;&#24615;&#21644;&#26497;&#23567;&#24615;&#30340;&#24418;&#24335;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#29983;&#25104;&#35299;&#37322;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;Anchors&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25152;&#33719;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#30701;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16190v1 Announce Type: new  Abstract: Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems. It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist. This further increases the confidence of the model. Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results. While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present. We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option. We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations. Obtained results show that our proposed method gives shorter explanations
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20165;&#23384;&#22312;&#24178;&#20928;&#20449;&#21495;&#29305;&#24449;&#30340;&#23376;&#31354;&#38388;&#24182;&#20002;&#24323;&#25200;&#21160;&#29305;&#24449;&#65292;&#20351;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.16176</link><description>&lt;p&gt;
&#23376;&#31354;&#38388;&#38450;&#24481;&#65306;&#36890;&#36807;&#23398;&#20064;&#24178;&#20928;&#20449;&#21495;&#30340;&#23376;&#31354;&#38388;&#26469;&#20002;&#24323;&#23545;&#25239;&#24615;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20165;&#23384;&#22312;&#24178;&#20928;&#20449;&#21495;&#29305;&#24449;&#30340;&#23376;&#31354;&#38388;&#24182;&#20002;&#24323;&#25200;&#21160;&#29305;&#24449;&#65292;&#20351;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#27491;&#24120;&#31034;&#20363;&#19978;&#25918;&#32622;&#31934;&#24515;&#21046;&#20316;&#30340;&#25200;&#21160;&#20197;&#27450;&#39575;DNNs&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31867;&#25915;&#20987;&#65292;&#38656;&#35201;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#25658;&#24102;&#30340;&#29305;&#24449;&#36827;&#34892;&#21051;&#30011;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#26679;&#26412;&#29305;&#24449;&#23376;&#31354;&#38388;&#36827;&#34892;&#35889;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#65292;&#26080;&#35770;&#26159;&#24178;&#20928;&#20449;&#21495;&#36824;&#26159;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#29305;&#24449;&#37117;&#26159;&#20887;&#20313;&#30340;&#65292;&#24182;&#20998;&#21035;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#23637;&#24320;&#65292;&#20114;&#30456;&#20043;&#38388;&#37325;&#21472;&#24456;&#23567;&#65292;&#32780;&#32463;&#20856;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#23558;&#25200;&#21160;&#29305;&#24449;&#25490;&#38500;&#22312;&#24178;&#20928;&#20449;&#21495;&#23376;&#31354;&#38388;&#20043;&#22806;&#12290;&#36825;&#20351;&#24471;DNNs&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#20165;&#23384;&#22312;&#24178;&#20928;&#20449;&#21495;&#29305;&#24449;&#30340;&#23376;&#31354;&#38388;&#65292;&#32780;&#20002;&#24323;&#25200;&#21160;&#29305;&#24449;&#65292;&#36825;&#26377;&#21161;&#20110;&#21306;&#20998;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#20026;&#20102;&#38450;&#27490;&#27531;&#20313;&#30340;&#25200;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#27425;&#26368;&#20248;&#26041;&#21521;&#27531;&#24046;&#32593;&#32476;(QPRN)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16176v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is ine
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#20854;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#25552;&#20379;&#20102;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#21327;&#26041;&#24046;&#30340;&#35299;&#26512;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16163</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#21327;&#26041;&#24046;&#20256;&#25773;&#30340;&#35299;&#26512;&#35299;
&lt;/p&gt;
&lt;p&gt;
An Analytic Solution to Covariance Propagation in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#20854;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#25552;&#20379;&#20102;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#21327;&#26041;&#24046;&#30340;&#35299;&#26512;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#34913;&#37327;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#28041;&#21450;&#26114;&#36149;&#25110;&#19981;&#20934;&#30830;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#36817;&#20284;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#22343;&#20540;&#21521;&#37327;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#20026;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;Heaviside&#12289;ReLU&#21644;GELU&#65289;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#21327;&#26041;&#24046;&#25552;&#20379;&#20102;&#35299;&#26512;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#20197;&#21450;&#35757;&#32451;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16163v1 Announce Type: cross  Abstract: Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#30422;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21019;&#24314;&#38543;&#26426;&#25513;&#30721;&#26469;&#32479;&#19968;&#25214;&#21040;&#24182;&#32416;&#27491;&#25925;&#38556;&#20256;&#24863;&#22120;&#65292;&#26377;&#25928;&#24615;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#39118;&#21147;&#28065;&#36718;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.16153</link><description>&lt;p&gt;
&#19968;&#20010;&#25513;&#30422;&#27169;&#22411;&#23601;&#36275;&#22815;&#23454;&#29616;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;
&lt;/p&gt;
&lt;p&gt;
One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#30422;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21019;&#24314;&#38543;&#26426;&#25513;&#30721;&#26469;&#32479;&#19968;&#25214;&#21040;&#24182;&#32416;&#27491;&#25925;&#38556;&#20256;&#24863;&#22120;&#65292;&#26377;&#25928;&#24615;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#39118;&#21147;&#28065;&#36718;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21487;&#38752;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#23545;&#20110;&#30830;&#20445;&#39118;&#21147;&#28065;&#36718;&#31561;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#38271;&#26399;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#30422;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;&#65288;FDIA&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#65292;&#24182;&#25429;&#25417;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#30340;&#25513;&#30422;&#26041;&#27861;&#21019;&#24314;&#38543;&#26426;&#25513;&#30721;&#65292;&#23427;&#23601;&#20687;&#19968;&#20010;&#25925;&#38556;&#65292;&#38024;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#20256;&#24863;&#22120;&#65292;&#20351;&#35757;&#32451;&#21644;&#25512;&#26029;&#20219;&#21153;&#32479;&#19968;&#65306;&#25214;&#21040;&#26377;&#25925;&#38556;&#30340;&#20256;&#24863;&#22120;&#24182;&#36827;&#34892;&#32416;&#27491;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;GE&#36817;&#28023;&#39118;&#21147;&#28065;&#36718;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#32416;&#27491;&#20256;&#24863;&#22120;&#25925;&#38556;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16153v1 Announce Type: cross  Abstract: Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines. In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and self-supervised learning. Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors. During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them. We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor fau
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16149</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#35843;&#26597;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
A Survey on Consumer IoT Traffic: Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#24050;&#32463;&#36827;&#20837;&#20102;&#20844;&#20247;&#29983;&#27963;&#12290;&#23613;&#31649;CIoT&#25552;&#39640;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#36825;&#19968;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#25214;&#20986;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20174;&#27969;&#37327;&#20998;&#26512;&#20013;&#20102;&#35299;CIoT&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#35843;&#26597;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#25506;&#35752;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#20013;&#30340;&#26032;&#29305;&#24449;&#12289;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;2018&#24180;1&#26376;&#33267;2023&#24180;12&#26376;&#25910;&#38598;&#20102;310&#31687;&#19982;CIoT&#27969;&#37327;&#20998;&#26512;&#26377;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#30340;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#35782;&#21035;&#20102;CIoT&#26032;&#29305;&#24449;&#30340;CIoT&#27969;&#37327;&#20998;&#26512;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20116;&#20010;&#24212;&#29992;&#30446;&#26631;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#12289;&#29992;&#25143;&#27963;&#21160;&#25512;&#26029;&#12289;&#24694;&#24847;&#34892;&#20026;&#26816;&#27979;&#12289;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#36890;&#20449;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24212;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;LSTM&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#21463;&#34920;&#38754;&#24352;&#21147;&#24433;&#21709;&#30340;&#27969;&#20307;&#27969;&#21160;&#20013;&#30340;&#33021;&#37327;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.16144</link><description>&lt;p&gt;
&#39044;&#27979;&#28082;&#28404;&#21160;&#21147;&#23398;&#20013;&#30340;&#33021;&#37327;&#39044;&#31639;&#65306;&#19968;&#31181;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24212;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;LSTM&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#21463;&#34920;&#38754;&#24352;&#21147;&#24433;&#21709;&#30340;&#27969;&#20307;&#27969;&#21160;&#20013;&#30340;&#33021;&#37327;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16144v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#20026;&#25506;&#32034;&#22797;&#26434;&#27969;&#21160;&#65288;&#21253;&#25324;&#22810;&#30456;&#21644;&#33258;&#30001;&#34920;&#38754;&#27969;&#21160;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#12290; &#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#23545;&#23398;&#20064;&#20174;&#30636;&#24577;&#36755;&#20837;&#21040;&#21160;&#24577;&#36755;&#20986;&#30340;&#26144;&#23556;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290; &#26412;&#30740;&#31350;&#24212;&#29992;LSTM&#26469;&#39044;&#27979;&#21463;&#34920;&#38754;&#24352;&#21147;&#24433;&#21709;&#30340;&#27969;&#20307;&#27969;&#21160;&#30340;&#30636;&#24577;&#21644;&#38745;&#24577;&#36755;&#20986;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28082;&#28404;&#21160;&#21147;&#23398;&#22330;&#26223;&#65306;&#20855;&#26377;&#19981;&#21516;&#21021;&#22987;&#24418;&#29366;&#30340;&#28082;&#28404;&#19982;&#22266;&#20307;&#34920;&#38754;&#30896;&#25758;&#65292;&#20197;&#21450;&#20004;&#20010;&#28082;&#28404;&#30896;&#25758;&#21518;&#20957;&#32858;&#12290; &#20165;&#20351;&#29992;&#26080;&#37327;&#32434;&#25968;&#21644;&#26469;&#33258;&#25968;&#20540;&#27169;&#25311;&#30340;&#20960;&#20309;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;LSTM&#21487;&#20197;&#39044;&#27979;&#33021;&#37327;&#39044;&#31639;&#12290; &#37319;&#29992;&#26631;&#35760;&#26684;&#28857;&#27861;&#21069;&#21521;&#36319;&#36394;&#26041;&#27861;&#32467;&#21512;&#26631;&#35760;&#26684;&#28857;&#27861;&#26377;&#38480;&#24046;&#20998;&#31574;&#30053;&#26469;&#27169;&#25311;&#28082;&#28404;&#21160;&#21147;&#23398;&#12290; &#20351;&#29992;&#39304;&#36865;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16144v1 Announce Type: cross  Abstract: Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows. The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs. This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects. Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision. Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget. The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics. Using a recurrent neural network (RNN) architecture fed wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181; CFAT &#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#38750;&#37325;&#21472;&#19977;&#35282;&#31383;&#21475;&#25216;&#26415;&#65292;&#19982;&#30697;&#24418;&#31383;&#21475;&#25216;&#26415;&#32467;&#21512;&#65292;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65292;&#20197;&#20943;&#23569;&#36793;&#30028;&#22833;&#30495;&#24182;&#22686;&#21152;&#29420;&#29305;&#31227;&#20301;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.16143</link><description>&lt;p&gt;
CFAT&#65306;&#37322;&#25918;&#19977;&#35282;&#31383;&#21475;&#36827;&#34892;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
CFAT: Unleashing TriangularWindows for Image Super-resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181; CFAT &#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#38750;&#37325;&#21472;&#19977;&#35282;&#31383;&#21475;&#25216;&#26415;&#65292;&#19982;&#30697;&#24418;&#31383;&#21475;&#25216;&#26415;&#32467;&#21512;&#65292;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65292;&#20197;&#20943;&#23569;&#36793;&#30028;&#22833;&#30495;&#24182;&#22686;&#21152;&#29420;&#29305;&#31227;&#20301;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20854;&#25429;&#25417;&#22797;&#26434;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#12290;&#30446;&#21069;&#22312;Transformer&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#37325;&#21472;&#30697;&#24418;&#20559;&#31227;&#31383;&#21475;&#25216;&#26415;&#26159;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22270;&#20687;&#25918;&#22823;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#36793;&#30028;&#22788;&#23384;&#22312;&#22833;&#30495;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#30340;&#29420;&#29305;&#31227;&#20301;&#27169;&#24335;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#24369;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#37325;&#21472;&#19977;&#35282;&#31383;&#21475;&#25216;&#26415;&#65292;&#23427;&#19982;&#30697;&#24418;&#31383;&#21475;&#21516;&#27493;&#24037;&#20316;&#65292;&#20197;&#20943;&#36731;&#36793;&#30028;&#32423;&#21035;&#30340;&#22833;&#30495;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#35775;&#38382;&#26356;&#22810;&#29420;&#29305;&#30340;&#31227;&#20301;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19977;&#35282;-&#30697;&#24418;&#31383;&#21475;&#26412;&#22320;&#27880;&#24847;&#21147;&#19982;&#22522;&#20110;&#36890;&#36947;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#22797;&#21512;&#34701;&#21512;&#27880;&#24847;&#21147;Transformer&#65288;CFAT&#65289;&#65292;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#12290;&#22240;&#27492;&#65292;CFAT&#23454;&#29616;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16143v1 Announce Type: cross  Abstract: Transformer-based models have revolutionized the field of image super-resolution (SR) by harnessing their inherent ability to capture complex contextual features. The overlapping rectangular shifted window technique used in transformer architecture nowadays is a common practice in super-resolution models to improve the quality and robustness of image upscaling. However, it suffers from distortion at the boundaries and has limited unique shifting modes. To overcome these weaknesses, we propose a non-overlapping triangular window technique that synchronously works with the rectangular one to mitigate boundary-level distortion and allows the model to access more unique sifting modes. In this paper, we propose a Composite Fusion Attention Transformer (CFAT) that incorporates triangular-rectangular window-based local attention with a channel-based global attention technique in image super-resolution. As a result, CFAT enables attention mech
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#22522;&#20110;&#30693;&#35782;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28041;&#21450;&#24494;&#35266;&#21644;&#23439;&#35266;&#30693;&#35782;&#65292;&#21253;&#25324;9&#20010;&#30693;&#35782;&#31867;&#21035;&#12289;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.16137</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;&#65306;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22522;&#20110;&#30693;&#35782;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28041;&#21450;&#24494;&#35266;&#21644;&#23439;&#35266;&#30693;&#35782;&#65292;&#21253;&#25324;9&#20010;&#30693;&#35782;&#31867;&#21035;&#12289;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#29616;&#22312;&#26159;&#39044;&#35757;&#32451;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#21464;&#25442;&#22120;&#65292;&#20197;&#21450;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22270;&#27169;&#22411;&#12290;&#25991;&#31456;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35282;&#19979;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21253;&#25324;&#24494;&#35266;&#65288;&#33410;&#28857;&#12289;&#38142;&#25509;&#31561;&#65289;&#21644;&#23439;&#35266;&#30693;&#35782;&#65288;&#31751;&#12289;&#20840;&#23616;&#32467;&#26500;&#31561;&#65289;&#12290;&#28085;&#30422;&#20102;&#20849;&#35745;9&#20010;&#30693;&#35782;&#31867;&#21035;&#21644;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16137v1 Announce Type: new  Abstract: Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#20102;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;34&#39033;&#20195;&#34920;&#24615;&#20114;&#34917;&#25512;&#33616;&#30740;&#31350;&#65292;&#21253;&#25324;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#19981;&#21516;&#30740;&#31350;&#38382;&#39064;&#19979;&#30340;&#27169;&#22411;&#20998;&#31867;&#19982;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.16135</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20114;&#34917;&#25512;&#33616;&#65306;&#23450;&#20041;&#12289;&#26041;&#27861;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#20102;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;34&#39033;&#20195;&#34920;&#24615;&#20114;&#34917;&#25512;&#33616;&#30740;&#31350;&#65292;&#21253;&#25324;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#19981;&#21516;&#30740;&#31350;&#38382;&#39064;&#19979;&#30340;&#27169;&#22411;&#20998;&#31867;&#19982;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20114;&#34917;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;2009&#24180;&#33267;2024&#24180;&#38388;&#36827;&#34892;&#30340;34&#39033;&#20195;&#34920;&#24615;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#29992;&#20110;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#20114;&#34917;&#20851;&#31995;&#30340;&#25968;&#25454;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#20114;&#34917;&#24615;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#26223;&#65292;&#20363;&#22914;&#38750;&#23545;&#31216;&#20114;&#34917;&#24615;&#12289;&#20135;&#21697;&#20043;&#38388;&#26367;&#20195;&#21644;&#20114;&#34917;&#20851;&#31995;&#20849;&#23384;&#65292;&#20197;&#21450;&#19981;&#21516;&#20135;&#21697;&#23545;&#20043;&#38388;&#30340;&#20114;&#34917;&#31243;&#24230;&#19981;&#21516;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26681;&#25454;&#20114;&#34917;&#25512;&#33616;&#30340;&#30740;&#31350;&#38382;&#39064;&#23545;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#22810;&#26679;&#24615;&#12289;&#20010;&#24615;&#21270;&#21644;&#20919;&#21551;&#21160;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30740;&#31350;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16135v1 Announce Type: cross  Abstract: In recent years, complementary recommendation has received extensive attention in the e-commerce domain. In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.16133</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;SSHPool
&lt;/p&gt;
&lt;p&gt;
SSHPool: The Separated Subgraph-based Hierarchical Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16133
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#65292;&#29992;&#20110;&#22270;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26679;&#26412;&#22270;&#30340;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#20998;&#38548;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;&#26412;&#22320;&#22270;&#21367;&#31215;&#21333;&#20803;&#20316;&#20026;&#23616;&#37096;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23558;&#27599;&#20010;&#23376;&#22270;&#21387;&#32553;&#25104;&#19968;&#20010;&#31895;&#31961;&#33410;&#28857;&#65292;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#31895;&#31961;&#22270;&#12290;&#30001;&#20110;&#36825;&#20123;&#23376;&#22270;&#30001;&#19981;&#21516;&#30340;&#31751;&#20998;&#38548;&#24320;&#65292;&#32467;&#26500;&#20449;&#24687;&#26080;&#27861;&#22312;&#23427;&#20204;&#20043;&#38388;&#20256;&#25773;&#65292;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#21487;&#20197;&#26174;&#33879;&#36991;&#20813;&#22823;&#22810;&#25968;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#32467;&#26524;&#31895;&#31961;&#22270;&#19978;&#23618;&#27425;&#22320;&#25191;&#34892;&#25152;&#25552;&#35758;&#30340;&#31243;&#24207;&#65292;SSHPool&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16133v1 Announce Type: new  Abstract: In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification. To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs. We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs). By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#31283;&#23450;&#30340;&#21306;&#38388;&#35266;&#27979;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21463;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#23433;&#20840;&#24615;&#65292;&#24182;&#33021;&#30417;&#27979;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#24182;&#26816;&#27979;&#25925;&#38556;&#12290;</title><link>https://arxiv.org/abs/2403.16132</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#19982;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Runtime Monitoring and Fault Detection for Neural Network-Controlled Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#31283;&#23450;&#30340;&#21306;&#38388;&#35266;&#27979;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21463;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#23433;&#20840;&#24615;&#65292;&#24182;&#33021;&#30417;&#27979;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#24182;&#26816;&#27979;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#25511;&#21046;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36235;&#21183;&#27491;&#22312;&#20852;&#36215;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#23384;&#22312;&#24178;&#25200;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#23433;&#20840;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#31283;&#23450;&#21306;&#38388;&#35266;&#27979;&#22120;&#65292;&#29992;&#20110;&#20026;&#31070;&#32463;&#32593;&#32476;&#12289;&#38750;&#32447;&#24615;&#20989;&#25968;&#21644;&#31995;&#32479;&#29366;&#24577;&#29983;&#25104;&#20934;&#30830;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#12290;&#25152;&#33719;&#24471;&#30340;&#21306;&#38388;&#34987;&#29992;&#20110;&#30417;&#25511;&#23454;&#26102;&#31995;&#32479;&#23433;&#20840;&#24615;&#65292;&#24182;&#26816;&#27979;&#31995;&#32479;&#36755;&#20986;&#25110;&#25191;&#34892;&#22120;&#20013;&#30340;&#25925;&#38556;&#12290;&#36890;&#36807;&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#31995;&#32479;&#36827;&#34892;&#27169;&#25311;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16132v1 Announce Type: cross  Abstract: There is an emerging trend in applying deep learning methods to control complex nonlinear systems. This paper considers enhancing the runtime safety of nonlinear systems controlled by neural networks in the presence of disturbance and measurement noise. A robustly stable interval observer is designed to generate sound and precise lower and upper bounds for the neural network, nonlinear function, and system state. The obtained interval is utilised to monitor the real-time system safety and detect faults in the system outputs or actuators. An adaptive cruise control vehicular system is simulated to demonstrate effectiveness of the proposed design.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;</title><link>https://arxiv.org/abs/2403.16130</link><description>&lt;p&gt;
AKBR: &#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AKBR: Learning Adaptive Kernel-based Representations for Graph Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;&#65288;AKBR&#65289;&#12290;&#19982;&#20165;&#36890;&#36807;&#35745;&#31639;&#22270;&#20043;&#38388;&#21516;&#26500;&#23376;&#32467;&#26500;&#23545;&#30340;&#25968;&#37327;&#26469;&#23450;&#20041;&#30340;&#26368;&#20808;&#36827;&#30340; R-&#21367;&#31215;&#22270;&#26680;&#19981;&#21516;&#65292;&#26080;&#27861;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#31471;&#21040;&#31471;&#23398;&#20064;&#26426;&#21046;&#65292;&#25152;&#25552;&#20986;&#30340;AKBR&#26041;&#27861;&#26088;&#22312;&#23450;&#20041;&#19968;&#20010;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22270;&#26500;&#24314;&#33258;&#36866;&#24212;&#26680;&#30697;&#38453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#21407;&#22987;&#22270;&#20013;&#19981;&#21516;&#23376;&#32467;&#26500;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;AKBR&#27169;&#22411;&#22240;&#27492;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#19981;&#21516;&#23376;&#32467;&#26500;&#30340;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#30001;&#20854;&#32467;&#26500;&#27880;&#24847;&#21147;&#25351;&#23450;&#30340;&#26356;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#25104;&#23545;&#22270;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;&#30001;&#20110;&#32467;&#26524;&#26680;&#30697;&#38453;&#30340;&#27599;&#19968;&#34892;...&#65288;&#27492;&#22788;&#34987;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16130v1 Announce Type: cross  Abstract: In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel mat
&lt;/p&gt;</description></item><item><title>&#22312;&#24322;&#26500;&#38598;&#32676;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Crius&#65292;&#19968;&#20010;&#29992;&#20110;&#26377;&#25928;&#35843;&#24230;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;&#35757;&#32451;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#31216;&#20026;"Cell"&#30340;&#26032;&#35843;&#24230;&#31890;&#24230;&#65292;&#20197;&#35299;&#20915;&#38598;&#32676;&#35843;&#24230;&#20013;&#20302;&#24320;&#38144;&#21644;&#20934;&#30830;&#24615;&#33021;&#25968;&#25454;&#33719;&#21462;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2403.16125</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#24322;&#26500;&#38598;&#32676;&#20013;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#30340;&#35843;&#24230;&#21644;&#24182;&#34892;&#21270;&#20195;&#30721;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16125
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24322;&#26500;&#38598;&#32676;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Crius&#65292;&#19968;&#20010;&#29992;&#20110;&#26377;&#25928;&#35843;&#24230;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;&#35757;&#32451;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#31216;&#20026;"Cell"&#30340;&#26032;&#35843;&#24230;&#31890;&#24230;&#65292;&#20197;&#35299;&#20915;&#38598;&#32676;&#35843;&#24230;&#20013;&#20302;&#24320;&#38144;&#21644;&#20934;&#30830;&#24615;&#33021;&#25968;&#25454;&#33719;&#21462;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#35843;&#24230;&#21644;&#33258;&#36866;&#24212;&#24182;&#34892;&#24615;&#32467;&#21512;&#65292;&#20026;&#25552;&#39640;&#24322;&#26500;GPU&#38598;&#32676;&#19978;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#25552;&#20379;&#20102;&#24040;&#22823;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#36866;&#24212;&#24182;&#34892;&#24615;&#25972;&#21512;&#21040;&#38598;&#32676;&#35843;&#24230;&#22120;&#20013;&#25193;&#23637;&#20102;&#38598;&#32676;&#35843;&#24230;&#31354;&#38388;&#12290;&#26032;&#31354;&#38388;&#26159;&#21407;&#22987;&#35843;&#24230;&#31354;&#38388;&#21644;&#33258;&#36866;&#24212;&#24182;&#34892;&#24615;&#30340;&#24182;&#34892;&#25506;&#32034;&#31354;&#38388;&#30340;&#20056;&#31215;&#65288;&#36824;&#21253;&#25324;&#27969;&#27700;&#32447;&#65292;&#24182;&#34892;&#12289;&#25968;&#25454;&#24182;&#34892;&#21644;&#24352;&#37327;&#24182;&#34892;&#65289;&#12290;&#25351;&#25968;&#32423;&#25193;&#22823;&#30340;&#35843;&#24230;&#31354;&#38388;&#21644;&#33258;&#36866;&#24212;&#24182;&#34892;&#24615;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#26368;&#20339;&#24182;&#34892;&#35745;&#21010;&#20849;&#21516;&#23548;&#33268;&#20102;&#20302;&#24320;&#38144;&#21644;&#20934;&#30830;&#24615;&#33021;&#25968;&#25454;&#33719;&#21462;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#38598;&#32676;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16125v1 Announce Type: cross  Abstract: Joint consideration of scheduling and adaptive parallelism offers great opportunities for improving the training efficiency of large models on heterogeneous GPU clusters. However, integrating adaptive parallelism into a cluster scheduler expands the cluster scheduling space. The new space is the product of the original scheduling space and the parallelism exploration space of adaptive parallelism (also a product of pipeline, data, and tensor parallelism). The exponentially enlarged scheduling space and ever-changing optimal parallelism plan from adaptive parallelism together result in the contradiction between low-overhead and accurate performance data acquisition for efficient cluster scheduling. This paper presents Crius, a training system for efficiently scheduling multiple large models with adaptive parallelism in a heterogeneous cluster. Crius proposes a novel scheduling granularity called Cell. It represents a job with determinis
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#34920;&#26126;&#65292;Neural Scene Flow Prior (NSFP)&#30340;&#24615;&#33021;&#19982;&#36755;&#20837;&#28857;&#20113;&#30340;&#25968;&#37327;&#21576;&#21453;&#27604;&#20851;&#31995;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#24103;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16116</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#24103;&#31070;&#32463;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multi-Frame Neural Scene Flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16116
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#34920;&#26126;&#65292;Neural Scene Flow Prior (NSFP)&#30340;&#24615;&#33021;&#19982;&#36755;&#20837;&#28857;&#20113;&#30340;&#25968;&#37327;&#21576;&#21453;&#27604;&#20851;&#31995;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#24103;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Scene Flow Prior (NSFP)&#21644;Fast Neural Scene Flow (FNSF)&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#30784;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#32479;&#19968;&#31283;&#23450;&#24615;&#30340;&#35270;&#35282;&#26469;&#23457;&#35270;NSFP&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25581;&#31034;&#20854;&#24615;&#33021;&#19982;&#36755;&#20837;&#28857;&#20113;&#25968;&#37327;&#21576;&#21453;&#27604;&#20851;&#31995;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;NSFP&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#36825;&#20123;&#29702;&#35770;&#27934;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#22810;&#24103;&#21382;&#21490;&#28857;&#20113;&#26469;&#25913;&#36827;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#28857;&#20113;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#24103;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16116v1 Announce Type: cross  Abstract: Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large out-of-distribution autonomous driving. Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear. Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds. This finding sheds light on NSFP's effectiveness in handling large-scale point cloud scene flow estimation tasks. Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds. Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#20854;&#22312;&#25918;&#23556;&#23398;&#25945;&#32946;&#12289;&#25253;&#21578;&#29983;&#25104;&#21644;&#24433;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;AI&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#38761;&#21629;&#12290;</title><link>https://arxiv.org/abs/2403.16112</link><description>&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#20013;&#24212;&#29992;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and challenges in the application of large artificial intelligence models in radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#20854;&#22312;&#25918;&#23556;&#23398;&#25945;&#32946;&#12289;&#25253;&#21578;&#29983;&#25104;&#21644;&#24433;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;AI&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ChatGPT&#30340;&#24433;&#21709;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22823;&#22411;&#27169;&#22411;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#36814;&#26469;&#20102;&#39640;&#28526;&#12290;&#38543;&#30528;&#20154;&#20204;&#20139;&#21463;&#30528;&#36825;&#31181;AI&#22823;&#22411;&#27169;&#22411;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#32454;&#20998;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#22823;&#22411;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#25918;&#23556;&#23398;&#25104;&#20687;&#39046;&#22495;&#20013;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#21490;&#12289;&#25216;&#26415;&#32454;&#33410;&#12289;&#24037;&#20316;&#27969;&#31243;&#12289;&#22810;&#27169;&#24335;&#22823;&#22411;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#22823;&#22411;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;AI&#22823;&#22411;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#25945;&#32946;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#12289;&#21333;&#27169;&#24335;&#21644;&#22810;&#27169;&#24335;&#25918;&#23556;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36824;&#24635;&#32467;&#20102;&#25918;&#23556;&#23398;&#20013;&#22823;&#22411;AI&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#25512;&#21160;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#24555;&#36895;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16112v1 Announce Type: cross  Abstract: Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development. As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field. This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models. Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology. Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16108</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#30340;Transformer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Transformer approach for Electricity Price Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;Transformer&#27169;&#22411;&#36827;&#34892;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#65288;EPF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#27809;&#26377;&#20351;&#29992;&#20854;&#20182;&#36882;&#24402;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#34920;&#26126;&#27880;&#24847;&#21147;&#23618;&#36275;&#20197;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#12290;&#35813;&#35770;&#25991;&#36824;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;EPF&#24037;&#20855;&#36827;&#34892;&#20102;&#23545;&#27169;&#22411;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20195;&#30721;&#20197;&#22686;&#24378;EPF&#30740;&#31350;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16108v1 Announce Type: cross  Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2403.16099</link><description>&lt;p&gt;
&#19968;&#20221;&#27861;&#22269;&#20551;&#26032;&#38395;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65306;&#20154;&#31867;&#19982;&#26426;&#22120;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Multi-Label Dataset of French Fake News: Human and Machine Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001; 8 &#20301;&#27880;&#37322;&#32773;&#20351;&#29992; 11 &#20010;&#26631;&#31614;&#27880;&#37322;&#30340;&#26469;&#33258; 17 &#20010;&#27861;&#22269;&#34987;&#19987;&#23478;&#26426;&#26500;&#35748;&#20026;&#19981;&#21487;&#38752;&#30340;&#26032;&#38395;&#26469;&#28304;&#36873;&#21462;&#30340; 100 &#31687;&#25991;&#26723;&#30340;&#35821;&#26009;&#24211; OBSINFOX&#12290;&#36890;&#36807;&#25910;&#38598;&#27604;&#36890;&#24120;&#26356;&#22810;&#30340;&#26631;&#31614;&#21644;&#27880;&#37322;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20154;&#31867;&#35748;&#20026;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20551;&#26032;&#38395;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#21160;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992; Gate Cloud &#36827;&#34892;&#20027;&#39064;&#21644;&#20307;&#35009;&#20998;&#26512;&#65292;&#36825;&#34920;&#26126;&#35821;&#26009;&#24211;&#20013;&#31867;&#20284;&#35773;&#21050;&#30340;&#25991;&#26412;&#26222;&#36941;&#23384;&#22312;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992; VAGO &#20027;&#35266;&#24615;&#20998;&#26512;&#22120;&#21450;&#20854;&#31070;&#32463;&#29256;&#26412;&#65292;&#20197;&#28548;&#28165;&#26631;&#31614;&#8220;&#20027;&#35266;&#8221;&#19982;&#26631;&#31614;&#8220;&#20551;&#26032;&#38395;&#8221;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;&#20197;&#19979;&#32593;&#22336;&#22312;&#32447;&#33719;&#21462;&#65306;https://github.com/obs-info/obsinfox
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16099v1 Announce Type: new  Abstract: We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;APL&#65288;&#38463;&#25289;&#20271;&#32534;&#31243;&#35821;&#35328;&#65289;&#65292;&#23427;&#20351;&#29992;LLM&#20316;&#20026;&#21322;&#32534;&#35793;&#22120;&#65292;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#20195;&#30721;&#36716;&#25442;&#20026;Python&#20195;&#30721;&#24182;&#36816;&#34892;&#65292;&#26500;&#24314;&#20102;&#23436;&#25972;&#30340;&#27969;&#27700;&#32447;&#12290;</title><link>https://arxiv.org/abs/2403.16087</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#38463;&#25289;&#20271;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs as Compiler for Arabic Programming Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;APL&#65288;&#38463;&#25289;&#20271;&#32534;&#31243;&#35821;&#35328;&#65289;&#65292;&#23427;&#20351;&#29992;LLM&#20316;&#20026;&#21322;&#32534;&#35793;&#22120;&#65292;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#20195;&#30721;&#36716;&#25442;&#20026;Python&#20195;&#30721;&#24182;&#36816;&#34892;&#65292;&#26500;&#24314;&#20102;&#23436;&#25972;&#30340;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;APL (Arabic Programming Language)&#65292;&#23427;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#21322;&#32534;&#35793;&#22120;&#65292;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#20195;&#30721;&#36716;&#25442;&#20026;Python&#20195;&#30721;&#65292;&#28982;&#21518;&#36816;&#34892;&#35813;&#20195;&#30721;&#12290;&#35774;&#35745;&#20102;&#20174;APL&#25991;&#26412;&#32467;&#26500;&#21040;&#25552;&#31034;&#65288;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65289;&#20877;&#21040;&#20351;&#29992;PyRunner&#36816;&#34892;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#30340;&#23436;&#25972;&#27969;&#27700;&#32447;&#12290;&#35813;&#39033;&#30446;&#21253;&#25324;&#19977;&#37096;&#20998;&#65306;Python&#24211;&#65292;&#20855;&#26377;&#31616;&#21333;&#30028;&#38754;&#30340;&#28216;&#20048;&#22330;&#21644;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16087v1 Announce Type: cross  Abstract: In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code. Designing a full pipeline from the structure of the APL text then a prompt (using prompt engineering) then running the prodcued python code using PyRunner. This project has a three parts first python library, a playground with simple interface and this research paper.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#25209;&#22788;&#29702;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;IBCB&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26681;&#25454;&#19987;&#23478;&#30340;&#34892;&#20026;&#28436;&#21464;&#21382;&#21490;&#23545;&#29615;&#22659;&#22870;&#21169;&#21442;&#25968;&#21644;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16075</link><description>&lt;p&gt;
IBCB: &#39640;&#25928;&#30340;&#36870;&#25209;&#22788;&#29702;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#34892;&#20026;&#28436;&#21464;&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16075
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#25209;&#22788;&#29702;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;IBCB&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26681;&#25454;&#19987;&#23478;&#30340;&#34892;&#20026;&#28436;&#21464;&#21382;&#21490;&#23545;&#29615;&#22659;&#22870;&#21169;&#21442;&#25968;&#21644;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#20851;&#27880;&#19987;&#23478;&#30340;&#34892;&#20026;&#26426;&#21046;&#24314;&#27169;&#65292;&#38656;&#35201;&#22823;&#37327;&#30001;&#26576;&#20010;&#22266;&#23450;&#19987;&#23478;&#29983;&#25104;&#30340;&#20132;&#20114;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#27969;&#24335;&#24212;&#29992;&#20013;&#65292;&#22914;&#27969;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#32447;&#20915;&#31574;&#32773;&#36890;&#24120;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#32447;&#20915;&#31574;&#32773;&#29983;&#25104;&#30340;&#20132;&#20114;&#21382;&#21490;&#21253;&#25324;&#20182;&#20204;&#20174;&#26032;&#25163;&#19987;&#23478;&#21040;&#26377;&#32463;&#39564;&#19987;&#23478;&#30340;&#34892;&#20026;&#28436;&#21464;&#12290;&#36825;&#32473;&#29616;&#26377;&#30340;&#21482;&#33021;&#21033;&#29992;&#26377;&#32463;&#39564;&#19987;&#23478;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#26032;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#25209;&#22788;&#29702;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;IBCB&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#22522;&#20110;&#19987;&#23478;&#34892;&#20026;&#28436;&#21464;&#21382;&#21490;&#30340;&#29615;&#22659;&#22870;&#21169;&#21442;&#25968;&#21644;&#23398;&#20064;&#31574;&#30053;&#30340;&#20272;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;IBCB&#23558;&#36870;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#31616;&#21333;&#30340;&#20108;&#27425;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16075v1 Announce Type: new  Abstract: Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25913;&#36827;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#26631;&#31614;&#20256;&#25773;&#27169;&#22411;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#21407;&#22987;&#27969;&#24418;&#27491;&#21017;&#21270;&#27169;&#22411;&#22312;&#23616;&#37096;&#21306;&#22495;&#24615;&#33021;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16059</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Manifold Regularization Classification Model Based On Improved Diffusion Map
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25913;&#36827;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#26631;&#31614;&#20256;&#25773;&#27169;&#22411;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#21407;&#22987;&#27969;&#24418;&#27491;&#21017;&#21270;&#27169;&#22411;&#22312;&#23616;&#37096;&#21306;&#22495;&#24615;&#33021;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#27491;&#21017;&#21270;&#27169;&#22411;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#21253;&#25324;&#23569;&#37327;&#26377;&#26631;&#31614;&#26679;&#26412;&#21644;&#22823;&#37327;&#26080;&#26631;&#31614;&#26679;&#26412;&#65292;&#29983;&#25104;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#27969;&#24418;&#33539;&#25968;&#38480;&#21046;&#20102;&#27169;&#22411;&#24615;&#33021;&#21482;&#23616;&#38480;&#20110;&#23616;&#37096;&#21306;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27969;&#24418;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#22686;&#24378;&#25193;&#25955;&#26144;&#23556;&#31639;&#27861;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;Neumann&#28909;&#26680;&#65292;&#20351;&#20854;&#33021;&#22815;&#20934;&#30830;&#25551;&#36848;&#27969;&#24418;&#19978;&#30340;&#26631;&#31614;&#20256;&#25773;&#36807;&#31243;&#12290;&#21033;&#29992;&#35813;&#30697;&#38453;&#65292;&#22312;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#19968;&#20010;&#25551;&#36848;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#19979;&#26631;&#31614;&#20998;&#24067;&#30340;&#26631;&#31614;&#20256;&#25773;&#20989;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#31614;&#20256;&#25773;&#20989;&#25968;&#25193;&#23637;&#21040;&#25972;&#20010;&#25968;&#25454;&#27969;&#24418;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#23637;&#30340;&#26631;&#31614;&#20256;&#25773;&#20989;&#25968;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16059v1 Announce Type: cross  Abstract: Manifold regularization model is a semi-supervised learning model that leverages the geometric structure of a dataset, comprising a small number of labeled samples and a large number of unlabeled samples, to generate classifiers. However, the original manifold norm limits the performance of models to local regions. To address this limitation, this paper proposes an approach to improve manifold regularization based on a label propagation model. We initially enhance the probability transition matrix of the diffusion map algorithm, which can be used to estimate the Neumann heat kernel, enabling it to accurately depict the label propagation process on the manifold. Using this matrix, we establish a label propagation function on the dataset to describe the distribution of labels at different time steps. Subsequently, we extend the label propagation function to the entire data manifold. We prove that the extended label propagation function c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21345;&#25176;&#22270;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#39044;&#27979;&#26410;&#23433;&#35013;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#24182;&#23454;&#29616;&#38271;&#26399;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.16049</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#21345;&#25176;&#22270;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#31995;&#32479;&#30340;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21345;&#25176;&#22270;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#39044;&#27979;&#26410;&#23433;&#35013;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#24182;&#23454;&#29616;&#38271;&#26399;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#39044;&#27979;&#26102;&#38388;&#27169;&#24335;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#36712;&#36857;&#32454;&#24494;&#19988;&#32463;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#39044;&#27979;&#26694;&#26550;&#19981;&#26029;&#34987;&#23436;&#21892;&#65292;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#26041;&#27861;&#12289;&#25968;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31995;&#32479;&#20043;&#19968;&#65292;&#20849;&#20139;&#20132;&#36890;&#31995;&#32479;&#22914;&#20849;&#20139;&#21333;&#36710;&#30001;&#20110;&#22478;&#24066;&#32422;&#26463;&#21644;&#29615;&#22659;&#38382;&#39064;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#30001;&#20110;&#31995;&#32479;&#30340;&#24320;&#25918;&#24615;&#21644;&#21508;&#31449;&#28857;&#20043;&#38388;&#30340;&#20351;&#29992;&#27169;&#24335;&#19981;&#24179;&#34913;&#65292;&#39044;&#27979;&#21333;&#36710;&#31449;&#28857;&#30340;&#31199;&#20511;&#21644;&#24402;&#36824;&#27169;&#24335;&#20173;&#28982;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21345;&#25176;&#22270;&#26041;&#27861;&#26469;&#39044;&#27979;&#31199;&#20511;&#21644;&#24402;&#36824;&#27169;&#24335;&#12290;&#21345;&#25176;&#22270;&#26041;&#27861;&#26377;&#21161;&#20110;&#23545;&#26032;&#23433;&#35013;&#31449;&#28857;&#30340;&#38656;&#27714;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#20123;&#26032;&#31449;&#28857;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#38271;&#26399;&#39044;&#27979;&#65292;&#36825;&#26159;&#20197;&#21069;&#23578;&#26410;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16049v1 Announce Type: new  Abstract: Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SSA-GCN&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#27169;&#25311;&#22270;&#32467;&#26500;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#21644;&#22797;&#26434;&#32593;&#32476;&#30340;&#35282;&#24230;&#25552;&#21462;&#26080;&#30417;&#30563;&#29305;&#24449;&#65292;&#20197;&#25552;&#21319;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16033</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SSA-GCN&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#27169;&#25311;&#22270;&#32467;&#26500;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#21644;&#22797;&#26434;&#32593;&#32476;&#30340;&#35282;&#24230;&#25552;&#21462;&#26080;&#30417;&#30563;&#29305;&#24449;&#65292;&#20197;&#25552;&#21319;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#65292;&#20063;&#31216;&#20026;&#22797;&#26434;&#32593;&#32476;&#25968;&#25454;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#20043;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#25552;&#21462;&#29305;&#23450;&#20219;&#21153;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#22312;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#22266;&#26377;&#35821;&#20041;&#21644;&#32467;&#26500;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#20041;-&#32467;&#26500;&#27880;&#24847;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SSA-GCN&#65289;&#65292;&#19981;&#20165;&#27169;&#25311;&#20102;&#22270;&#32467;&#26500;&#65292;&#36824;&#20174;&#24635;&#20307;&#19978;&#25552;&#21462;&#20102;&#26080;&#30417;&#30563;&#29305;&#24449;&#20197;&#22686;&#24378;&#39030;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;SSA-GCN&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#19977;&#20010;&#26041;&#38754;&#34920;&#29616;&#65306;&#39318;&#20808;&#65292;&#23427;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#30340;&#35282;&#24230;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#26469;&#33719;&#24471;&#35821;&#20041;&#20449;&#24687;&#65307;&#20854;&#27425;&#65292;&#23427;&#36890;&#36807;&#20174;&#22797;&#26434;&#32593;&#32476;&#30340;&#35282;&#24230;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#26469;&#33719;&#24471;&#32467;&#26500;&#20449;&#24687;&#65307;&#26368;&#21518;&#65292;&#23427;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#36825;&#20123;&#29305;&#24449;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16033v1 Announce Type: cross  Abstract: Graph data, also known as complex network data, is omnipresent across various domains and applications. Prior graph neural network models primarily focused on extracting task-specific structural features through supervised learning objectives, but they fell short in capturing the inherent semantic and structural features of the entire graph. In this paper, we introduce the semantic-structural attention-enhanced graph convolutional network (SSA-GCN), which not only models the graph structure but also extracts generalized unsupervised features to enhance vertex classification performance. The SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic information through unsupervised feature extraction from a knowledge graph perspective; secondly, it obtains structural information through unsupervised feature extraction from a complex network perspective; and finally, it integrates these features through a cross-attent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24403;&#21487;&#29992;&#37096;&#20998;&#22240;&#26524;&#39034;&#24207;&#21464;&#37327;&#26102;&#23398;&#20064;DAGs&#30340;&#20013;&#38388;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20272;&#35745;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16031</link><description>&lt;p&gt;
&#20174;&#20559;&#24207;&#20851;&#31995;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Directed Acyclic Graphs from Partial Orderings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24403;&#21487;&#29992;&#37096;&#20998;&#22240;&#26524;&#39034;&#24207;&#21464;&#37327;&#26102;&#23398;&#20064;DAGs&#30340;&#20013;&#38388;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20272;&#35745;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#36890;&#24120;&#29992;&#20110;&#27169;&#25311;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#23398;&#20064;DAG&#32467;&#26500;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36793;&#30340;&#26041;&#21521;&#21487;&#33021;&#26080;&#27861;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20986;&#26469;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#21487;&#29992;&#37096;&#20998;&#22240;&#26524;&#39034;&#24207;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;DAGs&#30340;&#20013;&#38388;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#37096;&#20998;&#39034;&#24207;&#30340;&#36890;&#29992;&#20272;&#35745;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#21644;&#39640;&#32500;&#38382;&#39064;&#30340;&#26377;&#25928;&#20272;&#35745;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#20248;&#21183;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16031v1 Announce Type: cross  Abstract: Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20026;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#30001;&#20010;&#24615;&#21270;PageRank&#65288;PPR&#65289;&#37319;&#26679;&#30340;&#20196;&#29260;&#21015;&#34920;&#65292;&#28982;&#21518;&#20165;&#22312;&#27492;&#21015;&#34920;&#19978;&#24212;&#29992;&#26631;&#20934;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26469;&#35745;&#31639;&#20854;&#33410;&#28857;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#22270;&#21464;&#25442;&#22120;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#30340;&#26679;&#26412;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16030</link><description>&lt;p&gt;
VCR-Graphormer&#65306;&#36890;&#36807;&#34394;&#25311;&#36830;&#25509;&#23454;&#29616;&#30340;&#23567;&#25209;&#37327;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16030
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20026;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#30001;&#20010;&#24615;&#21270;PageRank&#65288;PPR&#65289;&#37319;&#26679;&#30340;&#20196;&#29260;&#21015;&#34920;&#65292;&#28982;&#21518;&#20165;&#22312;&#27492;&#21015;&#34920;&#19978;&#24212;&#29992;&#26631;&#20934;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26469;&#35745;&#31639;&#20854;&#33410;&#28857;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#22270;&#21464;&#25442;&#22120;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#30340;&#26679;&#26412;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#25442;&#22120;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#25429;&#33719;&#22797;&#26434;&#25299;&#25169;&#21644;&#29305;&#24449;&#20449;&#24687;&#30340;&#34920;&#36798;&#24615;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#22270;&#21464;&#25442;&#22120;&#20256;&#32479;&#19978;&#23545;&#27599;&#23545;&#33410;&#28857;&#25191;&#34892;&#23494;&#38598;&#27880;&#24847;&#21147;&#65288;&#25110;&#20840;&#23616;&#27880;&#24847;&#21147;&#65289;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#21521;&#37327;&#65292;&#23548;&#33268;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#23545;&#20110;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#26159;&#26080;&#27861;&#25215;&#21463;&#30340;&#12290;&#22240;&#27492;&#65292;&#22270;&#21464;&#25442;&#22120;&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#27599;&#20010;&#23567;&#25209;&#37327;&#20013;&#30340;&#26377;&#38480;&#26679;&#26412;&#26080;&#27861;&#25903;&#25345;&#26377;&#25928;&#30340;&#23494;&#38598;&#27880;&#24847;&#21147;&#20197;&#32534;&#30721;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16030v1 Announce Type: new  Abstract: Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from comp
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#25968;&#25454;&#38598;&#31934;&#31616;&#20013;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#35843;&#26597;&#19982;&#25506;&#32034;&#65292;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.16028</link><description>&lt;p&gt;
&#25506;&#31350;&#25968;&#25454;&#38598;&#20559;&#24046;&#23545;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Dataset Bias on Dataset Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16028
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#25968;&#25454;&#38598;&#31934;&#31616;&#20013;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#35843;&#26597;&#19982;&#25506;&#32034;&#65292;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#65288;DD&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21512;&#25104;&#19968;&#20010;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#22522;&#26412;&#20449;&#24687;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#22987;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#65292;&#24182;&#26377;&#21161;&#20110;&#20943;&#36731;&#35757;&#32451;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;DD&#26041;&#27861;&#36890;&#24120;&#22312;&#20551;&#35774;&#25968;&#25454;&#38598;&#19981;&#20855;&#26377;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#36816;&#20316;&#65292;&#24573;&#35270;&#20102;&#25968;&#25454;&#38598;&#26412;&#36523;&#21487;&#33021;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#25968;&#25454;&#38598;&#20559;&#24046;&#23545;DD&#30340;&#24433;&#21709;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;DD&#39046;&#22495;&#30340;&#39318;&#27425;&#25506;&#32034;&#12290;&#37492;&#20110;&#30446;&#21069;&#27809;&#26377;&#36866;&#29992;&#20110;DD&#30340;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#20004;&#20010;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;CMNIST-DD&#21644;CCIFAR10-DD&#65292;&#20026;&#38543;&#21518;&#30340;&#20998;&#26512;&#22880;&#23450;&#22522;&#30784;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;DD&#26041;&#27861;&#22312;CMNIST-DD&#21644;CCIFAR10-DD&#19978;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#25353;&#29031;&#26631;&#20934;&#27969;&#31243;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16028v1 Announce Type: cross  Abstract: Dataset Distillation (DD) is a promising technique to synthesize a smaller dataset that preserves essential information from the original dataset. This synthetic dataset can serve as a substitute for the original large-scale one, and help alleviate the training workload. However, current DD methods typically operate under the assumption that the dataset is unbiased, overlooking potential bias issues within the dataset itself. To fill in this blank, we systematically investigate the influence of dataset bias on DD. To the best of our knowledge, this is the first exploration in the DD domain. Given that there are no suitable biased datasets for DD, we first construct two biased datasets, CMNIST-DD and CCIFAR10-DD, to establish a foundation for subsequent analysis. Then we utilize existing DD methods to generate synthetic datasets on CMNIST-DD and CCIFAR10-DD, and evaluate their performance following the standard process. Experiments demo
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#21152;&#36895;&#31283;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#32479;&#19968;&#27169;&#22359;&#65292;&#37325;&#28857;&#20851;&#27880;lcm-lora&#27169;&#22359;&#65292;&#25552;&#20379;&#20102;&#26080;&#26465;&#20214;&#31283;&#23450;&#25193;&#25955;&#21512;&#25104;&#21152;&#36895;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#25968;&#20540;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16024</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;STABLE-DIFFUSION&#30340;&#32479;&#19968;&#27169;&#22359;&#65306;LCM-LORA
&lt;/p&gt;
&lt;p&gt;
A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16024
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#21152;&#36895;&#31283;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#32479;&#19968;&#27169;&#22359;&#65292;&#37325;&#28857;&#20851;&#27880;lcm-lora&#27169;&#22359;&#65292;&#25552;&#20379;&#20102;&#26080;&#26465;&#20214;&#31283;&#23450;&#25193;&#25955;&#21512;&#25104;&#21152;&#36895;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#25968;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#29992;&#20110;&#21152;&#36895;&#31283;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#32479;&#19968;&#27169;&#22359;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;lcm-lora&#27169;&#22359;&#12290;&#31283;&#23450;&#25193;&#25955;&#36807;&#31243;&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21152;&#36895;&#36825;&#20123;&#36807;&#31243;&#23545;&#20110;&#25552;&#39640;&#35745;&#31639;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#22266;&#23450;&#28304;&#31163;&#25955;&#30697;&#38382;&#39064;&#30340;&#26631;&#20934;&#36845;&#20195;&#36807;&#31243;&#36890;&#24120;&#22312;&#20809;&#23398;&#21402;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#36739;&#24930;&#30340;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26080;&#26465;&#20214;&#31283;&#23450;&#25193;&#25955;&#21152;&#36895;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#20256;&#36755;&#26041;&#31243;&#21644;&#31163;&#25955;&#30697;&#38382;&#39064;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#26080;&#26465;&#20214;&#31283;&#23450;&#25193;&#25955;&#21512;&#25104;&#21152;&#36895;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#25968;&#20540;&#32467;&#26524;&#65292;&#20026;&#27169;&#22411;&#31163;&#25955;&#30697;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16024v1 Announce Type: new  Abstract: This paper presents a comprehensive study on the unified module for accelerating stable-diffusion processes, specifically focusing on the lcm-lora module. Stable-diffusion processes play a crucial role in various scientific and engineering domains, and their acceleration is of paramount importance for efficient computational performance. The standard iterative procedures for solving fixed-source discrete ordinates problems often exhibit slow convergence, particularly in optically thick scenarios. To address this challenge, unconditionally stable diffusion-acceleration methods have been developed, aiming to enhance the computational efficiency of transport equations and discrete ordinates problems. This study delves into the theoretical foundations and numerical results of unconditionally stable diffusion synthetic acceleration methods, providing insights into their stability and performance for model discrete ordinates problems. Furtherm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#22270;&#32593;&#32476;&#32467;&#26500;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#32852;&#37030;&#21442;&#25968;&#32858;&#21512;&#26041;&#27861;FLGNN&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#38544;&#31169;&#23433;&#20840;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23454;&#39564;&#21644;&#24046;&#20998;&#38544;&#31169;&#38450;&#24481;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.16004</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#22270;&#32593;&#32476;&#32467;&#26500;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#32852;&#37030;&#21442;&#25968;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#22270;&#32593;&#32476;&#32467;&#26500;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#32852;&#37030;&#21442;&#25968;&#32858;&#21512;&#26041;&#27861;FLGNN&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#38544;&#31169;&#23433;&#20840;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23454;&#39564;&#21644;&#24046;&#20998;&#38544;&#31169;&#38450;&#24481;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#30001;&#20110;&#20854;&#21327;&#20316;&#35757;&#32451;&#22810;&#20010;&#26469;&#28304;&#25968;&#25454;&#32780;&#19981;&#20250;&#27844;&#38706;&#38544;&#31169;&#30340;&#33021;&#21147;&#65292;&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#65292;&#30001;&#23458;&#25143;&#31471;&#25345;&#26377;&#30340;&#22270;&#30340;&#33410;&#28857;&#21644;&#32593;&#32476;&#32467;&#26500;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#21516;&#30340;&#65292;&#30452;&#25509;&#20849;&#20139;&#27169;&#22411;&#26799;&#24230;&#30340;&#32858;&#21512;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#22270;&#32852;&#37030;&#22330;&#26223;&#30340;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;FLGNN&#65292;&#24182;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27599;&#19968;&#23618;&#21442;&#25968;&#20849;&#20139;&#30340;&#32858;&#21512;&#25928;&#26524;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;FLGNN&#30340;&#26377;&#25928;&#24615;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20445;&#25252;FLGNN&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23454;&#39564;&#21644;&#24046;&#20998;&#38544;&#31169;&#38450;&#24481;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16004v1 Announce Type: cross  Abstract: Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy. However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario. Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model. The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets. Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense expe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30165;&#36857;&#22238;&#24402;&#27169;&#22411;&#19979;&#20197;&#39640;&#26031;&#27979;&#37327;&#30697;&#38453;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#31169;&#26377;&#21021;&#22987;&#21270;&#30340;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;DP&#21021;&#22987;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;Riemannian&#20248;&#21270;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#20272;&#35745;&#32467;&#26524;&#30340;&#38750;&#24179;&#20961;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.15999</link><description>&lt;p&gt;
&#20855;&#26377;&#20445;&#35777;&#31169;&#26377;&#21021;&#22987;&#21270;&#30340;&#36817;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#20302;&#31209;&#30165;&#36857;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal differentially private low-rank trace regression with guaranteed private initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30165;&#36857;&#22238;&#24402;&#27169;&#22411;&#19979;&#20197;&#39640;&#26031;&#27979;&#37327;&#30697;&#38453;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#31169;&#26377;&#21021;&#22987;&#21270;&#30340;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;DP&#21021;&#22987;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;Riemannian&#20248;&#21270;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#20272;&#35745;&#32467;&#26524;&#30340;&#38750;&#24179;&#20961;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#27979;&#37327;&#30697;&#38453;&#30340;&#30165;&#36857;&#22238;&#24402;&#27169;&#22411;&#19979;&#65292;&#23545;&#31209;&#20026;$r$&#30340;&#30697;&#38453;$M \in \RR^{d_1\times d_2}$&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20272;&#35745;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#31934;&#30830;&#34920;&#24449;&#20102;&#38750;&#31169;&#26377;&#35889;&#21021;&#22987;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#22312;Schatten-$q$&#33539;&#25968;&#19979;&#20272;&#35745;$M$&#30340;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#26497;&#23567;&#27010;&#29575;&#19979;&#30028;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;DP&#21021;&#22987;&#21270;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22823;&#23567;&#20026;$n \geq \wt O (r^2 (d_1\vee d_2))$&#12290;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;DP&#21021;&#22987;&#21270;&#33853;&#20837;&#22260;&#32469;$M$&#30340;&#23616;&#37096;&#29699;&#20869;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#20248;&#21270;&#30340;&#29992;&#20110;&#20272;&#35745;$M$&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65288;DP-RGrad&#65289;&#65292;&#36890;&#36807;DP&#21021;&#22987;&#21270;&#21644;&#26679;&#26412;&#22823;&#23567;$n \geq \wt O(r (d_1 + d_2))$&#23454;&#29616;&#20102;&#36817;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#21021;&#22987;&#21270;&#21644;&#26679;&#26412;&#22823;&#23567;&#26465;&#20214;&#19979;&#30340;&#25152;&#20272;&#35745;&#30340;$M$&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15999v1 Announce Type: cross  Abstract: We study differentially private (DP) estimation of a rank-$r$ matrix $M \in \RR^{d_1\times d_2}$ under the trace regression model with Gaussian measurement matrices. Theoretically, the sensitivity of non-private spectral initialization is precisely characterized, and the differential-privacy-constrained minimax lower bound for estimating $M$ under the Schatten-$q$ norm is established. Methodologically, the paper introduces a computationally efficient algorithm for DP-initialization with a sample size of $n \geq \wt O (r^2 (d_1\vee d_2))$. Under certain regularity conditions, the DP-initialization falls within a local ball surrounding $M$. We also propose a differentially private algorithm for estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence rate with the DP-initialization and sample size of $n \geq \wt O(r (d_1 + d_2))$. Finally, the paper discusses the non-trivial gap between the mi
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15989</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge-guided Machine Learning: Current Trends and Future Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15989
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31185;&#23398;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#24314;&#27169;&#20013;&#30340;&#20114;&#34917;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#26032;&#20852;&#39046;&#22495;&#31185;&#23398;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26088;&#22312;&#21033;&#29992;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20174;&#20351;&#29992;&#30340;&#31185;&#23398;&#30693;&#35782;&#31867;&#22411;&#12289;&#25506;&#35752;&#30340;&#30693;&#35782;-ML&#38598;&#25104;&#24418;&#24335;&#20197;&#21450;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25972;&#21512;&#31185;&#23398;&#30693;&#35782;&#30340;&#26041;&#27861;&#31561;&#26041;&#38754;&#35752;&#35770;&#20102;KGML&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#29615;&#22659;&#31185;&#23398;&#20013;&#21457;&#23637;&#30340;KGML&#26041;&#27861;&#30340;&#19968;&#20123;&#24120;&#35265;&#29992;&#20363;&#31867;&#21035;&#65292;&#20197;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20363;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15989v1 Announce Type: cross  Abstract: This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.
&lt;/p&gt;</description></item><item><title>CBGT-Net&#26159;&#19968;&#31181;&#21463;CBGT&#22238;&#36335;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#31215;&#32047;&#36275;&#22815;&#30340;&#35777;&#25454;&#21518;&#25165;&#23545;&#27969;&#25968;&#25454;&#20135;&#29983;&#20998;&#31867;&#20915;&#31574;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15974</link><description>&lt;p&gt;
CBGT-Net: &#19968;&#31181;&#29992;&#20110;&#23545;&#27969;&#25968;&#25454;&#36827;&#34892;&#31283;&#20581;&#20998;&#31867;&#30340;&#31867;&#33041;&#27169;&#20223;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15974
&lt;/p&gt;
&lt;p&gt;
CBGT-Net&#26159;&#19968;&#31181;&#21463;CBGT&#22238;&#36335;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#31215;&#32047;&#36275;&#22815;&#30340;&#35777;&#25454;&#21518;&#25165;&#23545;&#27969;&#25968;&#25454;&#20135;&#29983;&#20998;&#31867;&#20915;&#31574;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;CBGT-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21754;&#20083;&#21160;&#29289;&#22823;&#33041;&#20013;&#30382;&#36136;-&#22522;&#24213;&#33410;-&#19992;&#33041;&#65288;CBGT&#65289;&#22238;&#36335;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19981;&#21516;&#65292;CBGT-Net&#23398;&#20064;&#22312;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#27969;&#20013;&#36798;&#21040;&#36275;&#22815;&#35777;&#25454;&#26631;&#20934;&#21518;&#20135;&#29983;&#36755;&#20986;&#12290;&#23545;&#20110;&#27599;&#27425;&#35266;&#23519;&#65292;CBGT-Net&#29983;&#25104;&#19968;&#20010;&#30690;&#37327;&#65292;&#26126;&#30830;&#34920;&#31034;&#35266;&#23519;&#20026;&#27599;&#20010;&#28508;&#22312;&#20915;&#23450;&#25552;&#20379;&#30340;&#35777;&#25454;&#37327;&#65292;&#38543;&#26102;&#38388;&#32047;&#31215;&#35777;&#25454;&#65292;&#24182;&#22312;&#32047;&#31215;&#35777;&#25454;&#36229;&#36807;&#39044;&#23450;&#20041;&#38408;&#20540;&#26102;&#29983;&#25104;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#27169;&#22411;&#38656;&#35201;&#26681;&#25454;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#23567;&#34917;&#19969;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CBGT-Net&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15974v1 Announce Type: cross  Abstract: This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains. Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data. For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold. We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image. We show that the CBGT-Net provides improved accuracy and robustness compared t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;&#36827;&#34892;&#38382;&#39064;&#36172;&#21338;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23558;&#29305;&#24449;&#20174;102&#20010;&#20943;&#23569;&#21040;5&#20010;&#12290;</title><link>https://arxiv.org/abs/2403.15962</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#38382;&#39064;&#36172;&#21338;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Detection of Problem Gambling with Less Features Using Machine Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;&#36827;&#34892;&#38382;&#39064;&#36172;&#21338;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23558;&#29305;&#24449;&#20174;102&#20010;&#20943;&#23569;&#21040;5&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36172;&#21338;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#23545;&#29992;&#25143;&#27599;&#26085;&#34892;&#20026;&#25968;&#25454;&#30340;&#30417;&#25511;&#32780;&#25191;&#34892;&#20998;&#26512;&#29305;&#24449;&#12290;&#22312;&#25191;&#34892;&#38382;&#39064;&#36172;&#21338;&#26816;&#27979;&#26102;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20026;&#24314;&#31435;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#30456;&#23545;&#20016;&#23500;&#30340;&#20998;&#26512;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25910;&#38598;&#20998;&#26512;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;&#36827;&#34892;&#31934;&#30830;&#26816;&#27979;&#23558;&#26497;&#22823;&#20943;&#23569;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; PGN4&#65292;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#20998;&#26512;&#29305;&#24449;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#23558;102&#20010;&#29305;&#24449;&#20943;&#23569;&#21040;5&#20010;&#29305;&#24449;&#26102;&#65292;PGN4&#20165;&#36973;&#36935;&#20102;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#25490;&#21517;&#21069;5&#30340;&#29305;&#24449;&#30340;&#20849;&#21516;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15962v1 Announce Type: cross  Abstract: Analytic features in gambling study are performed based on the amount of data monitoring on user daily actions. While performing the detection of problem gambling, existing datasets provide relatively rich analytic features for building machine learning based model. However, considering the complexity and cost of collecting the analytic features in real applications, conducting precise detection with less features will tremendously reduce the cost of data collection. In this study, we propose a deep neural networks PGN4 that performs well when using limited analytic features. Through the experiment on two datasets, we discover that PGN4 only experiences a mere performance drop when cutting 102 features to 5 features. Besides, we find the commonality within the top 5 features from two datasets.
&lt;/p&gt;</description></item><item><title>&#28145;&#20837;&#25506;&#31350;&#25439;&#22833;&#21387;&#32553;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#29616;&#20195;&#25439;&#22833;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#36136;&#37327;&#25439;&#22833;&#22312;1%&#20197;&#19979;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;50-100&#20493;&#30340;&#21387;&#32553;&#27604;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.15953</link><description>&lt;p&gt;
&#20102;&#35299;&#25439;&#22833;&#21387;&#32553;&#22312;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding The Effectiveness of Lossy Compression in Machine Learning Training Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15953
&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#25506;&#31350;&#25439;&#22833;&#21387;&#32553;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#29616;&#20195;&#25439;&#22833;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#36136;&#37327;&#25439;&#22833;&#22312;1%&#20197;&#19979;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;50-100&#20493;&#30340;&#21387;&#32553;&#27604;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;ML/AI&#65289;&#25216;&#26415;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#20013;&#21464;&#24471;&#26085;&#30410;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#38656;&#35201;&#26041;&#27861;&#22312;&#24191;&#22495;&#32593;&#32476;&#65288;WAN&#65289;&#19978;&#20849;&#20139;&#25968;&#25454;&#25110;&#23558;&#20854;&#20174;&#36793;&#32536;&#35774;&#22791;&#20256;&#36755;&#21040;&#25968;&#25454;&#20013;&#24515;&#12290;&#25968;&#25454;&#21387;&#32553;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#25439;&#22833;&#21387;&#32553;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;ML/AI&#25968;&#25454;&#20943;&#23569;&#25216;&#26415;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#23427;&#23545;17&#31181;&#25968;&#25454;&#20943;&#23569;&#26041;&#27861;&#22312;7&#20010;ML/AI&#24212;&#29992;&#31243;&#24207;&#19978;&#36827;&#34892;&#20102;&#38750;&#24120;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#34920;&#26126;&#29616;&#20195;&#30340;&#25439;&#22833;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;50-100&#20493;&#30340;&#21387;&#32553;&#27604;&#25913;&#21892;&#65292;&#36136;&#37327;&#25439;&#22833;&#22312;1%&#20197;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#25351;&#23548;&#26410;&#26469;&#30340;&#20351;&#29992;&#21644;de
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15953v1 Announce Type: cross  Abstract: Learning and Artificial Intelligence (ML/AI) techniques have become increasingly prevalent in high performance computing (HPC). However, these methods depend on vast volumes of floating point data for training and validation which need methods to share the data on a wide area network (WAN) or to transfer it from edge devices to data centers. Data compression can be a solution to these problems, but an in-depth understanding of how lossy compression affects model quality is needed. Prior work largely considers a single application or compression method. We designed a systematic methodology for evaluating data reduction techniques for ML/AI, and we use it to perform a very comprehensive evaluation with 17 data reduction methods on 7 ML/AI applications to show modern lossy compression methods can achieve a 50-100x compression ratio improvement for a 1% or less loss in quality. We identify critical insights that guide the future use and de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>LlamBERT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#24182;&#29992;&#20110;&#24494;&#35843;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#30053;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15938</link><description>&lt;p&gt;
LlamBERT&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#35268;&#27169;&#12289;&#20302;&#25104;&#26412;&#30340;&#25968;&#25454;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
LlamBERT: Large-scale low-cost data annotation in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15938
&lt;/p&gt;
&lt;p&gt;
LlamBERT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#24182;&#29992;&#20110;&#24494;&#35843;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#30053;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;GPT-4&#21644;Llama 2&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#19982;&#23427;&#20204;&#30340;&#20351;&#29992;&#30456;&#20851;&#30340;&#39640;&#25104;&#26412;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#23545;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#30340;&#23567;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#23558;&#32467;&#26524;&#29992;&#20110;&#24494;&#35843;&#31867;&#20284;BERT&#21644;RoBERTa&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#12290;&#36825;&#19968;&#31574;&#30053;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;IMDb&#24433;&#35780;&#25968;&#25454;&#38598;&#21644;UMLS Meta-Thesaurus&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LlamBERT&#26041;&#27861;&#22312;&#31245;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15938v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;TD&#26356;&#26032;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#23436;&#20840;&#20998;&#25955;&#24335;MARL&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#36890;&#35759;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15935</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#36890;&#36807;&#26412;&#22320;TD&#26356;&#26032;&#23454;&#29616;&#26679;&#26412;&#21644;&#36890;&#35759;&#39640;&#25928;&#30340;&#23436;&#20840;&#20998;&#25955;&#24335;MARL&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15935
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;TD&#26356;&#26032;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#23436;&#20840;&#20998;&#25955;&#24335;MARL&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#36890;&#35759;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23436;&#20840;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;Actor-Critic&#26694;&#26550;&#20013;&#65292;MARL&#31574;&#30053;&#35780;&#20272;&#65288;PE&#65289;&#38382;&#39064;&#26159;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#20013;&#19968;&#32452;$N$&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#37051;&#23621;&#36890;&#20449;&#21512;&#20316;&#35780;&#20272;&#32473;&#23450;&#31574;&#30053;&#30340;&#20840;&#23616;&#29366;&#24577;&#20540;&#20989;&#25968;&#12290;&#23545;&#20110;MARL-PE&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#38477;&#20302;&#37319;&#26679;&#21644;&#36890;&#35759;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#22797;&#26434;&#24615;&#23450;&#20041;&#20026;&#25910;&#25947;&#21040;&#19968;&#20123;$\epsilon$-&#31283;&#23450;&#28857;&#25152;&#38656;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#21644;&#36890;&#35759;&#36718;&#27425;&#12290;&#20026;&#20102;&#38477;&#20302;MARL-PE&#20013;&#30340;&#36890;&#35759;&#22797;&#26434;&#24615;&#65292;&#19968;&#20010;&#8220;&#33258;&#28982;&#8221;&#30340;&#24819;&#27861;&#26159;&#22312;&#27599;&#27425;&#36830;&#32493;&#36890;&#35759;&#30340;&#36718;&#20043;&#38388;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;TD&#26356;&#26032;&#27493;&#39588;&#65292;&#20197;&#20943;&#23569;&#36890;&#35759;&#39057;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#22870;&#21169;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#30340;&#8220;&#20195;&#29702;&#28418;&#31227;&#8221;&#29616;&#35937;&#65292;&#26412;&#22320;TD&#26356;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15935v1 Announce Type: new  Abstract: In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $\epsilon$-stationary point. To lower communication complexity in MARL-PE, a "natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential "agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.15933</link><description>&lt;p&gt;
&#29702;&#35299;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#20013;&#30340;&#22495;&#22823;&#23567;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain-Size Generalization in Markov Logic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#65288;MLNs&#65289;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#20851;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#22810;&#20010;&#30740;&#31350;&#27880;&#24847;&#21040;&#65292;&#22312;&#32473;&#23450;&#22495;&#19978;&#23398;&#20064;&#30340;MLNs&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#22495;&#19978;&#27867;&#21270;&#24456;&#24046;&#12290;&#36825;&#31181;&#34892;&#20026;&#28304;&#20110;MLN&#22312;&#19981;&#21516;&#22495;&#22823;&#23567;&#19978;&#20351;&#29992;&#26102;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#20854;&#38480;&#21046;&#22312;MLN&#21442;&#25968;&#30340;&#26041;&#24046;&#33539;&#22260;&#20869;&#12290;&#21442;&#25968;&#26041;&#24046;&#36824;&#38480;&#21046;&#20102;&#20174;&#19981;&#21516;&#22495;&#22823;&#23567;&#20013;&#21462;&#20986;&#30340;MLN&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#23637;&#31034;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#65292;&#23545;&#24212;&#20110;&#22495;&#22823;&#23567;&#27867;&#21270;&#30340;&#20004;&#20010;&#33258;&#28982;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#25351;&#25968;&#38543;&#26426;&#22270;&#21644;&#20854;&#20182;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#30340;&#20851;&#31995;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24050;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#20250;&#20943;&#23569;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#23433;&#20840;&#24615;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#23433;&#20840;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#35745;&#31639;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23450;&#20041;&#20195;&#29702;&#38598;&#23454;&#29616;&#26377;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.15928</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#20572;&#27490;&#26102;&#38388;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15928
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#23433;&#20840;&#24615;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#23433;&#20840;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#35745;&#31639;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23450;&#20041;&#20195;&#29702;&#38598;&#23454;&#29616;&#26377;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#23433;&#20840;&#24615;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23613;&#31649;&#31185;&#23398;&#30028;&#24050;&#32463;&#24341;&#36215;&#24517;&#35201;&#30340;&#20851;&#27880;&#65292;&#20294;&#32771;&#34385;&#21040;&#38543;&#26426;&#20572;&#27490;&#26102;&#38388;&#65292;&#23398;&#20064;&#22312;&#23398;&#20064;&#38454;&#27573;&#19981;&#36829;&#21453;&#23433;&#20840;&#24615;&#32422;&#26463;&#30340;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#19968;&#20010;&#36807;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#31574;&#30053;&#20855;&#26377;&#36739;&#39640;&#30340;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#19981;&#36829;&#21453;&#23433;&#20840;&#24615;&#32422;&#26463;&#30340;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#25311;&#32467;&#26524;&#26469;&#23637;&#31034;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#31216;&#20026;&#20195;&#29702;&#38598;&#30340;&#29366;&#24577;&#31354;&#38388;&#23376;&#38598;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15928v1 Announce Type: new  Abstract: In this paper, we present an online reinforcement learning algorithm for constrained Markov decision processes with a safety constraint. Despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. To this end, we propose an algorithm based on linear programming that does not require a process model. We show that the learned policy is safe with high confidence. We also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. Finally, we provide simulation results to show the efficacy of the proposed algorithm. Further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#36712;&#36857;&#25277;&#26679;&#21644;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#25968;&#25454;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15908</link><description>&lt;p&gt;
&#20351;&#29992;&#36712;&#36857;&#25277;&#26679;&#30340;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#31574;&#30053;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15908
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#36712;&#36857;&#25277;&#26679;&#21644;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#25968;&#25454;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20854;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25913;&#21892;&#20102;&#25506;&#32034;&#24615;&#33021;&#24182;&#33719;&#24471;&#20102;&#26032;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#23398;&#20064;&#27969;&#31243;&#23548;&#33268;&#30340;&#31283;&#20581;&#31574;&#30053;&#27604;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#22122;&#22768;&#35266;&#27979;&#26356;&#19981;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36712;&#36857;&#25277;&#26679;&#21644;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#65288;DGCN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#26368;&#20248;&#25511;&#21046;&#29615;&#22659;&#20013;&#23454;&#29616;MBRL&#38382;&#39064;&#30340;&#25968;&#25454;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;DGCN&#19977;&#31181;&#19981;&#21516;&#30340;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#36712;&#36857;&#25277;&#26679;&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#36817;&#20284;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#30340;&#30693;&#21517;&#27979;&#35797;&#29615;&#22659;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#26041;&#27861;&#21644;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#32452;&#21512;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15908v1 Announce Type: new  Abstract: Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and prob
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.15905</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#37327;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20010;&#24615;&#21270;&#20197;&#35299;&#20915;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20391;&#37325;&#20110;&#24494;&#35843;&#23436;&#25972;&#22522;&#30784;&#27169;&#22411;&#25110;&#20854;&#26368;&#21518;&#20960;&#23618;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#33021;&#28304;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#8212;&#8212;&#30446;&#26631;&#22359;&#24494;&#35843;&#65288;TBFT&#65289;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#28418;&#31227;&#21644;&#20010;&#24615;&#21270;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#36755;&#20837;&#32423;&#21035;&#12289;&#29305;&#24449;&#32423;&#21035;&#21644;&#36755;&#20986;&#32423;&#21035;&#12290;&#38024;&#23545;&#27599;&#31181;&#31867;&#22411;&#65292;&#25105;&#20204;&#24494;&#35843;&#19981;&#21516;&#27169;&#22411;&#22359;&#20197;&#23454;&#29616;&#22312;&#38477;&#20302;&#33021;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36755;&#20837;&#32423;&#12289;&#29305;&#24449;&#32423;&#21644;&#36755;&#20986;&#32423;&#23545;&#24212;&#20110;&#24494;&#35843;&#27169;&#22411;&#30340;&#21069;&#31471;&#12289;&#20013;&#27573;&#21644;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#12290;</title><link>https://arxiv.org/abs/2403.15886</link><description>&lt;p&gt;
&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#23454;&#29616;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Leveraging Zero-Shot Prompting for Efficient Language Model Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15886
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLMs&#39640;&#25928;&#22320;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#29305;&#23450;&#20110;&#24212;&#29992;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#21644;&#20154;&#24037;&#21171;&#21160;&#12290;&#35813;&#25216;&#26415;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#20026;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#26631;&#31614;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#65292;&#20197;&#35299;&#20915;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;LLMs&#37096;&#32626;&#21040;&#29305;&#23450;&#24212;&#29992;&#25110;&#36793;&#32536;&#35774;&#22791;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#23398;&#29983;&#27169;&#22411;&#27169;&#20223;&#36825;&#20123;&#29702;&#30001;&#20197;&#21450;&#25945;&#24072;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#26469;&#22686;&#24378;&#24494;&#35843;&#21644;&#33976;&#39311;&#12290;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#65292;&#32771;&#34385;&#21040;&#20027;&#35201;&#25216;&#26415;&#20844;&#21496;LLM APIs&#30340;&#25353;&#35760;&#21495;&#35745;&#36153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35843;&#26597;&#20102;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15886v1 Announce Type: cross  Abstract: This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#36866;&#29992;&#20110;&#25152;&#26377;&#23454;&#29992;&#30340;&#24402;&#19968;&#21270;&#27969;&#26550;&#26500;&#65292;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#24182;&#20943;&#23567;&#20102;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.15881</link><description>&lt;p&gt;
&#24555;&#36895;&#32479;&#19968;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fast and Unified Path Gradient Estimators for Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15881
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#36866;&#29992;&#20110;&#25152;&#26377;&#23454;&#29992;&#30340;&#24402;&#19968;&#21270;&#27969;&#26550;&#26500;&#65292;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#24182;&#20943;&#23567;&#20102;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#27969;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#19982;&#21464;&#20998;&#25512;&#26029;&#30340;&#26631;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#35282;&#24230;&#30475;&#65292;&#23427;&#20204;&#24448;&#24448;&#26114;&#36149;&#19988;&#26080;&#27861;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20123;&#20851;&#38190;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#36866;&#29992;&#20110;&#25152;&#26377;&#23454;&#29992;&#30340;&#24402;&#19968;&#21270;&#27969;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20272;&#35745;&#22120;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#23545;&#27492;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#32771;&#34385;&#32473;&#23450;&#30446;&#26631;&#33021;&#37327;&#20989;&#25968;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#20010;&#33258;&#28982;&#31185;&#23398;&#24212;&#29992;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#24182;&#20943;&#23567;&#20102;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15881v1 Announce Type: new  Abstract: Recent work shows that path gradient estimators for normalizing flows have lower variance compared to standard estimators for variational inference, resulting in improved training. However, they are often prohibitively more expensive from a computational point of view and cannot be applied to maximum likelihood training in a scalable manner, which severely hinders their widespread adoption. In this work, we overcome these crucial limitations. Specifically, we propose a fast path gradient estimator which improves computational efficiency significantly and works for all normalizing flow architectures of practical relevance. We then show that this estimator can also be applied to maximum likelihood training for which it has a regularizing effect as it can take the form of a given target energy function into account. We empirically establish its superior performance and reduced variance for several natural sciences applications.
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15855</link><description>&lt;p&gt;
&#21021;&#22987;&#20540;&#21644;&#25299;&#25169;&#32467;&#26500;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initialisation and Topology Effects in Decentralised Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15855
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#20998;&#25955;&#24335;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22312;&#32593;&#32476;&#19978;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#23545;&#20010;&#20307;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#26412;&#22320;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#28040;&#38500;&#20102;&#21333;&#28857;&#25925;&#38556;&#21644;&#20013;&#22830;&#21327;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#19968;&#20010;&#31616;&#21270;&#30340;&#25968;&#20540;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#26089;&#26399;&#34892;&#20026;&#65292;&#20351;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20540;&#31574;&#30053;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19979;&#30340;&#27604;&#20363;&#34892;&#20026;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26356;&#22810;&#30740;&#31350;&#25171;&#24320;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
&lt;/p&gt;</description></item><item><title>TablePuppet&#25552;&#20986;&#20102;&#20851;&#31995;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#36830;&#25509;&#20013;&#30340;&#23398;&#20064;&#65288;LoJ&#65289;&#21644;&#32852;&#21512;&#20013;&#30340;&#23398;&#20064;&#65288;LoU&#65289;&#26469;&#22788;&#29702;&#20998;&#24067;&#24335;&#20851;&#31995;&#34920;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;</title><link>https://arxiv.org/abs/2403.15839</link><description>&lt;p&gt;
TablePuppet&#65306;&#20851;&#31995;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TablePuppet: A Generic Framework for Relational Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15839
&lt;/p&gt;
&lt;p&gt;
TablePuppet&#25552;&#20986;&#20102;&#20851;&#31995;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#36830;&#25509;&#20013;&#30340;&#23398;&#20064;&#65288;LoJ&#65289;&#21644;&#32852;&#21512;&#20013;&#30340;&#23398;&#20064;&#65288;LoU&#65289;&#26469;&#22788;&#29702;&#20998;&#24067;&#24335;&#20851;&#31995;&#34920;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#27861;&#23558;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#35270;&#20026;&#21333;&#20010;&#34920;&#65292;&#20998;&#20026;&#27700;&#24179;&#26041;&#24335;&#65288;&#25353;&#34892;&#65289;&#25110;&#22402;&#30452;&#26041;&#24335;&#65288;&#25353;&#21015;&#65289;&#20998;&#37197;&#32473;&#21442;&#19982;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20998;&#24067;&#22312;&#25968;&#25454;&#24211;&#20013;&#30340;&#20851;&#31995;&#34920;&#12290;&#36825;&#31181;&#24773;&#20917;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;SQL&#25805;&#20316;&#65292;&#22914;&#36830;&#25509;&#21644;&#21512;&#24182;&#25805;&#20316;&#65292;&#20197;&#33719;&#24471;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25805;&#20316;&#35201;&#20040;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#30452;&#25509;&#22312;&#20998;&#24067;&#24335;&#20851;&#31995;&#34920;&#19978;&#36816;&#34892;FL&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20851;&#31995;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TablePuppet&#65292;&#19968;&#20010;&#29992;&#20110;RFL&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#36830;&#25509;&#20013;&#30340;&#23398;&#20064;&#65288;LoJ&#65289;&#65292;&#28982;&#21518;&#26159;&#65288;2&#65289;&#32852;&#21512;&#20013;&#30340;&#23398;&#20064;&#65288;LoU&#65289;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;LoJ&#23558;&#23398;&#20064;&#25512;&#21521;&#21152;&#20837;&#30340;&#22402;&#30452;&#34920;&#65292;LoU&#36827;&#19968;&#27493;&#23558;&#23398;&#20064;&#25512;&#21521;&#27599;&#20010;&#22402;&#30452;&#34920;&#30340;&#27700;&#24179;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15839v1 Announce Type: new  Abstract: Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables?   In this paper, we formalize this problem as relational federated learning (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.15837</link><description>&lt;p&gt;
&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#20013;&#24515;&#25513;&#34109;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Centered Masking for Language-Image Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15837
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;GLIP&#65289;&#30340;&#39640;&#26031;&#25513;&#34109;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#30452;&#25509;&#21644;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#25513;&#34109;&#12290;GLIP&#22522;&#20110;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;FLIP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;CLIP&#27169;&#22411;&#26102;&#38543;&#26426;&#23631;&#34109;&#22270;&#20687;&#34917;&#19969;&#12290;GLIP&#23558;&#38543;&#26426;&#23631;&#34109;&#26367;&#25442;&#20026;&#20013;&#24515;&#25513;&#34109;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#65292;&#24182;&#21463;&#21040;&#22270;&#20687;&#20013;&#24515;&#37325;&#35201;&#24615;&#30340;&#21551;&#21457;&#12290;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#65292;GLIP&#20445;&#30041;&#20102;&#19982;FLIP&#30456;&#21516;&#30340;&#35745;&#31639;&#33410;&#30465;&#33021;&#21147;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25152;&#35777;&#23454;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GLIP&#30340;&#22909;&#22788;&#24456;&#23481;&#26131;&#33719;&#24471;&#65292;&#26080;&#38656;&#31934;&#32454;&#35843;&#25972;&#39640;&#26031;&#65292;&#20063;&#36866;&#29992;&#20110;&#21253;&#21547;&#26080;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#22270;&#29255;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15837v1 Announce Type: cross  Abstract: We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a vision-language model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.15826</link><description>&lt;p&gt;
&#36890;&#36807;Dropout&#23545;&#26102;&#38388;&#20219;&#21153;&#36827;&#34892;&#27604;&#20363;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Learning based Policy Optimization for Temporal Tasks via Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#24076;&#26395;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#30830;&#20445;&#35813;&#26234;&#33021;&#20307;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#20197;&#31163;&#25955;&#26102;&#38388;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;DT-STL&#65289;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#37325;&#26032;&#34920;&#36848;&#20026;&#24418;&#24335;&#21270;&#26694;&#26550;&#65288;&#22914;DT-STL&#65289;&#65292;&#19968;&#20010;&#20248;&#21183;&#26159;&#20801;&#35768;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#36712;&#36857;&#21644;&#19968;&#20010;DT-STL&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#35745;&#31639;&#40065;&#26834;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#36712;&#36857;&#19982;&#28385;&#36275;&#35813;&#20844;&#24335;&#30340;&#36712;&#36857;&#38598;&#20043;&#38388;&#30340;&#36817;&#20284;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#25105;&#20204;&#21033;&#29992;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#24182;&#20551;&#35774;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#36825;&#20123;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#19982;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#31867;&#20284;&#30340;&#22320;&#26041;&#65292;&#20854;&#20013;&#36882;&#24402;&#21333;&#20803;&#30340;&#25968;&#37327;&#19982;&#26234;&#33021;&#20307;&#30340;&#26102;&#38388;&#35270;&#37326;&#25104;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15826v1 Announce Type: cross  Abstract: This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment. We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in discrete-time Signal Temporal Logic (DT-STL). One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics. In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula. We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers. We show how this learning problem is similar to training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#36873;&#25321;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#39640;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#20302;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#26381;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#22810;&#25552;&#39640;&#30899;&#25490;&#25918;&#25928;&#29575;&#36798;80%&#12290;</title><link>https://arxiv.org/abs/2403.15824</link><description>&lt;p&gt;
&#22522;&#20110;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Carbon Intensity-Aware Adaptive Inference of DNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#36873;&#25321;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#39640;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#20302;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#26381;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#22810;&#25552;&#39640;&#30899;&#25490;&#25918;&#25928;&#29575;&#36798;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#25512;&#26029;&#20197;&#20854;&#24040;&#22823;&#30340;&#33021;&#32791;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#39640;&#30899;&#36275;&#36857;&#32780;&#38395;&#21517;&#65292;&#36890;&#36807;&#26681;&#25454;&#19968;&#22825;&#20013;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#21464;&#21270;&#35843;&#25972;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#20351;&#20854;&#26356;&#21152;&#21487;&#25345;&#32493;&#12290;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20302;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#22823;&#12289;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#39640;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#23567;&#12289;&#26356;&#20302;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#65292;&#21363;&#30899;&#25490;&#25918;&#25928;&#29575;&#65292;&#20197;&#37327;&#21270;&#33258;&#36866;&#24212;&#27169;&#22411;&#36873;&#25321;&#22312;&#30899;&#36275;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23558;&#35270;&#35273;&#35782;&#21035;&#26381;&#21153;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#39640;&#36798;80%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15824v1 Announce Type: cross  Abstract: DNN inference, known for its significant energy consumption and the resulting high carbon footprint, can be made more sustainable by adapting model size and accuracy to the varying carbon intensity throughout the day. Our heuristic algorithm uses larger, high-accuracy models during low-intensity periods and smaller, lower-accuracy ones during high-intensity periods. We also introduce a metric, carbon-emission efficiency, which quantitatively measures the efficacy of adaptive model selection in terms of carbon footprint. The evaluation showed that the proposed approach could improve the carbon emission efficiency in improving the accuracy of vision recognition services by up to 80%.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#25928;&#28151;&#21512;&#21521;&#37327;-&#20851;&#31995;&#25628;&#32034;&#30340;&#26367;&#20195;&#25968;&#25454;&#35775;&#38382;&#36335;&#24452;&#35774;&#35745;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15807</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#28151;&#21512;&#21521;&#37327;-&#20851;&#31995;&#25628;&#32034;&#25968;&#25454;&#35775;&#38382;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Access Paths for Mixed Vector-Relational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#25928;&#28151;&#21512;&#21521;&#37327;-&#20851;&#31995;&#25628;&#32034;&#30340;&#26367;&#20195;&#25968;&#25454;&#35775;&#38382;&#36335;&#24452;&#35774;&#35745;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#30340;&#24555;&#36895;&#22686;&#38271;&#20197;&#21450;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#30340;&#37319;&#29992;&#24341;&#21457;&#20102;&#23545;&#21019;&#24314;&#21521;&#37327;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#31934;&#30830;&#20294;&#31351;&#23613;&#30340;&#25195;&#25551;&#24335;&#25628;&#32034;&#65292;&#24182;&#25552;&#20986;&#30828;&#20214;&#20248;&#21270;&#21644;&#26367;&#20195;&#24352;&#37327;&#24335;&#20844;&#24335;&#21270;&#21644;&#25209;&#22788;&#29702;&#20197;&#25269;&#28040;&#25104;&#26412;&#12290;&#25105;&#20204;&#27010;&#36848;&#22797;&#26434;&#30340;&#35775;&#38382;&#36335;&#24452;&#35774;&#35745;&#31354;&#38388;&#65292;&#20027;&#35201;&#30001;&#20851;&#31995;&#36873;&#25321;&#24615;&#39537;&#21160;&#65292;&#20197;&#21450;&#32771;&#34385;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15807v1 Announce Type: cross  Abstract: The rapid growth of machine learning capabilities and the adoption of data processing methods using vector embeddings sparked a great interest in creating systems for vector data management. While the predominant approach of vector data management is to use specialized index structures for fast search over the entirety of the vector embeddings, once combined with other (meta)data, the search queries can also become selective on relational attributes - typical for analytical queries. As using vector indexes differs from traditional relational data access, we revisit and analyze alternative access paths for efficient mixed vector-relational search.   We first evaluate the accurate but exhaustive scan-based search and propose hardware optimizations and alternative tensor-based formulation and batching to offset the cost. We outline the complex access-path design space, primarily driven by relational selectivity, and the decisions to consi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#32553;&#25918;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#22635;&#34917;&#20102;&#30740;&#31350;&#20013;&#30340;&#32570;&#21475;&#12290;</title><link>https://arxiv.org/abs/2403.15790</link><description>&lt;p&gt;
ISS&#30331;&#26426;&#65306;&#19981;&#24179;&#34913;&#30340;&#33258;&#30417;&#30563;&#65306;&#21457;&#29616;&#29992;&#20110;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#32553;&#25918;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15790
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#32553;&#25918;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#22635;&#34917;&#20102;&#30740;&#31350;&#20013;&#30340;&#32570;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15790v1&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#19981;&#22826;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25506;&#35752;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#25152;&#24102;&#26469;&#30340;&#20855;&#20307;&#25361;&#25112;&#65292;&#37325;&#28857;&#25918;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#19978;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#21644;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26032;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#38477;&#32500;&#12290;&#23427;&#20204;&#20063;&#32463;&#24120;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#25152;&#35265;&#12290;&#22312;&#22788;&#29702;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#23450;&#24615;&#21464;&#37327;&#36890;&#24120;&#20351;&#29992;&#29420;&#28909;&#32534;&#30721;&#22120;&#19982;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65288;&#22343;&#26041;&#35823;&#24046;&#25110;&#20132;&#21449;&#29109;&#65289;&#36827;&#34892;&#32534;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#29305;&#21035;&#26159;&#24403;&#20998;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#20197;&#24179;&#34913;&#23398;&#20064;&#65306;&#19968;&#20010;&#22810;&#30417;&#30563;&#30340; Ba
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15790v1 Announce Type: new  Abstract: The field of imbalanced self-supervised learning, especially in the context of tabular data, has not been extensively studied. Existing research has predominantly focused on image datasets. This paper aims to fill this gap by examining the specific challenges posed by data imbalance in self-supervised learning in the domain of tabular data, with a primary focus on autoencoders. Autoencoders are widely employed for learning and constructing a new representation of a dataset, particularly for dimensionality reduction. They are also often used for generative model learning, as seen in variational autoencoders. When dealing with mixed tabular data, qualitative variables are often encoded using a one-hot encoder with a standard loss function (MSE or Cross Entropy). In this paper, we analyze the drawbacks of this approach, especially when categorical variables are imbalanced. We propose a novel metric to balance learning: a Multi-Supervised Ba
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15780</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#65292;&#21253;&#25324;&#37027;&#20123;&#30452;&#25509;&#28041;&#21450;&#20154;&#31867;&#30340;&#39046;&#22495;&#65292;&#24179;&#31561;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#30028;&#24840;&#21457;&#31361;&#20986;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#24615;&#23548;&#21521;&#26041;&#27861;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#25506;&#35752;&#24615;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#24615;&#33021;&#20248;&#21270;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q-Learning&#31639;&#27861;&#65292;&#21033;&#29992;&#20854;&#25910;&#25947;&#20445;&#35777;&#26469;&#30830;&#20445;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31449;&#28857;&#31867;&#21035;&#65288;&#20013;&#24515;&#12289;&#36793;&#32536;&#21644;&#36828;&#31243;&#65289;&#20043;&#38388;&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#36890;&#36807;&#22522;&#23612;&#31995;&#25968;&#26469;&#34913;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15780v1 Announce Type: cross  Abstract: As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic fairness has risen to prominence in the Artificial Intelligence community. On the other hand, in the context of Shared Micromobility Systems, the exploration of fairness-oriented approaches remains limited. Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic fairness in the operation and control of Shared Micromobility Services. Our study leverages the Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence guarantees to ensure the robustness of our proposed approach. Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories--central, peripheral, and remote. Through stra
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#21151;&#33021;&#25968;&#25454;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#35757;&#32451;&#38598;&#25104;&#25104;&#21592;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32452;&#21512;&#22522;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15778</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21151;&#33021;&#34920;&#31034;&#30340;&#38598;&#25104;&#23398;&#20064;&#65306;&#21151;&#33021;&#25237;&#31080;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#21151;&#33021;&#25968;&#25454;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#35757;&#32451;&#38598;&#25104;&#25104;&#21592;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32452;&#21512;&#22522;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20256;&#32479;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#24207;&#35266;&#27979;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#20013;&#65292;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;(FDA)&#20316;&#20026;&#19968;&#31181;&#27169;&#25311;&#21644;&#20998;&#26512;&#22825;&#28982;&#20026;&#26102;&#38388;&#22495;&#20869;&#30340;&#20989;&#25968;&#30340;&#25968;&#25454;&#30340;&#26694;&#26550;&#24050;&#32463;&#24191;&#27867;&#27969;&#34892;&#36215;&#26469;&#12290;&#34429;&#28982;&#22312;FDA&#25991;&#29486;&#20013;&#23545;&#30417;&#30563;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#20294;&#21151;&#33021;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#23398;&#20064;&#21364;&#26368;&#36817;&#25165;&#25104;&#20026;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#21518;&#32773;&#20174;&#21508;&#31181;&#32479;&#35745;&#35282;&#24230;&#21576;&#29616;&#20986;&#26410;&#32463;&#25506;&#32034;&#30340;&#26041;&#38754;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#30340;&#28966;&#28857;&#22312;&#20110;&#21151;&#33021;&#25968;&#25454;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#26088;&#22312;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#26469;&#35757;&#32451;&#38598;&#25104;&#25104;&#21592;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#26469;&#32452;&#21512;&#22522;&#27169;&#22411;&#39044;&#27979;&#12290;&#25152;&#35859;&#30340;&#21151;&#33021;&#25237;&#31080;Cla
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15778v1 Announce Type: cross  Abstract: Many conventional statistical and machine learning methods face challenges when applied directly to high dimensional temporal observations. In recent decades, Functional Data Analysis (FDA) has gained widespread popularity as a framework for modeling and analyzing data that are, by their nature, functions in the domain of time. Although supervised classification has been extensively explored in recent decades within the FDA literature, ensemble learning of functional classifiers has only recently emerged as a topic of significant interest. Thus, the latter subject presents unexplored facets and challenges from various statistical perspectives. The focal point of this paper lies in the realm of ensemble learning for functional data and aims to show how different functional data representations can be used to train ensemble members and how base model predictions can be combined through majority voting. The so-called Functional Voting Cla
&lt;/p&gt;</description></item><item><title>FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.15769</link><description>&lt;p&gt;
FusionINN&#65306;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#29992;&#20110;&#33041;&#32959;&#30244;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15769
&lt;/p&gt;
&lt;p&gt;
FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;&#22810;&#20010;&#28304;&#22270;&#20687;&#21512;&#24182;&#20026;&#21333;&#20010;&#34701;&#21512;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#19987;&#23478;&#65292;&#20165;&#20381;&#36182;&#34701;&#21512;&#22270;&#20687;&#21487;&#33021;&#19981;&#36275;&#20197;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#65292;&#22240;&#20026;&#34701;&#21512;&#26426;&#21046;&#28151;&#21512;&#20102;&#26469;&#33258;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38590;&#20197;&#35299;&#37322;&#28508;&#22312;&#30340;&#32959;&#30244;&#30149;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FusionINN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#36807;&#31243;&#23558;&#20854;&#20998;&#35299;&#22238;&#28304;&#22270;&#20687;&#12290;FusionINN&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#28508;&#22312;&#22270;&#20687;&#19982;&#34701;&#21512;&#22270;&#20687;&#19968;&#36215;&#65292;&#20197;&#20419;&#36827;&#20998;&#35299;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#20174;&#32780;&#20445;&#35777;&#26080;&#25439;&#30340;&#19968;&#23545;&#19968;&#20687;&#32032;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#34701;&#21512;&#22270;&#20687;&#30340;&#21487;&#20998;&#35299;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#21629;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15769v1 Announce Type: cross  Abstract: Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#27169;&#22411;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65288;BEND&#65289;&#65292;&#26377;&#25928;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.15766</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#65288;BEND&#65289;
&lt;/p&gt;
&lt;p&gt;
BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#27169;&#22411;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65288;BEND&#65289;&#65292;&#26377;&#25928;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bagging&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#26469;&#38477;&#20302;&#27169;&#22411;&#26041;&#24046;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65288;BEND&#65289;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#27169;&#22411;&#39640;&#25928;&#26500;&#24314;&#22522;&#26412;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15766v1 Announce Type: cross  Abstract: Bagging has achieved great success in the field of machine learning by integrating multiple base classifiers to build a single strong classifier to reduce model variance. The performance improvement of bagging mainly relies on the number and diversity of base classifiers. However, traditional deep learning model training methods are expensive to train individually and difficult to train multiple models with low similarity in a restricted dataset. Recently, diffusion models, which have been tremendously successful in the fields of imaging and vision, have been found to be effective in generating neural network model weights and biases with diversity. We creatively propose a Bagging deep learning training algorithm based on Efficient Neural network Diffusion (BEND). The originality of BEND comes from the first use of a neural network diffusion model to efficiently build base classifiers for bagging. Our approach is simple but effective, 
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;</title><link>https://arxiv.org/abs/2403.15757</link><description>&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
User-Side Realization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15757
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#26381;&#21153;&#24863;&#21040;&#19981;&#28385;&#24847;&#12290;&#30001;&#20110;&#26381;&#21153;&#24182;&#38750;&#37327;&#36523;&#23450;&#21046;&#32473;&#29992;&#25143;&#65292;&#22240;&#27492;&#19981;&#28385;&#24847;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#12290;&#38382;&#39064;&#22312;&#20110;&#65292;&#21363;&#20351;&#29992;&#25143;&#24863;&#21040;&#19981;&#28385;&#24847;&#65292;&#20182;&#20204;&#36890;&#24120;&#20063;&#27809;&#26377;&#35299;&#20915;&#19981;&#28385;&#30340;&#25163;&#27573;&#12290;&#29992;&#25143;&#26080;&#27861;&#20462;&#25913;&#26381;&#21153;&#30340;&#28304;&#20195;&#30721;&#65292;&#20063;&#26080;&#27861;&#24378;&#36843;&#26381;&#21153;&#25552;&#20379;&#21830;&#36827;&#34892;&#26356;&#25913;&#12290;&#29992;&#25143;&#21035;&#26080;&#36873;&#25321;&#65292;&#21482;&#33021;&#20445;&#25345;&#19981;&#28385;&#24847;&#25110;&#36864;&#20986;&#26381;&#21153;&#12290;&#29992;&#25143;&#31471;&#23454;&#29616;&#36890;&#36807;&#25552;&#20379;&#36890;&#29992;&#31639;&#27861;&#26469;&#22788;&#29702;&#29992;&#25143;&#31471;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15757v1 Announce Type: cross  Abstract: Users are dissatisfied with services. Since the service is not tailor-made for a user, it is natural for dissatisfaction to arise. The problem is, that even if users are dissatisfied, they often do not have the means to resolve their dissatisfaction. The user cannot alter the source code of the service, nor can they force the service provider to change. The user has no choice but to remain dissatisfied or quit the service. User-side realization offers proactive solutions to this problem by providing general algorithms to deal with common problems on the user's side. These algorithms run on the user's side and solve the problems without having the service provider change the service itself.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Hadamard&#31354;&#38388;&#20013;&#36827;&#34892;&#20984;&#20248;&#21270;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#20551;&#35774;&#19981;&#21516;&#65292;&#20854;&#22797;&#26434;&#24615;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#26354;&#29575;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.15749</link><description>&lt;p&gt;
Horoballs&#21644;&#27425;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Horoballs and the subgradient method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Hadamard&#31354;&#38388;&#20013;&#36827;&#34892;&#20984;&#20248;&#21270;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#20551;&#35774;&#19981;&#21516;&#65292;&#20854;&#22797;&#26434;&#24615;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#26354;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#32034;Hadamard&#31354;&#38388;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27425;&#26799;&#24230;&#31639;&#27861;&#30340;&#36845;&#20195;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#31867;&#26041;&#27861;&#20551;&#35774;&#24213;&#23618;&#31354;&#38388;&#26159;&#27969;&#24418;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#27979;&#22320;&#20984;&#30340;&#65306;&#36825;&#20123;&#26041;&#27861;&#26159;&#20351;&#29992;&#20999;&#31354;&#38388;&#21644;&#25351;&#25968;&#26144;&#23556;&#26469;&#25551;&#36848;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#36845;&#20195;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;Hadamard&#31354;&#38388;&#65292;&#26159;&#22312;&#24213;&#23618;&#31354;&#38388;&#26412;&#36523;&#20013;&#26500;&#24314;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#23458;&#35266;&#27700;&#24179;&#38598;&#30340;horospherical&#20984;&#24615;&#12290;&#23545;&#20110;&#36825;&#20010;&#21463;&#38480;&#21046;&#30340;&#23458;&#35266;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#36890;&#24120;&#24418;&#24335;&#30340;&#22797;&#26434;&#24615;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22797;&#26434;&#24615;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#26354;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15749v1 Announce Type: cross  Abstract: To explore convex optimization on Hadamard spaces, we consider an iteration in the style of a subgradient algorithm. Traditionally, such methods assume that the underlying spaces are manifolds and that the objectives are geodesically convex: the methods are described using tangent spaces and exponential maps. By contrast, our iteration applies in a general Hadamard space, is framed in the underlying space itself, and relies instead on horospherical convexity of the objective level sets. For this restricted class of objectives, we prove a complexity result of the usual form. Notably, the complexity does not depend on a lower bound on the space curvature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25991;&#26723;&#20013;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#24182;&#35782;&#21035;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#30830;&#35748;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28389;&#29992;&#20854;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2403.15740</link><description>&lt;p&gt;
Ghost Sentence&#65306;&#19968;&#31181;&#20379;&#26222;&#36890;&#29992;&#25143;&#20351;&#29992;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#36827;&#34892;&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15740
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25991;&#26723;&#20013;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#24182;&#35782;&#21035;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#30830;&#35748;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28389;&#29992;&#20854;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#29992;&#25143;&#25968;&#25454;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#24494;&#35843;&#21464;&#31181;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24314;&#35758;&#29992;&#25143;&#22312;&#20854;&#25991;&#26723;&#20013;&#21453;&#22797;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#65292;&#20351;LLMs&#33021;&#22815;&#35760;&#24518;&#36825;&#20123;&#23494;&#30721;&#12290;&#36825;&#20123;&#29992;&#25143;&#25991;&#26723;&#20013;&#38544;&#34255;&#30340;&#23494;&#30721;&#65292;&#34987;&#31216;&#20026;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#19968;&#26086;&#23427;&#20204;&#20986;&#29616;&#22312;LLMs&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#65292;&#29992;&#25143;&#23601;&#21487;&#20197;&#30830;&#20449;&#20182;&#20204;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#35757;&#32451;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#29256;&#26435;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#24189;&#28789;&#21477;&#23376;&#23450;&#20041;&#20102;&#8220;&#29992;&#25143;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#8221;&#20219;&#21153;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#12289;&#19981;&#21516;&#35268;&#27169;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#21518;$k$&#20010;&#21333;&#35789;&#39564;&#35777;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15740v1 Announce Type: new  Abstract: Web user data plays a central role in the ecosystem of pre-trained large language models (LLMs) and their fine-tuned variants. Billions of data are crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web users}} confirm if LLMs misuse their data without permission? In this work, we suggest that users repeatedly insert personal passphrases into their documents, enabling LLMs to memorize them. These concealed passphrases in user documents, referred to as \textit{ghost sentences}, once they are identified in the generated content of LLMs, users can be sure that their data is used for training. To explore the effectiveness and usage of this copyrighting tool, we define the \textit{user training data identification} task with ghost sentences. Multiple datasets from various sources at different scales are created and tested with LLMs of different sizes. For evaluation, we introduce a last $k$ words verification manner along 
&lt;/p&gt;</description></item><item><title>CrystalFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#31561;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.15734</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#32676;&#20449;&#24687;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Space Group Informed Transformer for Crystalline Materials Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15734
&lt;/p&gt;
&lt;p&gt;
CrystalFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#31561;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;CrystalFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#12290;&#31354;&#38388;&#32676;&#23545;&#31216;&#24615;&#26174;&#33879;&#31616;&#21270;&#20102;&#26230;&#20307;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#21033;&#29992;Wyckoff&#20301;&#32622;&#30340;&#26174;&#33879;&#31163;&#25955;&#21644;&#39034;&#24207;&#29305;&#24615;&#65292;CrystalFormer&#23398;&#20250;&#20102;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CrystalFormer&#22312;&#29983;&#25104;&#30340;&#26230;&#20307;&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#19982;&#26631;&#20934;&#22522;&#20934;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;CrystalFormer&#20174;&#25968;&#25454;&#20013;&#21560;&#25910;&#20102;&#21512;&#29702;&#30340;&#22266;&#20307;&#21270;&#23398;&#20449;&#24687;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#12290;CrystalFormer&#32479;&#19968;&#20102;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#32467;&#26500;&#25628;&#32034;&#21644;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15734v1 Announce Type: cross  Abstract: We introduce CrystalFormer, a transformer-based autoregressive model specifically designed for space group-controlled generation of crystalline materials. The space group symmetry significantly simplifies the crystal space, which is crucial for data and compute efficient generative modeling of crystalline materials. Leveraging the prominent discrete and sequential nature of the Wyckoff positions, CrystalFormer learns to generate crystals by directly predicting the species and locations of symmetry-inequivalent atoms in the unit cell. Our results demonstrate that CrystalFormer matches state-of-the-art performance on standard benchmarks for both validity, novelty, and stability of the generated crystalline materials. Our analysis also shows that CrystalFormer ingests sensible solid-state chemistry information from data for generative modeling. The CrystalFormer unifies symmetry-based structure search and generative pre-training in the re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#21450;&#32467;&#26500;&#35774;&#35745;&#65292;&#22312;&#29702;&#35770;&#19978;&#35748;&#35777;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15726</link><description>&lt;p&gt;
&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#35748;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Convection-Diffusion Equation: A Theoretically Certified Framework for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#21450;&#32467;&#26500;&#35774;&#35745;&#65292;&#22312;&#29702;&#35770;&#19978;&#35748;&#35777;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20174;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#30784;&#27169;&#22411;&#26144;&#23556;&#21040;&#19968;&#20010;&#22797;&#26434;&#20989;&#25968;&#12290;&#36890;&#36807;&#25166;&#23454;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#26144;&#23556;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#12290;&#36825;&#20010;&#22312;&#29702;&#35770;&#19978;&#35748;&#35777;&#30340;&#26694;&#26550;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#25968;&#23398;&#22522;&#30784;&#21644;&#26356;&#22810;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#23558;&#25193;&#25955;&#26426;&#21046;&#34701;&#20837;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15726v1 Announce Type: new  Abstract: In this paper, we study the partial differential equation models of neural networks. Neural network can be viewed as a map from a simple base model to a complicate function. Based on solid analysis, we show that this map can be formulated by a convection-diffusion equation. This theoretically certified framework gives mathematical foundation and more understanding of neural networks. Moreover, based on the convection-diffusion equation model, we design a novel network structure, which incorporates diffusion mechanism into network architecture. Extensive experiments on both benchmark datasets and real-world applications validate the performance of the proposed model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Ev-Edge &#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#20248;&#21270;&#25514;&#26045;&#25552;&#39640;&#22312;&#21830;&#21697;&#36793;&#32536;&#24179;&#21488;&#19978;&#25191;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;&#35270;&#35273;&#31639;&#27861;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.15717</link><description>&lt;p&gt;
Ev-Edge: &#22312;&#21830;&#21697;&#36793;&#32536;&#24179;&#21488;&#19978;&#23545;&#22522;&#20110;&#20107;&#20214;&#30340;&#35270;&#35273;&#31639;&#27861;&#36827;&#34892;&#39640;&#25928;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Ev-Edge: Efficient Execution of Event-based Vision Algorithms on Commodity Edge Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15717
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Ev-Edge &#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#20248;&#21270;&#25514;&#26045;&#25552;&#39640;&#22312;&#21830;&#21697;&#36793;&#32536;&#24179;&#21488;&#19978;&#25191;&#34892;&#22522;&#20110;&#20107;&#20214;&#30340;&#35270;&#35273;&#31639;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#24050;&#32463;&#25104;&#20026;&#33258;&#20027;&#23548;&#33322;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24863;&#30693;&#27169;&#24335;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#21487;&#20197;&#24573;&#30053;&#30340;&#36816;&#21160;&#27169;&#31946;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#20256;&#24863;&#22120;&#30340;&#24322;&#27493;&#26102;&#38388;&#20107;&#20214;&#27969;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38656;&#35201;&#19968;&#31181;&#30001;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#12289;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20197;&#21450;&#28151;&#21512;SNN-ANN&#31639;&#27861;&#32452;&#25104;&#30340;&#32452;&#21512;&#65292;&#20197;&#22312;&#19968;&#31995;&#21015;&#24863;&#30693;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21830;&#21697;&#36793;&#32536;&#24179;&#21488;&#19978;&#25191;&#34892;&#27492;&#31867;&#24037;&#20316;&#36127;&#36733;&#65288;&#35813;&#24179;&#21488;&#20855;&#26377;&#35832;&#22914;CPU&#12289;GPU&#21644;&#31070;&#32463;&#21152;&#36895;&#22120;&#20043;&#31867;&#30340;&#24322;&#26500;&#22788;&#29702;&#21333;&#20803;&#65289;&#20250;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36825;&#26159;&#30001;&#20110;&#20107;&#20214;&#27969;&#30340;&#19981;&#35268;&#21017;&#24615;&#36136;&#12289;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#29305;&#24449;&#19982;&#30828;&#20214;&#24179;&#21488;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Ev-Edge&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15717v1 Announce Type: new  Abstract: Event cameras have emerged as a promising sensing modality for autonomous navigation systems, owing to their high temporal resolution, high dynamic range and negligible motion blur. To process the asynchronous temporal event streams from such sensors, recent research has shown that a mix of Artificial Neural Networks (ANNs), Spiking Neural Networks (SNNs) as well as hybrid SNN-ANN algorithms are necessary to achieve high accuracies across a range of perception tasks. However, we observe that executing such workloads on commodity edge platforms which feature heterogeneous processing elements such as CPUs, GPUs and neural accelerators results in inferior performance. This is due to the mismatch between the irregular nature of event streams and diverse characteristics of algorithms on the one hand and the underlying hardware platform on the other. We propose Ev-Edge, a framework that contains three key optimizations to boost the performance
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15711</link><description>&lt;p&gt;
&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifiable Latent Neural Causal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#26088;&#22312;&#20174;&#20302;&#32423;&#35266;&#27979;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#39640;&#32423;&#22240;&#26524;&#34920;&#24449;&#12290;&#23427;&#29305;&#21035;&#25797;&#38271;&#39044;&#27979;&#22312;&#26410;&#35265;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#22240;&#20026;&#36825;&#20123;&#21464;&#21270;&#36890;&#24120;&#21487;&#20197;&#35299;&#37322;&#20026;&#24178;&#39044;&#30340;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;{&#24050;&#35265;}&#20998;&#24067;&#21464;&#21270;&#25104;&#20026;&#24110;&#21161;&#35782;&#21035;&#22240;&#26524;&#34920;&#24449;&#30340;&#33258;&#28982;&#31574;&#30053;&#65292;&#36827;&#32780;&#26377;&#21161;&#20110;&#39044;&#27979;&#20197;&#21069;{&#26410;&#35265;}&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#30830;&#23450;&#36825;&#20123;&#20998;&#24067;&#21464;&#21270;&#30340;&#31867;&#22411;&#65288;&#25110;&#26465;&#20214;&#65289;&#23545;&#20110;&#22240;&#26524;&#34920;&#24449;&#30340;&#21487;&#35782;&#21035;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#24037;&#20316;&#24314;&#31435;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#65292;&#34920;&#24449;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15711v1 Announce Type: new  Abstract: Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findin
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#26032;&#30340;Dynamic Signal Distribution (DSD)&#20998;&#31867;&#20219;&#21153;&#65292;&#27169;&#25311;&#22270;&#20687;&#30001;$k$&#20010;&#32500;&#24230;&#20026;$d$&#30340;&#34917;&#19969;&#32452;&#25104;&#65292;&#20197;&#35299;&#20915;CNNs&#30456;&#23545;&#20110;LCNs&#21644;FCNs&#30340;&#32479;&#35745;&#20248;&#21183;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15707</link><description>&lt;p&gt;
&#22320;&#22495;&#24615;&#21644;&#26435;&#37325;&#20849;&#20139;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65306;CNN&#12289;LCN&#21644;FCN&#20043;&#38388;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15707
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#26032;&#30340;Dynamic Signal Distribution (DSD)&#20998;&#31867;&#20219;&#21153;&#65292;&#27169;&#25311;&#22270;&#20687;&#30001;$k$&#20010;&#32500;&#24230;&#20026;$d$&#30340;&#34917;&#19969;&#32452;&#25104;&#65292;&#20197;&#35299;&#20915;CNNs&#30456;&#23545;&#20110;LCNs&#21644;FCNs&#30340;&#32479;&#35745;&#20248;&#21183;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22320;&#22495;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20854;&#26550;&#26500;&#20013;&#22266;&#26377;&#30340;&#22320;&#22495;&#24615;&#21644;&#26435;&#37325;&#20849;&#20139;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#29616;&#26377;&#30340;&#35797;&#22270;&#37327;&#21270;&#36825;&#20123;&#20559;&#24046;&#22312;CNNs&#19978;&#30456;&#23545;&#20110;&#23616;&#37096;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;LCNs&#65289;&#21644;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FCNs&#65289;&#30340;&#32479;&#35745;&#20248;&#21183;&#30340;&#23581;&#35797;&#21487;&#20197;&#24402;&#20026;&#20197;&#19979;&#20960;&#31867;&#65306;&#35201;&#20040;&#23427;&#20204;&#24573;&#35270;&#20248;&#21270;&#22120;&#65292;&#20165;&#25552;&#20379;&#20855;&#26377;&#32479;&#19968;&#25910;&#25947;&#19978;&#30028;&#20294;&#27809;&#26377;&#20998;&#38548;&#19979;&#30028;&#30340;&#32479;&#35745;&#25910;&#25947;&#24615;&#65292;&#35201;&#20040;&#32771;&#34385;&#21040;&#19981;&#30495;&#23454;&#22320;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#22320;&#22495;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21160;&#24577;&#20449;&#21495;&#20998;&#24067;&#65288;DSD&#65289;&#20998;&#31867;&#20219;&#21153;&#65292;&#23427;&#23558;&#22270;&#20687;&#24314;&#27169;&#20026;&#21253;&#21547;$k$&#20010;&#23610;&#23544;&#20026;$d$&#30340;&#34917;&#19969;&#65292;&#26631;&#31614;&#26159;de
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15707v1 Announce Type: cross  Abstract: Vision tasks are characterized by the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is de
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15706</link><description>&lt;p&gt;
G-ACIL&#65306;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15706
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#22312;&#39034;&#24207;&#20219;&#21153;&#19978;&#35757;&#32451;&#32593;&#32476;&#65292;&#27599;&#20010;&#20219;&#21153;&#26377;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#24191;&#20041;CIL(GCIL)&#26088;&#22312;&#35299;&#20915;&#26356;&#25509;&#36817;&#29616;&#23454;&#24773;&#26223;&#19979;&#30340;CIL&#38382;&#39064;&#65292;&#21363;&#26032;&#25968;&#25454;&#20855;&#26377;&#28151;&#21512;&#25968;&#25454;&#31867;&#21035;&#21644;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#22823;&#23567;&#65292;&#23548;&#33268;&#36951;&#24536;&#21152;&#21095;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;GCIL&#30340;&#23581;&#35797;&#35201;&#20040;&#24615;&#33021;&#19981;&#20339;&#65292;&#35201;&#20040;&#36890;&#36807;&#20445;&#23384;&#21382;&#21490;&#33539;&#20363;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;(G-ACIL)&#12290;G-ACIL&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;(&#19968;&#31181;&#26080;&#26799;&#24230;&#35757;&#32451;&#25216;&#26415;)&#65292;&#24182;&#20026;GCIL&#24773;&#26223;&#25552;&#20379;&#20998;&#26512;&#35299;(&#21363;&#38381;&#21512;&#24418;&#24335;)&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20256;&#20837;&#25968;&#25454;&#20998;&#35299;&#20026;&#26292;&#38706;&#31867;&#21644;&#26410;&#26292;&#38706;&#31867;&#65292;&#23454;&#29616;&#20102;&#22686;&#38271;&#31867;&#20043;&#38388;&#30340;&#31561;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15706v1 Announce Type: new  Abstract: Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32676;&#20307;&#27491;&#21017;&#21270;&#31574;&#30053;&#20272;&#35745;&#31867;&#21035;&#36719;&#26631;&#31614;&#20197;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#23545;&#22024;&#26434;&#26631;&#31614;&#30340;&#36807;&#25311;&#21512;&#24182;&#23398;&#20064;&#31867;&#38388;&#30456;&#20284;&#24615;&#20197;&#25913;&#21892;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15694</link><description>&lt;p&gt;
&#25968;&#25454;&#20928;&#21270;&#20013;&#30340;&#32676;&#20307;&#31119;&#21033;&#23454;&#20363;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Group Benefits Instances Selection for Data Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15694
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32676;&#20307;&#27491;&#21017;&#21270;&#31574;&#30053;&#20272;&#35745;&#31867;&#21035;&#36719;&#26631;&#31614;&#20197;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#23545;&#22024;&#26434;&#26631;&#31614;&#30340;&#36807;&#25311;&#21512;&#24182;&#23398;&#20064;&#31867;&#38388;&#30456;&#20284;&#24615;&#20197;&#25913;&#21892;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#20026;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#26159;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#21155;&#21183;&#65292;&#30452;&#25509;&#21033;&#29992;&#32593;&#32476;&#22270;&#20687;&#26469;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#33258;&#28982;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#36890;&#24120;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#22312;&#21512;&#25104;&#22024;&#26434;&#25968;&#25454;&#38598;&#19978;&#35774;&#35745;&#21644;&#27979;&#35797;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#22024;&#26434;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;GRIP&#21033;&#29992;&#19968;&#31181;&#32676;&#20307;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20272;&#35745;&#31867;&#21035;&#36719;&#26631;&#31614;&#20197;&#25552;&#39640;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#36719;&#26631;&#31614;&#30417;&#30563;&#38477;&#20302;&#20102;&#23545;&#22024;&#26434;&#26631;&#31614;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#23398;&#20064;&#20102;&#26377;&#21033;&#20110;&#20998;&#31867;&#30340;&#31867;&#38388;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#20840;&#23616;&#23454;&#20363;&#20928;&#21270;&#25805;&#20316;&#36890;&#36807;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15694v1 Announce Type: new  Abstract: Manually annotating datasets for training deep models is very labor-intensive and time-consuming. To overcome such inferiority, directly leveraging web images to conduct training data becomes a natural choice. Nevertheless, the presence of label noise in web data usually degrades the model performance. Existing methods for combating label noise are typically designed and tested on synthetic noisy datasets. However, they tend to fail to achieve satisfying results on real-world noisy datasets. To this end, we propose a method named GRIP to alleviate the noisy label problem for both synthetic and real-world datasets. Specifically, GRIP utilizes a group regularization strategy that estimates class soft labels to improve noise robustness. Soft label supervision reduces overfitting on noisy labels and learns inter-class similarities to benefit classification. Furthermore, an instance purification operation globally identifies noisy labels by m
&lt;/p&gt;</description></item><item><title>EAGLE&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.15690</link><description>&lt;p&gt;
EAGLE&#65306;&#38754;&#21521;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EAGLE: A Domain Generalization Framework for AI-generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15690
&lt;/p&gt;
&lt;p&gt;
EAGLE&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#36127;&#36131;&#20219;&#21644;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#30417;&#30563;&#24335;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#26087;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38543;&#30528;&#26032;LLMs&#30340;&#39057;&#32321;&#21457;&#24067;&#65292;&#26500;&#24314;&#29992;&#20110;&#35782;&#21035;&#36825;&#20123;&#26032;&#27169;&#22411;&#25991;&#26412;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#23558;&#38656;&#35201;&#26032;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;EAGLE&#21033;&#29992;&#36804;&#20170;&#20026;&#27490;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#23398;&#20064;&#36328;&#36825;&#20123;&#29983;&#25104;&#22120;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#26816;&#27979;&#30001;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;EAGLE&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#30340;&#34920;&#24449;&#33021;&#21147;&#26469;&#23398;&#20064;&#36825;&#31181;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15690v1 Announce Type: cross  Abstract: With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models. While supervised AI-generated text detectors perform well on text generated by older LLMs, with the frequent release of new LLMs, building supervised detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of AI-generated text from unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of self-supervised 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;DIB&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#21487;&#20197;&#36890;&#36807;&#25311;&#21512;&#30456;&#20114;&#20449;&#24687;&#26469;&#25552;&#20379;&#20998;&#26512;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#21464;&#20998;&#36924;&#36817;&#30340;&#38656;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15681</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20449;&#24687;&#29942;&#39048;&#22312;&#30830;&#23450;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable Information Bottleneck for Deterministic Multi-view Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15681
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;DIB&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#21487;&#20197;&#36890;&#36807;&#25311;&#21512;&#30456;&#20114;&#20449;&#24687;&#26469;&#25552;&#20379;&#20998;&#26512;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#21464;&#20998;&#36924;&#36817;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#20026;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;MVC&#65289;&#25552;&#20379;&#20102;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#21387;&#32553;&#22810;&#35270;&#35282;&#35266;&#23519;&#25968;&#25454;&#26469;&#20445;&#30041;&#22810;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;DIB&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25311;&#21512;&#30456;&#20114;&#20449;&#24687;&#32780;&#19981;&#38656;&#35201;&#21464;&#20998;&#36924;&#36817;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#21644;&#20998;&#26512;&#24615;&#30340;MVC&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#24402;&#19968;&#21270;&#26680;&#26684;&#25289;&#22982;&#30697;&#38453;&#30452;&#25509;&#25311;&#21512;&#39640;&#32500;&#31354;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#36741;&#21161;&#31070;&#32463;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15681v1 Announce Type: cross  Abstract: In recent several years, the information bottleneck (IB) principle provides an information-theoretic framework for deep multi-view clustering (MVC) by compressing multi-view observations while preserving the relevant information of multiple views. Although existing IB-based deep MVC methods have achieved huge success, they rely on variational approximation and distribution assumption to estimate the lower bound of mutual information, which is a notoriously hard and impractical problem in high-dimensional multi-view spaces. In this work, we propose a new differentiable information bottleneck (DIB) method, which provides a deterministic and analytical MVC solution by fitting the mutual information without the necessity of variational approximation. Specifically, we first propose to directly fit the mutual information of high-dimensional spaces by leveraging normalized kernel Gram matrix, which does not require any auxiliary neural estima
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24341;&#20837;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.15654</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#26412;&#22320;&#26356;&#26032;&#23545;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of Local Updates for Decentralized Learning under Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24341;&#20837;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20004;&#31181;&#22522;&#26412;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;Decentralized Gradient Tracking (DGT) &#21644; Decentralized Gradient Descent (DGD)&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#22659;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21152;&#20837; $K &gt; 1$ &#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#33021;&#22815;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110; $\mu$-&#24378;&#20984;&#21644; $L$-&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26412;&#22320; DGT &#26041;&#27861;&#23454;&#29616;&#20102;&#36890;&#20449;&#22797;&#26434;&#24230;&#20026; $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$&#65292;&#20854;&#20013; $\rho$ &#34913;&#37327;&#32593;&#32476;&#36830;&#36890;&#24615;&#65292;$\delta$ &#34920;&#31034;&#26412;&#22320;&#25439;&#22833;&#30340;&#20108;&#38454;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#34920;&#26126;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#65292;&#22686;&#21152; $K$ &#33021;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15654v1 Announce Type: new  Abstract: We revisit two fundamental decentralized optimization methods, Decentralized Gradient Tracking (DGT) and Decentralized Gradient Descent (DGD), with multiple local updates. We consider two settings and demonstrate that incorporating $K &gt; 1$ local update steps can reduce communication complexity. Specifically, for $\mu$-strongly convex and $L$-smooth loss functions, we proved that local DGT achieves communication complexity $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$, where $\rho$ measures the network connectivity and $\delta$ measures the second-order heterogeneity of the local loss. Our result reveals the tradeoff between communication and computation and shows increasing $K$ can effectively reduce communication costs when the data heterogeneity is low and the network is well-connected. We then consider the over-parameterization regime where the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Parametric Grid Convolutional Attention Networks&#65288;PGCANs&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#25552;&#39640;&#35299;&#20915;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#20449;&#24687;&#20256;&#25773;&#36895;&#29575;</title><link>https://arxiv.org/abs/2403.15652</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#21644;&#21367;&#31215;&#30340;&#21442;&#25968;&#21270;&#32534;&#30721;&#32531;&#35299;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#35889;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15652
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Parametric Grid Convolutional Attention Networks&#65288;PGCANs&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#25552;&#39640;&#35299;&#20915;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#20449;&#24687;&#20256;&#25773;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35299;&#20915;&#22312;&#24314;&#27169;&#21508;&#31181;&#31995;&#32479;&#21644;&#29289;&#29702;&#29616;&#35937;&#26102;&#33258;&#28982;&#20135;&#29983;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;PDE&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;DNN&#30340;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#65292;&#23427;&#20204;&#20063;&#20250;&#21463;&#21040;&#35889;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#23398;&#20064;&#20302;&#39057;&#35299;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Parametric Grid Convolutional Attention Networks (PGCANs)&#65292;&#21487;&#20197;&#22312;&#39046;&#22495;&#19981;&#20381;&#36182;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;PDE&#31995;&#32479;&#12290;PGCAN&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#29992;&#22522;&#20110;&#32593;&#26684;&#30340;&#32534;&#30721;&#22120;&#23545;&#36755;&#20837;&#31354;&#38388;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#20854;&#21442;&#25968;&#36890;&#36807;&#19968;&#20010;DNN&#35299;&#30721;&#22120;&#19982;&#36755;&#20986;&#30456;&#36830;&#25509;&#65292;&#21518;&#32773;&#21033;&#29992;&#27880;&#24847;&#21147;&#26469;&#20248;&#20808;&#36873;&#25321;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#26412;&#22320;&#21270;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#21367;&#31215;&#23618;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#25913;&#21892;&#20174;&#36793;&#30028;&#21040;&#22495;&#20869;&#37096;&#30340;&#20449;&#24687;&#20256;&#25773;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15652v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are increasingly used to solve partial differential equations (PDEs) that naturally arise while modeling a wide range of systems and physical phenomena. However, the accuracy of such DNNs decreases as the PDE complexity increases and they also suffer from spectral bias as they tend to learn the low-frequency solution characteristics. To address these issues, we introduce Parametric Grid Convolutional Attention Networks (PGCANs) that can solve PDE systems without leveraging any labeled data in the domain. The main idea of PGCAN is to parameterize the input space with a grid-based encoder whose parameters are connected to the output via a DNN decoder that leverages attention to prioritize feature training. Our encoder provides a localized learning ability and uses convolution layers to avoid overfitting and improve information propagation rate from the boundaries to the interior of the domain. We test the perfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24046;&#20998;&#31169;&#26377;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Next-Token Prediction of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#26085;&#30410;&#37325;&#35201;&#12290;DP-SGD&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#20197;&#19968;&#31181;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DP-SGD&#38656;&#35201;&#27604;SGD&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#36807;&#39640;&#20272;&#35745;&#23545;&#25163;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#20551;&#35774;&#21482;&#26377;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31169;&#26377;&#28151;&#21512;&#38598;&#21512;&#20998;&#24067;&#65288;PMixED&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#27599;&#20010;&#36755;&#20986;&#20998;&#24067;&#20174;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;LLM&#38598;&#21512;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#28982;&#21518;&#23545;&#25237;&#24433;&#20998;&#24067;&#36827;&#34892;&#24179;&#22343;&#24182;&#20174;&#20013;&#25277;&#26679;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31169;&#26377;&#39044;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;DP-SGD&#26356;&#36731;&#37327;&#21270;&#65292;&#22240;&#20026;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FedDG&#26550;&#26500;&#26041;&#27861;gPerXAN&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#26041;&#26696;&#21644;&#24341;&#23548;&#27491;&#21017;&#21270;&#22120;&#37197;&#21512;&#24037;&#20316;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26174;&#24335;&#32452;&#35013;&#35268;&#33539;&#21270;&#65292;&#26377;&#21161;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#23545;&#39046;&#22495;&#29305;&#24449;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#36807;&#28388;&#12290;</title><link>https://arxiv.org/abs/2403.15605</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#39640;&#25928;&#32452;&#21512;&#35268;&#33539;&#21270;&#23618;&#19982;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15605
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FedDG&#26550;&#26500;&#26041;&#27861;gPerXAN&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#26041;&#26696;&#21644;&#24341;&#23548;&#27491;&#21017;&#21270;&#22120;&#37197;&#21512;&#24037;&#20316;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26174;&#24335;&#32452;&#35013;&#35268;&#33539;&#21270;&#65292;&#26377;&#21161;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#23545;&#39046;&#22495;&#29305;&#24449;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36716;&#31227;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20005;&#23803;&#30340;&#38382;&#39064;&#65292;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#26410;&#30693;&#39046;&#22495;&#27979;&#35797;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65288;FedDG&#65289;&#26088;&#22312;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#20351;&#29992;&#21327;&#20316;&#23458;&#25143;&#31471;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#21487;&#33021;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#26410;&#30693;&#23458;&#25143;&#31471;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FedDG&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#25968;&#25454;&#27844;&#38706;&#38544;&#31169;&#39118;&#38505;&#65292;&#25110;&#32773;&#22312;&#23458;&#25143;&#31471;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20135;&#29983;&#26174;&#33879;&#24320;&#38144;&#65292;&#36825;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FedDG&#26550;&#26500;&#26041;&#27861;&#65292;&#21363;gPerXAN&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#35268;&#33539;&#21270;&#26041;&#26696;&#19982;&#24341;&#23548;&#27491;&#21017;&#21270;&#22120;&#37197;&#21512;&#24037;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#20010;&#24615;&#21270;&#26174;&#24335;&#32452;&#35013;&#35268;&#33539;&#21270;&#65292;&#20197;&#24378;&#21046;&#23458;&#25143;&#31471;&#27169;&#22411;&#26377;&#36873;&#25321;&#22320;&#36807;&#28388;&#23545;&#26412;&#22320;&#25968;&#25454;&#26377;&#20559;&#21521;&#30340;&#29305;&#23450;&#39046;&#22495;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15605v1 Announce Type: cross  Abstract: Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while ret
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340; sub-seasonal &#33267; seasonal &#22825;&#27668;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15598</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36816;&#33829;&#20122;&#23395;&#33410;&#22825;&#27668;&#39044;&#27979;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
An ensemble of data-driven weather prediction models for operational sub-seasonal forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15598
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340; sub-seasonal &#33267; seasonal &#22825;&#27668;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36816;&#32500;&#21363;&#25512;&#24191;&#30340;&#22810;&#27169;&#22411;&#38598;&#25104;&#22825;&#27668;&#39044;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#19982;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;&#65288;ECMWF&#65289;&#28023;&#27915;&#27169;&#22411;&#32806;&#21512;&#65292;&#22312;1&#24230;&#20998;&#36776;&#29575;&#19979;&#39044;&#27979;&#20840;&#29699;&#22825;&#27668;&#30340;4&#21608;&#21069;&#30651;&#12290;&#23545;&#20110;2&#31859;&#28201;&#24230;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#38598;&#25104;&#24179;&#22343;&#20248;&#20110;&#21407;&#22987;&#30340;ECMWF&#25193;&#23637;&#33539;&#22260;&#38598;&#25104;&#65292;&#20248;&#21183;&#20026;4-17%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#21069;&#30651;&#26102;&#38271;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#29992;&#32479;&#35745;&#20559;&#24046;&#26657;&#27491;&#21518;&#65292;ECMWF&#38598;&#25104;&#22312;4&#21608;&#26102;&#36739;&#38598;&#25104;&#26816;&#39564;&#32422;&#22909;3%&#12290;&#23545;&#20110;&#20854;&#20182;&#22320;&#38754;&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#38598;&#25104;&#20063;&#19982;ECMWF&#30340;&#38598;&#25104;&#30456;&#24046;&#19981;&#21040;&#20960;&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#27425;&#23395;&#33410;&#33267;&#23395;&#33410;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15598v1 Announce Type: cross  Abstract: We present an operations-ready multi-model ensemble weather forecasting system which uses hybrid data-driven weather prediction models coupled with the European Centre for Medium-range Weather Forecasts (ECMWF) ocean model to predict global weather at 1-degree resolution for 4 weeks of lead time. For predictions of 2-meter temperature, our ensemble on average outperforms the raw ECMWF extended-range ensemble by 4-17%, depending on the lead time. However, after applying statistical bias corrections, the ECMWF ensemble is about 3% better at 4 weeks. For other surface parameters, our ensemble is also within a few percentage points of ECMWF's ensemble. We demonstrate that it is possible to achieve near-state-of-the-art subseasonal-to-seasonal forecasts using a multi-model ensembling approach with data-driven weather prediction models.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26159;&#20851;&#20110;&#22312;&#23391;&#21152;&#25289;&#22269;&#32972;&#26223;&#19979;&#23545;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;&#36827;&#34892;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#30007;&#24615;&#21463;&#23475;&#32773;&#30340;&#23384;&#22312;&#12289;&#27169;&#24335;&#21644;&#28508;&#22312;&#22240;&#32032;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#23545;&#30007;&#24615;&#21463;&#23475;&#32773;&#30740;&#31350;&#31354;&#30333;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15594</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27934;&#35265;&#20998;&#26512;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;
&lt;/p&gt;
&lt;p&gt;
Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26159;&#20851;&#20110;&#22312;&#23391;&#21152;&#25289;&#22269;&#32972;&#26223;&#19979;&#23545;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;&#36827;&#34892;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#30007;&#24615;&#21463;&#23475;&#32773;&#30340;&#23384;&#22312;&#12289;&#27169;&#24335;&#21644;&#28508;&#22312;&#22240;&#32032;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#23545;&#30007;&#24615;&#21463;&#23475;&#32773;&#30740;&#31350;&#31354;&#30333;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23478;&#24237;&#26292;&#21147;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#20010;&#20851;&#20110;&#22899;&#24615;&#21463;&#23475;&#32773;&#30340;&#24615;&#21035;&#38382;&#39064;&#65292;&#22312;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#20851;&#27880;&#65292;&#23391;&#21152;&#25289;&#22269;&#29305;&#21035;&#26159;&#30007;&#24615;&#21463;&#23475;&#32773;&#20173;&#28982;&#20027;&#35201;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#22312;&#23391;&#21152;&#25289;&#22269;&#32972;&#26223;&#19979;&#23545;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;&#65288;MDV&#65289;&#36825;&#19968;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#20854;&#26222;&#36941;&#24615;&#12289;&#27169;&#24335;&#21644;&#28508;&#22312;&#22240;&#32032;&#12290;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#24378;&#35843;&#23478;&#24237;&#26292;&#21147;&#24773;&#22659;&#20013;&#22899;&#24615;&#30340;&#21463;&#23475;&#65292;&#23548;&#33268;&#23545;&#30007;&#24615;&#21463;&#23475;&#32773;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#20174;&#23391;&#21152;&#25289;&#22269;&#20027;&#35201;&#22478;&#24066;&#25910;&#38598;&#20102;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#20197;&#20102;&#35299;&#28508;&#22312;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;11&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#21253;&#25324;&#40664;&#35748;&#21644;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#65289;&#12289;2&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;4&#31181;&#38598;&#25104;&#27169;&#22411;&#12290;&#23613;&#31649;&#37319;&#29992;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;CatBoost&#30001;&#20110;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15594v1 Announce Type: cross  Abstract: Domestic violence, which is often perceived as a gendered issue among female victims, has gained increasing attention in recent years. Despite this focus, male victims of domestic abuse remain primarily overlooked, particularly in Bangladesh. Our study represents a pioneering exploration of the underexplored realm of male domestic violence (MDV) within the Bangladeshi context, shedding light on its prevalence, patterns, and underlying factors. Existing literature predominantly emphasizes female victimization in domestic violence scenarios, leading to an absence of research on male victims. We collected data from the major cities of Bangladesh and conducted exploratory data analysis to understand the underlying dynamics. We implemented 11 traditional machine learning models with default and optimized hyperparameters, 2 deep learning, and 4 ensemble models. Despite various approaches, CatBoost has emerged as the top performer due to its 
&lt;/p&gt;</description></item><item><title>FairerCLIP&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;RKHSs&#20013;&#20351;&#29992;&#20989;&#25968;&#21435;&#38500;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#20559;&#35265;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#24471;&#39044;&#27979;&#26356;&#20844;&#24179;&#19988;&#26356;&#33021;&#25269;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15593</link><description>&lt;p&gt;
FairerCLIP: &#22312;RKHSs&#20013;&#20351;&#29992;&#20989;&#25968;&#21435;&#38500;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15593
&lt;/p&gt;
&lt;p&gt;
FairerCLIP&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;RKHSs&#20013;&#20351;&#29992;&#20989;&#25968;&#21435;&#38500;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#20559;&#35265;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#24471;&#39044;&#27979;&#26356;&#20844;&#24179;&#19988;&#26356;&#33021;&#25269;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#25552;&#20379;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#32039;&#20945;&#36890;&#29992;&#34920;&#31034;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#22810;&#20010;&#19979;&#28216;&#38646;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;&#20013;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#35757;&#32451;&#36807;&#31243;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#20256;&#25773;&#25110;&#25918;&#22823;&#31038;&#20250;&#20559;&#35265;&#21644;2&#65289;&#23398;&#20064;&#20381;&#36182;&#34394;&#20551;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;FairerCLIP&#65292;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#26356;&#21152;&#20844;&#24179;&#19988;&#26356;&#33021;&#25269;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHSs&#65289;&#20013;&#32852;&#21512;&#21435;&#20559;CLIP&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#22810;&#20010;&#22909;&#22788;&#65306;1&#65289;&#28789;&#27963;&#24615;&#65306;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#26377;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#35201;&#20040;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#27809;&#26377;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;FairerCLIP&#33021;&#22815;&#36866;&#24212;&#20004;&#31181;&#23398;&#20064;&#24773;&#20917;&#12290;2&#65289;&#20248;&#21270;&#20415;&#21033;&#24615;&#65306;FairerCLIP&#23545;&#20110;&#36845;&#20195;&#20248;&#21270;&#38750;&#24120;&#21512;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15593v1 Announce Type: cross  Abstract: Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15576</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Data-centric Prediction Explanation via Kernelized Stein Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#28508;&#22312;&#34920;&#31034;&#26469;&#36830;&#25509;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#28857;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#39044;&#27979;&#21407;&#22240;&#30340;&#32447;&#32034;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#27604;&#22914;&#20135;&#29983;&#26174;&#30528;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#37322;&#65288;HD-Explain&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#65288;KSD&#65289;&#23646;&#24615;&#30340;&#31616;&#21333;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;KSD&#21807;&#19968;&#22320;&#20026;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20869;&#26680;&#20989;&#25968;&#65292;&#29992;&#20110;&#32534;&#30721;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#39046;&#22495;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;HD-Explain&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15576v1 Announce Type: new  Abstract: Existing example-based prediction explanation methods often bridge test and training data points through the model's parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain), a straightforward prediction explanation method exploiting properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperform
&lt;/p&gt;</description></item><item><title>SSL&#26041;&#27861;&#22522;&#20110;&#20266;&#26631;&#31614;&#23384;&#22312;&#26174;&#33879;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#23567;&#29109;&#21644;&#24341;&#20837;&#19968;&#20010;&#24809;&#32602;&#39033;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15567</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#25152;&#20449;&#20219;&#30340;&#65306;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Do not trust what you trust: Miscalibration in Semi-supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15567
&lt;/p&gt;
&lt;p&gt;
SSL&#26041;&#27861;&#22522;&#20110;&#20266;&#26631;&#31614;&#23384;&#22312;&#26174;&#33879;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#23567;&#29109;&#21644;&#24341;&#20837;&#19968;&#20010;&#24809;&#32602;&#39033;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#39640;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#24341;&#23548;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;&#36825;&#31181;&#31574;&#30053;&#30340;&#22266;&#26377;&#32570;&#28857;&#26469;&#33258;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#20266;&#26631;&#31614;&#20165;&#22522;&#20110;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#31243;&#24230;&#36827;&#34892;&#36807;&#28388;&#65292;&#32780;&#19981;&#32771;&#34385;&#20854;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#20266;&#26631;&#35760;&#36807;&#31243;&#20013;&#35780;&#20272;&#21644;&#22686;&#24378;&#32593;&#32476;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;SSL&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#24418;&#24335;&#21270;&#22320;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#26368;&#23567;&#29109;&#65288;Shannon&#29109;&#30340;&#19979;&#30028;&#65289;&#21487;&#33021;&#26159;&#35823;&#26657;&#20934;&#30340;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#38598;&#25104;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24809;&#32602;&#39033;&#65292;&#24378;&#21046;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#39044;&#27979;&#30340;logit&#36317;&#31163;&#20445;&#25345;&#36739;&#20302;&#65292;&#20174;&#32780;&#38450;&#27490;&#32593;&#32476;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15567v1 Announce Type: new  Abstract: State-of-the-art semi-supervised learning (SSL) approaches rely on highly confident predictions to serve as pseudo-labels that guide the training on unlabeled samples. An inherent drawback of this strategy stems from the quality of the uncertainty estimates, as pseudo-labels are filtered only based on their degree of uncertainty, regardless of the correctness of their predictions. Thus, assessing and enhancing the uncertainty of network predictions is of paramount importance in the pseudo-labeling process. In this work, we empirically demonstrate that SSL methods based on pseudo-labels are significantly miscalibrated, and formally demonstrate the minimization of the min-entropy, a lower bound of the Shannon entropy, as a potential cause for miscalibration. To alleviate this issue, we integrate a simple penalty term, which enforces the logit distances of the predictions on unlabeled samples to remain low, preventing the network prediction
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20083;&#33146;&#35299;&#21078;&#24863;&#30693;&#32593;&#32476;&#65292;&#29992;&#20110;&#25429;&#33719;&#32454;&#33268;&#22270;&#20687;&#32454;&#33410;&#21644;&#32534;&#30721;&#20083;&#33146;&#35299;&#21078;&#30340;&#26032;&#24179;&#28369;&#24230;&#39033;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#32908;&#32905;&#12289;&#20083;&#25151;&#21644;&#32959;&#30244;&#31867;&#21035;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15560</link><description>&lt;p&gt;
A2DMN&#65306;&#38754;&#21521;&#20083;&#33146;&#36229;&#22768;&#35821;&#20041;&#20998;&#21106;&#30340;&#35299;&#21078;&#24863;&#30693;&#25193;&#24352;&#22810;&#23610;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15560
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20083;&#33146;&#35299;&#21078;&#24863;&#30693;&#32593;&#32476;&#65292;&#29992;&#20110;&#25429;&#33719;&#32454;&#33268;&#22270;&#20687;&#32454;&#33410;&#21644;&#32534;&#30721;&#20083;&#33146;&#35299;&#21078;&#30340;&#26032;&#24179;&#28369;&#24230;&#39033;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#32908;&#32905;&#12289;&#20083;&#25151;&#21644;&#32959;&#30244;&#31867;&#21035;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;; &#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#22266;&#26377;&#22320;&#32570;&#20047;&#21033;&#29992;&#32452;&#32455;&#35299;&#21078;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#22270;&#20687;&#21306;&#22495;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#30001;&#20110;&#19981;&#26029;&#19979;&#37319;&#26679;&#25805;&#20316;&#32780;&#38590;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#36793;&#30028;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20083;&#33146;&#35299;&#21078;&#24863;&#30693;&#32593;&#32476;&#65292;&#29992;&#20110;&#25429;&#33719;&#31934;&#32454;&#22270;&#20687;&#32454;&#33410;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;&#20083;&#33146;&#35299;&#21078;&#30340;&#26032;&#24179;&#28369;&#24230;&#39033;&#12290;&#23427;&#36328;&#22810;&#20010;&#31354;&#38388;&#23610;&#24230;&#21512;&#24182;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#35821;&#20041;&#36793;&#30028;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#19982;&#20843;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;325&#24133;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#32908;&#32905;&#12289;&#20083;&#25151;&#21644;&#32959;&#30244;&#31867;&#21035;&#30340;&#20998;&#21106;&#65292;&#24182;&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15560v1 Announce Type: cross  Abstract: In recent years, convolutional neural networks for semantic segmentation of breast ultrasound (BUS) images have shown great success; however, two major challenges still exist. 1) Most current approaches inherently lack the ability to utilize tissue anatomy, resulting in misclassified image regions. 2) They struggle to produce accurate boundaries due to the repeated down-sampling operations. To address these issues, we propose a novel breast anatomy-aware network for capturing fine image details and a new smoothness term that encodes breast anatomy. It incorporates context information across multiple spatial scales to generate more accurate semantic boundaries. Extensive experiments are conducted to compare the proposed method and eight state-of-the-art approaches using a BUS dataset with 325 images. The results demonstrate the proposed method significantly improves the segmentation of the muscle, mammary, and tumor classes and produces
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.15527</link><description>&lt;p&gt;
&#20381;&#20174;&#22312;&#32447;&#27169;&#22411;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Conformal online model aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#20174;&#39044;&#27979;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#20570;&#20986;&#24378;&#28872;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#23427;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23558;&#28857;&#39044;&#27979;&#36716;&#25442;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#30340;&#38598;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20381;&#20174;&#39044;&#27979;&#21482;&#22312;&#20107;&#20808;&#30830;&#23450;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36215;&#20316;&#29992;&#12290;&#20381;&#20174;&#39044;&#27979;&#20013;&#30456;&#23545;&#36739;&#23569;&#28041;&#21450;&#30340;&#38382;&#39064;&#26159;&#27169;&#22411;&#36873;&#25321;&#21644;/&#25110;&#32858;&#21512;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#24212;&#35813;&#22914;&#20309;&#20381;&#20174;&#21270;&#20247;&#22810;&#39044;&#27979;&#26041;&#27861;&#65288;&#38543;&#26426;&#26862;&#26519;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#27491;&#21017;&#21270;&#32447;&#24615;&#27169;&#22411;&#31561;&#65289;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#26469;&#33258;&#22810;&#20010;&#31639;&#27861;&#30340;&#39044;&#27979;&#38598;&#36827;&#34892;&#25237;&#31080;&#65292;&#20854;&#20013;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#19978;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15527v1 Announce Type: cross  Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any black-box prediction model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Latent Neural Cellular Automata (LNCA)&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#20174;&#20256;&#32479;&#36755;&#20837;&#31354;&#38388;&#36716;&#31227;&#21040;&#29305;&#21035;&#35774;&#35745;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#27492;&#23454;&#29616;&#23545;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.15525</link><description>&lt;p&gt;
&#36164;&#28304;&#39640;&#25928;&#22270;&#20687;&#24674;&#22797;&#30340;&#28508;&#22312;&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Latent Neural Cellular Automata for Resource-Efficient Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15525
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Latent Neural Cellular Automata (LNCA)&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#20174;&#20256;&#32479;&#36755;&#20837;&#31354;&#38388;&#36716;&#31227;&#21040;&#29305;&#21035;&#35774;&#35745;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#27492;&#23454;&#29616;&#23545;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#20195;&#34920;&#20102;&#20256;&#32479;&#32454;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36716;&#25442;&#20989;&#25968;&#32780;&#24471;&#20197;&#22686;&#24378;&#12290;&#36825;&#31181;&#20174;&#25163;&#21160;&#21040;&#25968;&#25454;&#39537;&#21160;&#30340;&#36716;&#21464;&#26174;&#33879;&#22686;&#21152;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#24212;&#29992;&#20110;&#21253;&#25324;&#20869;&#23481;&#29983;&#25104;&#21644;&#20154;&#24037;&#29983;&#21629;&#22312;&#20869;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#35201;&#27714;&#30340;&#38459;&#30861;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#65288;LNCA&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#36164;&#28304;&#38480;&#21046;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35745;&#31639;&#20174;&#20256;&#32479;&#36755;&#20837;&#31354;&#38388;&#36716;&#31227;&#21040;&#19968;&#20010;&#32463;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20381;&#36182;&#20110;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#30340;&#32972;&#26223;&#19979;&#65292;&#26088;&#22312;&#20174;&#20854;&#36864;&#21270;&#29256;&#26412;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#36825;&#31181;&#20462;&#25913;&#19981;&#20165;&#25913;&#21892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15525v1 Announce Type: cross  Abstract: Neural cellular automata represent an evolution of the traditional cellular automata model, enhanced by the integration of a deep learning-based transition function. This shift from a manual to a data-driven approach significantly increases the adaptability of these models, enabling their application in diverse domains, including content generation and artificial life. However, their widespread application has been hampered by significant computational requirements. In this work, we introduce the Latent Neural Cellular Automata (LNCA) model, a novel architecture designed to address the resource limitations of neural cellular automata. Our approach shifts the computation from the conventional input space to a specially designed latent space, relying on a pre-trained autoencoder. We apply our model in the context of image restoration, which aims to reconstruct high-quality images from their degraded versions. This modification not only r
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2403.15524</link><description>&lt;p&gt;
PPA-Game&#65306;&#34920;&#24449;&#21644;&#23398;&#20064;&#22312;&#32447;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#30340;&#31454;&#20105;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15524
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27604;&#20363;&#24615;&#25910;&#30410;&#20998;&#37197;&#28216;&#25103;&#65288;PPA-Game&#65289;&#26469;&#27169;&#25311;&#20195;&#29702;&#32773;&#22914;&#20309;&#31454;&#20105;&#21487;&#20998;&#37197;&#36164;&#28304;&#21644;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#65292;&#31867;&#20284;&#20110;YouTube&#21644;TikTok&#31561;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#12290;&#26681;&#25454;&#24322;&#36136;&#26435;&#37325;&#20026;&#20195;&#29702;&#32773;&#20998;&#37197;&#25910;&#30410;&#65292;&#21453;&#26144;&#20102;&#21019;&#20316;&#32773;&#20043;&#38388;&#20869;&#23481;&#36136;&#37327;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#32431;&#32435;&#20160;&#22343;&#34913;&#65288;PNE&#65289;&#24182;&#19981;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#37117;&#26377;&#20445;&#35777;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#20250;&#35266;&#23519;&#21040;&#65292;&#20854;&#32570;&#20047;&#24773;&#20917;&#26159;&#32597;&#35265;&#30340;&#12290;&#38500;&#20102;&#20998;&#26512;&#38745;&#24577;&#25910;&#30410;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20195;&#29702;&#32773;&#20851;&#20110;&#36164;&#28304;&#25910;&#30410;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#23558;&#22810;&#29609;&#23478;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;$T$&#36718;&#20013;&#20419;&#36827;&#27599;&#20010;&#20195;&#29702;&#32773;&#32047;&#31215;&#25910;&#30410;&#30340;&#26368;&#22823;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20219;&#20309;&#20195;&#29702;&#32773;&#30340;&#36951;&#25022;&#22312;&#20219;&#20309;$\eta &gt; 0$&#19979;&#37117;&#21463;&#21040;$O(\log^{1 + \eta} T)$&#30340;&#38480;&#21046;&#12290;&#32463;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15524v1 Announce Type: cross  Abstract: We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how agents, akin to content creators on platforms like YouTube and TikTok, compete for divisible resources and consumers' attention. Payoffs are allocated to agents based on heterogeneous weights, reflecting the diversity in content quality among creators. Our analysis reveals that although a pure Nash equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed, with its absence being rare in our simulations. Beyond analyzing static payoffs, we further discuss the agents' online learning about resource payoffs by integrating a multi-player multi-armed bandit framework. We propose an online algorithm facilitating each agent's maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta &gt; 0$. Empirical results further validate the effectiveness of ou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.15523</link><description>&lt;p&gt;
&#37319;&#29992;&#22122;&#22768;&#26631;&#35760;&#30340;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#30740;&#31350;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards auditory attention decoding with noise-tagging: A pilot study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;(AAD)&#26088;&#22312;&#20174;&#22823;&#33041;&#27963;&#21160;&#20013;&#25552;&#21462;&#34987;&#20851;&#27880;&#30340;&#35828;&#35805;&#32773;&#65292;&#25552;&#20379;&#20102;&#31070;&#32463;&#23548;&#21521;&#21548;&#35273;&#35774;&#22791;&#21644;&#33041;&#26426;&#25509;&#21475;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;AAD&#65292;&#35813;&#21327;&#35758;&#24341;&#21457;&#20102;&#21487;&#38752;&#30340;&#32534;&#30721;&#35843;&#21046;&#35825;&#21457;&#30005;&#20301;&#65292;&#20294;&#22312;&#21548;&#35273;&#27169;&#24335;&#19979;&#30340;&#25506;&#32034;&#36824;&#24456;&#26377;&#38480;&#12290;&#30740;&#31350;&#21442;&#19982;&#32773;&#20381;&#27425;&#21576;&#29616;&#20004;&#20010;&#33655;&#20848;&#35821;&#35328;&#35821;&#38899;&#21050;&#28608;&#65292;&#36825;&#20123;&#21050;&#28608;&#34987;&#24133;&#24230;&#35843;&#21046;&#20026;&#20855;&#26377;&#21807;&#19968;&#20108;&#36827;&#21046;&#20266;&#38543;&#26426;&#22122;&#22768;&#30721;&#65292;&#26377;&#25928;&#22320;&#20026;&#20854;&#26631;&#35760;&#20102;&#38468;&#21152;&#21487;&#35299;&#30721;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26410;&#35843;&#21046;&#38899;&#39057;&#19982;&#20351;&#29992;&#19981;&#21516;&#35843;&#21046;&#28145;&#24230;&#35843;&#21046;&#30340;&#38899;&#39057;&#30340;&#35299;&#30721;&#65292;&#20197;&#21450;&#20256;&#32479;AAD&#26041;&#27861;&#19982;&#26631;&#20934;&#35299;&#30721;&#22122;&#22768;&#30721;&#26041;&#27861;&#30340;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#35797;&#28857;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#26410;&#35843;&#21046;&#38899;&#39057;&#30456;&#27604;&#65292;70&#33267;100%&#30340;&#35843;&#21046;&#28145;&#24230;&#30340;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15523v1 Announce Type: cross  Abstract: Auditory attention decoding (AAD) aims to extract from brain activity the attended speaker amidst candidate speakers, offering promising applications for neuro-steered hearing devices and brain-computer interfacing. This pilot study makes a first step towards AAD using the noise-tagging stimulus protocol, which evokes reliable code-modulated evoked potentials, but is minimally explored in the auditory modality. Participants were sequentially presented with two Dutch speech stimuli that were amplitude modulated with a unique binary pseudo-random noise-code, effectively tagging these with additional decodable information. We compared the decoding of unmodulated audio against audio modulated with various modulation depths, and a conventional AAD method against a standard method to decode noise-codes. Our pilot study revealed higher performances for the conventional method with 70 to 100 percent modulation depths compared to unmodulated au
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#30340;&#26032;&#22411;&#26041;&#27861;&#65288;UMM&#65289;&#65292;&#19982;&#30446;&#21069;&#20808;&#36827;&#30340;c-VEP&#38646;&#35757;&#32451;&#26041;&#27861;&#65288;CCA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26080;&#38656;&#26657;&#20934;&#30340;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15521</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26032;&#39046;&#22495;&#34892;&#36208;&#65306;&#38754;&#21521;c-VEP BCI&#30340;&#26080;&#26657;&#20934;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to walk on new ground: Calibration-free decoding for c-VEP BCI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#30340;&#26032;&#22411;&#26041;&#27861;&#65288;UMM&#65289;&#65292;&#19982;&#30446;&#21069;&#20808;&#36827;&#30340;c-VEP&#38646;&#35757;&#32451;&#26041;&#27861;&#65288;CCA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26080;&#38656;&#26657;&#20934;&#30340;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#38646;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#21487;&#29992;&#24615;&#65292;&#28040;&#38500;&#20102;&#26657;&#20934;&#20250;&#35805;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26681;&#26893;&#20110;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#65288;ERP&#65289;&#39046;&#22495;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#26080;&#30417;&#30563;&#22343;&#20540;&#26368;&#22823;&#21270;&#65288;UMM&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#32534;&#30721;&#35843;&#21046;&#35270;&#35273;&#35825;&#21457;&#30005;&#20301;&#65288;c-VEP&#65289;&#21050;&#28608;&#21327;&#35758;&#12290;&#25105;&#20204;&#23558;UMM&#19982;&#20351;&#29992;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30340;c-VEP&#38646;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#27604;&#36739;&#21253;&#25324;&#23545;CCA&#21644;UMM&#30340;&#21363;&#26102;&#20998;&#31867;&#20197;&#21450;&#23545;&#20808;&#21069;&#20998;&#31867;&#35797;&#39564;&#36827;&#34892;&#32047;&#31215;&#23398;&#20064;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;c-VEP&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#29420;&#29305;&#20248;&#21183;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#20026;&#26080;&#26657;&#20934;BCI&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#26045;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#36824;&#20026;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#21644;&#25913;&#36827;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15521v1 Announce Type: cross  Abstract: This study explores two zero-training methods aimed at enhancing the usability of brain-computer interfaces (BCIs) by eliminating the need for a calibration session. We introduce a novel method rooted in the event-related potential (ERP) domain, unsupervised mean maximization (UMM), to the fast code-modulated visual evoked potential (c-VEP) stimulus protocol. We compare UMM to the state-of-the-art c-VEP zero-training method that uses canonical correlation analysis (CCA). The comparison includes instantaneous classification and classification with cumulative learning from previously classified trials for both CCA and UMM. Our study shows the effectiveness of both methods in navigating the complexities of a c-VEP dataset, highlighting their differences and distinct strengths. This research not only provides insights into the practical implementation of calibration-free BCI methods but also paves the way for further exploration and refine
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;GNN&#21644;Transformer&#30340;&#21512;&#20316;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#26500;&#24314;&#20102;GTC&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;GNN&#30340;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#21644;Transformer&#30340;&#20840;&#23616;&#20449;&#24687;&#24314;&#27169;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15520</link><description>&lt;p&gt;
GTC&#65306;GNN-Transformer&#33258;&#30417;&#30563;&#24322;&#26500;&#22270;&#34920;&#31034;&#30340;&#20849;&#36717;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;GNN&#21644;Transformer&#30340;&#21512;&#20316;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#26500;&#24314;&#20102;GTC&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;GNN&#30340;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#21644;Transformer&#30340;&#20840;&#23616;&#20449;&#24687;&#24314;&#27169;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20855;&#26377;&#20256;&#36882;&#20449;&#24687;&#30340;&#26426;&#21046;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#32858;&#21512;&#26412;&#22320;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#26368;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#24179;&#28369;&#19968;&#30452;&#38459;&#30861;&#30528;GNN&#36827;&#19968;&#27493;&#28145;&#20837;&#21644;&#25429;&#33719;&#22810;&#36339;&#37051;&#23621;&#12290;&#19982;GNN&#19981;&#21516;&#65292;Transformer&#21487;&#20197;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#36866;&#24403;&#30340;Transformer&#32467;&#26500;&#26469;&#24314;&#27169;&#20840;&#23616;&#20449;&#24687;&#21644;&#22810;&#36339;&#20132;&#20114;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;GNN&#21644;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#25972;&#21512;GNN&#30340;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#21644;Transformer&#30340;&#20840;&#23616;&#20449;&#24687;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#28040;&#38500;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;GNN-Transformer&#30340;&#21327;&#21516;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#26500;&#24314;&#20102;GTC&#26550;&#26500;&#12290;GTC&#21033;&#29992;GNN&#21644;Transformer&#20998;&#25903;&#20998;&#21035;&#23545;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#30340;&#33410;&#28857;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15520v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the most powerful weapon for various graph tasks due to the message-passing mechanism's great local information aggregation ability. However, over-smoothing has always hindered GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs, Transformers can model global information and multi-hop interactions via multi-head self-attention and a proper Transformer structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine GNN and Transformer, integrating both GNN's local information aggregation and Transformer's global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for GNN-Transformer and constructs GTC architecture. GTC leverages the GNN and Transformer branch to encode node information from different views respectively, and establishes contrast
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#30340;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21452;&#37325;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.15517</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#31209;&#21644;&#29305;&#24449;&#20016;&#23500;&#24615;&#26469;&#25913;&#21892;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#21069;&#21521;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#30340;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21452;&#37325;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26159;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#36880;&#28176;&#23398;&#20064;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#21152;&#22522;&#30784;&#38454;&#27573;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#21512;&#24182;&#26356;&#22810;&#19982;&#26410;&#35265;&#26032;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;RFR&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#21452;&#37325;&#30446;&#26631;&#65306;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15517v1 Announce Type: new  Abstract: Class Incremental Learning (CIL) constitutes a pivotal subfield within continual learning, aimed at enabling models to progressively learn new classification tasks while retaining knowledge obtained from prior tasks. Although previous studies have predominantly focused on backward compatible approaches to mitigate catastrophic forgetting, recent investigations have introduced forward compatible methods to enhance performance on novel tasks and complement existing backward compatible methods. In this study, we introduce an effective-Rank based Feature Richness enhancement (RFR) method, designed for improving forward compatibility. Specifically, this method increases the effective rank of representations during the base session, thereby facilitating the incorporation of more informative features pertinent to unseen novel tasks. Consequently, RFR achieves dual objectives in backward and forward compatibility: minimizing feature extractor mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31227;&#21160;&#28508;&#22312;&#29305;&#24449;&#12289;&#37325;&#26500;&#29983;&#25104;&#27169;&#31946;&#29256;&#26412;&#20197;&#21450;&#37319;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27604;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15512</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#25552;&#39640;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31227;&#21160;&#28508;&#22312;&#29305;&#24449;&#12289;&#37325;&#26500;&#29983;&#25104;&#27169;&#31946;&#29256;&#26412;&#20197;&#21450;&#37319;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27604;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20294;&#30452;&#25509;&#26041;&#27861;&#65288;&#22914;mixup&#21644;cutout&#65289;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#21463;&#38480;&#20110;&#20854;&#31163;&#25955;&#29305;&#24615;&#12290;&#26412;&#25991;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#26368;&#26032;&#30740;&#31350;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#31283;&#20581;&#24615;&#12290;&#35813;&#25216;&#26415;&#39318;&#20808;&#19987;&#27880;&#20110;&#23558;&#28508;&#22312;&#29305;&#24449;&#31227;&#36817;&#20915;&#31574;&#36793;&#30028;&#65292;&#28982;&#21518;&#36827;&#34892;&#37325;&#26500;&#20197;&#29983;&#25104;&#19968;&#20010;&#24102;&#26377;&#36719;&#26631;&#31614;&#30340;&#27169;&#31946;&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#24314;&#35758;&#20351;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15512v1 Announce Type: cross  Abstract: Efforts to leverage deep learning models in low-resource regimes have led to numerous augmentation studies. However, the direct application of methods such as mixup and cutout to text data, is limited due to their discrete characteristics. While methods using pretrained language models have exhibited efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware data augmentation strategy to enhance robustness using pretrained language models. The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;(MIAE)&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;MIAE&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#19979;&#23558;&#24322;&#26500;&#36755;&#20837;&#36716;&#25442;&#20026;&#36739;&#20302;&#32500;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20998;&#31867;&#22120;&#21306;&#20998;&#27491;&#24120;&#34892;&#20026;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15511</link><description>&lt;p&gt;
IoT&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;&#24341;&#23548;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;(MIAE)&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;MIAE&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#19979;&#23558;&#24322;&#26500;&#36755;&#20837;&#36716;&#25442;&#20026;&#36739;&#20302;&#32500;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20998;&#31867;&#22120;&#21306;&#20998;&#27491;&#24120;&#34892;&#20026;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDSs)&#21463;&#30410;&#20110;IoT&#25968;&#25454;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#65292;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#22312;IoT IDSs&#20013;&#35757;&#32451;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;(MIAE)&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;MIAE&#30001;&#22810;&#20010;&#23376;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#19981;&#21516;&#26469;&#28304;&#30340;&#36755;&#20837;&#12290; MIAE&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#23558;&#24322;&#26500;&#36755;&#20837;&#36716;&#25442;&#20026;&#36739;&#20302;&#32500;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20998;&#31867;&#22120;&#21306;&#20998;&#27491;&#24120;&#34892;&#20026;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15511v1 Announce Type: cross  Abstract: While intrusion detection systems (IDSs) benefit from the diversity and generalization of IoT data features, the data diversity (e.g., the heterogeneity and high dimensions of data) also makes it difficult to train effective machine learning models in IoT IDSs. This also leads to potentially redundant/noisy features that may decrease the accuracy of the detection engine in IDSs. This paper first introduces a novel neural network architecture called Multiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that can process inputs from different sources with different characteristics. The MIAE model is trained in an unsupervised learning mode to transform the heterogeneous inputs into lower-dimensional representation, which helps classifiers distinguish between normal behaviour and different types of attacks. To distil and retain more relevant features but remove less important/redundant ones during the training process,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SLU&#22810;&#20219;&#21153;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#38450;&#27490;&#35821;&#38899;&#35782;&#21035;&#21644;&#36523;&#20221;&#35782;&#21035;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15510</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving End-to-End Spoken Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SLU&#22810;&#20219;&#21153;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#38450;&#27490;&#35821;&#38899;&#35782;&#21035;&#21644;&#36523;&#20221;&#35782;&#21035;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#26159;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#25143;&#30028;&#38754;&#12290;&#20154;&#31867;&#30340;&#35328;&#35821;&#21487;&#33021;&#21253;&#21547;&#22823;&#37327;&#29992;&#25143;&#25935;&#24863;&#20449;&#24687;&#65292;&#22914;&#24615;&#21035;&#12289;&#36523;&#20221;&#21644;&#25935;&#24863;&#20869;&#23481;&#12290;&#22240;&#27492;&#20986;&#29616;&#20102;&#26032;&#22411;&#23433;&#20840;&#21644;&#38544;&#31169;&#20405;&#29359;&#12290;&#29992;&#25143;&#19981;&#24076;&#26395;&#23558;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#26292;&#38706;&#32473;&#19981;&#21487;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#30340;&#24694;&#24847;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;SLU&#31995;&#32479;&#38656;&#35201;&#30830;&#20445;&#28508;&#22312;&#30340;&#24694;&#24847;&#25915;&#20987;&#32773;&#26080;&#27861;&#25512;&#26029;&#20986;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#21516;&#26102;&#21448;&#24212;&#36991;&#20813;&#22823;&#22823;&#25439;&#23475;SLU&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;SLU&#22810;&#20219;&#21153;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#36523;&#20221;&#35782;&#21035;&#65288;IR&#65289;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#38544;&#34255;&#23618;&#20998;&#31163;&#25216;&#26415;&#65292;&#20351;SLU&#20449;&#24687;&#20165;&#20998;&#24067;&#22312;&#38544;&#34255;&#23618;&#30340;&#29305;&#23450;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15510v1 Announce Type: cross  Abstract: Spoken language understanding (SLU), one of the key enabling technologies for human-computer interaction in IoT devices, provides an easy-to-use user interface. Human speech can contain a lot of user-sensitive information, such as gender, identity, and sensitive content. New types of security and privacy breaches have thus emerged. Users do not want to expose their personal sensitive information to malicious attacks by untrusted third parties. Thus, the SLU system needs to ensure that a potential malicious attacker cannot deduce the sensitive attributes of the users, while it should avoid greatly compromising the SLU accuracy. To address the above challenge, this paper proposes a novel SLU multi-task privacy-preserving model to prevent both the speech recognition (ASR) and identity recognition (IR) attacks. The model uses the hidden layer separation technique so that SLU information is distributed only in a specific portion of the hidd
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;(TAE)&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#36716;&#25442;&#20026;&#21487;&#20998;&#31163;&#34920;&#31034;&#26469;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#28151;&#21512;&#34920;&#31034;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15509</link><description>&lt;p&gt;
&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#30340;&#21487;&#20998;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15509
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;(TAE)&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#36716;&#25442;&#20026;&#21487;&#20998;&#31163;&#34920;&#31034;&#26469;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#28151;&#21512;&#34920;&#31034;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#31561;&#35768;&#22810;&#38382;&#39064;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;AEs&#34920;&#31034;&#20013;&#28151;&#21512;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;TAE&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;TAE&#23558;&#28508;&#22312;&#34920;&#31034;&#30830;&#23450;&#22320;&#36716;&#25442;&#20026;&#26356;&#26131;&#21306;&#20998;&#30340;&#34920;&#31034;&#65292;&#21363;\textit{&#21487;&#20998;&#31163;&#34920;&#31034;}&#65292;&#24182;&#22312;&#36755;&#20986;&#31471;&#37325;&#24314;&#21487;&#20998;&#31163;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15509v1 Announce Type: cross  Abstract: Representation Learning (RL) plays a pivotal role in the success of many problems including cyberattack detection. Most of the RL methods for cyberattack detection are based on the latent vector of Auto-Encoder (AE) models. An AE transforms raw data into a new latent representation that better exposes the underlying characteristics of the input data. Thus, it is very useful for identifying cyberattacks. However, due to the heterogeneity and sophistication of cyberattacks, the representation of AEs is often entangled/mixed resulting in the difficulty for downstream attack detection models. To tackle this problem, we propose a novel mod called Twin Auto-Encoder (TAE). TAE deterministically transforms the latent representation into a more distinguishable representation namely the \textit{separable representation} and the reconstructsuct the separable representation at the output. The output of TAE called the \textit{reconstruction represe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15502</link><description>&lt;p&gt;
&#22312;&#32447;&#25991;&#23383;&#33258;&#21160;&#23436;&#25104;&#30340;&#39034;&#24207;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Sequential Decision-Making for Inline Text Autocomplete
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15502
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#29616;&#20195;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#21151;&#33021;&#65292;&#24212;&#29992;&#20110;&#35832;&#22914;&#28040;&#24687;&#20256;&#36882;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#39046;&#22495;&#12290;&#36890;&#24120;&#65292;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#20174;&#20855;&#26377;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38408;&#20540;&#24182;&#27809;&#26377;&#30452;&#25509;&#32771;&#34385;&#29992;&#25143;&#22240;&#26174;&#31034;&#24314;&#35758;&#32780;&#26045;&#21152;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#20363;&#22914;&#20174;&#36755;&#20837;&#20999;&#25442;&#21040;&#38405;&#35835;&#24314;&#35758;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#24314;&#35758;&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#26469;&#25913;&#36827;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#38543;&#26102;&#38388;&#19982;&#30446;&#26631;&#29992;&#25143;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23398;&#20064;&#24314;&#35758;&#31574;&#30053;&#12290;&#36825;&#31181;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#23558;&#35748;&#30693;&#36127;&#33655;&#22240;&#32032;&#32435;&#20837;&#35757;&#32451;&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#30340;&#30446;&#26631;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#29702;&#35770;&#26041;&#38754;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15502v1 Announce Type: new  Abstract: Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use reinforcement learning to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretica
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22240;&#26524;&#20002;&#22833;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#20013;&#22240;&#20002;&#22833;&#20540;&#24341;&#36215;&#30340;&#20559;&#20506;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15500</link><description>&lt;p&gt;
&#22312;&#32570;&#22833;&#20540;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#65306;&#19968;&#20010;&#22240;&#26524;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15500
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22240;&#26524;&#20002;&#22833;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#20013;&#22240;&#20002;&#22833;&#20540;&#24341;&#36215;&#30340;&#20559;&#20506;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#65288;GRNI&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#20013;&#23384;&#22312;&#38646;&#20540;&#65306;&#19968;&#20123;&#26159;&#20195;&#34920;&#27809;&#26377;&#22522;&#22240;&#34920;&#36798;&#30340;&#29983;&#29289;&#38646;&#20540;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#26159;&#30001;&#20110;&#27979;&#24207;&#36807;&#31243;&#65288;&#20063;&#31216;&#20026;&#20002;&#22833;&#20540;&#65289;&#23548;&#33268;&#30340;&#25216;&#26415;&#24615;&#38646;&#20540;&#65292;&#36825;&#21487;&#33021;&#36890;&#36807;&#25197;&#26354;&#27979;&#23450;&#22522;&#22240;&#34920;&#36798;&#30340;&#32852;&#21512;&#20998;&#24067;&#32780;&#20559;&#20506;GRNI&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#25554;&#34917;&#22788;&#29702;&#20002;&#22833;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#20837;&#20266;&#20851;&#31995;&#65292;&#22240;&#20026;&#30495;&#23454;&#30340;&#32852;&#21512;&#20998;&#24067;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#26524;&#22270;&#27169;&#22411;&#26469;&#34920;&#24449;&#20002;&#22833;&#26426;&#21046;&#65292;&#21363;&#22240;&#26524;&#20002;&#22833;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15500v1 Announce Type: cross  Abstract: Gene regulatory network inference (GRNI) is a challenging problem, particularly owing to the presence of zeros in single-cell RNA sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts), which may bias GRNI by distorting the joint distribution of the measured gene expressions. Existing approaches typically handle dropout error via imputation, which may introduce spurious relations as the true joint distribution is generally unidentifiable. To tackle this issue, we introduce a causal graphical model to characterize the dropout mechanism, namely, Causal Dropout Model. We provide a simple yet effective theoretical result: interestingly, the conditional independence (CI) relations in the data with dropouts, after deleting the samples with zero values (regardless if technical or not) for the conditioned variables, are asymptoticall
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Causal Machine Learning&#26041;&#27861;&#20998;&#26512;&#30005;&#21147;&#24066;&#22330;&#20013;&#23450;&#20215;&#25919;&#31574;&#23545;CO2&#27700;&#24179;&#30340;&#24433;&#21709;&#65292;&#25361;&#25112;&#20256;&#32479;&#26234;&#24935;&#65292;&#21457;&#29616;&#21487;&#33021;&#22686;&#21152;CO2&#24378;&#24230;&#65292;&#24182;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20803;&#31639;&#27861;&#22686;&#24378;&#30740;&#31350;&#28145;&#24230;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15499</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20803;&#23398;&#20064;&#22120;&#23545;&#30005;&#21147;&#24066;&#22330;&#20013;CO2&#20943;&#25490;&#31574;&#30053;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15499
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Causal Machine Learning&#26041;&#27861;&#20998;&#26512;&#30005;&#21147;&#24066;&#22330;&#20013;&#23450;&#20215;&#25919;&#31574;&#23545;CO2&#27700;&#24179;&#30340;&#24433;&#21709;&#65292;&#25361;&#25112;&#20256;&#32479;&#26234;&#24935;&#65292;&#21457;&#29616;&#21487;&#33021;&#22686;&#21152;CO2&#24378;&#24230;&#65292;&#24182;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20803;&#31639;&#27861;&#22686;&#24378;&#30740;&#31350;&#28145;&#24230;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#65288;CausalML&#65289;&#32479;&#35745;&#26041;&#27861;&#65292;&#20998;&#26512;&#30005;&#21147;&#23450;&#20215;&#25919;&#31574;&#23545;&#23478;&#24237;&#37096;&#38376;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#35843;&#26597;&#28508;&#22312;&#32467;&#26524;&#19982;&#22788;&#29702;&#25928;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20854;&#20013;&#23450;&#20215;&#25919;&#31574;&#30340;&#21464;&#21270;&#26159;&#22788;&#29702;&#25928;&#26524;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25361;&#25112;&#20102;&#22260;&#32469;&#22522;&#20110;&#28608;&#21169;&#30340;&#30005;&#21147;&#23450;&#20215;&#30340;&#20256;&#32479;&#26234;&#24935;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#36825;&#20123;&#25919;&#31574;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;CO2&#24378;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20803;&#31639;&#27861;&#65292;&#21453;&#26144;&#20102;&#24403;&#20195;&#32479;&#35745;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#30340;&#22240;&#26524;&#20998;&#26512;&#28145;&#24230;&#12290;&#35813;&#30740;&#31350;&#23545;&#23398;&#20064;&#32773;X&#12289;T&#12289;S&#21644;R&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#22522;&#20110;&#35268;&#23450;&#38382;&#39064;&#30340;&#20855;&#20307;&#30446;&#26631;&#21644;&#32972;&#26223;&#32454;&#24494;&#20043;&#22788;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#19978;&#30340;&#25345;&#32493;&#23545;&#35805;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15499v1 Announce Type: cross  Abstract: This study employs the Causal Machine Learning (CausalML) statistical method to analyze the influence of electricity pricing policies on carbon dioxide (CO2) levels in the household sector. Investigating the causality between potential outcomes and treatment effects, where changes in pricing policies are the treatment, our analysis challenges the conventional wisdom surrounding incentive-based electricity pricing. The study's findings suggest that adopting such policies may inadvertently increase CO2 intensity. Additionally, we integrate a machine learning-based meta-algorithm, reflecting a contemporary statistical approach, to enhance the depth of our causal analysis. The study conducts a comparative analysis of learners X, T, S, and R to ascertain the optimal methods based on the defined question's specified goals and contextual nuances. This research contributes valuable insights to the ongoing dialogue on sustainable development pr
&lt;/p&gt;</description></item><item><title>&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;</title><link>https://arxiv.org/abs/2403.15498</link><description>&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#19990;&#30028;&#27169;&#22411;&#21644;&#28508;&#21464;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15498
&lt;/p&gt;
&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#24615;&#33021;&#26469;&#28304;&#30340;&#35752;&#35770;&#12290;&#26159;&#20165;&#20165;&#23398;&#20064;&#21477;&#27861;&#27169;&#24335;&#21644;&#34920;&#38754;&#32479;&#35745;&#32467;&#26524;&#65292;&#36824;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#35821;&#20041;&#21644;&#19990;&#30028;&#27169;&#22411;&#65311;&#25105;&#20204;&#22312;&#35937;&#26827;&#36825;&#20010;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#28216;&#25103;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#21644;&#23545;&#27604;&#28608;&#27963;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#23613;&#31649;&#27169;&#22411;&#27809;&#26377;&#20808;&#39564;&#30340;&#28216;&#25103;&#30693;&#35782;&#65292;&#20165;&#20165;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20851;&#20110;&#26827;&#30424;&#29366;&#24577;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15498v1 Announce Type: cross  Abstract: Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Benford's law&#20316;&#20026;&#19968;&#31181;&#26816;&#27979;&#35270;&#35273;&#27169;&#22411;&#20013;&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#36807;&#28388;&#24322;&#24120;&#25968;&#25454;&#28857;&#21644;&#26631;&#35760;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.15497</link><description>&lt;p&gt;
&#36890;&#36807;&#32479;&#35745;&#25216;&#26415;&#22312;&#35270;&#35273;&#27169;&#22411;&#20013;&#26816;&#27979;&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
On the Detection of Anomalous or Out-Of-Distribution Data in Vision Models Using Statistical Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Benford's law&#20316;&#20026;&#19968;&#31181;&#26816;&#27979;&#35270;&#35273;&#27169;&#22411;&#20013;&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#36807;&#28388;&#24322;&#24120;&#25968;&#25454;&#28857;&#21644;&#26631;&#35760;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#21644;&#24322;&#24120;&#36755;&#20837;&#26159;&#24403;&#20170;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#28431;&#27934;&#65292;&#32463;&#24120;&#23548;&#33268;&#31995;&#32479;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#30340;&#25968;&#25454;&#33539;&#22260;&#24191;&#27867;&#65292;&#22240;&#27492;&#26816;&#27979;&#38750;&#20856;&#22411;&#36755;&#20837;&#26159;&#19968;&#39033;&#22256;&#38590;&#19988;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26412;&#31119;&#29305;&#23450;&#24459;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#30495;&#23454;&#21644;&#21463;&#25439;&#36755;&#20837;&#20043;&#38388;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#35768;&#22810;&#24773;&#22659;&#19979;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24322;&#24120;&#25968;&#25454;&#28857;&#30340;&#36807;&#28388;&#22120;&#65292;&#26631;&#24535;&#30528;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#24076;&#26395;&#23601;&#36825;&#20123;&#24212;&#29992;&#21644;&#36825;&#31181;&#25216;&#26415;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15497v1 Announce Type: cross  Abstract: Out-of-distribution data and anomalous inputs are vulnerabilities of machine learning systems today, often causing systems to make incorrect predictions. The diverse range of data on which these models are used makes detecting atypical inputs a difficult and important task. We assess a tool, Benford's law, as a method used to quantify the difference between real and corrupted inputs. We believe that in many settings, it could function as a filter for anomalous data points and for signalling out-of-distribution data. We hope to open a discussion on these applications and further areas where this technique is underexplored.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#24182;&#35760;&#24405;&#20102;&#26080;&#38464;&#34746;&#24815;&#23548;&#31995;&#32479;&#65288;GFINS&#65289;&#21644;&#22810;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;MIMU&#65289;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.15494</link><description>&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#21644;&#26080;&#38464;&#34746;&#24815;&#23548;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Multiple and Gyro-Free Inertial Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15494
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#24182;&#35760;&#24405;&#20102;&#26080;&#38464;&#34746;&#24815;&#23548;&#31995;&#32479;&#65288;GFINS&#65289;&#21644;&#22810;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;MIMU&#65289;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#24815;&#24615;&#23548;&#33322;&#31995;&#32479;&#65288;INS&#65289;&#21033;&#29992;&#19977;&#20010;&#27491;&#20132;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#26469;&#30830;&#23450;&#24179;&#21488;&#30340;&#20301;&#32622;&#12289;&#36895;&#24230;&#21644;&#26041;&#21521;&#12290;INS&#26377;&#26080;&#25968;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#33258;&#20027;&#24179;&#21488;&#21644;&#29289;&#32852;&#32593;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19982;INS&#30340;&#25972;&#21512;&#65292;&#31361;&#20986;&#20102;&#37325;&#22823;&#30340;&#21019;&#26032;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#23613;&#31649;&#36825;&#19968;&#39046;&#22495;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;INS&#25968;&#25454;&#38598;&#20063;&#19981;&#26029;&#22686;&#21152;&#65292;&#20294;&#23545;&#20110;&#26080;&#38464;&#34746;&#24815;&#23548;&#31995;&#32479;&#65288;GFINS&#65289;&#21644;&#22810;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;MIMU&#65289;&#26550;&#26500;&#21364;&#27809;&#26377;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#24182;&#28608;&#21457;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;54&#20010;&#24815;&#24615;&#20256;&#24863;&#22120;&#20998;&#32452;&#25104;&#20061;&#20010;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65292;&#35774;&#35745;&#24182;&#35760;&#24405;&#20102;GFINS&#21644;MIMU&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#21487;&#29992;&#20110;&#23450;&#20041;&#21644;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;MIMU&#21644;GFINS&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15494v1 Announce Type: cross  Abstract: An inertial navigation system (INS) utilizes three orthogonal accelerometers and gyroscopes to determine platform position, velocity, and orientation. There are countless applications for INS, including robotics, autonomous platforms, and the internet of things. Recent research explores the integration of data-driven methods with INS, highlighting significant innovations, improving accuracy and efficiency. Despite the growing interest in this field and the availability of INS datasets, no datasets are available for gyro-free INS (GFINS) and multiple inertial measurement unit (MIMU) architectures. To fill this gap and to stimulate further research in this field, we designed and recorded GFINS and MIMU datasets using 54 inertial sensors grouped in nine inertial measurement units. These sensors can be used to define and evaluate different types of MIMU and GFINS architectures. The inertial sensors were arranged in three different sensor c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20010;&#20307;&#30340;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#21892;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15489</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
EEG decoding with conditional identification information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15489
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20010;&#20307;&#30340;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#21892;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35299;&#30721;&#23545;&#20110;&#25581;&#31034;&#20154;&#31867;&#22823;&#33041;&#24182;&#25512;&#21160;&#33041;&#26426;&#25509;&#21475;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#39640;&#22122;&#38899;&#27700;&#24179;&#21644;&#20010;&#20307;&#38388;&#22266;&#26377;&#21464;&#21270;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#24402;&#21151;&#20110;&#20854;&#20808;&#36827;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DNN&#22312;&#35299;&#30721;&#26410;&#35265;&#20010;&#20307;&#30340;&#33041;&#30005;&#22270;&#26679;&#26412;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#20307;&#30340;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#36890;&#36807;&#33041;&#30005;&#22270;&#21644;&#20010;&#20154;&#29305;&#24449;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;WithMe&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#21253;&#21547;&#36825;&#20123;&#26631;&#35782;&#31526;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#38598;&#20013;&#30340;&#20027;&#20307;&#21644;&#26410;&#35265;&#20027;&#20307;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#22686;&#24378;&#26174;&#31034;&#20102;&#25913;&#36827;&#33041;&#30005;&#22270;&#35299;&#30721;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15489v1 Announce Type: cross  Abstract: Decoding EEG signals is crucial for unraveling human brain and advancing brain-computer interfaces. Traditional machine learning algorithms have been hindered by the high noise levels and inherent inter-person variations in EEG signals. Recent advances in deep neural networks (DNNs) have shown promise, owing to their advanced nonlinear modeling capabilities. However, DNN still faces challenge in decoding EEG samples of unseen individuals. To address this, this paper introduces a novel approach by incorporating the conditional identification information of each individual into the neural network, thereby enhancing model representation through the synergistic interaction of EEG and personal traits. We test our model on the WithMe dataset and demonstrated that the inclusion of these identifiers substantially boosts accuracy for both subjects in the training set and unseen subjects. This enhancement suggests promising potential for improvi
&lt;/p&gt;</description></item><item><title>MOGAM&#27169;&#22411;&#26159;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#19978;&#30340;&#38480;&#21046;&#32780;&#25552;&#20986;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15485</link><description>&lt;p&gt;
MOGAM&#65306;&#19968;&#31181;&#29992;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#38754;&#21521;&#23545;&#35937;&#22270;&#27880;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15485
&lt;/p&gt;
&lt;p&gt;
MOGAM&#27169;&#22411;&#26159;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#19978;&#30340;&#38480;&#21046;&#32780;&#25552;&#20986;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#22312;&#25233;&#37057;&#30151;&#27835;&#30103;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#20010;&#20307;&#22312;&#35813;&#24179;&#21488;&#34920;&#36798;&#24773;&#32490;&#65292;&#26088;&#22312;&#23454;&#29616;&#25233;&#37057;&#30151;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#29305;&#23450;&#29305;&#24449;&#65292;&#23548;&#33268;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#35270;&#39057;&#65289;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#38754;&#21521;&#23545;&#35937;&#22270;&#27880;&#24847;&#27169;&#22411;&#65288;MOGAM&#65289;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#25552;&#20379;&#26356;&#20855;&#21487;&#20280;&#32553;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#20026;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#25233;&#37057;&#30151;&#30340;&#30495;&#23454;&#30151;&#29366;&#65292;&#25105;&#20204;&#20165;&#25910;&#38598;&#20102;&#20855;&#26377;&#20020;&#24202;&#35786;&#26029;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#26085;&#24535;&#12290;&#20026;&#20102;&#21033;&#29992;&#35270;&#39057;&#26085;&#24535;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#39069;&#22806;&#30340;&#20803;&#25968;&#25454;&#65292;&#22914;&#35270;&#39057;&#26085;&#24535;&#30340;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15485v1 Announce Type: cross  Abstract: Early detection plays a crucial role in the treatment of depression. Therefore, numerous studies have focused on social media platforms, where individuals express their emotions, aiming to achieve early detection of depression. However, the majority of existing approaches often rely on specific features, leading to limited scalability across different types of social media datasets, such as text, images, or videos. To overcome this limitation, we introduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can be applied to diverse types of data, offering a more scalable and versatile solution. Furthermore, to ensure that our model can capture authentic symptoms of depression, we only include vlogs from users with a clinical diagnosis. To leverage the diverse features of vlogs, we adopt a multimodal approach and collect additional metadata such as the title, description, and duration of the vlogs. To effectively aggregat
&lt;/p&gt;</description></item><item><title>RakutenAI-7B&#26159;&#19968;&#22871;&#26085;&#26412;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26085;&#26412;LM Harness&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20998;&#21035;&#21457;&#24067;&#20102;&#25351;&#23548;&#21644;&#32842;&#22825;&#35843;&#25972;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15484</link><description>&lt;p&gt;
RakutenAI-7B&#65306;&#20026;&#26085;&#26412;&#35821;&#35328;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RakutenAI-7B: Extending Large Language Models for Japanese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15484
&lt;/p&gt;
&lt;p&gt;
RakutenAI-7B&#26159;&#19968;&#22871;&#26085;&#26412;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26085;&#26412;LM Harness&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20998;&#21035;&#21457;&#24067;&#20102;&#25351;&#23548;&#21644;&#32842;&#22825;&#35843;&#25972;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;RakutenAI-7B&#65292;&#36825;&#26159;&#19968;&#22871;&#20197;&#26085;&#26412;&#20026;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26085;&#26412;LM Harness&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#20248;&#20110;&#24320;&#25918;&#30340;7B&#27169;&#22411;&#12290;&#38500;&#20102;&#22522;&#30784;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#26681;&#25454;&#25351;&#23548;&#21644;&#32842;&#22825;&#36827;&#34892;&#35843;&#25972;&#30340;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;RakutenAI-7B-instruct&#21644;RakutenAI-7B-chat&#65292;&#20351;&#29992;Apache 2.0&#35768;&#21487;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15484v1 Announce Type: new  Abstract: We introduce RakutenAI-7B, a suite of Japanese-oriented large language models that achieve the best performance on the Japanese LM Harness benchmarks among the open 7B models. Along with the foundation model, we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#22686;&#24378;&#30340;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;Gram&#35282;&#22330;&#32534;&#30721;&#25216;&#26415;&#21644;&#26799;&#24230;&#24809;&#32602;Wasserstein&#36317;&#31163;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25193;&#23637;&#21407;&#22987;&#35757;&#32451;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.15483</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#22686;&#24378;&#30340;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15483
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#22686;&#24378;&#30340;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#20351;&#29992;Gram&#35282;&#22330;&#32534;&#30721;&#25216;&#26415;&#21644;&#26799;&#24230;&#24809;&#32602;Wasserstein&#36317;&#31163;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25193;&#23637;&#21407;&#22987;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#26377;&#25928;&#25429;&#25417;&#28378;&#21160;&#36724;&#25215;&#26102;&#22495;&#20449;&#21495;&#20043;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#27169;&#22411;&#31934;&#24230;&#21463;&#26679;&#26412;&#25968;&#37327;&#21644;&#36136;&#37327;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#22686;&#24378;&#30340;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;Gram&#35282;&#22330;&#32534;&#30721;&#25216;&#26415;&#23545;&#28378;&#21160;&#36724;&#25215;&#30340;&#26102;&#22495;&#20449;&#21495;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#29983;&#25104;&#29305;&#24449;&#22270;&#20197;&#20445;&#30041;&#25391;&#21160;&#20449;&#21495;&#30340;&#23436;&#25972;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#20026;&#35757;&#32451;&#38598;&#12289;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#12290;&#20854;&#20013;&#65292;&#35757;&#32451;&#38598;&#36755;&#20837;&#26799;&#24230;&#24809;&#32602;Wasserstein&#36317;&#31163;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#21040;&#20855;&#26377;&#31867;&#20284;&#29305;&#24449;&#30340;&#26032;&#26679;&#26412;&#65292;&#20174;&#32780;&#25193;&#23637;&#21407;&#22987;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15483v1 Announce Type: cross  Abstract: In order to solve the problem that current convolutional neural networks can not capture the correlation features between the time domain signals of rolling bearings effectively, and the model accuracy is limited by the number and quality of samples, a rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model is proposed. Firstly, Gram angular field coding technique is used to encode the time domain signal of the rolling bearing and generate the feature map to retain the complete information of the vibration signal. Then, the re-sulting data is divided into a training set, a validation set, and a test set. Among them, the training set is input into the gradient penalty Wasserstein distance generation adversarial network to complete the training, and a new sample with similar features to the training sample is obtained, and then the original training set is expanded. N
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#25552;&#20379;&#22810;&#32423;&#35814;&#32454;&#21453;&#39304;&#65292;&#36171;&#33021;&#35268;&#27169;&#21270;&#30340;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.15482</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#32423;&#21453;&#39304;&#65292;&#20026;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15482
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#25552;&#20379;&#22810;&#32423;&#35814;&#32454;&#21453;&#39304;&#65292;&#36171;&#33021;&#35268;&#27169;&#21270;&#30340;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#26159;&#22521;&#20859;&#20855;&#26377;&#20020;&#24202;&#25216;&#33021;&#30340;&#21516;&#34892;&#36741;&#23548;&#21592;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21453;&#39304;&#26426;&#21046;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#30417;&#30563;&#12290;&#21516;&#34892;&#36741;&#23548;&#21592;&#32463;&#24120;&#32570;&#20047;&#26426;&#21046;&#26469;&#20174;&#32463;&#39564;&#20016;&#23500;&#30340;&#23548;&#24072;&#37027;&#37324;&#33719;&#24471;&#35814;&#32454;&#30340;&#21453;&#39304;&#65292;&#36825;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#25903;&#25345;&#20351;&#29992;&#21516;&#34892;&#36741;&#23548;&#30340;&#22823;&#37327;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#24773;&#22659;&#21270;&#21644;&#22810;&#32423;&#21453;&#39304;&#65292;&#20197;&#36171;&#33021;&#35268;&#27169;&#21270;&#30340;&#21021;&#23398;&#32773;&#21516;&#34892;&#36741;&#23548;&#21592;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#19982;&#19968;&#32452;&#39640;&#32423;&#24515;&#29702;&#27835;&#30103;&#30563;&#23548;&#20849;&#21516;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#32423;&#21453;&#39304;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#23436;&#25972;&#21453;&#39304;&#27880;&#37322;&#30340;400&#27425;&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21453;&#39304;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15482v1 Announce Type: new  Abstract: Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualita
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19982;&#22270;&#21464;&#25442;&#22120;&#38598;&#25104;&#65292;&#35774;&#35745;&#20102;&#33033;&#20914;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#31232;&#30095;&#21152;&#27861;&#21644;&#25513;&#30721;&#25805;&#20316;&#21462;&#20195;&#30697;&#38453;&#20056;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#23454;&#29616;&#20102;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.15480</link><description>&lt;p&gt;
SpikeGraphormer&#65306;&#19968;&#31181;&#20855;&#26377;&#33033;&#20914;&#22270;&#27880;&#24847;&#21147;&#30340;&#39640;&#24615;&#33021;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19982;&#22270;&#21464;&#25442;&#22120;&#38598;&#25104;&#65292;&#35774;&#35745;&#20102;&#33033;&#20914;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#31232;&#30095;&#21152;&#27861;&#21644;&#25513;&#30721;&#25805;&#20316;&#21462;&#20195;&#30697;&#38453;&#20056;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#23454;&#29616;&#20102;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#21464;&#25442;&#22120;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22266;&#26377;&#38480;&#21046;&#24182;&#22686;&#24378;&#22270;&#34920;&#31034;&#24615;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#33410;&#28857;&#20219;&#21153;&#65292;&#22270;&#21464;&#25442;&#22120;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21644;&#20108;&#36827;&#21046;&#33033;&#20914;&#23646;&#24615;&#65292;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#33021;&#37327;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#23558;SNN&#19982;&#22270;&#21464;&#25442;&#22120;&#36827;&#34892;&#38598;&#25104;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33033;&#20914;&#22270;&#27880;&#24847;&#21147;&#65288;SGA&#65289;&#27169;&#22359;&#12290;&#30697;&#38453;&#20056;&#27861;&#34987;&#31232;&#30095;&#21152;&#27861;&#21644;&#25513;&#30721;&#25805;&#20316;&#25152;&#21462;&#20195;&#12290;&#32447;&#24615;&#22797;&#26434;&#24615;&#20351;&#24471;&#21487;&#20197;&#22312;&#20855;&#26377;&#26377;&#38480;GPU&#20869;&#23384;&#30340;&#22823;&#35268;&#27169;&#22270;&#19978;&#36827;&#34892;&#25152;&#26377;&#33410;&#28857;&#38388;&#30340;&#20132;&#20114;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#23558;SNN&#24341;&#20837;&#22270;&#21464;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;SpikeGraph&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15480v1 Announce Type: cross  Abstract: Recently, Graph Transformers have emerged as a promising solution to alleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance graph representation performance. Unfortunately, Graph Transformers are computationally expensive due to the quadratic complexity inherent in self-attention when applied over large-scale graphs, especially for node tasks. In contrast, spiking neural networks (SNNs), with event-driven and binary spikes properties, can perform energy-efficient computation. In this work, we propose a novel insight into integrating SNNs with Graph Transformers and design a Spiking Graph Attention (SGA) module. The matrix multiplication is replaced by sparse addition and mask operations. The linear complexity enables all-pair node interactions on large-scale graphs with limited GPU memory. To our knowledge, our work is the first attempt to introduce SNNs into Graph Transformers. Furthermore, we design SpikeGraph
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.15476</link><description>&lt;p&gt;
&#23398;&#20064;&#25512;&#26029;&#29983;&#25104;&#35270;&#35273;&#27010;&#24565;&#30340;&#27169;&#26495;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Infer Generative Template Programs for Visual Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15476
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#21487;&#20197;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#28789;&#27963;&#25484;&#25569;&#35270;&#35273;&#27010;&#24565;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#23398;&#20064;&#22914;&#20309;&#20197;&#19968;&#31181;&#36890;&#29992;&#26041;&#24335;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#65306;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#30340;&#31243;&#24207;&#34920;&#36798;&#24335;&#65292;&#25351;&#23450;&#20102;&#36755;&#20837;&#27010;&#24565;&#20013;&#24120;&#35265;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#22810;&#20010;&#19982;&#27010;&#24565;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#36890;&#36807;&#35299;&#26512;&#36827;&#34892;&#23569;&#26679;&#26412;&#29983;&#25104;&#21644;&#20849;&#20998;&#21106;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#20174;&#21253;&#21547;&#27010;&#24565;&#20998;&#32452;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#35273;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65306;2D&#24067;&#23616;&#12289;Omniglot&#23383;&#31526;&#21644;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#22312;&#26377;&#38480;&#39046;&#22495;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#20102;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15476v1 Announce Type: cross  Abstract: People grasp flexible visual concepts from a few examples. We explore a neurosymbolic system that learns how to infer programs that capture visual concepts in a domain-general fashion. We introduce Template Programs: programmatic expressions from a domain-specific language that specify structural and parametric patterns common to an input concept. Our framework supports multiple concept-related tasks, including few-shot generation and co-segmentation through parsing. We develop a learning paradigm that allows us to train networks that infer Template Programs directly from visual datasets that contain concept groupings. We run experiments across multiple visual domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our method outperforms task-specific alternatives, and performs competitively against domain-specific approaches for the limited domains where they exist.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15474</link><description>&lt;p&gt;
EC-IoU: &#36890;&#36807;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#35843;&#25972;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#65288;EC-IoU&#65289;&#24230;&#37327;&#26469;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#26469;&#20248;&#21270;&#24191;&#27867;&#20351;&#29992;&#30340;IoU&#24230;&#37327;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#33258;&#25105;&#20195;&#29702;&#20154;&#30340;&#35270;&#35282;&#35206;&#30422;&#26356;&#36817;&#30340;&#22320;&#38754;&#30495;&#23454;&#23545;&#35937;&#28857;&#30340;&#39044;&#27979;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;EC-IoU&#24230;&#37327;&#21487;&#20197;&#29992;&#20110;&#20856;&#22411;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#36873;&#25321;&#26377;&#26356;&#39640;&#23433;&#20840;&#24615;&#34920;&#29616;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#23427;&#36824;&#21487;&#20197;&#38598;&#25104;&#21040;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#20013;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#23613;&#31649;&#38754;&#21521;&#23433;&#20840;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;EC-IoU&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22343;&#20540;&#24179;&#22343;&#31934;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20248;&#20110;&#20351;&#29992;IoU&#35757;&#32451;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15474v1 Announce Type: cross  Abstract: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24320;&#21457;&#30340;&#31561;&#36317;NMT&#31995;&#32479;&#65292;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21477;&#23545;&#20013;&#38899;&#32032;&#35745;&#25968;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.15469</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#32032;&#35745;&#25968;&#27604;&#22870;&#21169;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#31561;&#36317;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24320;&#21457;&#30340;&#31561;&#36317;NMT&#31995;&#32479;&#65292;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21477;&#23545;&#20013;&#38899;&#32032;&#35745;&#25968;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#35270;&#39057;&#37197;&#38899;&#65288;AVD&#65289;&#27969;&#27700;&#32447;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65292;&#21363;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#12290;AVD&#31649;&#36947;&#20013;&#20351;&#29992;&#31561;&#36317;-NMT&#31639;&#27861;&#26469;&#35843;&#33410;&#21512;&#25104;&#36755;&#20986;&#25991;&#26412;&#30340;&#38271;&#24230;&#65292;&#20197;&#30830;&#20445;&#22312;&#37197;&#38899;&#36807;&#31243;&#20043;&#21518;&#65292;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#23545;&#40784;&#21516;&#27493;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#35843;&#25972;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#25991;&#26412;&#20013;&#23383;&#31526;&#21644;&#21333;&#35789;&#30340;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#35843;&#25972;&#38899;&#32032;&#30340;&#25968;&#37327;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#35821;&#38899;&#25345;&#32493;&#26102;&#38388;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24320;&#21457;&#31561;&#36317;NMT&#31995;&#32479;&#65292;&#37325;&#28857;&#20248;&#21270;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21477;&#23545;&#20013;&#38899;&#32032;&#35745;&#25968;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15469v1 Announce Type: new  Abstract: Traditional Automatic Video Dubbing (AVD) pipeline consists of three key modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms are employed to regulate the length of the synthesized output text. This is done to guarantee synchronization with respect to the alignment of video and audio subsequent to the dubbing process. Previous approaches have focused on aligning the number of characters and words in the source and target language texts of Machine Translation models. However, our approach aims to align the number of phonemes instead, as they are closely associated with speech duration. In this paper, we present the development of an isometric NMT system using Reinforcement Learning (RL), with a focus on optimizing the alignment of phoneme counts in the source and target language sentence pairs. To evaluate our models, we propose the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#23545;&#27169;&#31946;&#30340;&#36710;&#29260;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#27604;&#36739;&#20102;Real-ESRGAN&#12289;A-ESRGAN&#21644;StarSRGAN&#19977;&#31181;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15466</link><description>&lt;p&gt;
&#21033;&#29992;&#36229;&#20998;&#36776;&#29575;&#25104;&#20687;&#25216;&#26415;&#35782;&#21035;&#20302;&#20998;&#36776;&#29575;&#27169;&#31946;&#36710;&#29260;&#65306;Real-ESRGAN&#12289;A-ESRGAN&#21644;StarSRGAN&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Super-Resolution Imaging for Recognition of Low-Resolution Blurred License Plates: A Comparative Study of Real-ESRGAN, A-ESRGAN, and StarSRGAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#23545;&#27169;&#31946;&#30340;&#36710;&#29260;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#27604;&#36739;&#20102;Real-ESRGAN&#12289;A-ESRGAN&#21644;StarSRGAN&#19977;&#31181;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#24378;&#21170;&#21457;&#23637;&#65292;&#36710;&#29260;&#35782;&#21035;&#25216;&#26415;&#29616;&#22312;&#21487;&#20197;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#36866;&#24403;&#24212;&#29992;&#65292;&#22914;&#36947;&#36335;&#30417;&#25511;&#12289;&#34987;&#30423;&#36710;&#36742;&#36861;&#36394;&#12289;&#20572;&#36710;&#22330;&#20986;&#20837;&#21475;&#26816;&#27979;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24212;&#29992;&#27491;&#24120;&#36816;&#34892;&#30340;&#21069;&#25552;&#26159;&#36710;&#29260;&#24517;&#39035;&#36275;&#22815;&#8220;&#28165;&#26224;&#8221;&#65292;&#25165;&#33021;&#34987;&#31995;&#32479;&#35782;&#21035;&#20986;&#27491;&#30830;&#30340;&#36710;&#29260;&#21495;&#30721;&#12290;&#22914;&#26524;&#30001;&#20110;&#22806;&#37096;&#22240;&#32032;&#20351;&#36710;&#29260;&#21464;&#27169;&#31946;&#65292;&#37027;&#20040;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#23558;&#22823;&#22823;&#38477;&#20302;&#12290;&#23613;&#31649;&#21488;&#28286;&#26377;&#35768;&#22810;&#36947;&#36335;&#30417;&#25511;&#25668;&#20687;&#22836;&#65292;&#20294;&#22823;&#22810;&#25968;&#25668;&#20687;&#22836;&#30340;&#36136;&#37327;&#19981;&#20339;&#65292;&#32463;&#24120;&#23548;&#33268;&#30001;&#20110;&#29031;&#29255;&#20998;&#36776;&#29575;&#20302;&#32780;&#26080;&#27861;&#35782;&#21035;&#36710;&#29260;&#21495;&#30721;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#22788;&#29702;&#27169;&#31946;&#30340;&#36710;&#29260;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#23558;&#23545;&#19977;&#20010;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65306;Real-ESRGAN&#12289;A-ESRGAN&#21644;StarSRGAN&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15466v1 Announce Type: cross  Abstract: With the robust development of technology, license plate recognition technology can now be properly applied in various scenarios, such as road monitoring, tracking of stolen vehicles, detection at parking lot entrances and exits, and so on. However, the precondition for these applications to function normally is that the license plate must be 'clear' enough to be recognized by the system with the correct license plate number. If the license plate becomes blurred due to some external factors, then the accuracy of recognition will be greatly reduced. Although there are many road surveillance cameras in Taiwan, the quality of most cameras is not good, often leading to the inability to recognize license plate numbers due to low photo resolution. Therefore, this study focuses on using super-resolution technology to process blurred license plates. This study will mainly fine-tune three super-resolution models: Real-ESRGAN, A-ESRGAN, and Star
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15465</link><description>&lt;p&gt;
&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;$n$-gram&#32467;&#26500;&#30340;transformer&#65292;&#20363;&#22914;&#24213;&#23618;&#30340;ChatGPT&#12290;Transformer&#25552;&#20379;&#20102;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#21333;&#35789;&#24207;&#21015;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#36825;&#20123;&#27010;&#29575;&#35745;&#31639;&#39640;&#21487;&#33021;&#24615;&#21333;&#35789;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#35745;&#31639;&#20174;&#32473;&#23450;&#21021;&#22987;&#29366;&#24577;&#24320;&#22987;&#30340;&#26368;&#20248;&#65288;&#21363;&#26368;&#26377;&#21487;&#33021;&#65289;&#21333;&#35789;&#24207;&#21015;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$N$&#21644;$n$-gram&#35789;&#27719;&#37327;&#30340;&#20302;&#38454;&#22810;&#39033;&#24335;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;$N$&#20010;&#21333;&#35789;&#30340;&#39640;&#21487;&#33021;&#24615;&#24207;&#21015;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#20013;&#30340;&#23637;&#24320;&#26041;&#27861;&#65292;&#19968;&#31181;&#21333;&#31574;&#30053;&#36845;&#20195;&#65292;&#21487;&#20197;&#25913;&#21892;&#20219;&#20309;&#32473;&#23450;&#21551;&#21457;&#24335;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#21551;&#21457;&#24335;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#39640;&#27010;&#29575;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#12289;&#31034;&#20363;&#21644;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15465v1 Announce Type: cross  Abstract: In this paper we consider a transformer with an $n$-gram structure, such as the one underlying ChatGPT. The transformer provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15464</link><description>&lt;p&gt;
&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#30142;&#30149;&#39044;&#27979;&#65306;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15464v1 &#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#25688;&#35201;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#21253;&#21547;&#23545;&#20581;&#24247;&#30456;&#20851;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#30142;&#30149;&#39044;&#27979;&#65292;&#26377;&#20215;&#20540;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#23558;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;(&#20363;&#22914;&#35786;&#26029;&#12289;&#23454;&#39564;&#23460;&#12289;&#22788;&#26041;)&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#21465;&#36848;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#38754;&#21521;EHR&#39044;&#27979;&#30340;&#25552;&#31034;&#31574;&#30053;&#35780;&#20272;&#20102;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#65306;&#19968;&#20010;&#36827;&#34892;&#39044;&#27979;&#24182;&#29983;&#25104;&#25512;&#29702;&#36807;&#31243;&#30340;&#39044;&#27979;&#20195;&#29702;&#65292;&#20197;&#21450;&#20998;&#26512;&#19981;&#27491;&#30830;&#39044;&#27979;&#24182;&#20026;&#25913;&#21892;&#39044;&#27979;&#20195;&#29702;&#25512;&#29702;&#25552;&#20379;&#25351;&#23548;&#30340;&#25209;&#35780;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;LLMs&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15464v1 Announce Type: cross  Abstract: Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#27492;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20316;&#20026;&#21487;&#38752;&#22522;&#20934;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.15463</link><description>&lt;p&gt;
&#25581;&#31034;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24322;&#24120;&#65306;&#36830;&#32493;&#23398;&#20064;&#20013;&#20687;&#32032;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15463
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#27492;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20316;&#20026;&#21487;&#38752;&#22522;&#20934;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19968;&#20010;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22270;&#20687;&#26102;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#24456;&#23569;&#20851;&#27880;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#26032;&#25968;&#25454;&#38543;&#26102;&#38388;&#21040;&#26469;&#65292;&#30446;&#26631;&#26159;&#22312;&#26032;&#26087;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#32463;&#20856;&#35774;&#32622;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#20197;&#22312;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22522;&#20110;&#20687;&#32032;&#30340;&#24322;&#24120;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#65292;&#24182;&#20316;&#20026;&#35813;&#39046;&#22495;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#35752;&#35770;&#20102;&#21738;&#20123;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21644;&#21738;&#20123;&#26041;&#27861;&#26063;&#30475;&#36215;&#26469;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15463v1 Announce Type: cross  Abstract: Anomaly Detection is a relevant problem in numerous real-world applications, especially when dealing with images. However, little attention has been paid to the issue of changes over time in the input data distribution, which may cause a significant decrease in performance. In this study, we investigate the problem of Pixel-Level Anomaly Detection in the Continual Learning setting, where new data arrives over time and the goal is to perform well on new and old data. We implement several state-of-the-art techniques to solve the Anomaly Detection problem in the classic setting and adapt them to work in the Continual Learning setting. To validate the approaches, we use a real-world dataset of images with pixel-based anomalies to provide a reliable benchmark and serve as a foundation for further advancements in the field. We provide a comprehensive analysis, discussing which Anomaly Detection methods and which families of approaches seem m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24320;&#21457;&#20266;&#26631;&#35760;&#21644;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#37326;&#28779;&#29123;&#26009;&#26144;&#23556;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15462</link><description>&lt;p&gt;
FUELVISION: &#19968;&#31181;&#29992;&#20110;&#37326;&#28779;&#29123;&#26009;&#26144;&#23556;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#22810;&#27169;&#22411;&#38598;&#25104;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15462
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24320;&#21457;&#20266;&#26631;&#35760;&#21644;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#37326;&#28779;&#29123;&#26009;&#26144;&#23556;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29123;&#26009;&#26465;&#20214;&#30340;&#20934;&#30830;&#35780;&#20272;&#26159;&#39044;&#27979;&#28779;&#28798;&#28857;&#29123;&#21644;&#34892;&#20026;&#20197;&#21450;&#39118;&#38505;&#31649;&#29702;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#21253;&#25324;Landsat-8&#20809;&#23398;&#24433;&#20687;&#12289;&#20004;&#31181;SAR&#65288;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65289;&#24433;&#20687;Sentinel-1&#65288;C&#27874;&#27573;&#65289;&#21644;PALSAR&#65288;L&#27874;&#27573;&#65289;&#20197;&#21450;&#22320;&#24418;&#29305;&#24449;&#22312;&#20869;&#30340;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#25429;&#33719;&#29123;&#26009;&#31867;&#22411;&#21644;&#20998;&#24067;&#30340;&#32508;&#21512;&#20449;&#24687;&#12290;&#36890;&#36807;&#35757;&#32451;&#38598;&#25104;&#27169;&#22411;&#26469;&#39044;&#27979;&#31867;&#20284;&#8220;Scott and Burgan 40&#8221;&#36825;&#26679;&#30340;&#26223;&#35266;&#23610;&#24230;&#29123;&#26009;&#65292;&#20351;&#29992;&#32654;&#22269;&#20892;&#19994;&#37096;&#26519;&#21153;&#23616;&#33719;&#24471;&#30340;&#26862;&#26519;&#28165;&#26597;&#19982;&#20998;&#26512;&#65288;FIA&#65289;&#30000;&#38388;&#35843;&#26597;&#22320;&#22359;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#36275;&#65292;&#36825;&#31181;&#22522;&#26412;&#26041;&#27861;&#20135;&#29983;&#20102;&#30456;&#23545;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#38024;&#23545;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#65292;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#24320;&#21457;&#20102;&#20266;&#26631;&#35760;&#21644;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#22686;&#24378;&#26469;&#33258;&#21152;&#21033;&#31119;&#23612;&#20122;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15462v1 Announce Type: cross  Abstract: Accurate assessment of fuel conditions is a prerequisite for fire ignition and behavior prediction, and risk management. The method proposed herein leverages diverse data sources including Landsat-8 optical imagery, Sentinel-1 (C-band) Synthetic Aperture Radar (SAR) imagery, PALSAR (L-band) SAR imagery, and terrain features to capture comprehensive information about fuel types and distributions. An ensemble model was trained to predict landscape-scale fuels such as the 'Scott and Burgan 40' using the as-received Forest Inventory and Analysis (FIA) field survey plot data obtained from the USDA Forest Service. However, this basic approach yielded relatively poor results due to the inadequate amount of training data. Pseudo-labeled and fully synthetic datasets were developed using generative AI approaches to address the limitations of ground truth data availability. These synthetic datasets were used for augmenting the FIA data from Calif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;&#21644;&#27602;&#24615;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;BERT&#21644;GPT&#27169;&#22411;&#22312;DOTA 2&#28216;&#25103;&#23545;&#25112;&#30340;&#32842;&#22825;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15458</link><description>&lt;p&gt;
&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;&#21644;&#27602;&#24615;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;BERT&#21644;GPT&#27169;&#22411;&#22312;DOTA 2&#28216;&#25103;&#23545;&#25112;&#30340;&#32842;&#22825;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29609;&#22312;&#32447;&#25163;&#26426;&#21644;&#30005;&#33041;&#28216;&#25103;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#19982;&#29609;&#23478;&#20043;&#38388;&#30340;&#26377;&#27602;&#34892;&#20026;&#21644;&#28389;&#29992;&#27807;&#36890;&#26377;&#20851;&#12290;&#22522;&#20110;&#19981;&#21516;&#30340;&#25253;&#21578;&#21644;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#21644;&#27602;&#24615;&#23545;&#29609;&#23478;&#28216;&#25103;&#34920;&#29616;&#21644;&#25972;&#20307;&#24184;&#31119;&#24863;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20998;&#31867;&#25110;&#26816;&#27979;&#28216;&#25103;&#20869;&#22403;&#22334;&#35805;&#25110;&#26377;&#27602;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#24182;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;BERT&#21644;GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#28216;&#25103;&#20869;&#32842;&#22825;&#20013;&#30340;&#27602;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;API&#65292;&#25910;&#38598;&#20102;&#26469;&#33258;DOTA 2&#28216;&#25103;&#23545;&#25112;&#30340;&#28216;&#25103;&#20869;&#32842;&#22825;&#25968;&#25454;&#65292;&#32463;&#36807;&#22788;&#29702;&#12289;&#23457;&#26597;&#21644;&#26631;&#35760;&#20026;&#38750;&#27602;&#24615;&#12289;&#36731;&#24494;&#65288;&#27602;&#24615;&#65289;&#21644;&#26377;&#27602;&#12290;&#35813;&#30740;&#31350;&#33021;&#22815;&#25910;&#38598;&#32422;&#20004;&#21315;&#20010;&#28216;&#25103;&#20869;&#32842;&#22825;&#20197;&#35757;&#32451;&#21644;&#27979;&#35797;BERT&#65288;Base-uncased&#65289;&#12289;BERT&#65288;Large-uncased&#65289;&#21644;GPT-3&#27169;&#22411;&#12290;&#22522;&#20110;&#36825;&#19977;&#31181;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#65292;&#26412;&#30740;&#31350;&#24471;&#20986;&#32467;&#35770;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15458v1 Announce Type: new  Abstract: Common problems in playing online mobile and computer games were related to toxic behavior and abusive communication among players. Based on different reports and studies, the study also discusses the impact of online hate speech and toxicity on players' in-game performance and overall well-being. This study investigates the capability of pre-trained language models to classify or detect trash talk or toxic in-game messages The study employs and evaluates the performance of pre-trained BERT and GPT language models in detecting toxicity within in-game chats. Using publicly available APIs, in-game chat data from DOTA 2 game matches were collected, processed, reviewed, and labeled as non-toxic, mild (toxicity), and toxic. The study was able to collect around two thousand in-game chats to train and test BERT (Base-uncased), BERT (Large-uncased), and GPT-3 models. Based on the three models' state-of-the-art performance, this study concludes p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.15455</link><description>&lt;p&gt;
&#25913;&#36827;&#25991;&#26412;&#27969;&#20013;&#29992;&#20110;&#24494;&#35843;SentenceBERT&#30340;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#25991;&#26412;&#25968;&#25454;&#30340;&#28608;&#22686;&#20026;&#26426;&#26500;&#21644;&#20844;&#21496;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#30417;&#27979;&#20844;&#20247;&#23545;&#20854;&#26381;&#21153;&#21644;&#20135;&#21697;&#30340;&#24847;&#35265;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#24555;&#36895;&#29983;&#25104;&#65292;&#22788;&#29702;&#20381;&#27425;&#21040;&#36798;&#12289;&#28508;&#22312;&#26080;&#38480;&#30340;&#25991;&#26412;&#27969;&#30340;&#25991;&#26412;&#27969;&#25366;&#25496;&#35774;&#32622;&#36890;&#24120;&#27604;&#20256;&#32479;&#30340;&#25209;&#37327;&#23398;&#20064;&#26356;&#21512;&#36866;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#22240;&#20854;&#22312;&#27969;&#24335;&#20869;&#23481;&#20013;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65288;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#29616;&#35937;&#65289;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#23545;&#31934;&#24515;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#20934;&#30830;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#30340;SBERT&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15455v1 Announce Type: new  Abstract: The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four differ
&lt;/p&gt;</description></item><item><title>&#23545;&#31216;&#30772;&#22351;&#25216;&#26415;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#21069;&#21521;&#27169;&#22411;&#23545;&#31216;&#24615;&#36896;&#25104;&#30340;&#22256;&#38590;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.15448</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#23398;&#20064;&#22312;&#20301;&#30456;&#24674;&#22797;&#20013;&#23384;&#22312;&#20160;&#20040;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is Wrong with End-to-End Learning for Phase Retrieval?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15448
&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#30772;&#22351;&#25216;&#26415;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#21069;&#21521;&#27169;&#22411;&#23545;&#31216;&#24615;&#36896;&#25104;&#30340;&#22256;&#38590;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#25104;&#20687;&#31185;&#23398;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#21069;&#21521;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#24456;&#24120;&#35265;&#12290;&#24403;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#20869;&#22312;&#23545;&#31216;&#24615;&#21487;&#33021;&#23548;&#33268;&#24456;&#22823;&#30340;&#23398;&#20064;&#22256;&#38590;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;&#36825;&#20123;&#22256;&#38590;&#26159;&#22914;&#20309;&#20135;&#29983;&#30340;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#22914;&#20309;&#36890;&#36807;&#22312;&#20219;&#20309;&#23398;&#20064;&#20043;&#21069;&#23545;&#35757;&#32451;&#38598;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#21363;&#23545;&#31216;&#30772;&#22351;&#65292;&#26469;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#12290;&#25105;&#20204;&#20197;&#36828;&#22330;&#30456;&#20301;&#24674;&#22797;&#65288;FFPR&#65289;&#20026;&#20363;&#65292;&#36825;&#23545;&#35768;&#22810;&#31185;&#23398;&#25104;&#20687;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#23545;&#31216;&#30772;&#22351;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#23545;&#31216;&#30772;&#22351;&#30340;&#25968;&#23398;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15448v1 Announce Type: cross  Abstract: For nonlinear inverse problems that are prevalent in imaging science, symmetries in the forward model are common. When data-driven deep learning approaches are used to solve such problems, these intrinsic symmetries can cause substantial learning difficulties. In this paper, we explain how such difficulties arise and, more importantly, how to overcome them by preprocessing the training set before any learning, i.e., symmetry breaking. We take far-field phase retrieval (FFPR), which is central to many areas of scientific imaging, as an example and show that symmetric breaking can substantially improve data-driven learning. We also formulate the mathematical principle of symmetry breaking.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;LDA/HDP&#27169;&#22411;&#25552;&#21462;&#22810;&#35821;&#35328;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20027;&#39064;&#21160;&#24577;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#35821;&#35328;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.15445</link><description>&lt;p&gt;
&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25581;&#31034;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22810;&#35821;&#35328;&#20027;&#39064;&#21160;&#24577;&#21644;&#36235;&#21183;&#35782;&#21035;&#65306;LDA/HDP&#27169;&#22411;&#22686;&#24378;&#30340;&#26032;&#22411;&#25968;&#25454;&#32763;&#35793;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Decoding Multilingual Topic Dynamics and Trend Identification through ARIMA Time Series Analysis on Social Networks: A Novel Data Translation Framework Enhanced by LDA/HDP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;LDA/HDP&#27169;&#22411;&#25552;&#21462;&#22810;&#35821;&#35328;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20027;&#39064;&#21160;&#24577;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#35821;&#35328;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#30772;&#35793;&#22810;&#35821;&#35328;&#20027;&#39064;&#21160;&#24577;&#65292;&#24182;&#35782;&#21035;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#12290;&#25105;&#20204;&#20851;&#27880;&#31361;&#23612;&#26031;&#31038;&#20132;&#32593;&#32476;&#20013;&#22312;&#20896;&#29366;&#30149;&#27602;&#22823;&#27969;&#34892;&#26399;&#38388;&#20197;&#21450;&#20854;&#20182;&#26174;&#33879;&#20027;&#39064;&#65288;&#22914;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#20013;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#19982;&#36825;&#20123;&#20027;&#39064;&#30456;&#20851;&#30340;&#21508;&#31181;&#22810;&#35821;&#35328;&#35780;&#35770;&#36827;&#34892;&#32858;&#21512;&#12290;&#28982;&#21518;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#31934;&#28860;&#12290;&#25105;&#20204;&#24341;&#20837;&#25105;&#20204;&#30340;&#26080;&#33521;&#35821;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26469;&#22788;&#29702;&#35821;&#35328;&#24046;&#24322;&#12290;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#35777;&#27979;&#35797;&#26174;&#31034;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;F1&#20540;&#65292;&#31361;&#26174;&#20102;&#23427;&#36866;&#29992;&#20110;&#35821;&#35328;&#19968;&#33268;&#20219;&#21153;&#30340;&#29305;&#28857;&#12290;&#28145;&#20837;&#30740;&#31350;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;LDA&#21644;HDP&#27169;&#22411;&#65292;&#20174;&#32763;&#35793;&#20869;&#23481;&#20013;&#25552;&#21462;&#30456;&#20851;&#20027;&#39064;&#12290;&#36825;&#23548;&#33268;&#24212;&#29992;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26469;&#35299;&#30721;&#19981;&#26029;&#21464;&#21270;&#30340;&#20027;&#39064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15445v1 Announce Type: cross  Abstract: In this study, the authors present a novel methodology adept at decoding multilingual topic dynamics and identifying communication trends during crises. We focus on dialogues within Tunisian social networks during the Coronavirus Pandemic and other notable themes like sports and politics. We start by aggregating a varied multilingual corpus of comments relevant to these subjects. This dataset undergoes rigorous refinement during data preprocessing. We then introduce our No-English-to-English Machine Translation approach to handle linguistic differences. Empirical tests of this method showed high accuracy and F1 scores, highlighting its suitability for linguistically coherent tasks. Delving deeper, advanced modeling techniques, specifically LDA and HDP models are employed to extract pertinent topics from the translated content. This leads to applying ARIMA time series analysis to decode evolving topic trends. Applying our method to a mu
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#20154;&#31867;&#27963;&#21160;/&#21160;&#20316;&#35782;&#21035;&#20013;&#23454;&#29616;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#65292;&#25506;&#35752;&#20102;IMU&#25968;&#25454;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;HAR&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#35201;&#24615;&#25506;&#35752;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;HAR&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.15444</link><description>&lt;p&gt;
&#22522;&#20110;IMU&#30340;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#20154;&#31867;&#27963;&#21160;/&#21160;&#20316;&#35782;&#21035;&#20013;&#23454;&#29616;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#65292;&#25506;&#35752;&#20102;IMU&#25968;&#25454;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;HAR&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#35201;&#24615;&#25506;&#35752;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;HAR&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#27963;&#22312;&#19968;&#20010;&#22810;&#24863;&#30693;&#19990;&#30028;&#20013;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20173;&#28982;&#23616;&#38480;&#20110;&#23545;&#20154;&#20307;&#36816;&#21160;&#21644;&#34892;&#20026;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29702;&#35299;&#12290;&#20107;&#23454;&#19978;&#65292;&#23545;&#20154;&#31867;&#36816;&#21160;&#30340;&#23436;&#25972;&#24773;&#22659;&#24847;&#35782;&#26368;&#22909;&#26159;&#36890;&#36807;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#26469;&#29702;&#35299;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20154;&#31867;&#27963;&#21160;/&#21160;&#20316;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#20013;&#20256;&#36882;&#21644;&#21033;&#29992;&#30693;&#35782;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;IMU&#25968;&#25454;&#21450;&#20854;&#22312;&#36328;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#20197;&#21450;&#30740;&#31350;HAR&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#21644;&#25277;&#35937;&#24615;&#23558;HAR&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28982;&#21518;&#27604;&#36739;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;HAR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21306;&#20998;&#21644;&#35814;&#32454;&#38416;&#36848;&#20102;&#25991;&#29486;&#20013;&#35768;&#22810;&#30456;&#20851;&#20294;&#19981;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#65292;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#22495;&#33258;&#36866;&#24212;&#12289;&#34920;&#31034;&#23398;&#20064;&#12289;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#24182;&#25551;&#36848;&#20102;&#36328;&#27169;&#24577;&#23398;&#20064;&#22914;&#20309;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15444v1 Announce Type: cross  Abstract: Despite living in a multi-sensory world, most AI models are limited to textual and visual understanding of human motion and behavior. In fact, full situational awareness of human motion could best be understood through a combination of sensors. In this survey we investigate how knowledge can be transferred and utilized amongst modalities for Human Activity/Action Recognition (HAR), i.e. cross-modality transfer learning. We motivate the importance and potential of IMU data and its applicability in cross-modality learning as well as the importance of studying the HAR problem. We categorize HAR related tasks by time and abstractness and then compare various types of multimodal HAR datasets. We also distinguish and expound on many related but inconsistently used terms in the literature, such as transfer learning, domain adaptation, representation learning, sensor fusion, and multimodal learning, and describe how cross-modal learning fits w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15443</link><description>&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15443
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#36880;&#28176;&#24694;&#21270;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20027;&#35201;&#24433;&#21709;&#35760;&#24518;&#12289;&#24605;&#32500;&#21644;&#34892;&#20026;&#31561;&#35748;&#30693;&#21151;&#33021;&#12290;&#26412;&#30149;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38454;&#27573;&#65292;&#21363;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65292;&#38750;&#24120;&#37325;&#35201;&#23613;&#26089;&#35786;&#26029;&#65292;&#22240;&#20026;&#19968;&#20123;&#36880;&#28176;&#21457;&#23637;&#20026;&#30149;&#30151;&#30340;MCI&#24739;&#32773;&#20250;&#21457;&#23637;&#20026;&#36825;&#31181;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#32452;&#65306;&#25511;&#21046;&#27491;&#24120;&#65288;CN&#65289;&#12289;&#36880;&#28176;&#21457;&#23637;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;pMCI&#65289;&#12289;&#31283;&#23450;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;sMCI&#65289;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31867;&#26159;&#22522;&#20110;&#23545;&#20174;ADNI&#25968;&#25454;&#38598;&#33719;&#24471;&#30340;PET&#25195;&#25551;&#22270;&#20687;&#30340;&#24443;&#24213;&#26816;&#26597;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#30340;&#24443;&#24213;&#29702;&#35299;&#12290;&#24050;&#32463;&#20351;&#29992;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;VGG16&#12289;AlexNet&#21644;&#33258;&#23450;&#20041;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15443v1 Announce Type: cross  Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects cognitive functions such as memory, thinking, and behavior. In this disease, there is a critical phase, mild cognitive impairment, that is really important to be diagnosed early since some patients with progressive MCI will develop the disease. This study delves into the challenging task of classifying Alzheimer's disease into four distinct groups: control normal (CN), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). This classification is based on a thorough examination of PET scan images obtained from the ADNI dataset, which provides a thorough understanding of the disease's progression. Several deep-learning and traditional machine-learning models have been used to detect Alzheimer's disease. In this paper, three deep-learning models, namely VGG16 and AlexNet, and a custom Convolu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;Geometric Bayesian Flow Networks (GeoBFN)&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#30340;&#21487;&#24494;&#20998;&#21442;&#25968;&#31354;&#38388;&#20013;&#23545;&#19981;&#21516;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24615;&#21644;&#22122;&#22768;&#25935;&#24863;&#24615;&#30340;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#30340;&#33258;&#28982;&#25311;&#21512;&#12290;GeoBFN&#36890;&#36807;&#20248;&#21270;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;3D&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15441</link><description>&lt;p&gt;
&#32479;&#19968;&#29983;&#25104;&#24314;&#27169;&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#30340;3D&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Unified Generative Modeling of 3D Molecules via Bayesian Flow Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;Geometric Bayesian Flow Networks (GeoBFN)&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#30340;&#21487;&#24494;&#20998;&#21442;&#25968;&#31354;&#38388;&#20013;&#23545;&#19981;&#21516;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24615;&#21644;&#22122;&#22768;&#25935;&#24863;&#24615;&#30340;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#30340;&#33258;&#28982;&#25311;&#21512;&#12290;GeoBFN&#36890;&#36807;&#20248;&#21270;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;3D&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#28304;&#33258;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#31616;&#21270;&#36830;&#32493;&#24615;&#20551;&#35774;&#65292;&#23613;&#31649;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20998;&#23376;&#20960;&#20309;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#29305;&#24615;&#65292;&#24456;&#38590;&#30452;&#25509;&#24212;&#29992;&#20110;&#20960;&#20309;&#29983;&#25104;&#24212;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#20960;&#20309;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;GeoBFN&#65289;&#65292;&#36890;&#36807;&#22312;&#21487;&#24494;&#20998;&#21442;&#25968;&#31354;&#38388;&#20013;&#24314;&#27169;&#19981;&#21516;&#27169;&#24577;&#65292;&#33258;&#28982;&#22320;&#36866;&#37197;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;GeoBFN&#36890;&#36807;&#22312;&#20998;&#24067;&#21442;&#25968;&#19978;&#21512;&#25104;&#31227;&#21464;&#30456;&#20114;&#20381;&#36182;&#24314;&#27169;&#65292;&#20445;&#25345;&#20102;SE-(3)&#19981;&#21464;&#23494;&#24230;&#24314;&#27169;&#23646;&#24615;&#65292;&#24182;&#32479;&#19968;&#20102;&#19981;&#21516;&#27169;&#24577;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#36890;&#36807;&#20248;&#21270;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeoBFN&#22312;&#22810;&#20010;3D&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#65288;&#22312;QM9&#20013;&#31283;&#23450;&#29983;&#25104;90.87%&#30340;&#20998;&#23376;&#65292;&#22312;&#21407;&#23376;&#26041;&#38754;&#31283;&#23450;&#29983;&#25104;85.6%&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15441v1 Announce Type: cross  Abstract: Advanced generative model (e.g., diffusion model) derived from simplified continuity assumptions of data distribution, though showing promising progress, has been difficult to apply directly to geometry generation applications due to the multi-modality and noise-sensitive nature of molecule geometry. This work introduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits molecule geometry by modeling diverse modalities in the differentiable parameter space of distributions. GeoBFN maintains the SE-(3) invariant density modeling property by incorporating equivariant inter-dependency modeling on parameters of distributions and unifying the probabilistic modeling of different modalities. Through optimized training and sampling techniques, we demonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D molecule generation benchmarks in terms of generation quality (90.87% molecule stability in QM9 and 85.6% atom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#21098;&#19982;&#24674;&#22797;&#30340;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#27493;&#23398;&#20064;&#31639;&#27861;&#21644;&#20462;&#21098;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#22312;&#24322;&#26500;&#35774;&#22791;&#22330;&#26223;&#20013;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15439</link><description>&lt;p&gt;
&#22522;&#20110;&#20462;&#21098;&#19982;&#24674;&#22797;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning based on Pruning and Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15439
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#21098;&#19982;&#24674;&#22797;&#30340;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#27493;&#23398;&#20064;&#31639;&#27861;&#21644;&#20462;&#21098;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#22312;&#24322;&#26500;&#35774;&#22791;&#22330;&#26223;&#20013;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#24322;&#26500;&#29615;&#22659;&#30340;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29616;&#23454;&#29615;&#22659;&#20013;&#23458;&#25143;&#31471;&#30340;&#32593;&#32476;&#36895;&#24230;&#22810;&#26679;&#24615;&#12290;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#24322;&#27493;&#23398;&#20064;&#31639;&#27861;&#21644;&#20462;&#21098;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#28041;&#21450;&#24322;&#26500;&#35774;&#22791;&#30340;&#22330;&#26223;&#20013;&#30340;&#20302;&#25928;&#38382;&#39064;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24322;&#27493;&#31639;&#27861;&#20013;&#26576;&#20123;&#23458;&#25143;&#31471;&#35757;&#32451;&#19981;&#36275;&#30340;&#28382;&#21518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#37327;&#24674;&#22797;&#65292;&#35813;&#26694;&#26550;&#21152;&#24555;&#20102;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#36807;&#31243;&#30340;&#20248;&#21270;&#65292;&#21253;&#25324;&#32531;&#20914;&#26426;&#21046;&#65292;&#20351;&#24471;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#33021;&#22815;&#31867;&#20284;&#20110;&#21516;&#27493;&#23398;&#20064;&#36827;&#34892;&#25805;&#20316;&#12290;&#21478;&#22806;&#65292;&#26381;&#21153;&#22120;&#21521;&#23458;&#25143;&#31471;&#20256;&#36755;&#20840;&#23616;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#20063;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15439v1 Announce Type: new  Abstract: A novel federated learning training framework for heterogeneous environments is presented, taking into account the diverse network speeds of clients in realistic settings. This framework integrates asynchronous learning algorithms and pruning techniques, effectively addressing the inefficiencies of traditional federated learning algorithms in scenarios involving heterogeneous devices, as well as tackling the staleness issue and inadequate training of certain clients in asynchronous algorithms. Through the incremental restoration of model size during training, the framework expedites model training while preserving model accuracy. Furthermore, enhancements to the federated learning aggregation process are introduced, incorporating a buffering mechanism to enable asynchronous federated learning to operate akin to synchronous learning. Additionally, optimizations in the process of the server transmitting the global model to clients reduce c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;BCI&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#65292;&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#31163;&#32447;&#24615;&#33021;&#27700;&#24179;&#65292;&#21516;&#26102;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#35266;&#23519;&#22522;&#30784;&#19978;&#23454;&#29616;&#25968;&#25454;&#30340;&#25345;&#32493;&#37325;&#26032;&#23545;&#40784;&#65292;&#23637;&#31034;&#20102;&#22312;&#36328;&#20027;&#20307;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#23454;&#39564;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.15438</link><description>&lt;p&gt;
BCI&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30340;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Adaptive Deep Learning Method For BCI Motor Imagery Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15438
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;BCI&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#65292;&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#31163;&#32447;&#24615;&#33021;&#27700;&#24179;&#65292;&#21516;&#26102;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#35266;&#23519;&#22522;&#30784;&#19978;&#23454;&#29616;&#25968;&#25454;&#30340;&#25345;&#32493;&#37325;&#26032;&#23545;&#40784;&#65292;&#23637;&#31034;&#20102;&#22312;&#36328;&#20027;&#20307;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32447;&#20351;&#29992;&#32780;&#19981;&#38656;&#35201;&#30417;&#30563;&#65292;&#21516;&#26102;&#36798;&#21040;&#31163;&#32447;&#24615;&#33021;&#27700;&#24179;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#65292;&#21516;&#26102;&#22522;&#20110;&#23454;&#26102;&#35266;&#23519;&#25345;&#32493;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#23545;&#40784;&#65292;&#26080;&#35770;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#36824;&#26159;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#32771;&#34385;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#20027;&#20307;&#22330;&#26223;&#19979;&#65292;&#20174;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#23545;&#36816;&#21160;&#24819;&#35937;&#36827;&#34892;&#35299;&#30721;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#21487;&#37325;&#29616;&#24615;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15438v1 Announce Type: cross  Abstract: In the context of Brain-Computer Interfaces, we propose an adaptive method that reaches offline performance level while being usable online without requiring supervision. Interestingly, our method does not require retraining the model, as it consists in using a frozen efficient deep learning backbone while continuously realigning data, both at input and latent spaces, based on streaming observations. We demonstrate its efficiency for Motor Imagery brain decoding from electroencephalography data, considering challenging cross-subject scenarios. For reproducibility, we share the code of our experiments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#24050;&#30693;&#34920;&#36798;&#24335;&#21644;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#24515;&#33039;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#29289;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#20998;&#31163;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.15433</link><description>&lt;p&gt;
HyPer-EP: &#20026;&#24515;&#33039;&#30005;&#29983;&#29702;&#32780;&#20803;&#23398;&#20064;&#30340;&#28151;&#21512;&#20010;&#24615;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HyPer-EP: Meta-Learning Hybrid Personalized Models for Cardiac Electrophysiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#24050;&#30693;&#34920;&#36798;&#24335;&#21644;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#24515;&#33039;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#29289;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#20998;&#31163;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#34394;&#25311;&#24515;&#33039;&#27169;&#22411;&#22312;&#20020;&#24202;&#19978;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#28508;&#21147;&#65292;&#23613;&#31649;&#22312;&#32473;&#23450;&#24739;&#32773;&#29305;&#23450;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20854;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#25104;&#26412;&#39640;&#26114;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#38169;&#35823;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#31616;&#21270;&#21644;&#20551;&#35774;&#25152;&#36896;&#25104;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#25968;&#25454;&#30417;&#30563;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;&#20010;&#24615;&#21270;&#24515;&#33039;&#25968;&#23383;&#23402;&#29983;&#25551;&#36848;&#20026;&#22522;&#20110;&#29289;&#29702;&#24050;&#30693;&#34920;&#36798;&#24335;&#21644;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26410;&#30693;&#19982;&#29616;&#23454;&#20043;&#38388;&#24046;&#36317;&#30340;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#28151;&#21512;&#27169;&#22411;&#20013;&#22522;&#20110;&#29289;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#20998;&#31163;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15433v1 Announce Type: cross  Abstract: Personalized virtual heart models have demonstrated increasing potential for clinical use, although the estimation of their parameters given patient-specific data remain a challenge. Traditional physics-based modeling approaches are computationally costly and often neglect the inherent structural errors in these models due to model simplifications and assumptions. Modern deep learning approaches, on the other hand, rely heavily on data supervision and lacks interpretability. In this paper, we present a novel hybrid modeling framework to describe a personalized cardiac digital twin as a combination of a physics-based known expression augmented by neural network modeling of its unknown gap to reality. We then present a novel meta-learning framework to enable the separate identification of both the physics-based and neural components in the hybrid model. We demonstrate the feasibility and generality of this hybrid modeling framework with 
&lt;/p&gt;</description></item><item><title>BRIEDGE&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#21644;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#23454;&#29616;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Informer&#30340;ProbSparse&#33258;&#27880;&#24847;&#26426;&#21046;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15432</link><description>&lt;p&gt;
BRIEDGE: EEG&#33258;&#36866;&#24212;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
BRIEDGE: EEG-Adaptive Edge AI for Multi-Brain to Multi-Robot Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15432
&lt;/p&gt;
&lt;p&gt;
BRIEDGE&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#21644;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#23454;&#29616;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Informer&#30340;ProbSparse&#33258;&#27880;&#24847;&#26426;&#21046;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817; EEG &#22522;&#20110;&#33041;&#26426;&#25509;&#21475;&#25216;&#26415;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#36890;&#36807;&#25972;&#21512;&#20256;&#24863;&#12289;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#25511;&#21046;&#23454;&#29616;&#33041;&#21040;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; BRIEDGE &#20316;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#21644;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#23454;&#29616;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;&#65292;&#22914;&#22270;1&#25152;&#31034;&#12290;&#27491;&#22914;&#25152;&#25551;&#32472;&#30340;&#37027;&#26679;&#65292;&#36793;&#32536;&#31227;&#21160;&#26381;&#21153;&#22120;&#25110;&#36793;&#32536;&#20415;&#25658;&#26381;&#21153;&#22120;&#23558;&#20174;&#29992;&#25143;&#25910;&#38598; EEG &#25968;&#25454;&#65292;&#24182;&#21033;&#29992; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#28982;&#21518;&#23545; EEG &#22522;&#30784;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#25968;&#25454;&#20256;&#36755;&#36807;&#31243;&#20013;&#35299;&#30721;&#25104;&#21629;&#20196;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25552;&#21462;&#24322;&#36136; EEG &#25968;&#25454;&#30340;&#32852;&#21512;&#29305;&#24449;&#20197;&#21450;&#22686;&#24378;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;BRIEDGE &#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110; Informer &#30340; ProbSparse &#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#21516;&#26102;&#65292;&#24179;&#34892;&#21644;&#23433;&#20840;&#30340;&#36827;&#34892;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15432v1 Announce Type: cross  Abstract: Recent advances in EEG-based BCI technologies have revealed the potential of brain-to-robot collaboration through the integration of sensing, computing, communication, and control. In this paper, we present BRIEDGE as an end-to-end system for multi-brain to multi-robot interaction through an EEG-adaptive neural network and an encoding-decoding communication framework, as illustrated in Fig.1. As depicted, the edge mobile server or edge portable server will collect EEG data from the users and utilize the EEG-adaptive neural network to identify the users' intentions. The encoding-decoding communication framework then encodes the EEG-based semantic information and decodes it into commands in the process of data transmission. To better extract the joint features of heterogeneous EEG data as well as enhance classification accuracy, BRIEDGE introduces an informer-based ProbSparse self-attention mechanism. Meanwhile, parallel and secure trans
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26631;&#20934;&#26657;&#20934;&#20250;&#35805;&#21644;&#22522;&#20110;EMG&#30340;&#26032;BCI&#25511;&#21046;&#20250;&#35805;&#65292;&#35266;&#23519;&#21040;&#20102;&#24863;&#35273;&#36816;&#21160;&#33410;&#24459;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#33539;&#20363;&#24341;&#20837;&#30340;&#39069;&#22806;&#20934;&#22791;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15431</link><description>&lt;p&gt;
&#23558;BCI&#27169;&#22411;&#20174;&#26657;&#20934;&#36716;&#31227;&#21040;&#25511;&#21046;&#65306;&#35266;&#23519;EEG&#29305;&#24449;&#20013;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Transferring BCI models from calibration to control: Observing shifts in EEG features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15431
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26631;&#20934;&#26657;&#20934;&#20250;&#35805;&#21644;&#22522;&#20110;EMG&#30340;&#26032;BCI&#25511;&#21046;&#20250;&#35805;&#65292;&#35266;&#23519;&#21040;&#20102;&#24863;&#35273;&#36816;&#21160;&#33410;&#24459;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#33539;&#20363;&#24341;&#20837;&#30340;&#39069;&#22806;&#20934;&#22791;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCI&#65289;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24320;&#21457;&#36234;&#26469;&#36234;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#36981;&#24490;&#31163;&#25955;&#33539;&#20363;&#65292;&#22312;&#36825;&#20123;&#33539;&#20363;&#20013;&#65292;&#21442;&#19982;&#32773;&#23450;&#26399;&#25191;&#34892;&#36816;&#21160;&#24819;&#35937;&#12290;&#24403;&#29992;&#25143;&#23581;&#35797;&#20351;&#29992;&#36825;&#26679;&#30340;BCI&#25191;&#34892;&#25511;&#21046;&#20219;&#21153;&#26102;&#65292;EEG&#27169;&#24335;&#21487;&#33021;&#20250;&#21457;&#29983;&#20160;&#20040;&#21464;&#21270;&#36890;&#24120;&#19981;&#26126;&#30830;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#27867;&#21270;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21253;&#21547;&#26631;&#20934;&#26657;&#20934;&#20250;&#35805;&#21644;&#22522;&#20110;EMG&#30340;&#26032;BCI&#25511;&#21046;&#20250;&#35805;&#30340;&#26032;&#33539;&#20363;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35266;&#23519;&#21040;&#24863;&#35273;&#36816;&#21160;&#33410;&#24459;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#35266;&#23519;&#21040;&#25511;&#21046;&#33539;&#20363;&#24341;&#20837;&#30340;&#39069;&#22806;&#20934;&#22791;&#25928;&#26524;&#12290;&#22312;&#36816;&#21160;&#30456;&#20851;&#30382;&#23618;&#30005;&#20301;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26657;&#20934;&#19982;&#25511;&#21046;&#20250;&#35805;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;CSP&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#22312;&#26657;&#20934;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#23545;BCI&#25511;&#21046;&#39550;&#39542;&#25968;&#25454;&#36827;&#34892;&#20986;&#20154;&#24847;&#26009;&#22320;&#33391;&#22909;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15431v1 Announce Type: cross  Abstract: Public Motor Imagery-based brain-computer interface (BCI) datasets are being used to develop increasingly good classifiers. However, they usually follow discrete paradigms where participants perform Motor Imagery at regularly timed intervals. It is often unclear what changes may happen in the EEG patterns when users attempt to perform a control task with such a BCI. This may lead to generalisation errors. We demonstrate a new paradigm containing a standard calibration session and a novel BCI control session based on EMG. This allows us to observe similarities in sensorimotor rhythms, and observe the additional preparation effects introduced by the control paradigm. In the Movement Related Cortical Potentials we found large differences between the calibration and control sessions. We demonstrate a CSP-based Machine Learning model trained on the calibration data that can make surprisingly good predictions on the BCI-controlled driving da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.15426</link><description>&lt;p&gt;
&#25945;&#32946;&#29615;&#22659;&#19979;&#38598;&#25104;&#24378;&#20808;&#39564;&#27169;&#22359;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#30340;&#19977;&#38454;&#27573;SFT&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15426
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20808;&#39564;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#35777;&#26126;&#27604;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#26356;&#26377;&#31454;&#20105;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22120;&#21644;&#37325;&#21472;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20581;&#22766;&#30340;&#20998;&#31867;&#65292;&#23558;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#20998;&#19977;&#25209;&#27880;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;LORA&#24494;&#35843;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#27169;&#22359;&#65292;&#23558;&#31995;&#32479;&#25552;&#31034;&#12289;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#20219;&#21153;&#20998;&#21106;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#23545;&#22522;&#20110;&#20808;&#39564;&#30340;&#24494;&#35843;&#27169;&#22411;&#24212;&#29992;&#20102;&#21387;&#32553;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#38543;&#21518;&#22312;&#36755;&#20986;&#31471;&#36827;&#34892;&#25991;&#26412;&#36807;&#28388;&#20197;&#33719;&#24471;&#22686;&#37327;&#24341;&#23548;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20195;&#34920;&#20102;&#30495;&#27491;&#20197;&#20016;&#23500;&#30340;&#25945;&#32946;&#30693;&#35782;&#12289;&#20998;&#27493;&#25351;&#23548;&#30340;&#29305;&#28857;&#20307;&#29616;&#23548;&#24072;&#35282;&#33394;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15426v1 Announce Type: cross  Abstract: In this paper, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Deep Temporal State Domain Adaptation&#65288;DTSDA&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#34892;&#20026;&#21464;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15424</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#26102;&#38388;&#20851;&#31995;&#20449;&#24687;&#30340;&#28145;&#24230;&#39046;&#22495;&#33258;&#36866;&#24212;&#36827;&#34892;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-user activity recognition using deep domain adaptation with temporal relation information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Deep Temporal State Domain Adaptation&#65288;DTSDA&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#34892;&#20026;&#21464;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#26222;&#36866;&#35745;&#31639;&#30340;&#22522;&#30707;&#65292;&#20855;&#26377;&#22312;&#20581;&#24247;&#30417;&#27979;&#21644;&#29615;&#22659;&#36741;&#21161;&#29983;&#27963;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#36816;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#20013;&#65292;&#36825;&#19968;&#20551;&#35774;&#22240;&#20998;&#24067;&#20043;&#22806;&#30340;&#25361;&#25112;&#32780;&#26080;&#25928;&#65292;&#21253;&#25324;&#26469;&#33258;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#24046;&#24322;&#12289;&#38543;&#26102;&#38388;&#21464;&#21270;&#20197;&#21450;&#20010;&#20307;&#34892;&#20026;&#21464;&#24322;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#21518;&#32773;&#65292;&#25506;&#32034;&#36328;&#29992;&#25143;HAR&#38382;&#39064;&#65292;&#20854;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#34892;&#20026;&#21464;&#24322;&#23548;&#33268;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Temporal State Domain Adaptation&#65288;DTSDA&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#36328;&#29992;&#25143;HAR&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#37327;&#36523;&#23450;&#21046;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15424v1 Announce Type: cross  Abstract: Human Activity Recognition (HAR) is a cornerstone of ubiquitous computing, with promising applications in diverse fields such as health monitoring and ambient assisted living. Despite significant advancements, sensor-based HAR methods often operate under the assumption that training and testing data have identical distributions. However, in many real-world scenarios, particularly in sensor-based HAR, this assumption is invalidated by out-of-distribution ($\displaystyle o.o.d.$) challenges, including differences from heterogeneous sensors, change over time, and individual behavioural variability. This paper centres on the latter, exploring the cross-user HAR problem where behavioural variability across individuals results in differing data distributions. To address this challenge, we introduce the Deep Temporal State Domain Adaptation (DTSDA) model, an innovative approach tailored for time series domain adaptation in cross-user HAR. Con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20851;&#31995;&#26368;&#20248;&#36755;&#36816;&#30340;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;i.i.d.&#20551;&#35774;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2403.15423</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#20851;&#31995;&#26368;&#20248;&#36755;&#36816;&#30340;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-user activity recognition via temporal relation optimal transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20851;&#31995;&#26368;&#20248;&#36755;&#36816;&#30340;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;i.i.d.&#20551;&#35774;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#27867;&#21270;&#27169;&#22411;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#26377;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#19968;&#20551;&#35774;&#19981;&#25104;&#31435;&#65292;&#25910;&#38598;&#30340;&#35757;&#32451;&#21644;&#30446;&#26631;&#27979;&#35797;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#22343;&#21248;&#20998;&#24067;&#65292;&#22914;&#36328;&#29992;&#25143;HAR&#30340;&#24773;&#20917;&#12290;&#22495;&#33258;&#36866;&#24212;&#26159;&#36328;&#29992;&#25143;HAR&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22495;&#33258;&#36866;&#24212;&#30340;&#24037;&#20316;&#22522;&#20110;&#27599;&#20010;&#22495;&#20013;&#30340;&#26679;&#26412;&#26159;i.i.d.&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#20197;&#23545;&#40784;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#19968;&#20851;&#20110;i.i.d.&#30340;&#24378;&#20551;&#35774;&#23545;&#20110;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#21487;&#33021;&#19981;&#22826;&#36866;&#29992;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#32454;&#20998;&#21644;&#29305;&#24449;&#24418;&#25104;&#30340;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15423v1 Announce Type: cross  Abstract: Current research on human activity recognition (HAR) mainly assumes that training and testing data are drawn from the same distribution to achieve a generalised model, which means all the data are considered to be independent and identically distributed $\displaystyle (i.i.d.) $. In many real-world applications, this assumption does not hold, and collected training and target testing datasets have non-uniform distribution, such as in the case of cross-user HAR. Domain adaptation is a promising approach for cross-user HAR tasks. Existing domain adaptation works based on the assumption that samples in each domain are $\displaystyle i.i.d. $ and do not consider the knowledge of temporal relation hidden in time series data for aligning data distribution. This strong assumption of $\displaystyle i.i.d. $ may not be suitable for time series-related domain adaptation methods because the samples formed by time series segmentation and feature e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#25913;&#21892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#26356;&#24555;&#22320;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15422</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;--&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Sensor-based Human Activity Recognition with Data Heterogeneity -- A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15422
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#25913;&#21892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#26356;&#24555;&#22320;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15422v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#22312;&#26222;&#36866;&#35745;&#31639;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#22810;&#32500;&#35266;&#23519;&#20998;&#26512;&#34892;&#20026;&#12290; &#23613;&#31649;&#30740;&#31350;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;HAR&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#26041;&#38754;&#12290; &#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#21508;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#20855;&#26377;&#22343;&#21248;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#19982;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#20154;&#31867;&#27963;&#21160;&#20013;&#30340;&#22810;&#26679;&#24615;&#30456;&#30683;&#30462;&#12290; &#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#26377;&#21161;&#20110;&#24320;&#21457;&#20010;&#24615;&#21270;&#12289;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#20943;&#23569;&#26631;&#27880;&#25968;&#25454;&#12290; &#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#22788;&#29702;HAR&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12289;&#24212;&#29992;&#30456;&#24212;&#30340;&#36866;&#24403;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#24635;&#32467;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15422v1 Announce Type: cross  Abstract: Sensor-based Human Activity Recognition (HAR) is crucial in ubiquitous computing, analysing behaviours through multi-dimensional observations. Despite research progress, HAR confronts challenges, particularly in data distribution assumptions. Most studies often assume uniform data distributions across datasets, contrasting with the varied nature of practical sensor data in human activities. Addressing data heterogeneity issues can improve performance, reduce computational costs, and aid in developing personalized, adaptive models with less annotated data. This review investigates how machine learning addresses data heterogeneity in HAR, by categorizing data heterogeneity types, applying corresponding suitable machine learning methods, summarizing available datasets, and discussing future challenges.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#25935;&#25463;&#35823;&#24046;&#26657;&#27491;&#65292;&#25552;&#21319;&#20102;&#20256;&#32479;&#25163;&#21183;&#35782;&#21035;&#27169;&#22411;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.15421</link><description>&lt;p&gt;
&#38754;&#21521;&#20302;&#21151;&#32791;&#24212;&#29992;&#30340;&#25935;&#25463;&#25163;&#21183;&#35782;&#21035;&#65306;&#36890;&#29992;&#24615;&#23450;&#21046;&#21270;
&lt;/p&gt;
&lt;p&gt;
Agile gesture recognition for low-power applications: customisation for generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15421
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#25935;&#25463;&#35823;&#24046;&#26657;&#27491;&#65292;&#25552;&#21319;&#20102;&#20256;&#32479;&#25163;&#21183;&#35782;&#21035;&#27169;&#22411;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#21183;&#35782;&#21035;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#28966;&#28857;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#25163;&#37096;&#22270;&#20687;&#36830;&#32493;&#27969;&#30340;&#35775;&#38382;&#24773;&#22659;&#19978;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#20302;&#21151;&#32791;&#20256;&#24863;&#22120;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#25163;&#21183;&#35782;&#21035;&#25216;&#26415;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#24335;&#35782;&#21035;&#31995;&#32479;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21644;&#25935;&#25463;&#35823;&#24046;&#26657;&#27491;&#65292;&#26088;&#22312;&#25552;&#39640;&#20256;&#32479;&#25163;&#21183;&#35782;&#21035;&#27169;&#22411;&#22312;&#24102;&#26377;li&#30340;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15421v1 Announce Type: cross  Abstract: Automated hand gesture recognition has long been a focal point in the AI community. Traditionally, research in this field has predominantly focused on scenarios with access to a continuous flow of hand's images. This focus has been driven by the widespread use of cameras and the abundant availability of image data. However, there is an increasing demand for gesture recognition technologies that operate on low-power sensor devices. This is due to the rising concerns for data leakage and end-user privacy, as well as the limited battery capacity and the computing power in low-cost devices. Moreover, the challenge in data collection for individually designed hardware also hinders the generalisation of a gesture recognition model.   In this study, we unveil a novel methodology for pattern recognition systems using adaptive and agile error correction, designed to enhance the performance of legacy gesture recognition models on devices with li
&lt;/p&gt;</description></item><item><title>GKEDM&#25554;&#20214;&#27169;&#22359;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#21644;&#32858;&#21512;&#22270;&#20449;&#24687;&#65292;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#24182;&#25552;&#39640;GCN&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#30693;&#35782;&#33976;&#39311;&#30340;&#36741;&#21161;&#36716;&#31227;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.15419</link><description>&lt;p&gt;
&#20165;&#38656;&#27880;&#24847;&#21147;&#21363;&#21487;&#25552;&#21319;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Attention is all you need for boosting graph convolutional neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15419
&lt;/p&gt;
&lt;p&gt;
GKEDM&#25554;&#20214;&#27169;&#22359;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#21644;&#32858;&#21512;&#22270;&#20449;&#24687;&#65292;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#24182;&#25552;&#39640;GCN&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#30693;&#35782;&#33976;&#39311;&#30340;&#36741;&#21161;&#36716;&#31227;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#22788;&#29702;&#38750;&#32593;&#26684;&#22495;&#30340;&#22270;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#22270;&#20013;&#30340;&#25299;&#25169;&#36923;&#36753;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#34701;&#21512;&#21040;&#33410;&#28857;&#30340;&#26368;&#32456;&#34920;&#31034;&#20013;&#12290;GCNs&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#34507;&#30333;&#36136;&#20998;&#23376;&#32467;&#26500;&#31561;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#30693;&#35782;&#22686;&#24378;&#21644;&#33976;&#39311;&#27169;&#22359;&#65288;GKEDM&#65289;&#30340;&#25554;&#20214;&#27169;&#22359;&#12290;GKEDM&#21487;&#20197;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#21644;&#32858;&#21512;&#22270;&#20449;&#24687;&#65292;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#24182;&#25552;&#39640;GCN&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;GKEDM&#36824;&#21487;&#20197;&#20316;&#20026;&#30693;&#35782;&#33976;&#39311;&#30340;&#36741;&#21161;&#36716;&#31227;&#22120;&#12290;&#36890;&#36807;&#19968;&#31181;&#29305;&#27530;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#33976;&#39311;&#26041;&#27861;&#65292;GKEDM&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15419v1 Announce Type: new  Abstract: Graph Convolutional Neural Networks (GCNs) possess strong capabilities for processing graph data in non-grid domains. They can capture the topological logical structure and node features in graphs and integrate them into nodes' final representations. GCNs have been extensively studied in various fields, such as recommendation systems, social networks, and protein molecular structures. With the increasing application of graph neural networks, research has focused on improving their performance while compressing their size. In this work, a plug-in module named Graph Knowledge Enhancement and Distillation Module (GKEDM) is proposed. GKEDM can enhance node representations and improve the performance of GCNs by extracting and aggregating graph information via multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary transferor for knowledge distillation. With a specially designed attention distillation method, GKEDM can dis
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#20855;&#26377;&#26368;&#20339;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15417</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#25216;&#26415;&#22686;&#24378;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Automatic Modulation Recognition for IoT Applications Using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15417
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#20855;&#26377;&#26368;&#20339;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;(AMR)&#23545;&#20110;&#30830;&#23450;&#20256;&#20837;&#20449;&#21495;&#30340;&#35843;&#21046;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#22788;&#29702;&#21644;&#26368;&#23567;&#36164;&#28304;&#20351;&#29992;&#65292;&#36825;&#23545;&#29289;&#32852;&#32593;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#32593;&#32476;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15417v1 Announce Type: cross  Abstract: Automatic modulation recognition (AMR) is critical for determining the modulation type of incoming signals. Integrating advanced deep learning approaches enables rapid processing and minimal resource usage, essential for IoT applications. We have introduced a novel method using Transformer networks for efficient AMR, designed specifically to address the constraints on model size prevalent in IoT environments. Our extensive experiments reveal that our proposed method outperformed advanced deep learning techniques, achieving the highest recognition accuracy.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#21152;&#36895;&#25910;&#25947;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#23545;&#35282;Hessian&#30697;&#38453;&#65292;&#24182;&#24212;&#29992;&#27169;&#31946;&#25512;&#29702;&#20110;&#22810;&#20010;&#36229;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15416</link><description>&lt;p&gt;
&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#27169;&#31946;&#36229;&#21442;&#25968;&#26356;&#26032;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fuzzy hyperparameters update in a second order optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15416
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#21152;&#36895;&#25910;&#25947;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#23545;&#35282;Hessian&#30697;&#38453;&#65292;&#24182;&#24212;&#29992;&#27169;&#31946;&#25512;&#29702;&#20110;&#22810;&#20010;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25552;&#20986;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#20108;&#38454;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23558;&#20171;&#32461;&#23545;&#35282;Hessian&#30697;&#38453;&#30340;&#22312;&#32447;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#65292;&#20197;&#21450;&#23545;&#20960;&#20010;&#36229;&#21442;&#25968;&#36827;&#34892;&#27169;&#31946;&#25512;&#29702;&#12290;&#24050;&#21462;&#24471;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15416v1 Announce Type: cross  Abstract: This research will present a hybrid approach to accelerate convergence in a second order optimization. An online finite difference approximation of the diagonal Hessian matrix will be introduced, along with fuzzy inferencing of several hyperparameters. Competitive results have been achieved
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#40654;&#26364;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#19981;&#21516;EEG&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#22312;&#33041;&#26426;&#25509;&#21475;&#20219;&#21153;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15415</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#30340;&#40654;&#26364;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#24322;&#26500;&#33041;&#30005;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Physics-informed and Unsupervised Riemannian Domain Adaptation for Machine Learning on Heterogeneous EEG Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15415
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#40654;&#26364;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#19981;&#21516;EEG&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#22312;&#33041;&#26426;&#25509;&#21475;&#20219;&#21153;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20250;&#35805;&#12289;&#21463;&#35797;&#32773;&#21644;&#35774;&#22791;&#30340;&#21464;&#24322;&#24615;&#65292;&#23558;&#33041;&#30005;&#22270; (EEG) &#25968;&#25454;&#38598;&#29992;&#20110;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064; (ML) &#20855;&#26377;&#25361;&#25112;&#24615;&#12290;ML&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#65292;&#30001;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20256;&#24863;&#22120;&#25968;&#37327;&#21644;&#20301;&#32622;&#30340;&#21464;&#21270;&#32780;&#22797;&#26434;&#21270;&#20102;&#20998;&#26512;&#12290;&#31616;&#21333;&#30340;&#36890;&#36947;&#36873;&#25321;&#20250;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22312;&#20849;&#20139;&#23569;&#37327;&#36890;&#36947;&#30340;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;EEG&#20449;&#21495;&#29289;&#29702;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22330;&#25554;&#20540;&#23558;EEG&#36890;&#36947;&#26144;&#23556;&#21040;&#22266;&#23450;&#20301;&#32622;&#65292;&#20419;&#36827;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#20998;&#31867;&#27969;&#31243;&#21644;&#36801;&#31227;&#23398;&#20064;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33041;&#26426;&#25509;&#21475; (BCI) &#20219;&#21153;&#21644;&#28508;&#22312;&#29983;&#29289;&#26631;&#24535;&#29289;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#19982;&#19968;&#31181;&#31216;&#20026;&#36229;&#36234;&#32500;&#24230;&#30340;&#22522;&#20110;&#32479;&#35745;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;&#30340;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15415v1 Announce Type: cross  Abstract: Combining electroencephalogram (EEG) datasets for supervised machine learning (ML) is challenging due to session, subject, and device variability. ML algorithms typically require identical features at train and test time, complicating analysis due to varying sensor numbers and positions across datasets. Simple channel selection discards valuable data, leading to poorer performance, especially with datasets sharing few channels. To address this, we propose an unsupervised approach leveraging EEG signal physics. We map EEG channels to fixed positions using field interpolation, facilitating source-free domain adaptation. Leveraging Riemannian geometry classification pipelines and transfer learning steps, our method demonstrates robust performance in brain-computer interface (BCI) tasks and potential biomarker applications. Comparative analysis against a statistical-based approach known as Dimensionality Transcending, a signal-based imputa
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#32806;&#21512;&#21457;&#29983;&#22120;&#20998;&#35299;&#30340;&#27010;&#24565;&#65292;&#25512;&#24191;&#20102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;SPCA&#65289;&#65292;&#24182;&#36890;&#36807;&#33041;&#30005;&#22270;&#21644;&#33041;&#30913;&#22270;&#25968;&#25454;&#30340;&#34701;&#21512;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#22312;&#35782;&#21035;&#20849;&#21516;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15409</link><description>&lt;p&gt;
&#32806;&#21512;&#21457;&#29983;&#22120;&#20998;&#35299;&#29992;&#20110;&#34701;&#21512;&#33041;&#30005;&#22270;&#21644;&#33041;&#30913;&#22270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Coupled generator decomposition for fusion of electro- and magnetoencephalography data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15409
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#32806;&#21512;&#21457;&#29983;&#22120;&#20998;&#35299;&#30340;&#27010;&#24565;&#65292;&#25512;&#24191;&#20102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;SPCA&#65289;&#65292;&#24182;&#36890;&#36807;&#33041;&#30005;&#22270;&#21644;&#33041;&#30913;&#22270;&#25968;&#25454;&#30340;&#34701;&#21512;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#22312;&#35782;&#21035;&#20849;&#21516;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34701;&#21512;&#24314;&#27169;&#21487;&#20197;&#35782;&#21035;&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#21516;&#26102;&#32771;&#34385;&#28304;&#29305;&#24322;&#24615;&#21464;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;\textit{&#32806;&#21512;&#21457;&#29983;&#22120;&#20998;&#35299;}&#30340;&#27010;&#24565;&#65292;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#25512;&#24191;&#20102;&#25968;&#25454;&#34701;&#21512;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;SPCA&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21463;&#35797;&#32773;&#12289;&#22810;&#27169;&#24577;&#65288;&#33041;&#30005;&#22270;&#21644;&#33041;&#30913;&#22270;&#65288;EEG&#21644;MEG&#65289;&#65289;&#31070;&#32463;&#24433;&#20687;&#23454;&#39564;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#35782;&#21035;&#23545;&#38754;&#37096;&#30693;&#35273;&#21050;&#28608;&#30340;&#20849;&#21516;&#29305;&#24449;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#27169;&#24577;&#21644;&#21463;&#35797;&#32773;&#29305;&#24322;&#24615;&#21464;&#24322;&#12290;&#36890;&#36807;&#33041;&#30005;&#22270;/&#33041;&#30913;&#22270;&#35797;&#39564;&#30340;&#25286;&#20998;&#20132;&#21449;&#39564;&#35777;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22797;&#26434;&#24615;&#27169;&#22411;&#30340;&#26368;&#20339;&#27169;&#22411;&#39034;&#24207;&#21644;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#23558;&#20854;&#19982;&#20551;&#35774;&#23545;&#21050;&#28608;&#26377;&#20849;&#20139;&#33041;&#21453;&#24212;&#30340;&#32676;&#20307;&#27700;&#24179;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;&#65292;&#19982;&#28151;&#20081;&#30340;&#38754;&#23380;&#30456;&#27604;&#65292;&#32422;$\sim170ms$&#39070;&#19979;&#22238;&#38754;&#21306;&#28608;&#27963;&#21457;&#29983;&#20102;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15409v1 Announce Type: cross  Abstract: Data fusion modeling can identify common features across diverse data sources while accounting for source-specific variability. Here we introduce the concept of a \textit{coupled generator decomposition} and demonstrate how it generalizes sparse principal component analysis (SPCA) for data fusion. Leveraging data from a multisubject, multimodal (electro- and magnetoencephalography (EEG and MEG)) neuroimaging experiment, we demonstrate the efficacy of the framework in identifying common features in response to face perception stimuli, while accommodating modality- and subject-specific variability. Through split-half cross-validation of EEG/MEG trials, we investigate the optimal model order and regularization strengths for models of varying complexity, comparing these to a group-level model assuming shared brain responses to stimuli. Our findings reveal altered $\sim170ms$ fusiform face area activation for scrambled faces, as opposed to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32467;&#21512;30&#31186;ECG&#35760;&#24405;&#21644;&#38271;&#26399;HRV&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#20272;&#31639;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#29983;&#23384;&#27169;&#22411;&#65306;XGBoost&#27169;&#22411;&#21644;ResNet&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15408</link><description>&lt;p&gt;
&#22522;&#20110;&#30701;&#26102;ECG&#21644;&#37319;&#26679;&#38271;&#26399;HRV&#30340;&#22810;&#27169;&#24577;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Heart Failure Risk Estimation based on Short ECG and Sampled Long-Term HRV
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32467;&#21512;30&#31186;ECG&#35760;&#24405;&#21644;&#38271;&#26399;HRV&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#20272;&#31639;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#29983;&#23384;&#27169;&#22411;&#65306;XGBoost&#27169;&#22411;&#21644;ResNet&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#21253;&#25324;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#65292;&#20173;&#28982;&#26159;&#20840;&#29699;&#20027;&#35201;&#30340;&#27515;&#20129;&#21407;&#22240;&#65292;&#24120;&#24120;&#38590;&#20197;&#26089;&#26399;&#26816;&#27979;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#21487;&#35775;&#38382;&#21644;&#26377;&#25928;&#30340;&#39118;&#38505;&#35780;&#20272;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35786;&#26029;&#27979;&#35797;&#65292;&#36890;&#24120;&#22312;&#30151;&#29366;&#21457;&#20316;&#21518;&#36827;&#34892;&#12290;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25216;&#26415;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21147;&#37327;&#27491;&#25104;&#20026;&#26234;&#33021;&#21307;&#30103;&#39046;&#22495;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#32467;&#21512;30&#31186;ECG&#35760;&#24405;&#21644;&#36817;&#20284;&#30340;&#38271;&#26399;&#24515;&#29575;&#21464;&#24322;&#24615;&#65288;HRV&#65289;&#25968;&#25454;&#65292;&#26469;&#20272;&#31639;HF&#20303;&#38498;&#39118;&#38505;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#29983;&#23384;&#27169;&#22411;&#65306;&#19968;&#20010;XGBoost&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#22833;&#36133;&#26102;&#38388;&#65288;AFT&#65289;&#65292;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;ECG&#29305;&#24449;&#65307;&#20197;&#21450;&#19968;&#20010;&#20174;&#21407;&#22987;ECG&#20013;&#23398;&#20064;&#30340;ResNet&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25105;&#20204;&#20174;&#36229;&#32423;&#38271;&#26399;HRV&#20013;&#25552;&#21462;&#30340;&#26032;&#39062;&#38271;&#26399;HRVs&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15408v1 Announce Type: cross  Abstract: Cardiovascular diseases, including Heart Failure (HF), remain a leading global cause of mortality, often evading early detection. In this context, accessible and effective risk assessment is indispensable. Traditional approaches rely on resource-intensive diagnostic tests, typically administered after the onset of symptoms. The widespread availability of electrocardiogram (ECG) technology and the power of Machine Learning are emerging as viable alternatives within smart healthcare. In this paper, we propose several multi-modal approaches that combine 30-second ECG recordings and approximate long-term Heart Rate Variability (HRV) data to estimate the risk of HF hospitalization. We introduce two survival models: an XGBoost model with Accelerated Failure Time (AFT) incorporating comprehensive ECG features and a ResNet model that learns from the raw ECG. We extend these with our novel long-term HRVs extracted from the combination of ultra-
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#24040;&#22823;&#25913;&#36827;&#65292;&#20294;&#30446;&#21069;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#21462;&#20195;&#20154;&#31867;&#25945;&#24072;&#65292;&#36719;&#20214;&#29702;&#35299;&#38382;&#39064;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#28508;&#21147;&#30340;&#24605;&#32771;&#12290;</title><link>https://arxiv.org/abs/2403.15399</link><description>&lt;p&gt;
ChatGPT&#22312;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#36816;&#29992;&#65306;&#21462;&#24471;&#36827;&#23637;&#65292;&#30041;&#24453;&#36827;&#19968;&#27493;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in Linear Algebra: Strides Forward, Steps to Go
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15399
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#24040;&#22823;&#25913;&#36827;&#65292;&#20294;&#30446;&#21069;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#21462;&#20195;&#20154;&#31867;&#25945;&#24072;&#65292;&#36719;&#20214;&#29702;&#35299;&#38382;&#39064;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#28508;&#21147;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#26086;&#26032;&#25216;&#26415;&#20986;&#29616;&#65292;&#25945;&#32946;&#30028;&#23601;&#20250;&#25506;&#32034;&#20854;&#23454;&#29992;&#24615;&#20197;&#21450;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20851;&#20110;&#22522;&#30784;&#32447;&#24615;&#20195;&#25968;&#20027;&#39064;&#30340;ChatGPT&#20250;&#35805;&#12290;&#25105;&#20204;&#21453;&#24605;&#20102;&#36807;&#21435;&#19968;&#24180;&#20869;ChatGPT&#22312;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#25152;&#36827;&#34892;&#30340;&#36807;&#31243;&#65292;&#24378;&#35843;&#20102;&#22312;&#24212;&#23545;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#19978;&#25152;&#21462;&#24471;&#30340;&#24040;&#22823;&#25913;&#36827;&#12290;&#23588;&#20854;&#26159;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20010;&#36719;&#20214;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#25945;&#23398;&#21161;&#25163;&#65292;&#29978;&#33267;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21462;&#20195;&#20154;&#31867;&#25945;&#24072;&#30340;&#38382;&#39064;&#12290;&#25130;&#33267;&#26412;&#25991;&#25776;&#20889;&#26102;&#65292;&#31572;&#26696;&#36890;&#24120;&#26159;&#21542;&#23450;&#30340;&#12290;&#23545;&#20110;&#21487;&#20197;&#20026;&#20043;&#31215;&#26497;&#30340;&#26041;&#38754;&#65292;&#32473;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#21407;&#22987;&#24037;&#31243;&#30340;&#21453;&#24605;&#12290;&#19982;&#35813;&#36719;&#20214;&#30340;&#20132;&#27969;&#32473;&#20154;&#19968;&#31181;&#22312;&#19982;&#20154;&#31867;&#20132;&#35848;&#30340;&#21360;&#35937;&#65292;&#26377;&#26102;&#20250;&#20135;&#29983;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#36719;&#20214;&#26159;&#21542;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35835;&#32773;&#30340;&#27880;&#24847;&#21147;&#34987;&#24341;&#21521;&#20102;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15399v1 Announce Type: cross  Abstract: As soon as a new technology emerges, the education community explores its affordances and the possibilities to apply it in education. In this paper, we analyze sessions with ChatGPT around topics in basic Linear Algebra. We reflect the process undertaken by the ChatGPT along the recent year in our area of interest, emphasising the vast improvement that has been done in grappling with Linear Algebra problems. In particular, the question whether this software can be a teaching assistant or even somehow replace the human teacher, is addressed. As of the time this paper is written, the answer is generally negative. For the small part where the answer can be positive, some reflections about an original instrumental genesis are given.   Communication with the software gives the impression to talk to a human, and sometimes the question is whether the software understands the question or not. Therefore, the reader's attention is drawn to the f
&lt;/p&gt;</description></item><item><title>&#23558;&#27169;&#22411;&#25253;&#21578;&#20013;&#30340;&#36947;&#24503;&#32771;&#34385;&#31867;&#21035;&#37325;&#26032;&#20998;&#31867;&#20026;&#21487;&#20449;&#24230;&#21644;&#39118;&#38505;&#31649;&#29702;&#31867;&#21035;&#65292;&#38024;&#23545;&#21487;&#20449;&#24230;AI&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.15394</link><description>&lt;p&gt;
2024&#24180;&#12298;&#27169;&#22411;&#25253;&#21578;&#30340;&#27169;&#22411;&#21345;&#12299;&#65306;&#20197;&#21487;&#20449;&#24230;&#21644;&#39118;&#38505;&#31649;&#29702;&#37325;&#26032;&#20998;&#31867;&#36947;&#24503;&#32771;&#34385;&#30340;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
"Model Cards for Model Reporting" in 2024: Reclassifying Category of Ethical Considerations in Terms of Trustworthiness and Risk Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15394
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#22411;&#25253;&#21578;&#20013;&#30340;&#36947;&#24503;&#32771;&#34385;&#31867;&#21035;&#37325;&#26032;&#20998;&#31867;&#20026;&#21487;&#20449;&#24230;&#21644;&#39118;&#38505;&#31649;&#29702;&#31867;&#21035;&#65292;&#38024;&#23545;&#21487;&#20449;&#24230;AI&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#24180;&#65292;&#39064;&#20026;&#12298;&#27169;&#22411;&#25253;&#21578;&#30340;&#27169;&#22411;&#21345;&#12299;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#35760;&#24405;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#40723;&#21169;&#36879;&#26126;&#25253;&#21578;&#32451;&#20064;&#65292;&#38024;&#23545;&#19968;&#31995;&#21015;&#26126;&#30830;&#23450;&#20041;&#30340;&#31867;&#21035;&#12290;&#35813;&#35770;&#25991;&#20013;&#35814;&#32454;&#20171;&#32461;&#30340;&#19968;&#20010;&#31867;&#21035;&#26159;&#36947;&#24503;&#32771;&#34385;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#25454;&#12289;&#20154;&#31867;&#29983;&#21629;&#12289;&#32531;&#35299;&#25514;&#26045;&#12289;&#39118;&#38505;&#21644;&#21361;&#23475;&#20197;&#21450;&#29992;&#20363;&#31561;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#35758;&#37325;&#26032;&#20998;&#31867;&#21407;&#22987;&#27169;&#22411;&#21345;&#20013;&#30340;&#36825;&#19968;&#31867;&#21035;&#65292;&#36825;&#26159;&#22240;&#20026;&#26368;&#36817;&#25104;&#29087;&#30340;&#39046;&#22495;&#31216;&#20026;&#21487;&#20449;&#24230;&#20154;&#24037;&#26234;&#33021;&#65292;&#35813;&#26415;&#35821;&#20998;&#26512;&#31639;&#27861;&#23646;&#24615;&#26159;&#21542;&#34920;&#26126;AI&#31995;&#32479;&#20540;&#24471;&#21463;&#21040;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#20219;&#12290;&#22312;&#25105;&#20204;&#23545;&#21487;&#20449;&#24230;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19977;&#20010;&#22791;&#21463;&#23562;&#37325;&#30340;&#32452;&#32455;&#8212;&#8212;&#27431;&#30431;&#22996;&#21592;&#20250;AI&#39640;&#32423;&#19987;&#23478;&#32452;&#12289;&#32463;&#27982;&#21512;&#20316;&#19982;&#21457;&#23637;&#32452;&#32455;&#21644;&#32654;&#22269;&#22269;&#23478;&#26631;&#20934;&#19982;&#25216;&#26415;&#30740;&#31350;&#25152;&#8212;&#8212;&#20182;&#20204;&#23601;&#21487;&#20449;&#24230;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#25776;&#20889;&#20102;&#20934;&#21017;&#12290;&#36825;&#20123;&#26368;&#36817;&#30340;&#20986;&#29256;&#29289;&#27719;&#32858;&#20102;&#35768;&#22810;&#20849;&#21516;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15394v1 Announce Type: cross  Abstract: In 2019, the paper entitled "Model Cards for Model Reporting" introduced a new tool for documenting model performance and encouraged the practice of transparent reporting for a defined list of categories. One of the categories detailed in that paper is ethical considerations, which includes the subcategories of data, human life, mitigations, risks and harms, and use cases. We propose to reclassify this category in the original model card due to the recent maturing of the field known as trustworthy AI, a term which analyzes whether the algorithmic properties of the model indicate that the AI system is deserving of trust from its stakeholders. In our examination of trustworthy AI, we highlight three respected organizations - the European Commission's High-Level Expert Group on AI, the OECD, and the U.S.-based NIST - that have written guidelines on various aspects of trustworthy AI. These recent publications converge on numerous character
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;Reddit&#29992;&#25143;&#24086;&#23376;&#65292;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#65292;&#24110;&#21161;&#25913;&#21892;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#30340;&#30417;&#27979;&#21644;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15393</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20174;Reddit&#24086;&#23376;&#20013;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;
&lt;/p&gt;
&lt;p&gt;
Detection of Opioid Users from Reddit Posts via an Attention-based Bidirectional Recurrent Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;Reddit&#29992;&#25143;&#24086;&#23376;&#65292;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#65292;&#24110;&#21161;&#25913;&#21892;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#30340;&#30417;&#27979;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#25351;&#30340;&#26159;&#22240;&#38463;&#29255;&#31867;&#33647;&#29289;&#36807;&#37327;&#20351;&#29992;&#21644;&#25104;&#30270;&#32780;&#23548;&#33268;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20303;&#38498;&#21644;&#27515;&#20129;&#26696;&#20363;&#65292;&#24050;&#32463;&#25104;&#20026;&#32654;&#22269;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#27492;&#21361;&#26426;&#65292;&#32852;&#37030;&#21644;&#22320;&#26041;&#25919;&#24220;&#20197;&#21450;&#21355;&#29983;&#31038;&#21306;&#24050;&#32463;&#21046;&#23450;&#20102;&#35768;&#22810;&#31574;&#30053;&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#20581;&#24247;&#30417;&#27979;&#26469;&#25552;&#39640;&#25105;&#20204;&#23545;&#21361;&#26426;&#30340;&#20102;&#35299;&#26159;&#24403;&#21153;&#20043;&#24613;&#20043;&#19968;&#12290;&#38500;&#20102;&#30452;&#25509;&#27979;&#35797;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20063;&#21487;&#33021;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26469;&#26816;&#27979;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#65292;&#22240;&#20026;&#35768;&#22810;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#21487;&#33021;&#36873;&#25321;&#19981;&#20570;&#27979;&#35797;&#65292;&#20294;&#21487;&#33021;&#20250;&#21311;&#21517;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20182;&#20204;&#30340;&#32463;&#21382;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25910;&#38598;&#24182;&#20998;&#26512;&#20102;&#26469;&#33258;&#27969;&#34892;&#31038;&#20132;&#32593;&#32476;Reddit&#30340;&#29992;&#25143;&#24086;&#23376;&#65292;&#20197;&#30830;&#23450;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15393v1 Announce Type: new  Abstract: The opioid epidemic, referring to the growing hospitalizations and deaths because of overdose of opioid usage and addiction, has become a severe health problem in the United States. Many strategies have been developed by the federal and local governments and health communities to combat this crisis. Among them, improving our understanding of the epidemic through better health surveillance is one of the top priorities. In addition to direct testing, machine learning approaches may also allow us to detect opioid users by analyzing data from social media because many opioid users may choose not to do the tests but may share their experiences on social media anonymously. In this paper, we take advantage of recent advances in machine learning, collect and analyze user posts from a popular social network Reddit with the goal to identify opioid users. Posts from more than 1,000 users who have posted on three sub-reddits over a period of one mon
&lt;/p&gt;</description></item><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.15048</link><description>&lt;p&gt;
&#21345;&#36890;&#24187;&#35273;&#26816;&#27979;: &#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#25104;&#39046;&#22495;&#20013;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24187;&#35273;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39118;&#26684;&#22914;&#21345;&#36890;&#20154;&#29289;&#20013;&#21253;&#21547;&#20102;&#24863;&#30693;&#19978;&#20851;&#38190;&#30340;&#32570;&#38519;&#65292;&#20381;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26816;&#27979;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#30340;&#35270;&#35273;&#24187;&#35273;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#65288;PA-ICVL&#65289;&#19982;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#21516;&#26102;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#12290;&#36890;&#36807;&#20174;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#23039;&#21183;&#20272;&#35745;&#22120;&#20013;&#33719;&#24471;&#23039;&#21183;&#25351;&#23548;&#65292;&#25105;&#20204;&#20351;VLM&#33021;&#22815;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35782;&#21035;&#35270;&#35273;&#24187;&#35273;&#26041;&#38754;&#65292;&#19982;&#20165;&#20381;&#36182;&#20110;RGB&#22270;&#20687;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20943;&#36731;&#35270;&#35273;&#24187;&#35273;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
&lt;/p&gt;</description></item><item><title>&#24341;&#21147;&#21452;&#23545;&#20598;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#39044;&#35774;&#30340;&#29366;&#24577;&#26041;&#31243;&#25512;&#23548;&#20986;&#23545;&#24212;&#30340;&#24341;&#21147;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.14763</link><description>&lt;p&gt;
&#20174;&#29366;&#24577;&#26041;&#31243;&#21040;&#24341;&#21147;&#23545;&#20598;
&lt;/p&gt;
&lt;p&gt;
Gravitational Duals from Equations of State
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14763
&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#21452;&#23545;&#20598;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#39044;&#35774;&#30340;&#29366;&#24577;&#26041;&#31243;&#25512;&#23548;&#20986;&#23545;&#24212;&#30340;&#24341;&#21147;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#21452;&#23545;&#20598;&#29702;&#35770;&#23558;&#20116;&#32500;&#24341;&#21147;&#29702;&#35770;&#19982;&#22235;&#32500;&#37327;&#23376;&#22330;&#35770;&#22312;&#24179;&#30452;&#31354;&#38388;&#20013;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#31181;&#26144;&#23556;&#19979;&#65292;&#22330;&#35770;&#30340;&#29366;&#24577;&#26041;&#31243;&#34987;&#32534;&#30721;&#22312;&#24341;&#21147;&#29702;&#35770;&#30340;&#40657;&#27934;&#35299;&#20013;&#12290;&#35299;&#20116;&#32500;&#29233;&#22240;&#26031;&#22374;&#26041;&#31243;&#20197;&#30830;&#23450;&#29366;&#24577;&#26041;&#31243;&#26159;&#19968;&#20010;&#31639;&#27861;&#24615;&#30340;&#12289;&#30452;&#25509;&#30340;&#38382;&#39064;&#12290;&#30830;&#23450;&#24341;&#21147;&#29702;&#35770;&#20174;&#32780;&#20135;&#29983;&#29305;&#23450;&#29366;&#24577;&#26041;&#31243;&#26159;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12289;&#21453;&#21521;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31639;&#27861;&#19981;&#20165;&#21463;&#25968;&#25454;&#39537;&#21160;&#65292;&#36824;&#21463;&#29233;&#22240;&#26031;&#22374;&#26041;&#31243;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#20132;&#21449;&#28857;&#12289;&#19968;&#32423;&#21644;&#20108;&#32423;&#30456;&#21464;&#30340;&#29702;&#35770;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14763v1 Announce Type: cross  Abstract: Holography relates gravitational theories in five dimensions to four-dimensional quantum field theories in flat space. Under this map, the equation of state of the field theory is encoded in the black hole solutions of the gravitational theory. Solving the five-dimensional Einstein's equations to determine the equation of state is an algorithmic, direct problem. Determining the gravitational theory that gives rise to a prescribed equation of state is a much more challenging, inverse problem. We present a novel approach to solve this problem based on physics-informed neural networks. The resulting algorithm is not only data-driven but also informed by the physics of the Einstein's equations. We successfully apply it to theories with crossovers, first- and second-order phase transitions.
&lt;/p&gt;</description></item><item><title>&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14689</link><description>&lt;p&gt;
&#21457;&#23637;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#20135;&#19994;&#26631;&#20934;&#65306;&#25361;&#25112;&#12289;&#31574;&#30053;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14689
&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#25215;&#35834;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#33258;&#21160;&#21270;&#34892;&#25919;&#21644;&#25945;&#23398;&#20219;&#21153;&#20197;&#21450;&#38477;&#20302;&#20869;&#23481;&#21019;&#24314;&#25104;&#26412;&#26469;&#38761;&#26032;&#25945;&#32946;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#32570;&#20047;&#26631;&#20934;&#21270;&#23454;&#36341;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#20998;&#25955;&#65292;&#32473;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#25552;&#20379;&#23545;&#24403;&#21069;&#23616;&#21183;&#12289;&#25361;&#25112;&#21644;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31574;&#30053;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#22987;&#36890;&#36807;&#30740;&#31350;AIED&#22312;&#19981;&#21516;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21253;&#25324;&#31995;&#32479;&#20114;&#25805;&#20316;&#24615;&#12289;&#26412;&#20307;&#26144;&#23556;&#12289;&#25968;&#25454;&#38598;&#25104;&#12289;&#35780;&#20272;&#21644;&#36947;&#24503;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14689v1 Announce Type: cross  Abstract: The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#32447;&#24615;&#27169;&#22411;&#21464;&#20307;&#31561;&#25928;&#20110;&#26631;&#20934;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.14587</link><description>&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Linear Time Series Forecasting Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14587
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#32447;&#24615;&#27169;&#22411;&#21464;&#20307;&#31561;&#25928;&#20110;&#26631;&#20934;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31616;&#21333;&#65292;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21363;&#20351;&#19982;&#26356;&#28145;&#23618;&#27425;&#21644;&#26356;&#26114;&#36149;&#30340;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#20063;&#26159;&#22914;&#27492;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#32447;&#24615;&#27169;&#22411;&#30340;&#21464;&#20307;&#65292;&#36890;&#24120;&#21253;&#25324;&#26576;&#31181;&#24418;&#24335;&#30340;&#29305;&#24449;&#35268;&#33539;&#21270;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#36825;&#20123;&#32447;&#24615;&#27169;&#22411;&#26550;&#26500;&#21487;&#20197;&#34920;&#36798;&#30340;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#21464;&#20307;&#31561;&#25928;&#65292;&#24182;&#22312;&#21151;&#33021;&#19978;&#26080;&#27861;&#21306;&#20998;&#26631;&#20934;&#30340;&#38750;&#32422;&#26463;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#20026;&#27599;&#31181;&#32447;&#24615;&#21464;&#20307;&#25551;&#36848;&#20102;&#27169;&#22411;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#37117;&#21487;&#20197;&#37325;&#26032;&#35299;&#37322;&#20026;&#22312;&#36866;&#24403;&#25193;&#20805;&#30340;&#29305;&#24449;&#38598;&#19978;&#30340;&#38750;&#32422;&#26463;&#32447;&#24615;&#22238;&#24402;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#22343;&#26041;&#25439;&#22833;&#20989;&#25968;&#26102;&#21487;&#20197;&#24471;&#21040;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#24453;&#26816;&#26597;&#30340;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20960;&#20046;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14587v1 Announce Type: new  Abstract: Despite their simplicity, linear models perform well at time series forecasting, even when pitted against deeper and more expensive models. A number of variations to the linear model have been proposed, often including some form of feature normalisation that improves model generalisation. In this paper we analyse the sets of functions expressible using these linear model architectures. In so doing we show that several popular variants of linear models for time series forecasting are equivalent and functionally indistinguishable from standard, unconstrained linear regression. We characterise the model classes for each linear variant. We demonstrate that each model can be reinterpreted as unconstrained linear regression over a suitably augmented feature set, and therefore admit closed-form solutions when using a mean-squared loss function. We provide experimental evidence that the models under inspection learn nearly identical solutions, a
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14566</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#25913;&#36827;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey on Concept-based Approaches For Model Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14566
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#30340;&#37325;&#28857;&#24050;&#32463;&#20174;&#20165;&#20165;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36716;&#21464;&#20026;&#20351;DNN&#26356;&#26131;&#35299;&#37322;&#32473;&#20154;&#31867;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#35266;&#23519;&#21040;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#26174;&#33879;&#24615;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#29992;&#25152;&#35859;&#30340;&#27010;&#24565;&#22312;&#31616;&#21333;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#26415;&#35821;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#27010;&#24565;&#26159;&#25968;&#25454;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#21333;&#20803;&#65292;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#22522;&#30707;&#12290;&#29992;&#27010;&#24565;&#30340;&#35299;&#37322;&#33021;&#22815;&#26816;&#27979;&#21040;&#20266;&#20851;&#32852;&#12289;&#22266;&#26377;&#20559;&#35265;&#25110;&#32874;&#26126;&#27721;&#12290;&#38543;&#30528;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#30340;&#20986;&#29616;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#27010;&#24565;&#34920;&#31034;&#26041;&#27861;&#21644;&#33258;&#21160;&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#27010;&#24565;&#36827;&#34892;&#20107;&#21518;&#27169;&#22411;&#35299;&#32544;&#35780;&#20272;&#65292;&#32780;&#20854;&#20182;&#20154;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#20107;&#21069;&#35757;&#32451;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#26032;&#30340;&#65292;&#26377;&#35768;&#22810;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14566v1 Announce Type: new  Abstract: The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans. The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts. Concepts are human interpretable units of data and are the thinking ground of humans. Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans. With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms. Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training. The concept-based approaches are new, with many representatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;mixup&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#31867;&#20869;&#28151;&#21512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14137</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#34917;&#30340;&#31867;&#20869;&#21644;&#31867;&#38388;Mixup&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14137
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;mixup&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#31867;&#20869;&#28151;&#21512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MixUp&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#21516;&#19968;&#31867;&#21035;&#20869;&#30340;&#28151;&#21512;&#65288;&#31867;&#20869;Mixup&#65289;&#65292;&#23548;&#33268;&#21516;&#19968;&#31867;&#21035;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#34987;&#20302;&#20272;&#12290;&#20854;&#27425;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#28151;&#21512;&#65288;&#31867;&#38388;Mixup&#65289;&#26377;&#25928;&#22686;&#24378;&#20102;&#31867;&#38388;&#21487;&#20998;&#31163;&#24615;&#65292;&#20294;&#22312;&#36890;&#36807;&#20854;&#28151;&#21512;&#25805;&#20316;&#25913;&#36827;&#31867;&#20869;&#20957;&#32858;&#21147;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;mixup&#26041;&#27861;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;mixup&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#31867;&#20869;Mixup&#65292;&#20197;&#21152;&#24378;&#31867;&#20869;&#20957;&#32858;&#24615;-&#36825;&#26159;&#30446;&#21069;&#30340;mixup&#25216;&#26415;&#27809;&#26377;&#25552;&#20379;&#30340;&#29305;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#23567;&#25209;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23567;&#25209;&#37327;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26410;&#22686;&#24378;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#29983;&#25104;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14137v1 Announce Type: cross  Abstract: MixUp and its variants, such as Manifold MixUp, have two key limitations in image classification tasks. First, they often neglect mixing within the same class (intra-class mixup), leading to an underutilization of the relationships among samples within the same class. Second, although these methods effectively enhance inter-class separability by mixing between different classes (inter-class mixup), they fall short in improving intra-class cohesion through their mixing operations, limiting their classification performance. To tackle these issues, we propose a novel mixup method and a comprehensive integrated solution.Our mixup approach specifically targets intra-class mixup, an aspect commonly overlooked, to strengthen intra-class cohesion-a feature not provided by current mixup techniques.For each mini-batch, our method utilizes feature representations of unaugmented original images from each class within the mini-batch to generate a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.14092</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#23454;&#26102;&#20943;&#23569;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14092
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#26174;&#33879;&#22686;&#21152;&#33021;&#28304;&#28040;&#32791;&#65292;&#30899;&#25490;&#25918;&#20302;&#30340;&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#27491;&#25104;&#20026;&#20840;&#29699;&#25919;&#24220;&#21644;&#20225;&#19994;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#38656;&#35201;&#22312;&#20919;&#21364;&#21644;IT&#36127;&#36733;&#20013;&#36827;&#34892;&#21151;&#32791;&#20248;&#21270;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#22522;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#32593;&#20013;&#30340;&#21487;&#29992;&#24615;&#26469;&#35843;&#25972;&#28789;&#27963;&#36127;&#36733;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#19981;&#38388;&#26029;&#30005;&#28304;&#20013;&#30340;&#30005;&#27744;&#23384;&#20648;&#65292;&#20351;&#29992;&#21327;&#20316;&#20195;&#29702;&#12290;&#36825;&#20123;&#20248;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#23545;&#21464;&#21270;&#30340;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#32593;&#30899;&#25490;&#25918;&#24378;&#24230;&#65289;&#30340;&#20381;&#36182;&#20351;&#24471;&#36825;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#30340;&#23454;&#26102;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30899;&#36275;&#36857;&#20943;&#23569;&#65288;DC-CFR&#65289;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#35282;&#24230;&#30340;&#25968;&#25454;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14092v1 Announce Type: cross  Abstract: As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BART&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38382;&#31572;&#26694;&#26550;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#23450;&#24773;&#32490;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#39044;&#27979;&#31572;&#26696;&#36328;&#24230;&#20301;&#32622;&#65292;&#23454;&#29616;&#23545;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.14050</link><description>&lt;p&gt;
&#20351;&#29992;BART&#20174;&#25512;&#25991;&#20013;&#25552;&#21462;&#24773;&#32490;&#30701;&#35821;
&lt;/p&gt;
&lt;p&gt;
Extracting Emotion Phrases from Tweets using BART
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BART&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38382;&#31572;&#26694;&#26550;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#23450;&#24773;&#32490;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#39044;&#27979;&#31572;&#26696;&#36328;&#24230;&#20301;&#32622;&#65292;&#23454;&#29616;&#23545;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#39033;&#26088;&#22312;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20013;&#24773;&#32490;&#26041;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#20027;&#35201;&#26159;&#23545;&#25991;&#26412;&#30340;&#25972;&#20307;&#26497;&#24615;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#20256;&#36798;&#24773;&#32490;&#30340;&#20855;&#20307;&#30701;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#26694;&#26550;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#21452;&#21521;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;BART&#65289;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20174;&#32473;&#23450;&#25991;&#26412;&#20013;&#25552;&#21462;&#25918;&#22823;&#32473;&#23450;&#24773;&#24863;&#26497;&#24615;&#30340;&#30701;&#35821;&#12290;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#30830;&#23450;&#35201;&#25552;&#21462;&#30340;&#29305;&#23450;&#24773;&#32490;&#65292;&#28982;&#21518;&#24341;&#23548;BART&#19987;&#27880;&#20110;&#25991;&#26412;&#20013;&#30456;&#20851;&#30340;&#24773;&#24863;&#32447;&#32034;&#12290;&#25105;&#20204;&#22312;BART&#20013;&#20351;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#25991;&#26412;&#20013;&#31572;&#26696;&#36328;&#24230;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#20301;&#32622;&#65292;&#20174;&#32780;&#24110;&#21161;&#30830;&#23450;&#25552;&#21462;&#30340;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14050v2 Announce Type: replace  Abstract: Sentiment analysis is a natural language processing task that aims to identify and extract the emotional aspects of a text. However, many existing sentiment analysis methods primarily classify the overall polarity of a text, overlooking the specific phrases that convey sentiment. In this paper, we applied an approach to sentiment analysis based on a question-answering framework. Our approach leverages the power of Bidirectional Autoregressive Transformer (BART), a pre-trained sequence-to-sequence model, to extract a phrase from a given text that amplifies a given sentiment polarity. We create a natural language question that identifies the specific emotion to extract and then guide BART to pay attention to the relevant emotional cues in the text. We use a classifier within BART to predict the start and end positions of the answer span within the text, which helps to identify the precise boundaries of the extracted emotion phrase. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13841</link><description>&lt;p&gt;
&#25972;&#21512;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#29992;&#20110;&#20010;&#24615;&#21270;&#24773;&#24863;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#29366;&#24577;&#20316;&#20026;&#24773;&#24863;&#30340;&#25351;&#26631;&#23545;&#25972;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#22312;&#20854;&#21457;&#20316;&#21069;&#20934;&#30830;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#26469;&#33258;&#21487;&#31359;&#25140;&#21644;&#31227;&#21160;&#35774;&#22791;&#30340;&#25968;&#25454;&#36827;&#34892;&#19982;&#30701;&#26399;&#24773;&#24863;&#26816;&#27979;&#12290;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#19987;&#27880;&#20110;&#23458;&#35266;&#30340;&#24863;&#23448;&#27979;&#37327;&#65292;&#24448;&#24448;&#24573;&#30053;&#20854;&#20182;&#24418;&#24335;&#30340;&#33258;&#25105;&#25253;&#21578;&#20449;&#24687;&#65292;&#22914;&#26085;&#35760;&#21644;&#31508;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#19968;&#20010;transformer&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23458;&#35266;&#25351;&#26631;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32437;&#21521;&#30740;&#31350;&#65292;&#25307;&#21215;&#20102;&#22823;&#23398;&#29983;&#24182;&#22312;&#19968;&#24180;&#20869;&#23545;&#20854;&#36827;&#34892;&#30417;&#27979;&#65292;&#25910;&#38598;&#20102;&#21253;&#25324;&#29983;&#29702;&#12289;&#29615;&#22659;&#12289;&#30561;&#30496;&#12289;&#20195;&#35874;&#21644;&#36523;&#20307;&#27963;&#21160;&#21442;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#26085;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13841v1 Announce Type: cross  Abstract: Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the partici
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23384;&#22312;&#22024;&#26434;&#36890;&#20449;&#36890;&#36947;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedNMUT&#65292;&#36890;&#36807;&#26799;&#24230;&#36319;&#36394;&#20943;&#23567;&#25968;&#25454;&#24322;&#36136;&#24615;&#24433;&#21709;&#24182;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22312;&#22122;&#22768;&#21442;&#25968;&#20013;&#23454;&#29616;&#23458;&#25143;&#31471;&#20849;&#35782;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#20256;&#32479;&#21442;&#25968;&#28151;&#21512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13247</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65306;&#22312;&#20449;&#24687;&#20998;&#20139;&#19981;&#23436;&#20840;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#26356;&#26032;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23384;&#22312;&#22024;&#26434;&#36890;&#20449;&#36890;&#36947;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedNMUT&#65292;&#36890;&#36807;&#26799;&#24230;&#36319;&#36394;&#20943;&#23567;&#25968;&#25454;&#24322;&#36136;&#24615;&#24433;&#21709;&#24182;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22312;&#22122;&#22768;&#21442;&#25968;&#20013;&#23454;&#29616;&#23458;&#25143;&#31471;&#20849;&#35782;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#20256;&#32479;&#21442;&#25968;&#28151;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#25955;&#24335;&#22024;&#26434;&#27169;&#22411;&#26356;&#26032;&#36319;&#36394;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;FedNMUT&#65289;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#22312;&#21453;&#26144;&#20449;&#24687;&#20132;&#25442;&#19981;&#23436;&#25972;&#30340;&#22024;&#26434;&#36890;&#20449;&#36890;&#36947;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#36816;&#34892;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#26799;&#24230;&#36319;&#36394;&#26469;&#26368;&#23567;&#21270;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23558;&#22122;&#22768;&#32435;&#20837;&#20854;&#21442;&#25968;&#20013;&#65292;&#20197;&#27169;&#25311;&#22024;&#26434;&#36890;&#20449;&#28192;&#36947;&#30340;&#26465;&#20214;&#65292;&#20174;&#32780;&#36890;&#36807;&#36890;&#20449;&#22270;&#25299;&#25169;&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#35782;&#12290;FedNMUT&#23558;&#21442;&#25968;&#20849;&#20139;&#21644;&#22122;&#22768;&#32435;&#20837;&#20316;&#20026;&#20248;&#20808;&#32771;&#34385;&#65292;&#20197;&#22686;&#24378;&#20998;&#25955;&#24335;&#23398;&#20064;&#31995;&#32479;&#23545;&#22024;&#26434;&#36890;&#20449;&#30340;&#25269;&#25239;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;&#22312;&#24615;&#33021;&#19978;&#65292;FedNMUT&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20256;&#32479;&#30340;&#21442;&#25968;&#28151;&#21512;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13247v1 Announce Type: new  Abstract: A novel Decentralized Noisy Model Update Tracking Federated Learning algorithm (FedNMUT) is proposed, which is tailored to function efficiently in the presence of noisy communication channels that reflect imperfect information exchange. This algorithm uses gradient tracking to minimize the impact of data heterogeneity while minimizing communication overhead. The proposed algorithm incorporates noise into its parameters to mimic the conditions of noisy communication channels, thereby enabling consensus among clients through a communication graph topology in such challenging environments. FedNMUT prioritizes parameter sharing and noise incorporation to increase the resilience of decentralized learning systems against noisy communications. Through theoretical and empirical validation, it is demonstrated that the performance of FedNMUT is superior compared to the existing state-of-the-art methods and conventional parameter-mixing approaches 
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11782</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20174;&#20559;&#22909;&#21644;&#36873;&#25321;&#20013;&#23398;&#20064;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A tutorial on learning from preferences and choices with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24314;&#27169;&#20301;&#20110;&#32463;&#27982;&#23398;&#12289;&#20915;&#31574;&#29702;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#30340;&#20559;&#22909;&#21450;&#20854;&#36873;&#25321;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#26356;&#25509;&#36817;&#20182;&#20204;&#26399;&#26395;&#30340;&#20135;&#21697;&#65292;&#20026;&#36328;&#39046;&#22495;&#30340;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#24212;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;&#27492;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36830;&#36143;&#12289;&#20840;&#38754;&#30340;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#28436;&#31034;&#22914;&#20309;&#23558;&#29702;&#24615;&#21407;&#21017;&#65288;&#26469;&#33258;&#32463;&#27982;&#23398;&#21644;&#20915;&#31574;&#29702;&#35770;&#65289;&#26080;&#32541;&#22320;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#21512;&#36866;&#22320;&#23450;&#21046;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#19968;&#26694;&#26550;&#20351;&#24471;&#33021;&#22815;&#26500;&#24314;&#28085;&#30422;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#12289;&#36776;&#35782;&#38480;&#21046;&#21644;&#23545;&#35937;&#21644;&#26631;&#31614;&#20559;&#22909;&#30340;&#22810;&#37325;&#20914;&#31361;&#25928;&#29992;&#24773;&#26223;&#30340;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#20803;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#22235;&#20803;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#29305;&#24449;&#20043;&#38388;&#20851;&#31995;&#30340;&#21516;&#26102;&#22312;&#25925;&#38556;&#20998;&#31867;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11722</link><description>&lt;p&gt;
&#20351;&#29992;&#22235;&#20803;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#22235;&#20803;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11722
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#20803;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#22235;&#20803;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#29305;&#24449;&#20043;&#38388;&#20851;&#31995;&#30340;&#21516;&#26102;&#22312;&#25925;&#38556;&#20998;&#31867;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22235;&#20803;&#25968;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#38271;&#26102;&#38388;&#24207;&#21015;&#21010;&#20998;&#20026;&#25968;&#25454;&#27573;&#65292;&#25552;&#21462;&#36825;&#20123;&#22359;&#30340;&#26368;&#23567;&#20540;&#12289;&#26368;&#22823;&#20540;&#12289;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#20316;&#20026;&#20195;&#34920;&#24615;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#23553;&#35013;&#22312;&#22235;&#20803;&#25968;&#20013;&#65292;&#24471;&#21040;&#19968;&#20010;&#22235;&#20803;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#20351;&#29992;&#22235;&#20803;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21704;&#23494;&#39039;&#31215;&#26469;&#20445;&#30041;&#36825;&#20123;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20010;&#22235;&#20803;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20351;&#29992;GHR&#24494;&#31215;&#20998;&#30340;&#22235;&#20803;&#25968;&#21453;&#21521;&#20256;&#25773;&#65292;&#36825;&#23545;&#20110;&#22235;&#20803;&#25968;&#31354;&#38388;&#20013;&#30340;&#26377;&#25928;&#20056;&#31215;&#21644;&#38142;&#35268;&#21017;&#26159;&#24517;&#38656;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25512;&#23548;&#26356;&#26032;&#35268;&#21017;&#19982;&#33258;&#21160;&#24494;&#20998;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;Tennessee Eastman&#25968;&#25454;&#38598;&#65292;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#20351;&#29992;&#21387;&#32553;&#25968;&#25454;&#36827;&#34892;&#25925;&#38556;&#20998;&#31867;&#65306;&#19968;&#20010;&#23436;&#20840;&#30417;&#30563;&#30340;&#35774;&#32622;&#21644;&#21478;&#19968;&#20010;&#21322;&#30417;&#30563;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11722v1 Announce Type: new  Abstract: We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series. This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product. To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space. Furthermore, we investigate the connection between the derived update rules and automatic differentiation. We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully supervised one and i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.11259</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11259
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#26381;&#21153;&#22120;&#20013;&#25918;&#32622;&#24212;&#29992;&#31243;&#24207;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#35768;&#22810;&#26381;&#21153;&#22120;&#12289;&#29992;&#25143;&#21450;&#20854;&#35831;&#27714;&#12290;&#29616;&#26377;&#31639;&#27861;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#35299;&#20915;&#20855;&#26377;&#37325;&#22823;&#19981;&#30830;&#23450;&#24615;&#24773;&#26223;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#21516;&#26102;&#32771;&#34385;&#25152;&#26377;&#25216;&#26415;&#32422;&#26463;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#27169;&#25311;&#20102;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#20013;&#37096;&#32626;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#35745;&#23558;&#23398;&#20064;&#22914;&#20309;&#26681;&#25454;&#29992;&#25143;&#21644;&#26381;&#21153;&#22120;&#30340;&#31354;&#38388;&#20301;&#32622;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#12290;&#26412;&#30740;&#31350;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#36890;&#36807;&#21464;&#21270;&#21442;&#25968;&#22914;&#29992;&#25143;&#20301;&#32622;&#12289;&#35831;&#27714;&#36895;&#29575;&#21644;&#35299;&#20915;&#20248;&#21270;&#27169;&#22411;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#35760;&#24405;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#36317;&#31163;&#21487;&#29992;&#26381;&#21153;&#22120;&#30340;&#36317;&#31163;&#29305;&#24449;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11259v1 Announce Type: cross  Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36873;&#39033;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Hierarchical Policy learning&#26469;&#35299;&#20915;&#39640;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10855</link><description>&lt;p&gt;
&#20351;&#29992;&#36873;&#39033;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Options
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36873;&#39033;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Hierarchical Policy learning&#26469;&#35299;&#20915;&#39640;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#24314;&#31435;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#25913;&#36827;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#39640;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20197;&#20998;&#23618;&#26041;&#24335;&#20998;&#35299;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65292;&#26469;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#31532;&#19968;&#31456;&#25105;&#20204;&#29087;&#24713;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#19968;&#20123;&#26368;&#36817;&#30340;&#25216;&#26415;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20998;&#23618;&#31574;&#30053;&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#21333;&#20010;&#22522;&#26412;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20010;&#23618;&#27425;&#32467;&#26500;&#30001;&#39030;&#23618;&#30340;&#31649;&#29702;&#20195;&#29702;&#21644;&#24213;&#23618;&#30340;&#21592;&#24037;&#20195;&#29702;&#32452;&#25104;&#12290;&#22312;&#26368;&#21518;&#19968;&#31456;&#65292;&#20063;&#26159;&#26412;&#35770;&#25991;&#30340;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#23581;&#35797;&#29420;&#31435;&#20110;&#31649;&#29702;&#32423;&#21035;&#23398;&#20064;&#23618;&#27425;&#32467;&#26500;&#30340;&#20302;&#23618;&#20803;&#32032;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;Eigenoption&#8221;&#12290;&#22522;&#20110;&#29615;&#22659;&#30340;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10855v1 Announce Type: new  Abstract: The current thesis aims to explore the reinforcement learning field and build on existing methods to produce improved ones to tackle the problem of learning in high-dimensional and complex environments. It addresses such goals by decomposing learning tasks in a hierarchical fashion known as Hierarchical Reinforcement Learning.   We start in the first chapter by getting familiar with the Markov Decision Process framework and presenting some of its recent techniques that the following chapters use. We then proceed to build our Hierarchical Policy learning as an answer to the limitations of a single primitive policy. The hierarchy is composed of a manager agent at the top and employee agents at the lower level.   In the last chapter, which is the core of this thesis, we attempt to learn lower-level elements of the hierarchy independently of the manager level in what is known as the "Eigenoption". Based on the graph structure of the environm
&lt;/p&gt;</description></item><item><title>LightIt&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26174;&#24335;&#29031;&#26126;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#29031;&#26126;&#25511;&#21046;&#65292;&#21516;&#26102;&#35757;&#32451;&#20102;&#19968;&#20010;&#36523;&#20221;&#20445;&#25345;&#30340;&#37325;&#29031;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10615</link><description>&lt;p&gt;
LightIt&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#29031;&#26126;&#24314;&#27169;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LightIt: Illumination Modeling and Control for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10615
&lt;/p&gt;
&lt;p&gt;
LightIt&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26174;&#24335;&#29031;&#26126;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#29031;&#26126;&#25511;&#21046;&#65292;&#21516;&#26102;&#35757;&#32451;&#20102;&#19968;&#20010;&#36523;&#20221;&#20445;&#25345;&#30340;&#37325;&#29031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LightIt&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26174;&#24335;&#29031;&#26126;&#25511;&#21046;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#29983;&#25104;&#26041;&#27861;&#32570;&#20047;&#29031;&#26126;&#25511;&#21046;&#65292;&#32780;&#36825;&#23545;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#35768;&#22810;&#33402;&#26415;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#27604;&#22914;&#35774;&#32622;&#25972;&#20307;&#24773;&#32490;&#25110;&#30005;&#24433;&#22806;&#35266;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20197;&#36974;&#34109;&#21644;&#27861;&#32447;&#22270;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#27425;&#21453;&#23556;&#36974;&#34109;&#26469;&#24314;&#27169;&#29031;&#26126;&#65292;&#21253;&#25324;&#25237;&#23556;&#38452;&#24433;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#36974;&#34109;&#20272;&#35745;&#27169;&#22359;&#26469;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#36974;&#34109;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20272;&#35745;&#30340;&#36974;&#34109;&#21644;&#27861;&#32447;&#20316;&#20026;&#36755;&#20837;&#35757;&#32451;&#25511;&#21046;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#29031;&#26126;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#20010;&#20445;&#25345;&#36523;&#20221;&#30340;&#37325;&#29031;&#27169;&#22411;&#65292;&#20197;&#22270;&#20687;&#21644;&#30446;&#26631;&#36974;&#34109;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#12289;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10615v1 Announce Type: cross  Abstract: We introduce LightIt, a method for explicit illumination control for image generation. Recent generative methods lack lighting control, which is crucial to numerous artistic aspects of image generation such as setting the overall mood or cinematic appearance. To overcome these limitations, we propose to condition the generation on shading and normal maps. We model the lighting with single bounce shading, which includes cast shadows. We first train a shading estimation module to generate a dataset of real-world images and shading pairs. Then, we train a control network using the estimated shading and normals as input. Our method demonstrates high-quality image generation and lighting control in numerous scenes. Additionally, we use our generated dataset to train an identity-preserving relighting model, conditioned on an image and a target shading. Our method is the first that enables the generation of images with controllable, consisten
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;</title><link>https://arxiv.org/abs/2403.09506</link><description>&lt;p&gt;
&#19981;&#35201;&#20197;&#22806;&#34920;&#21028;&#26029;: &#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30446;&#26631;&#35782;&#21035;&#20013;&#30340;&#35757;&#32451;&#27969;&#31243;&#22312;&#25968;&#25454;&#22686;&#24378;&#26102;&#24573;&#30053;&#20102;&#33394;&#35843;&#25238;&#21160;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#20250;&#24102;&#26469;&#23545;&#20998;&#31867;&#26377;&#23475;&#30340;&#22806;&#35266;&#21464;&#21270;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20063;&#26159;&#20302;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33394;&#35843;&#21464;&#21270;&#22312;&#35270;&#39057;&#35782;&#21035;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#21464;&#21270;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#23545;&#20110;&#21253;&#21547;&#36816;&#21160;&#20449;&#24687;&#30340;&#35270;&#39057;&#26469;&#35828;&#65292;&#38745;&#24577;&#22806;&#35266;&#19981;&#26159;&#37027;&#20040;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#65292;&#23427;&#22312;&#35270;&#39057;&#20013;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#65292;&#38544;&#24335;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#36816;&#21160;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SwapMix&#30340;&#25805;&#20316;&#65292;&#29992;&#20110;&#39640;&#25928;&#20462;&#25913;&#35270;&#39057;&#26679;&#26412;&#30340;&#22806;&#35266;&#65292;&#24182;&#24341;&#20837;&#20102;&#21464;&#24322;&#23545;&#40784;&#65288;VA&#65289;&#26469;&#35299;&#20915;SwapMix&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36843;&#20351;&#27169;&#22411;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09506v1 Announce Type: cross  Abstract: Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to le
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09267</link><description>&lt;p&gt;
&#28145;&#24230;&#38480;&#20215;&#35746;&#21333;&#31807;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Limit Order Book Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#32929;&#31080;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#23574;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#22312;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#19978;&#20132;&#26131;&#30340;&#19968;&#32452;&#24322;&#36136;&#32929;&#31080;&#30340;&#39640;&#39057;&#38480;&#20215;&#35746;&#21333;&#31807;&#20013;&#38388;&#20215;&#26684;&#21464;&#21160;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#8220;LOBFrame&#8221;&#65292;&#19968;&#20010;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#38480;&#20215;&#35746;&#21333;&#31807;&#25968;&#25454;&#65292;&#24182;&#23450;&#37327;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#21452;&#37325;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#32929;&#31080;&#30340;&#24494;&#35266;&#32467;&#26500;&#29305;&#24449;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#39640;&#39044;&#27979;&#33021;&#21147;&#19981;&#19968;&#23450;&#23545;&#24212;&#21487;&#25805;&#20316;&#30340;&#20132;&#26131;&#20449;&#21495;&#12290;&#25105;&#20204;&#35748;&#20026;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;&#38480;&#20215;&#35746;&#21333;&#31807;&#29615;&#22659;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#20934;&#30830;&#39044;&#27979;&#30340;&#27010;&#29575;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#29983;&#25104;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20559;&#35265;&#25968;&#25454;&#19978;&#35757;&#32451;&#20844;&#24179;&#30340;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#26631;&#27880;&#34394;&#20551;&#23646;&#24615;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#30340;&#34394;&#20551;&#23646;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#26174;&#31034;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06606</link><description>&lt;p&gt;
&#20998;&#24067;&#29983;&#25104;&#22686;&#24378;&#20844;&#24179;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributionally Generative Augmentation for Fair Facial Attribute Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#29983;&#25104;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20559;&#35265;&#25968;&#25454;&#19978;&#35757;&#32451;&#20844;&#24179;&#30340;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#26631;&#27880;&#34394;&#20551;&#23646;&#24615;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#30340;&#34394;&#20551;&#23646;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#26174;&#31034;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#65288;FAC&#65289;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#35757;&#32451;&#30340;FAC&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23376;&#32676;&#20307;&#20013;&#23637;&#31034;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#19981;&#20844;&#24179;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#20854;&#20013;&#19968;&#20123;&#34394;&#20551;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#30007;&#24615;&#65289;&#22312;&#32479;&#35745;&#19978;&#19982;&#30446;&#26631;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#24494;&#31505;&#65289;&#30456;&#20851;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#34394;&#20551;&#23646;&#24615;&#30340;&#26631;&#31614;&#65292;&#20294;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20844;&#24179;&#30340;FAC&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#26126;&#30830;&#23637;&#31034;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#65292;&#23427;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#38543;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#22270;&#20687;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#36753;&#34394;&#20551;&#23646;&#24615;&#65292;&#38543;&#26426;&#25277;&#26679;&#19968;&#23450;&#31243;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06606v1 Announce Type: cross  Abstract: Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04810</link><description>&lt;p&gt;
&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Restricted Bayesian Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#36816;&#34892;&#26041;&#24335;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12289;&#36807;&#25311;&#21512;&#12289;&#27424;&#25311;&#21512;&#12289;&#26799;&#24230;&#28040;&#22833;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#31283;&#20581;&#30340;&#25910;&#25947;&#20540;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#30340;&#20984;&#24615;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#30340;&#27010;&#24565;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#32500;&#24230;&#24046;&#24322;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.03967</link><description>&lt;p&gt;
&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#23545;&#23545;&#25239;&#33030;&#24369;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#30340;&#27010;&#24565;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#32500;&#24230;&#24046;&#24322;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23384;&#22312;&#19988;&#23545;&#20154;&#31867;&#26469;&#35828;&#20960;&#20046;&#26080;&#27861;&#23519;&#35273;&#36825;&#19968;&#20107;&#23454;&#65292;&#22312;&#29702;&#35770;&#19978;&#20173;&#28982;&#30456;&#24403;&#31070;&#31192;&#12290;&#25991;&#31456;&#24341;&#20837;&#20102;&#20004;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#27010;&#24565;&#65306;&#33258;&#28982;&#25110;&#22312;&#27969;&#24418;&#19978;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#21487;&#20197;&#34987;&#20154;&#31867;/&#31070;&#35861;&#24863;&#30693;&#21040;&#30340;&#65307;&#38750;&#33258;&#28982;&#25110;&#33073;&#31163;&#27969;&#24418;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21017;&#26080;&#27861;&#34987;&#24863;&#30693;&#21040;&#12290;&#25991;&#31456;&#35748;&#20026;&#33073;&#31163;&#27969;&#24418;&#30340;&#25915;&#20987;&#23384;&#22312;&#26159;&#25968;&#25454;&#22266;&#26377;&#32500;&#24230;&#19982;&#29615;&#22659;&#32500;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#24517;&#28982;&#32467;&#26524;&#12290;&#23545;&#20110;2&#23618;ReLU&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#32500;&#24230;&#24046;&#24322;&#19981;&#24433;&#21709;&#20174;&#35266;&#27979;&#25968;&#25454;&#31354;&#38388;&#20013;&#25277;&#21462;&#26679;&#26412;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23427;&#20173;&#20250;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#25552;&#20379;&#20102;&#22312;/&#33073;&#31163;&#27969;&#24418;&#25915;&#20987;&#30340;$\ell_2,\ell_{\infty}$&#25915;&#20987;&#24378;&#24230;&#19982;&#32500;&#24230;&#24046;&#24322;&#20043;&#38388;&#26126;&#30830;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03881</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Latent Dataset Distillation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#24102;&#26469;&#23384;&#20648;&#25361;&#25112;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20123;&#38750;&#24433;&#21709;&#21147;&#26679;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#34987;&#24573;&#30053;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#33976;&#39311;&#25104;&#19968;&#32452;&#21387;&#32553;&#26679;&#26412;&#65288;&#21512;&#25104;&#26679;&#26412;&#65289;&#65292;&#21363;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36873;&#25321;&#29992;&#20110;&#36830;&#25509;&#21407;&#22987;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26550;&#26500;&#65288;&#36890;&#24120;&#26159;ConvNet&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#33976;&#39311;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#21017;&#26368;&#32456;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20363;&#22914;128x128&#21450;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.02930</link><description>&lt;p&gt;
BASS&#30340;&#20877;&#23457;&#35270;--&#21033;&#29992;&#32479;&#19968;&#35821;&#20041;&#22270;&#25552;&#21319;&#25277;&#35937;&#25688;&#35201;--&#19968;&#39033;&#22797;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02930
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;BASS&#26694;&#26550;&#30340;&#35814;&#32454;&#22797;&#21046;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#35821;&#20041;&#22270;&#27010;&#24565;&#30340;&#25277;&#35937;&#25688;&#35201;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#22797;&#21046;&#20851;&#38190;&#32452;&#20214;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#19968;&#20010;&#28040;&#34701;&#30740;&#31350;&#26469;&#31995;&#32479;&#22320;&#38548;&#31163;&#22312;&#22797;&#21046;&#26032;&#39062;&#32452;&#20214;&#26102;&#26681;&#28304;&#20110;&#38169;&#35823;&#26469;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#21363;&#20351;&#26159;&#34987;&#21512;&#29702;&#30465;&#30053;&#30340;&#32454;&#33410;&#23545;&#20110;&#22797;&#21046;&#20687;BASS&#36825;&#26679;&#30340;&#20808;&#36827;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02930v1 Announce Type: new  Abstract: We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Paraformer&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65292;&#35774;&#35745;&#20102;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#32508;&#21512;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;</title><link>https://arxiv.org/abs/2403.02746</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#31934;&#30830;&#25351;&#23548;&#30340;&#23398;&#20064;&#65306;&#20174;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#26631;&#31614;&#26356;&#26032;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Paraformer&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65292;&#35774;&#35745;&#20102;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#32508;&#21512;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#26159;&#35843;&#26597;&#22320;&#29699;&#34920;&#38754;&#21644;&#35299;&#20915;&#20154;&#31867;&#38754;&#20020;&#30340;&#35768;&#22810;&#25361;&#25112;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#22320;&#38754;&#32454;&#33410;&#12289;&#21508;&#31181;&#22320;&#35980;&#21644;&#24191;&#27867;&#22320;&#29702;&#21306;&#22495;&#20869;&#20934;&#30830;&#35757;&#32451;&#26631;&#31614;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65288;Paraformer&#65289;&#65292;&#21363;&#20302;&#21040;&#39640;&#32593;&#32476;&#65288;L2HNet&#65289;V2&#65292;&#29992;&#20110;&#22312;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#30340;&#26131;&#33719;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#29616;&#26377;&#30340;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#26041;&#27861;&#26174;&#31034;&#20102;CNN&#22312;&#20445;&#30041;&#23616;&#37096;&#22320;&#38754;&#32454;&#33410;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#21508;&#31181;&#22320;&#35980;&#20013;&#20840;&#23616;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Paraformer&#20013;&#30340;&#24182;&#34892;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21253;&#25324;&#19968;&#20010;&#26080;&#38477;&#37319;&#26679;CNN&#20998;&#25903;&#21644;&#19968;&#20010;Transformer&#20998;&#25903;&#65292;&#26469;&#20849;&#21516;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02746v1 Announce Type: cross  Abstract: Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;RL&#25511;&#21046;&#31574;&#30053;&#21644;&#35782;&#21035;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15893</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#24182;&#34892;&#23398;&#20064;&#31574;&#30053;&#21644;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;RL&#25511;&#21046;&#31574;&#30053;&#21644;&#35782;&#21035;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31574;&#30053;&#38754;&#20020;&#30528;&#30830;&#20445;&#23433;&#20840;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#23433;&#20840;RL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#39044;&#23450;&#20041;&#30340;&#23433;&#20840;&#32422;&#26463;&#32435;&#20837;&#21040;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#23545;&#39044;&#23450;&#20041;&#23433;&#20840;&#32422;&#26463;&#30340;&#20381;&#36182;&#22312;&#23433;&#20840;&#25511;&#21046;RL&#20219;&#21153;&#20013;&#20855;&#26377;&#38480;&#21046;&#65292;&#22240;&#20026;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#26080;&#27861;&#24471;&#21040;&#25110;&#19981;&#22815;&#36866;&#24212;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;&#30340;RL&#25511;&#21046;&#31574;&#30053;&#24182;&#30830;&#23450;&#32473;&#23450;&#29615;&#22659;&#30340;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;pSTL&#65289;&#23433;&#20840;&#35268;&#33539;&#21644;&#19968;&#20010;&#23567;&#30340;&#21021;&#22987;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#20219;&#21153;&#65292;&#24039;&#22937;&#22320;&#23558;&#21463;&#38480;&#31574;&#30053;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15893v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has revolutionized decision-making across a wide range of domains over the past few decades. Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety. Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process. However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable. Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment. Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy op
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#36890;&#36807;&#22312;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;STG&#36801;&#31227;&#23398;&#20064;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#36229;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#23450;&#22478;&#24066;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11922</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#31354;&#22270;&#36801;&#31227;&#23398;&#20064;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#36890;&#36807;&#22312;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;STG&#36801;&#31227;&#23398;&#20064;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#36229;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#23450;&#22478;&#24066;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#65288;STG&#65289;&#23398;&#20064;&#23545;&#20110;&#26234;&#24935;&#22478;&#24066;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22312;&#35768;&#22810;&#22478;&#24066;&#21644;&#22320;&#21306;&#24448;&#24448;&#23384;&#22312;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#29992;&#20110;STG&#36801;&#31227;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32463;&#36807;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26469;&#25191;&#34892;STG&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11922v1 Announce Type: new  Abstract: Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11858</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#38543;&#26426;Hessian&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Hessian Fitting on Lie Group
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;Hessian&#25311;&#21512;&#20934;&#21017;&#65292;&#21487;&#29992;&#20110;&#25512;&#23548;&#22823;&#37096;&#20998;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;BFGS&#12289;&#39640;&#26031;&#29275;&#39039;&#12289;AdaGrad&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#29575;&#65292;&#20363;&#22914;&#65292;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#21644;&#23545;&#31216;&#27491;&#23450;&#65288;SPL&#65289;&#30697;&#38453;&#21644;&#26576;&#20123;&#26446;&#32676;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#32447;&#24615;&#36895;&#29575;&#12290;&#22312;&#29305;&#23450;&#19988;&#36275;&#22815;&#19968;&#33324;&#30340;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#26159;&#24378;&#20984;&#30340;&#12290;&#20026;&#20102;&#30830;&#35748;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#26377;&#22122;&#22768;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#12289;&#26102;&#21464;&#30340;Hessians&#21644;&#20302;&#31934;&#24230;&#31639;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20381;&#36182;&#20110;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07310</link><description>&lt;p&gt;
BioNeRF: &#29992;&#20110;&#35270;&#22270;&#21512;&#25104;&#30340;&#29983;&#29289;&#21512;&#29702;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07310
&lt;/p&gt;
&lt;p&gt;
BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BioNeRF&#65292;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#32593;&#32476;&#26435;&#37325;&#26469;&#23384;&#20648;&#22330;&#26223;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;BioNeRF&#23454;&#29616;&#20102;&#19968;&#31181;&#21463;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#36755;&#20837;&#34701;&#21512;&#25104;&#20869;&#23384;&#31867;&#20284;&#30340;&#32467;&#26500;&#65292;&#25552;&#39640;&#23384;&#20648;&#33021;&#21147;&#24182;&#25552;&#21462;&#26356;&#22810;&#20869;&#22312;&#21644;&#30456;&#20851;&#20449;&#24687;&#12290;BioNeRF&#36824;&#27169;&#20223;&#20102;&#37329;&#23383;&#22612;&#32454;&#32990;&#20013;&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#19968;&#31181;&#34892;&#20026;&#65292;&#20854;&#20013;&#20869;&#23384;&#20316;&#20026;&#19978;&#19979;&#25991;&#25552;&#20379;&#65292;&#24182;&#19982;&#20004;&#20010;&#21518;&#32493;&#31070;&#32463;&#27169;&#22411;&#30340;&#36755;&#20837;&#30456;&#32467;&#21512;&#65292;&#19968;&#20010;&#36127;&#36131;&#29983;&#25104;&#23481;&#31215;&#23494;&#24230;&#65292;&#21478;&#19968;&#20010;&#36127;&#36131;&#28210;&#26579;&#22330;&#26223;&#30340;&#39068;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioNeRF&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.
&lt;/p&gt;</description></item><item><title>JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04599</link><description>&lt;p&gt;
&#35265; JEANIE&#65306;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30456;&#20284;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04599
&lt;/p&gt;
&lt;p&gt;
JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24207;&#21015;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24178;&#25200;&#24615;&#21464;&#21270;&#65292;&#21253;&#25324;&#21160;&#20316;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#20027;&#20307;&#23039;&#21183;&#65292;&#23548;&#33268;&#22312;&#27604;&#36739;&#20004;&#32452;&#24103;&#25110;&#35780;&#20272;&#20004;&#20010;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#26102;&#20135;&#29983;&#26102;&#38388;-&#35270;&#35282;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#23545;&#27604;&#30340;&#32852;&#21512;&#26102;&#38388;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#23545;&#40784;&#26041;&#27861;&#65288;JEANIE&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#33021;&#22815;&#22312;&#19977;&#32500;&#20013;&#36731;&#26494;&#25805;&#20316;&#25668;&#20687;&#26426;&#21644;&#20027;&#20307;&#23039;&#21183;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#65288;FSAR&#65289;&#19978;&#35780;&#20272;&#20102;JEANIE&#65292;&#20854;&#20013;&#30001;&#20110;&#26032;&#31867;&#21035;&#26679;&#26412;&#26377;&#38480;&#65292;&#36890;&#36807;&#21305;&#37197;&#22909;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#65288;&#32452;&#25104;&#24207;&#21015;&#30340;&#26102;&#38388;&#22359;&#65289;&#26469;&#25490;&#38500;&#24178;&#25200;&#21464;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38024;&#23545;&#26597;&#35810;&#24207;&#21015;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#25668;&#20687;&#26426;&#20301;&#32622;&#21019;&#24314;&#22810;&#20010;&#35270;&#35282;&#12290;&#23545;&#20110;&#25903;&#25345;&#24207;&#21015;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27169;&#25311;&#20986;&#30340;&#26597;&#35810;&#24207;&#21015;&#36827;&#34892;&#21305;&#37197;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#25903;&#25345;&#26102;&#38388;&#22359;&#21487;&#20197;&#19982;&#35270;&#35282;&#27169;&#25311;&#30340;&#26597;&#35810;&#24207;&#21015;&#21305;&#37197;&#65292;&#22914;DTW&#12290;
&lt;/p&gt;
&lt;p&gt;
Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be m
&lt;/p&gt;</description></item><item><title>Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02423</link><description>&lt;p&gt;
Uni-RLHF: &#29992;&#20110;&#22810;&#26679;&#21270;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#29992;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02423
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#25163;&#21160;&#22870;&#21169;&#35774;&#35745;&#65292;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#29615;&#22659;&#20013;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#31867;&#22411;&#23545;RLHF&#30340;&#36827;&#27493;&#36827;&#34892;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27880;&#37322;&#24179;&#21488;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#32479;&#19968;&#22522;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Uni-RLHF&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;RLHF&#37327;&#36523;&#23450;&#21046;&#30340;&#32508;&#21512;&#31995;&#32479;&#23454;&#29616;&#12290;&#23427;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#23436;&#25972;&#30340;&#20174;&#30495;&#23454;&#20154;&#31867;&#21453;&#39304;&#21040;&#23454;&#38469;&#38382;&#39064;&#21457;&#23637;&#30340;&#24037;&#20316;&#27969;&#12290;Uni-RLHF&#21253;&#21547;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#36890;&#29992;&#30340;&#22810;&#21453;&#39304;&#27880;&#37322;&#24179;&#21488;&#65292;2&#65289;&#22823;&#35268;&#27169;&#30340;&#20247;&#21253;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;3&#65289;&#27169;&#22359;&#21270;&#30340;&#31163;&#32447;RLHF&#22522;&#20934;&#23454;&#29616;&#12290;Uni-RLHF&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#21453;&#39304;&#31867;&#22411;&#65292;&#24182;&#19982;&#20027;&#35201;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of main
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02023</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#39044;&#27979;&#30001;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24615;&#32780;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#28369;&#21160;&#31383;&#21475;&#26469;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#37096;&#20998;&#22312;&#30701;&#31383;&#21475;&#20869;&#34987;&#25429;&#25417;&#21040;&#30340;&#38271;&#26399;&#21464;&#21270;&#65288;&#21363;&#22806;&#31383;&#21475;&#21464;&#21270;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32858;&#28966;&#38271;&#26399;&#21464;&#21270;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#25439;&#22833;&#23558;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#26500;&#24314;&#27491;&#36127;&#23545;&#12290;&#24403;&#19982;&#25105;&#20204;&#30340;&#20998;&#35299;&#32593;&#32476;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, es
&lt;/p&gt;</description></item><item><title>LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2401.17919</link><description>&lt;p&gt;
LOCOST: &#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LOCOST: State-Space Models for Long Document Abstractive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17919
&lt;/p&gt;
&lt;p&gt;
LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#32534;&#30721;&#38271;&#24207;&#21015;&#21644;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#30340;&#20302;&#22797;&#26434;&#24230;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCOST&#65306;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26550;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;O&#65288;L log L&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#27604;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#27700;&#24179;&#19978;&#36798;&#21040;&#20102;&#19982;&#30456;&#21516;&#22823;&#23567;&#30340;&#26368;&#20248;&#31232;&#30095;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;93-96%&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;50%&#30340;&#20869;&#23384;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;87%&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;LOCOST&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20026;&#23436;&#25972;&#20070;&#25688;&#35201;&#21270;&#35774;&#23450;&#20102;&#26032;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#20026;&#38271;&#36755;&#20837;&#22788;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#25216;&#26415;&#22312;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;</title><link>https://arxiv.org/abs/2401.03695</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale Empirical Study on Improving the Fairness of Image Classification Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#25216;&#26415;&#22312;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#19968;&#30452;&#26159;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#37319;&#32435;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#35777;&#23454;&#22312;&#21508;&#33258;&#30340;&#24773;&#22659;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20043;&#38388;&#20173;&#28982;&#27809;&#26377;&#36827;&#34892;&#31995;&#32479;&#30340;&#35780;&#20272;&#20197;&#20415;&#22312;&#30456;&#21516;&#24773;&#22659;&#19979;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#36825;&#20351;&#24471;&#29702;&#35299;&#23427;&#20204;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21464;&#24471;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#36827;&#34892;&#39318;&#27425;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#20840;&#38754;&#27604;&#36739;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#22330;&#26223;&#65292;&#21033;&#29992;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#24120;&#29992;&#24615;&#33021;&#25351;&#26631;&#65292;&#24635;&#20849;&#35780;&#20272;&#20102;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340; 13 &#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03695v2 Announce Type: replace-cross  Abstract: Fairness has been a critical issue that affects the adoption of deep learning models in real practice. To improve model fairness, many existing methods have been proposed and evaluated to be effective in their own contexts. However, there is still no systematic evaluation among them for a comprehensive comparison under the same context, which makes it hard to understand the performance distinction among them, hindering the research progress and practical adoption of them. To fill this gap, this paper endeavours to conduct the first large-scale empirical study to comprehensively compare the performance of existing state-of-the-art fairness improving techniques. Specifically, we target the widely-used application scenario of image classification, and utilized three different datasets and five commonly-used performance metrics to assess in total 13 methods from diverse categories. Our findings reveal substantial variations in the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#20013;&#30740;&#31350;&#27169;&#22359;&#21270;&#31995;&#32479;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22359;&#21270;&#31995;&#32479;&#22312;&#21457;&#29616;&#38544;&#34255;&#32452;&#21512;&#32467;&#26500;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#35782;&#21035;&#28508;&#22312;&#27169;&#22359;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15001</link><description>&lt;p&gt;
&#21457;&#29616;&#33021;&#22815;&#36890;&#29992;&#32452;&#21512;&#30340;&#27169;&#22359;&#21270;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Discovering modular solutions that generalize compositionally
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15001
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#20013;&#30740;&#31350;&#27169;&#22359;&#21270;&#31995;&#32479;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22359;&#21270;&#31995;&#32479;&#22312;&#21457;&#29616;&#38544;&#34255;&#32452;&#21512;&#32467;&#26500;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#35782;&#21035;&#28508;&#22312;&#27169;&#22359;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#21487;&#20197;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#12289;&#29420;&#31435;&#30340;&#37096;&#20998;&#12290;&#21457;&#29616;&#36825;&#31181;&#28508;&#22312;&#30340;&#32452;&#21512;&#32467;&#26500;&#26377;&#21487;&#33021;&#23454;&#29616;&#32452;&#21512;&#27867;&#21270;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#26368;&#24378;&#22823;&#30340;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#28789;&#27963;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#20351;&#27169;&#22411;&#26356;&#27169;&#22359;&#21270;&#20284;&#20046;&#26159;&#33258;&#28982;&#30340;&#65292;&#20197;&#24110;&#21161;&#25429;&#25417;&#35768;&#22810;&#20219;&#21153;&#30340;&#32452;&#21512;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#27169;&#22359;&#21270;&#31995;&#32479;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#33021;&#22815;&#21457;&#29616;&#38544;&#34255;&#30340;&#32452;&#21512;&#32467;&#26500;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#20854;&#20013;&#25945;&#24072;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#25105;&#20204;&#23436;&#20840;&#25511;&#21046;&#30528;&#22320;&#38754;&#30495;&#23454;&#27169;&#22359;&#30340;&#32452;&#21512;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#32452;&#21512;&#27867;&#21270;&#30340;&#38382;&#39064;&#19982;&#21457;&#29616;&#28508;&#22312;&#27169;&#22359;&#30340;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20195;&#34920;&#19968;&#20010;&#36890;&#29992;&#31867;&#21035;&#30340;&#20056;&#27861;&#30456;&#20114;&#20316;&#29992;&#30340;&#36229;&#32593;&#32476;&#20013;&#30740;&#31350;&#20102;&#27169;&#22359;&#21270;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#32431;&#31929;&#30340;&#32447;&#24615;&#21464;&#25442;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15001v2 Announce Type: replace  Abstract: Many complex tasks can be decomposed into simpler, independent parts. Discovering such underlying compositional structure has the potential to enable compositional generalization. Despite progress, our most powerful systems struggle to compose flexibly. It therefore seems natural to make models more modular to help capture the compositional nature of many tasks. However, it is unclear under which circumstances modular systems can discover hidden compositional structure. To shed light on this question, we study a teacher-student setting with a modular teacher where we have full control over the composition of ground truth modules. This allows us to relate the problem of compositional generalization to that of identification of the underlying modules. In particular we study modularity in hypernetworks representing a general class of multiplicative interactions. We show theoretically that identification up to linear transformation purel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.11834</link><description>&lt;p&gt;
&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#34892;&#20154;&#21160;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#25311;&#34892;&#20154;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36947;&#36335;&#65292;&#24182;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30340;MARL&#20195;&#29702;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20195;&#29702;&#23398;&#20064;&#36991;&#24320;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20219;&#21153;&#65306;&#31364;&#30452;&#25509;&#36335;&#24452;&#21644;&#23485;&#32469;&#36947;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#36208;&#24266;&#20013;&#30340;&#21452;&#21521;&#34892;&#20154;&#27969;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#23494;&#24230;&#19981;&#22826;&#39640;&#26102;&#65292;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35889;&#22495;&#20013;&#23398;&#20064;&#31639;&#23376;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20445;&#35777;&#20102;&#39640;&#25554;&#20540;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.05654</link><description>&lt;p&gt;
&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#30340;&#35889;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spectral methods for Neural Integral Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35889;&#22495;&#20013;&#23398;&#20064;&#31639;&#23376;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20445;&#35777;&#20102;&#39640;&#25554;&#20540;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05654v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#36328;&#20132;&#25688;&#35201;&#65306;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#26159;&#22522;&#20110;&#31215;&#20998;&#26041;&#31243;&#29702;&#35770;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#27169;&#22411;&#30001;&#31215;&#20998;&#31639;&#23376;&#21644;&#36890;&#36807;&#20248;&#21270;&#36807;&#31243;&#23398;&#20064;&#30340;&#30456;&#24212;&#26041;&#31243;&#65288;&#31532;&#20108;&#31181;&#65289;&#32452;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#31215;&#20998;&#31639;&#23376;&#30340;&#38750;&#23616;&#37096;&#29305;&#24615;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#35889;&#22495;&#20013;&#23398;&#20064;&#19968;&#20010;&#31639;&#23376;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#35777;&#39640;&#25554;&#20540;&#31934;&#24230;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#36817;&#20284;&#33021;&#21147;&#21644;&#25910;&#25947;&#21040;&#25968;&#20540;&#26041;&#27861;&#35299;&#30340;&#21508;&#31181;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#23637;&#31034;&#25152;&#24471;&#27169;&#22411;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05654v3 Announce Type: replace-cross  Abstract: Neural integral equations are deep learning models based on the theory of integral equations, where the model consists of an integral operator and the corresponding equation (of the second kind) which is learned through an optimization procedure. This approach allows to leverage the nonlocal properties of integral operators in machine learning, but it is computationally expensive. In this article, we introduce a framework for neural integral equations based on spectral methods that allows us to learn an operator in the spectral domain, resulting in a cheaper computational cost, as well as in high interpolation accuracy. We study the properties of our methods and show various theoretical guarantees regarding the approximation capabilities of the model, and convergence to solutions of the numerical methods. We provide numerical experiments to demonstrate the practical effectiveness of the resulting model.
&lt;/p&gt;</description></item><item><title>I-PHYRE&#26159;&#19968;&#20010;&#25361;&#25112;&#20195;&#29702;&#21516;&#26102;&#23637;&#31034;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#12289;&#22810;&#27493;&#35268;&#21010;&#21644;&#23601;&#22320;&#24178;&#39044;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2312.03009</link><description>&lt;p&gt;
I-PHYRE: &#20132;&#20114;&#24335;&#29289;&#29702;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
I-PHYRE: Interactive Physical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03009
&lt;/p&gt;
&lt;p&gt;
I-PHYRE&#26159;&#19968;&#20010;&#25361;&#25112;&#20195;&#29702;&#21516;&#26102;&#23637;&#31034;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#12289;&#22810;&#27493;&#35268;&#21010;&#21644;&#23601;&#22320;&#24178;&#39044;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#20027;&#35201;&#35780;&#20272;&#38745;&#24577;&#22330;&#26223;&#20013;&#30340;&#29289;&#29702;&#25512;&#29702;&#65292;&#23384;&#22312;&#35780;&#20272;&#20195;&#29702;&#19982;&#21160;&#24577;&#20107;&#20214;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#30340;&#32570;&#21475;&#12290;&#23613;&#31649;&#24403;&#20195;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#20462;&#25913;&#21021;&#22987;&#22330;&#26223;&#37197;&#32622;&#24182;&#35266;&#23519;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23454;&#26102;&#19982;&#20107;&#20214;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;I-PHYRE&#65292;&#19968;&#20010;&#26694;&#26550;&#25361;&#25112;&#20195;&#29702;&#21516;&#26102;&#23637;&#31034;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#12289;&#22810;&#27493;&#35268;&#21010;&#21644;&#23601;&#22320;&#24178;&#39044;&#12290;&#36825;&#37324;&#65292;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#25351;&#30340;&#26159;&#24555;&#36895;&#12289;&#36817;&#20284;&#29702;&#35299;&#29289;&#29702;&#23398;&#20197;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65307;&#22810;&#27493;&#34920;&#31034;&#22312;I-PHYRE&#20013;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#24207;&#21015;&#35268;&#21010;&#65292;&#32771;&#34385;&#21040;&#27599;&#27425;&#24178;&#39044;&#37117;&#21487;&#33021;&#26174;&#33879;&#25913;&#21464;&#21518;&#32493;&#36873;&#25321;&#65307;&#32780;&#23601;&#22320;&#24847;&#21619;&#30528;&#22312;&#22330;&#26223;&#20869;&#21450;&#26102;&#36827;&#34892;&#29289;&#20307;&#25805;&#20316;&#30340;&#24517;&#35201;&#24615;&#65292;&#22312;&#36825;&#37324;&#65292;&#24494;&#23567;&#30340;&#26102;&#38388;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#22235;&#20010;&#28216;&#25103;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03009v2 Announce Type: replace  Abstract: Current evaluation protocols predominantly assess physical reasoning in stationary scenes, creating a gap in evaluating agents' abilities to interact with dynamic events. While contemporary methods allow agents to modify initial scene configurations and observe consequences, they lack the capability to interact with events in real time. To address this, we introduce I-PHYRE, a framework that challenges agents to simultaneously exhibit intuitive physical reasoning, multi-step planning, and in-situ intervention. Here, intuitive physical reasoning refers to a quick, approximate understanding of physics to address complex problems; multi-step denotes the need for extensive sequence planning in I-PHYRE, considering each intervention can significantly alter subsequent choices; and in-situ implies the necessity for timely object manipulation within a scene, where minor timing deviations can result in task failure. We formulate four game spl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D3PO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30452;&#25509;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#28040;&#38500;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#21069;&#25552;&#19979;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;DPO&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#30340;&#20869;&#23384;&#38656;&#27714;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.13231</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D3PO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30452;&#25509;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#28040;&#38500;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#21069;&#25552;&#19979;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;DPO&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#30340;&#20869;&#23384;&#38656;&#27714;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#22312;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#39318;&#20808;&#26159;&#36890;&#36807;&#35757;&#32451;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26469;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#12289;&#26368;&#20339;&#26550;&#26500;&#21644;&#25163;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#26082;&#32791;&#26102;&#21448;&#25104;&#26412;&#39640;&#26114;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#21435;&#22122;&#36807;&#31243;&#30340;&#22823;&#37327;GPU&#20869;&#23384;&#38656;&#27714;&#38459;&#30861;&#20102;DPO&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;D3PO&#65289;&#26041;&#27861;&#26469;&#30452;&#25509;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;D3PO&#25552;&#20379;&#20102;&#25913;&#36827;&#65292;&#20294;&#22312;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#30340;&#21516;&#26102;&#20173;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13231v3 Announce Type: replace-cross  Abstract: Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#36712;&#36857;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#36710;&#36742;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#28151;&#21512;&#20132;&#36890;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;</title><link>https://arxiv.org/abs/2311.12261</link><description>&lt;p&gt;
EnduRL: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#28151;&#21512;&#20132;&#36890;&#22312;&#30495;&#23454;&#19990;&#30028;&#25200;&#21160;&#19979;&#30340;&#23433;&#20840;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic Under Real-World Perturbations Via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12261
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#36712;&#36857;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#36710;&#36742;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#28151;&#21512;&#20132;&#36890;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12261v2 &#20844;&#21578;&#31867;&#22411;: replace-cross &#25688;&#35201;: &#20154;&#39550;&#39542;&#36710;&#36742;&#65288;HVs&#65289;&#25918;&#22823;&#20102;&#20132;&#36890;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#25200;&#21160;&#65292;&#23548;&#33268;&#25317;&#22581;-&#36825;&#26159;&#22686;&#21152;&#29123;&#27833;&#28040;&#32791;&#12289;&#22686;&#21152;&#30896;&#25758;&#39118;&#38505;&#20197;&#21450;&#20943;&#23569;&#36947;&#36335;&#23481;&#37327;&#21033;&#29992;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#20154;&#36710;&#36742;&#65288;RVs&#65289;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#27492;&#31867;&#30740;&#31350;&#20381;&#36182;&#20110;&#20351;&#29992;&#31616;&#21270;&#30340;&#20154;&#36710;&#36319;&#39536;&#34892;&#20026;&#27169;&#22411;&#30340;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#34892;&#36710;&#36712;&#36857;&#24182;&#25552;&#21462;&#20102;&#21508;&#31181;&#21152;&#36895;&#24230;&#26354;&#32447;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#26354;&#32447;&#32467;&#21512;&#21040;&#27169;&#25311;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;RVs&#20197;&#32531;&#35299;&#25317;&#22581;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#31181;&#28151;&#21512;&#20132;&#36890;&#29615;&#22659;&#65288;&#29615;&#24418;&#21644;&#29942;&#39048;&#65289;&#20013;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#35780;&#20272;&#20102;&#28151;&#21512;&#20132;&#36890;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20132;&#36890;&#23494;&#24230;&#12289;&#37197;&#32622;&#21644;RV&#28183;&#36879;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#19979;&#65292;&#20808;&#21069;&#30340;RV&#25511;&#21046;&#22120;&#36935;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12261v2 Announce Type: replace-cross  Abstract: Human-driven vehicles (HVs) amplify naturally occurring perturbations in traffic, leading to congestion--a major contributor to increased fuel consumption, higher collision risks, and reduced road capacity utilization. While previous research demonstrates that Robot Vehicles (RVs) can be leveraged to mitigate these issues, most such studies rely on simulations with simplistic models of human car-following behaviors. In this work, we analyze real-world driving trajectories and extract a wide range of acceleration profiles. We then incorporates these profiles into simulations for training RVs to mitigate congestion. We evaluate the safety, efficiency, and stability of mixed traffic via comprehensive experiments conducted in two mixed traffic environments (Ring and Bottleneck) at various traffic densities, configurations, and RV penetration rates. The results show that under real-world perturbations, prior RV controllers experienc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#22024;&#26434;&#26631;&#31614;&#23545;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2311.11202</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#25552;&#39640;&#25968;&#25454;&#21487;&#20449;&#24230;&#65306;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#22024;&#26434;&#26631;&#31614;&#23545;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11202v2&#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;-&#36328;&#25991;&#26723;&#25688;&#35201;&#65306;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#23545;&#40784;&#36807;&#31243;&#20013;&#21487;&#33021;&#21463;&#21040;&#19981;&#24076;&#26395;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#27880;&#35299;&#30340;&#27491;&#30830;&#24615;&#65292;&#21363;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#29992;&#20110;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22914;Jigsaw Civil Comments&#12289;Anthropic Harmless&#21644;Red Team&#12289;PKU BeaverTails&#21644;SafeRLHF&#12290;&#32771;&#34385;&#21040;&#20154;&#20204;&#28165;&#27927;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#21644;&#38590;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#31574;&#21010;&#35821;&#35328;&#25968;&#25454;&#20013;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11202v2 Announce Type: replace-cross  Abstract: Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless &amp; Red Team, PKU BeaverTails &amp; SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we 
&lt;/p&gt;</description></item><item><title>Tactics2D&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#21151;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25506;&#32034;&#23398;&#20064;&#39537;&#21160;&#30340;&#39550;&#39542;&#20915;&#31574;&#27169;&#22411;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.11058</link><description>&lt;p&gt;
Tactics2D&#65306;&#19968;&#31181;&#20855;&#26377;&#29983;&#25104;&#22330;&#26223;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#29992;&#20110;&#39550;&#39542;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Tactics2D: A Reinforcement Learning Environment Library with Generative Scenarios for Driving Decision-making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11058
&lt;/p&gt;
&lt;p&gt;
Tactics2D&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#21151;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25506;&#32034;&#23398;&#20064;&#39537;&#21160;&#30340;&#39550;&#39542;&#20915;&#31574;&#27169;&#22411;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tactics2D&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#22330;&#26223;&#30340;&#21151;&#33021;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#24320;&#31665;&#21363;&#29992;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#25506;&#32034;&#22522;&#20110;&#23398;&#20064;&#30340;&#39550;&#39542;&#20915;&#31574;&#27169;&#22411;&#12290;&#35813;&#24211;&#23454;&#29616;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#29983;&#25104;&#20114;&#21160;&#20132;&#36890;&#22330;&#26223;&#12290;Tactics2D&#30340;&#26174;&#33879;&#29305;&#28857;&#21253;&#25324;&#19982;&#29616;&#23454;&#19990;&#30028;&#26085;&#24535;&#21644;&#25968;&#25454;&#26684;&#24335;&#30340;&#24191;&#27867;&#20860;&#23481;&#24615;&#65292;&#21487;&#23450;&#21046;&#30340;&#20132;&#36890;&#22330;&#26223;&#32452;&#20214;&#20197;&#21450;&#20016;&#23500;&#30340;&#20869;&#32622;&#21151;&#33021;&#27169;&#26495;&#12290;Tactics2D&#32771;&#34385;&#21040;&#29992;&#25143;&#21451;&#22909;&#24615;&#32780;&#24320;&#21457;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25991;&#26723;&#21644;&#20132;&#20114;&#24335;&#22312;&#32447;&#25945;&#31243;&#12290;&#35813;&#36719;&#20214;&#20445;&#25345;&#20102;&#31283;&#22266;&#30340;&#21487;&#38752;&#24615;&#65292;&#36229;&#36807;90%&#30340;&#20195;&#30721;&#36890;&#36807;&#20102;&#21333;&#20803;&#27979;&#35797;&#12290;&#35201;&#35775;&#38382;&#28304;&#20195;&#30721;&#24182;&#21442;&#19982;&#35752;&#35770;&#65292;&#35831;&#35775;&#38382;Tactics2D&#30340;&#23448;&#26041;GitHub&#39029;&#38754;https://github.com/WoodOxen/Tactics2D&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11058v2 Announce Type: replace  Abstract: Tactics2D is an open-source Reinforcement Learning environment library featured with auto-generation of diverse and challenging traffic scenarios. Its primary goal is to provide an out-of-the-box toolkit for researchers to explore learning-based driving decision-making models. This library implements both rule-based and data-driven approaches to generate interactive traffic scenarios. Noteworthy features of Tactics2D include expansive compatibility with real-world log and data formats, customizable traffic scenario components, and rich built-in functional templates. Developed with user-friendliness in mind, Tactics2D offers detailed documentation and an interactive online tutorial. The software maintains robust reliability, with over 90% code passing unit testing. For access to the source code and participation in discussions, visit the official GitHub page for Tactcis2D at https://github.com/WoodOxen/Tactics2D.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#26497;&#38480;&#20013;&#22270;&#19978;&#20449;&#21495;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;Poincar\'e&#19981;&#31561;&#24335;&#24182;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.10610</link><description>&lt;p&gt;
&#20449;&#21495;&#22312;&#22823;&#22270;&#19978;&#30340;&#37319;&#26679;&#30340;Poincar\'e&#19981;&#31561;&#24335;&#21644;&#19968;&#33268;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Poincar\'e Inequality and Consistency Results for Signal Sampling on Large Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10610
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#26497;&#38480;&#20013;&#22270;&#19978;&#20449;&#21495;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;Poincar\'e&#19981;&#31561;&#24335;&#24182;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#38543;&#30528;&#22270;&#30340;&#22823;&#23567;&#32780;&#22686;&#21152;&#12290;&#23545;&#22270;&#36827;&#34892;&#23376;&#37319;&#26679;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#22312;&#22270;&#19978;&#36827;&#34892;&#37319;&#26679;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#22270;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#30340;&#12290;&#29616;&#26377;&#30340;&#22270;&#37319;&#26679;&#25216;&#26415;&#19981;&#20165;&#38656;&#35201;&#35745;&#31639;&#22823;&#30697;&#38453;&#30340;&#35889;&#65292;&#32780;&#19988;&#22312;&#22270;&#21457;&#29983;&#21464;&#21270;&#65288;&#20363;&#22914;&#22686;&#38271;&#65289;&#26102;&#38656;&#35201;&#37325;&#22797;&#36825;&#20123;&#35745;&#31639;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#19968;&#31181;&#22270;&#26497;&#38480;--&#22270;&#19978;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#19978;&#20449;&#21495;&#30340;Poincar\'e&#19981;&#31561;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#28385;&#36275;&#36825;&#19968;&#19981;&#31561;&#24335;&#30340;&#33410;&#28857;&#23376;&#38598;&#30340;&#34917;&#38598;&#26159;&#22270;&#19978;&#20449;&#21495;Paley-Wiener&#31354;&#38388;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#12290;&#36890;&#36807;&#19982;&#35889;&#32858;&#31867;&#21644;&#39640;&#26031;&#28040;&#20803;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#37319;&#26679;&#38598;&#26159;&#19968;&#33268;&#30340;&#65292;&#21363;&#25910;&#25947;&#30340;&#22270;&#24207;&#21015;&#19978;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#25910;&#25947;&#21040;&#22270;&#26497;&#38480;&#19978;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10610v2 Announce Type: replace  Abstract: Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit -- the graphon. We prove a Poincar\'e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related
&lt;/p&gt;</description></item><item><title>&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;</title><link>https://arxiv.org/abs/2311.08118</link><description>&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#37051;&#23621;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Neighbor Explainability for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08118
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#36817;&#24180;&#26469;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#30830;&#23450;&#27599;&#20010;&#37051;&#23621;&#23545;&#20110; GNN &#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#21508;&#31181;&#24050;&#30693;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#34987;&#37325;&#26032;&#26500;&#36896;&#20197;&#33719;&#21462;&#37051;&#23621;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312; GNN &#39046;&#22495;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25552;&#20379;&#30340;&#35299;&#37322;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#22312;&#20351;&#29992;&#27809;&#26377;&#33258;&#29615;&#30340; GNNs &#26102;&#26410;&#33021;&#35782;&#21035;&#37325;&#35201;&#30340;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08118v2 Announce Type: replace-cross  Abstract: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22240;&#26524;&#22270;&#19978;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#30340; agent&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24403;&#21069;&#22240;&#26524;&#38382;&#31572;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#35299;&#37322;&#25110;&#35777;&#25454;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.02760</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Causal Question Answering with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22240;&#26524;&#22270;&#19978;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#30340; agent&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24403;&#21069;&#22240;&#26524;&#38382;&#31572;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#35299;&#37322;&#25110;&#35777;&#25454;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#38382;&#39064;&#25506;&#31350;&#19981;&#21516;&#20107;&#20214;&#25110;&#29616;&#35937;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#23427;&#20204;&#23545;&#21508;&#31181;&#29992;&#20363;&#37117;&#24456;&#37325;&#35201;&#65292;&#21253;&#25324;&#34394;&#25311;&#21161;&#25163;&#21644;&#25628;&#32034;&#24341;&#25806;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#22240;&#26524;&#38382;&#31572;&#26041;&#27861;&#19981;&#33021;&#20026;&#20854;&#31572;&#26696;&#25552;&#20379;&#35299;&#37322;&#25110;&#35777;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29992;&#22240;&#26524;&#22270;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#21517;&#35789;&#30701;&#35821;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25552;&#20379;&#20851;&#31995;&#30340;&#26469;&#28304;&#25968;&#25454;&#12290;&#21463;&#26368;&#36817;&#23558;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#24212;&#29992;&#20110;&#30693;&#35782;&#22270;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#20107;&#23454;&#26816;&#26597;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22240;&#26524;&#22270;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#30340; agent&#65292;&#23427;&#23398;&#20064;&#36890;&#36807;&#22270;&#25628;&#32034;&#26469;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#31243;&#24207;&#24341;&#23548; agent &#22788;&#29702;&#22823;&#35268;&#27169;&#25805;&#20316;&#31354;&#38388;&#21644;sp
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02760v2 Announce Type: replace  Abstract: Causal questions inquire about causal relationships between different events or phenomena. They are important for a variety of use cases, including virtual assistants and search engines. However, many current approaches to causal question answering cannot provide explanations or evidence for their answers. Hence, in this paper, we aim to answer causal questions with a causality graph, a large-scale dataset of causal relations between noun phrases along with the relations' provenance data. Inspired by recent, successful applications of reinforcement learning to knowledge graph tasks, such as link prediction and fact-checking, we explore the application of reinforcement learning on a causality graph for causal question answering. We introduce an Actor-Critic-based agent which learns to search through the graph to answer causal questions. We bootstrap the agent with a supervised learning procedure to deal with large action spaces and sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#20801;&#35768;&#23450;&#20301;&#22810;&#20010;&#21464;&#37327;&#30340;&#24178;&#39044;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#39318;&#27425;&#24471;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.02695</link><description>&lt;p&gt;
&#20174;&#22810;&#33410;&#28857;&#24178;&#39044;&#20013;&#35782;&#21035;&#32447;&#24615;&#28151;&#21512;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#20801;&#35768;&#23450;&#20301;&#22810;&#20010;&#21464;&#37327;&#30340;&#24178;&#39044;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#39318;&#27425;&#24471;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#26029;&#20174;&#20302;&#32423;&#35266;&#23519;&#20013;&#24471;&#20986;&#39640;&#32423;&#22240;&#26524;&#21464;&#37327;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#31216;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#26412;&#36136;&#19978;&#26159;&#27424;&#32422;&#26463;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#36817;&#24037;&#20316;&#38598;&#20013;&#22312;&#23548;&#33268;&#28508;&#22312;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21487;&#35782;&#21035;&#24615;&#30340;&#21508;&#31181;&#20551;&#35774;&#19978;&#12290;&#22823;&#37327;&#20043;&#21069;&#30340;&#26041;&#27861;&#32771;&#34385;&#22312;&#22240;&#26524;&#27169;&#22411;&#19978;&#19981;&#21516;&#24178;&#39044;&#19979;&#25910;&#38598;&#30340;&#22810;&#29615;&#22659;&#25968;&#25454;&#12290;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#24037;&#20316;&#20849;&#21516;&#28857;&#26159;&#23545;&#27599;&#20010;&#29615;&#22659;&#20013;&#21482;&#24178;&#39044;&#19968;&#20010;&#21464;&#37327;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#24182;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20801;&#35768;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#36890;&#36807;&#24178;&#39044;&#23450;&#20301;&#22810;&#20010;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#20851;&#20110;&#21508;&#20010;&#29615;&#22659;&#20013;&#24178;&#39044;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#30340;&#19968;&#33324;&#20551;&#35774;&#65292;&#20854;&#20013;&#20063;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02695v2 Announce Type: replace-cross  Abstract: The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also include
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#22120;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#38646;&#36801;&#31227;</title><link>https://arxiv.org/abs/2310.18847</link><description>&lt;p&gt;
&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#39044;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Bird's Eye View Based Pretrained World model for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#22120;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#38646;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sim2Real&#36716;&#31227;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#20174;&#24265;&#20215;&#30340;&#27169;&#25311;&#22120;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#22312;&#20256;&#32479;&#19990;&#30028;&#27169;&#22411;&#20013;&#34701;&#21512;&#32452;&#20214;&#65292;&#23436;&#20840;&#22312;&#27169;&#25311;&#22120;&#20013;&#35757;&#32451;&#65292;&#21487;&#20197;&#38646;&#36801;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#12290;&#20026;&#20102;&#20419;&#36827;&#36716;&#31227;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#22270;&#20687;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#36890;&#36807;&#20808;&#23398;&#20064;&#23558;&#22797;&#26434;&#30340;&#22522;&#20110;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#30340;RGB&#22270;&#20687;&#36716;&#25442;&#20026;BEV&#34920;&#31034;&#65292;&#28982;&#21518;&#23398;&#20064;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#22312;&#27169;&#25311;&#20013;&#23548;&#33322;&#12290;&#24403;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#26426;&#22120;&#20154;&#20351;&#29992;&#23558;FPV&#22522;&#30784;RGB&#22270;&#20687;&#36716;&#25442;&#20026;FPV&#21040;BEV&#36716;&#25442;&#22120;&#23398;&#20064;&#30340;&#23884;&#20837;&#30340;&#24863;&#30693;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#19979;&#28216;&#31574;&#30053;&#20351;&#29992;&#12290;&#21033;&#29992;&#29366;&#24577;&#26816;&#26597;&#27169;&#22359;&#65292;&#20351;&#29992;&#38170;&#23450;&#22270;&#20687;&#21644;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18847v2 Announce Type: replace-cross  Abstract: Sim2Real transfer has gained popularity because it helps transfer from inexpensive simulators to real world. This paper presents a novel system that fuses components in a traditional World Model into a robust system, trained entirely within a simulator, that Zero-Shot transfers to the real world. To facilitate transfer, we use an intermediary representation that is based on \textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a simulator by first learning to translate from complex \textit{First-Person View (FPV)} based RGB images to BEV representations, then learning to navigate using those representations. Later, when tested in the real world, the robot uses the perception model that translates FPV-based RGB images to embeddings that were learned by the FPV to BEV translator and that can be used by the downstream policy. The incorporation of state-checking modules using \textit{Anchor images} and Mixtur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#32467;&#26500;&#30340;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#20234;&#36763;&#26426;&#33258;&#24049;&#20272;&#35745;&#30340;&#20559;&#23548;&#25968;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#37327;&#23376;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.18411</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#32463;&#20856;&#21644;&#37327;&#23376;&#20234;&#36763;&#26426;&#30340;&#36890;&#29992;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A general learning scheme for classical and quantum Ising machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#32467;&#26500;&#30340;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#20234;&#36763;&#26426;&#33258;&#24049;&#20272;&#35745;&#30340;&#20559;&#23548;&#25968;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#37327;&#23376;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#36763;&#26426;&#26159;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25214;&#21040;&#20234;&#36763;&#27169;&#22411;&#22522;&#24577;&#30340;&#20219;&#20309;&#30828;&#20214;&#12290;&#30456;&#20851;&#31034;&#20363;&#21253;&#25324;&#30456;&#24178;&#20234;&#36763;&#26426;&#21644;&#37327;&#23376;&#36864;&#28779;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#32467;&#26500;&#30340;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#25968;&#23398;&#34920;&#24449;&#65292;&#35813;&#36807;&#31243;&#26159;&#22522;&#20110;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20559;&#23548;&#25968;&#19981;&#26159;&#26174;&#24335;&#35745;&#31639;&#30340;&#65292;&#32780;&#26159;&#30001;&#20234;&#36763;&#26426;&#33258;&#24049;&#20272;&#35745;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25152;&#25552;&#20986;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25191;&#34892;&#30340;&#19968;&#20123;&#23454;&#39564;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#25351;&#20986;&#20102;&#20234;&#36763;&#26426;&#20026;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#37327;&#23376;&#39046;&#22495;&#65292;&#37327;&#23376;&#36164;&#28304;&#34987;&#29992;&#20110;&#27169;&#22411;&#30340;&#25191;&#34892;&#21644;&#35757;&#32451;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18411v2 Announce Type: replace  Abstract: An Ising machine is any hardware specifically designed for finding the ground state of the Ising model. Relevant examples are coherent Ising machines and quantum annealers. In this paper, we propose a new machine learning model that is based on the Ising structure and can be efficiently trained using gradient descent. We provide a mathematical characterization of the training process, which is based upon optimizing a loss function whose partial derivatives are not explicitly calculated but estimated by the Ising machine itself. Moreover, we present some experimental results on the training and execution of the proposed learning model. These results point out new possibilities offered by Ising machines for different learning tasks. In particular, in the quantum realm, the quantum resources are used for both the execution and the training of the model, providing a promising perspective in quantum machine learning.
&lt;/p&gt;</description></item><item><title>BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.14714</link><description>&lt;p&gt;
BatteryML&#65306;&#19968;&#20010;&#29992;&#20110;&#30005;&#27744;&#34928;&#20943;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
BatteryML:An Open-source platform for Machine Learning on Battery Degradation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14714
&lt;/p&gt;
&lt;p&gt;
BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#34928;&#20943;&#20173;&#28982;&#26159;&#33021;&#28304;&#23384;&#20648;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#25512;&#21160;&#27934;&#23519;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24037;&#20855;&#27491;&#22312;&#23835;&#36215;&#12290;&#28982;&#32780;&#65292;&#30005;&#21270;&#23398;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39046;&#22495;&#24102;&#26469;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#32463;&#24120;&#22312;&#22788;&#29702;&#30005;&#27744;&#31185;&#23398;&#30340;&#22797;&#26434;&#24615;&#19978;&#33510;&#33510;&#25379;&#25166;&#65292;&#32780;&#30005;&#27744;&#30740;&#31350;&#20154;&#21592;&#21017;&#38754;&#20020;&#30528;&#23558;&#22797;&#26434;&#27169;&#22411;&#35843;&#25972;&#21040;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#28085;&#30422;&#25968;&#25454;&#26684;&#24335;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#32479;&#19968;&#26631;&#20934;&#12290;&#37492;&#20110;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BatteryML - &#19968;&#20010;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#19988;&#24320;&#28304;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#32479;&#19968;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#20256;&#32479;&#21644;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;&#36825;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14714v4 Announce Type: replace-cross  Abstract: Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often grapple with the intricacies of battery science, while battery researchers face hurdles in adapting intricate models tailored to specific datasets. Beyond this, a cohesive standard for battery degradation modeling, inclusive of data formats and evaluative benchmarks, is conspicuously absent. Recognizing these impediments, we present BatteryML - a one-step, all-encompass, and open-source platform designed to unify data preprocessing, feature extraction, and the implementation of both traditional and state-of-the-art models. This streamlined approach promises to enhance the practicality and efficiency of research applications. BatteryML s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RTSUM&#26694;&#26550;&#65292;&#21033;&#29992;&#20851;&#31995;&#19977;&#20803;&#32452;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25688;&#35201;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#32423;&#26174;&#33879;&#24615;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2310.13895</link><description>&lt;p&gt;
RTSUM&#65306;&#22522;&#20110;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#21487;&#35299;&#37322;&#25688;&#35201;&#19982;&#22810;&#32423;&#26174;&#33879;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13895
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RTSUM&#26694;&#26550;&#65292;&#21033;&#29992;&#20851;&#31995;&#19977;&#20803;&#32452;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25688;&#35201;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#32423;&#26174;&#33879;&#24615;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RTSUM&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#19977;&#20803;&#32452;&#20316;&#20026;&#25688;&#35201;&#22522;&#26412;&#21333;&#20803;&#30340;&#26080;&#30417;&#30563;&#25688;&#35201;&#26694;&#26550;&#12290;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#65292;RTSUM&#39318;&#20808;&#36890;&#36807;&#22810;&#32423;&#26174;&#33879;&#24615;&#35780;&#20998;&#36873;&#25321;&#26174;&#33879;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#65292;&#28982;&#21518;&#21033;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#36873;&#23450;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#29983;&#25104;&#31616;&#27905;&#25688;&#35201;&#12290;&#22312;RTSUM&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#37322;&#24615;&#25688;&#35201;&#24037;&#20855;&#30340;&#32593;&#32476;&#28436;&#31034;&#65292;&#25552;&#20379;&#20102;&#23545;&#36755;&#20986;&#25688;&#35201;&#30340;&#32454;&#31890;&#24230;&#35299;&#37322;&#12290;&#36890;&#36807;&#25903;&#25345;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#22312;&#19977;&#20010;&#19981;&#21516;&#32423;&#21035;&#19978;&#21487;&#35270;&#21270;&#25991;&#26412;&#21333;&#20803;&#30340;&#26174;&#33879;&#24615;&#65306;&#21477;&#23376;&#12289;&#20851;&#31995;&#19977;&#20803;&#32452;&#21644;&#30701;&#35821;&#12290;&#20195;&#30721;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13895v2 Announce Type: replace  Abstract: In this paper, we present RTSUM, an unsupervised summarization framework that utilizes relation triples as the basic unit for summarization. Given an input document, RTSUM first selects salient relation triples via multi-level salience scoring and then generates a concise summary from the selected relation triples by using a text-to-text language model. On the basis of RTSUM, we also develop a web demo for an interpretable summarizing tool, providing fine-grained interpretations with the output summary. With support for customization options, our tool visualizes the salience for textual units at three distinct levels: sentences, relation triples, and phrases. The codes,are publicly available.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#20195;&#29702;&#22312;&#22788;&#29702;&#22797;&#26434;&#25361;&#25112;&#26102;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#36991;&#20813;&#25191;&#34892;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08446</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#23454;&#29616;&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Multi-Modal Reasoning via Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08446
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20195;&#29702;&#22312;&#22788;&#29702;&#22797;&#26434;&#25361;&#25112;&#26102;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#36991;&#20813;&#25191;&#34892;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26222;&#36941;&#25215;&#35748;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#24037;&#20855;&#23398;&#20064;&#21644;&#33258;&#20027;&#20195;&#29702;&#30740;&#31350;&#20013;&#40723;&#33310;&#20102;&#30740;&#31350;&#12290;LLM&#20805;&#24403;&#20195;&#29702;&#30340;&#8220;&#22823;&#33041;&#8221;&#65292;&#20026;&#21327;&#20316;&#22810;&#27493;&#20219;&#21153;&#27714;&#35299;&#38598;&#25104;&#22810;&#20010;&#24037;&#20855;&#12290;&#22810;&#27169;&#24577;&#20195;&#29702;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#25361;&#25112;&#65292;&#22312;&#22788;&#29702;&#30452;&#35266;&#20219;&#21153;&#26102;&#19981;&#20687;&#35843;&#29992;&#35745;&#31639;&#22120;&#25110;&#22825;&#27668;API&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#24573;&#35270;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65306;&#23427;&#20204;&#20027;&#35201;&#19987;&#27880;&#20110;&#35745;&#21010;&#21644;&#25191;&#34892;&#38454;&#27573;&#65292;&#21482;&#20250;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#35843;&#29992;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20351;&#25191;&#34892;&#21464;&#24471;&#33030;&#24369;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20854;&#20182;&#20256;&#32479;&#30340;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#35201;&#20040;&#19982;&#22810;&#27169;&#24577;&#20195;&#29702;&#22330;&#26223;&#19981;&#20860;&#23481;&#25110;&#19981;&#29702;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#22810;&#27493;&#25512;&#29702;&#20135;&#29983;&#30340;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08446v2 Announce Type: replace-cross  Abstract: The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#25506;&#35752;&#20102;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2310.05884</link><description>&lt;p&gt;
&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#30475;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Meta-Learning Perspective on Transformers for Causal Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#25506;&#35752;&#20102;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#24320;&#21457;&#22823;&#22411;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21464;&#24471;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#20854;&#33021;&#21147;&#30340;&#26426;&#21046;&#23578;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35757;&#32451;&#36807;&#31243;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#35270;&#35282;&#26469;&#25506;&#31350;Transformer&#26550;&#26500;&#22312;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;Transformer&#20869;&#37096;&#30340;&#19968;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20010;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#29702;&#35770;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#30340;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#23454;&#39564;&#35777;&#23454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05884v2 Announce Type: replace-cross  Abstract: The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process within the Transformer. Further, within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments in various settings.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;HyMNet&#65292;&#19968;&#31181;&#32467;&#21512;&#30524;&#24213;&#22270;&#20687;&#21644;&#24515;&#33039;&#20195;&#35874;&#39118;&#38505;&#22240;&#32032;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#25913;&#21892;&#39640;&#34880;&#21387;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.01099</link><description>&lt;p&gt;
HyMNet&#65306;&#19968;&#31181;&#21033;&#29992;&#30524;&#24213;&#29031;&#29255;&#21644;&#24515;&#33039;&#20195;&#35874;&#39118;&#38505;&#22240;&#23376;&#36827;&#34892;&#39640;&#34880;&#21387;&#20998;&#31867;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
HyMNet: a Multimodal Deep Learning System for Hypertension Classification using Fundus Photographs and Cardiometabolic Risk Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01099
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;HyMNet&#65292;&#19968;&#31181;&#32467;&#21512;&#30524;&#24213;&#22270;&#20687;&#21644;&#24515;&#33039;&#20195;&#35874;&#39118;&#38505;&#22240;&#32032;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#25913;&#21892;&#39640;&#34880;&#21387;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#39640;&#34880;&#21387;&#65288;HTN&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#26512;&#21333;&#19968;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;HTN&#39118;&#38505;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;HyMNet&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#65288;MMDL&#65289;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#30524;&#24213;&#22270;&#20687;&#21644;&#24515;&#33039;&#20195;&#35874;&#39118;&#38505;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#24180;&#40836;&#21644;&#24615;&#21035;&#65292;&#20197;&#25913;&#21892;&#39640;&#34880;&#21387;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;MMDL&#31995;&#32479;&#20351;&#29992;&#20102;&#22312;160&#19975;&#24352;&#35270;&#32593;&#33180;&#22270;&#20687;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;RETFound&#26469;&#36827;&#34892;&#30524;&#24213;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#36335;&#24452;&#12290;&#36825;&#20004;&#20010;&#36335;&#24452;&#36890;&#36807;&#36830;&#25509;&#27599;&#20010;&#36335;&#24452;&#30340;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#28982;&#21518;&#23558;&#20854;&#36865;&#20837;&#34701;&#21512;&#32593;&#32476;&#12290;&#35813;&#31995;&#32479;&#22312;&#27801;&#29305;&#22269;&#27665;&#35686;&#21355;&#21355;&#29983;&#20107;&#21153;&#37096;&#25910;&#38598;&#30340;1,243&#21517;&#20010;&#20307;&#30340;5,016&#24352;&#30524;&#24213;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01099v2 Announce Type: replace-cross  Abstract: In recent years, deep learning has shown promise in predicting hypertension (HTN) from fundus images. However, most prior research has primarily focused on analyzing a single type of data, which may not capture the full complexity of HTN risk. To address this limitation, this study introduces a multimodal deep learning (MMDL) system, dubbed HyMNet, which combines fundus images and cardiometabolic risk factors, specifically age and gender, to improve hypertension detection capabilities. Our MMDL system uses RETFound, a foundation model pre-trained on 1.6 million retinal images, for the fundus path and a fully connected neural network for the age and gender path. The two paths are jointly trained by concatenating the feature vectors from each path that are then fed into a fusion network. The system was trained on 5,016 retinal images from 1,243 individuals collected from the Saudi Ministry of National Guard Health Affairs. The re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2310.00724</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;:&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subtractive Mixture Models via Squaring: Representation and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00724
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#20256;&#32479;&#19978;&#26159;&#36890;&#36807;&#23558;&#20960;&#20010;&#20998;&#24067;&#20316;&#20026;&#32452;&#20214;&#30456;&#21152;&#26469;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#12290;&#20801;&#35768;&#28151;&#21512;&#20943;&#21435;&#27010;&#29575;&#36136;&#37327;&#25110;&#23494;&#24230;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#24314;&#27169;&#22797;&#26434;&#20998;&#24067;&#25152;&#38656;&#30340;&#32452;&#20214;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#31181;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#24182;&#30830;&#20445;&#23427;&#20204;&#20173;&#28982;&#32534;&#30721;&#38750;&#36127;&#20989;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24179;&#26041;&#26469;&#23398;&#20064;&#21644;&#25191;&#34892;&#28145;&#24230;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#27010;&#29575;&#30005;&#36335;&#26694;&#26550;&#20013;&#36827;&#34892;&#36825;&#20123;&#30740;&#31350;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#24352;&#37327;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#24182;&#27867;&#21270;&#20854;&#20182;&#20943;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20801;&#35768;&#20943;&#27861;&#30340;&#24179;&#26041;&#30005;&#36335;&#31867;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#20855;&#26377;&#25351;&#25968;&#32423;&#26356;&#20855;&#34920;&#36798;&#21147;&#65307;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#19978;&#23454;&#35777;&#23637;&#31034;&#20102;&#36825;&#31181;&#22686;&#21152;&#30340;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>LAINR&#36890;&#36807;&#24341;&#20837;&#29699;&#38754;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;SINR&#65289;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;&#25552;&#39640;&#20102;&#21516;&#21270;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.09574</link><description>&lt;p&gt;
&#37319;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#28508;&#21464;&#21516;&#21270;&#26410;&#30693;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Latent assimilation with implicit neural representations for unknown dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09574
&lt;/p&gt;
&lt;p&gt;
LAINR&#36890;&#36807;&#24341;&#20837;&#29699;&#38754;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;SINR&#65289;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;&#25552;&#39640;&#20102;&#21516;&#21270;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24120;&#24120;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#30001;&#20110;&#25968;&#25454;&#32500;&#24230;&#21644;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#19981;&#23436;&#20840;&#29702;&#35299;&#32780;&#23548;&#33268;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;&#20855;&#26377;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#28508;&#21464;&#21516;&#21270;&#65288;LAINR&#65289;&#12290;&#36890;&#36807;&#24341;&#20837;&#29699;&#38754;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;SINR&#65289;&#20197;&#21450;&#23545;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;LAINR&#25552;&#39640;&#20102;&#21516;&#21270;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LAINR&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09574v2 Announce Type: replace  Abstract: Data assimilation is crucial in a wide range of applications, but it often faces challenges such as high computational costs due to data dimensionality and incomplete understanding of underlying mechanisms. To address these challenges, this study presents a novel assimilation framework, termed Latent Assimilation with Implicit Neural Representations (LAINR). By introducing Spherical Implicit Neural Representations (SINR) along with a data-driven uncertainty estimator of the trained neural networks, LAINR enhances efficiency in assimilation process. Experimental results indicate that LAINR holds certain advantage over existing methods based on AutoEncoders, both in terms of accuracy and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28508;&#22312;&#38543;&#26426;&#24494;&#20998;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#23398;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#36712;&#36857;&#29983;&#25104;&#21644;&#39044;&#27979;&#20013;&#30340;&#29289;&#29702;&#29616;&#23454;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2309.09317</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#38543;&#26426;&#24494;&#20998;&#24314;&#27169;&#30340;&#36816;&#21160;&#23398;&#24863;&#30693;&#36712;&#36857;&#29983;&#25104;&#19982;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28508;&#22312;&#38543;&#26426;&#24494;&#20998;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#23398;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#36712;&#36857;&#29983;&#25104;&#21644;&#39044;&#27979;&#20013;&#30340;&#29289;&#29702;&#29616;&#23454;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#29983;&#25104;&#21644;&#36712;&#36857;&#39044;&#27979;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20004;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#21069;&#32773;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#29983;&#25104;&#21508;&#31181;&#27979;&#35797;&#36712;&#36857;&#65292;&#21518;&#32773;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;&#36817;&#24180;&#26469;&#65292;&#26032;&#20852;&#30340;&#25968;&#25454;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22914;&#20309;&#30830;&#20445;&#29983;&#25104;/&#39044;&#27979;&#30340;&#36712;&#36857;&#22312;&#29289;&#29702;&#19978;&#21512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20010;&#25361;&#25112;&#30340;&#26681;&#28304;&#22312;&#20110;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20316;&#20026;&#19981;&#36879;&#26126;&#40657;&#21283;&#23376;&#36816;&#34892;&#65292;&#24182;&#19981;&#36981;&#23432;&#29289;&#29702;&#27861;&#21017;&#65292;&#32780;&#29616;&#26377;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#20379;&#29289;&#29702;&#21487;&#34892;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#39044;&#23450;&#20041;&#27169;&#22411;&#32467;&#26500;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09317v2 Announce Type: replace  Abstract: Trajectory generation and trajectory prediction are two critical tasks in autonomous driving, which generate various trajectories for testing during development and predict the trajectories of surrounding vehicles during operation, respectively. In recent years, emerging data-driven deep learning-based methods have shown great promise for these two tasks in learning various traffic scenarios and improving average performance without assuming physical models. However, it remains a challenging problem for these methods to ensure that the generated/predicted trajectories are physically realistic. This challenge arises because learning-based approaches often function as opaque black boxes and do not adhere to physical laws. Conversely, existing model-based methods provide physically feasible results but are constrained by predefined model structures, limiting their capabilities to address complex scenarios. To address the limitations of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#39118;&#30005;&#22330;&#20013;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#26500;&#24314;&#36328;&#26102;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#39118;&#30005;&#22330;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.03472</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#27425;&#32467;&#26500;&#25552;&#39640;&#39118;&#21147;&#21457;&#30005;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the forecast accuracy of wind power by leveraging multiple hierarchical structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.03472
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#39118;&#30005;&#22330;&#20013;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#26500;&#24314;&#36328;&#26102;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#39118;&#30005;&#22330;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#23545;&#20840;&#29699;&#20943;&#30899;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#29305;&#21035;&#26159;&#39118;&#33021;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#39118;&#33021;&#21457;&#30005;&#21463;&#27668;&#20505;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#12290;&#26368;&#36817;&#36890;&#36807;&#21327;&#35843;&#23454;&#29616;&#30340;&#23618;&#27425;&#39044;&#27979;&#22312;&#30701;&#26399;&#20869;&#26174;&#33879;&#25552;&#39640;&#20102;&#39118;&#33021;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#39118;&#30005;&#22330;&#20013;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#26500;&#24314;&#27178;&#26102;&#23618;&#27425;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#36328;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#32500;&#24230;&#30340;&#25972;&#21512;&#22914;&#20309;&#22686;&#21152;&#39118;&#30005;&#22330;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36328;&#26102;&#38388;&#21327;&#35843;&#22312;&#22810;&#20010;&#26102;&#38388;&#27719;&#24635;&#20013;&#20248;&#20110;&#21333;&#29420;&#36328;&#27178;&#25130;&#38754;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36328;&#26102;&#21327;&#35843;&#39044;&#27979;&#34920;&#29616;&#20986;&#23545;&#36739;&#31895;&#26102;&#38388;&#32858;&#21512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.03472v2 Announce Type: replace  Abstract: Renewable energy generation is of utmost importance for global decarbonization. Forecasting renewable energies, particularly wind energy, is challenging due to the inherent uncertainty in wind energy generation, which depends on weather conditions. Recent advances in hierarchical forecasting through reconciliation have demonstrated a significant increase in the quality of wind energy forecasts for short-term periods. We leverage the cross-sectional and temporal hierarchical structure of turbines in wind farms and build cross-temporal hierarchies to further investigate how integrated cross-sectional and temporal dimensions can add value to forecast accuracy in wind farms. We found that cross-temporal reconciliation was superior to individual cross-sectional reconciliation at multiple temporal aggregations. Additionally, machine learning based forecasts that were cross-temporally reconciled demonstrated high accuracy at coarser tempora
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20803;&#22312;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#26089;&#26399;&#38454;&#27573;&#20250;&#23581;&#35797;&#19982;&#36755;&#20837;&#25968;&#25454;&#23545;&#40784;&#65292;&#23545;&#40784;&#26102;&#38388;&#19978;&#30028;&#20026;$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$&#65292;&#22312;&#23545;&#40784;&#38454;&#27573;&#36807;&#21518;&#25439;&#22833;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\frac{1}{t})$</title><link>https://arxiv.org/abs/2307.12851</link><description>&lt;p&gt;
&#20855;&#26377;&#23567;&#21021;&#22987;&#21270;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#26089;&#26399;&#31070;&#32463;&#20803;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12851
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#22312;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#26089;&#26399;&#38454;&#27573;&#20250;&#23581;&#35797;&#19982;&#36755;&#20837;&#25968;&#25454;&#23545;&#40784;&#65292;&#23545;&#40784;&#26102;&#38388;&#19978;&#30028;&#20026;$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$&#65292;&#22312;&#23545;&#40784;&#38454;&#27573;&#36807;&#21518;&#25439;&#22833;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\frac{1}{t})$
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#27969;&#21644;&#23567;&#21021;&#22987;&#21270;&#23545;&#21452;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#24102;&#26377;&#33391;&#22909;&#20998;&#31163;&#30340;&#36755;&#20837;&#21521;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65306;&#20219;&#20309;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#36755;&#20837;&#25968;&#25454;&#23545;&#27491;&#30456;&#20851;&#65292;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#36755;&#20837;&#25968;&#25454;&#23545;&#36127;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#31532;&#19968;&#23618;&#20013;&#30340;&#31070;&#32463;&#20803;&#35797;&#22270;&#19982;&#31532;&#20108;&#23618;&#19978;&#30340;&#26435;&#37325;&#23545;&#24212;&#30340;&#27491;&#25968;&#25454;&#25110;&#36127;&#25968;&#25454;&#23545;&#40784;&#12290;&#23545;&#31070;&#32463;&#20803;&#26041;&#21521;&#21160;&#24577;&#30340;&#20180;&#32454;&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$&#30340;&#26102;&#38388;&#19978;&#30028;&#65292;&#36825;&#26159;&#20026;&#20102;&#35753;&#25152;&#26377;&#31070;&#32463;&#20803;&#19982;&#36755;&#20837;&#25968;&#25454;&#33391;&#22909;&#23545;&#40784;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#20854;&#20013;$n$&#26159;&#25968;&#25454;&#28857;&#30340;&#20010;&#25968;&#65292;$\mu$&#34913;&#37327;&#25968;&#25454;&#20998;&#31163;&#30340;&#31243;&#24230;&#12290;&#22312;&#26089;&#26399;&#23545;&#40784;&#38454;&#27573;&#20043;&#21518;&#65292;&#25439;&#22833;&#20197;$\mathcal{O}(\frac{1}{t})$&#30340;&#36895;&#24230;&#25910;&#25947;&#21040;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.12851v2 Announce Type: replace  Abstract: This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#19968;&#31181;&#26032;&#22411;&#38544;&#31169;&#25915;&#20987;&#8212;&#8212;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#65288;CDA&#65289;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2306.08929</link><description>&lt;p&gt;
&#20851;&#20110;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#30340;&#38887;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the resilience of Collaborative Learning-based Recommender Systems Against Community Detection Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#19968;&#31181;&#26032;&#22411;&#38544;&#31169;&#25915;&#20987;&#8212;&#8212;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#65288;CDA&#65289;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#28304;&#20110;&#21327;&#20316;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#21644;&#20843;&#21350;&#23398;&#20064;&#65289;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#21442;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#21516;&#26102;&#22312;&#20854;&#35774;&#22791;&#19978;&#20445;&#30041;&#24050;&#28040;&#36153;&#39033;&#30446;&#30340;&#21382;&#21490;&#35760;&#24405;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20045;&#19968;&#30475;&#20284;&#20046;&#26377;&#21033;&#20110;&#20445;&#25252;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21327;&#20316;&#23398;&#20064;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#19968;&#31181;&#31216;&#20026;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#65288;CDA&#65289;&#30340;&#26032;&#22411;&#38544;&#31169;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;&#36825;&#31181;&#25915;&#20987;&#20351;&#24471;&#23545;&#25163;&#33021;&#22815;&#22522;&#20110;&#19968;&#20010;&#36873;&#25321;&#30340;&#39033;&#30446;&#38598;&#65288;&#22914;&#35782;&#21035;&#23545;&#29305;&#23450;&#20852;&#36259;&#28857;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#65289;&#26469;&#35782;&#21035;&#31038;&#21306;&#25104;&#21592;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#30495;&#23454;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08929v2 Announce Type: replace-cross  Abstract: Collaborative-learning-based recommender systems emerged following the success of collaborative learning techniques such as Federated Learning (FL) and Gossip Learning (GL). In these systems, users participate in the training of a recommender system while maintaining their history of consumed items on their devices. While these solutions seemed appealing for preserving the privacy of the participants at first glance, recent studies have revealed that collaborative learning can be vulnerable to various privacy attacks. In this paper, we study the resilience of collaborative learning-based recommender systems against a novel privacy attack called Community Detection Attack (CDA). This attack enables an adversary to identify community members based on a chosen set of items (eg., identifying users interested in specific points-of-interest). Through experiments on three real recommendation datasets using two state-of-the-art recomme
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#24565;&#35782;&#21035;&#25216;&#26415;&#20174;&#24222;&#22823;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#25968;&#25454;&#38598;&#20013;&#25214;&#21040;&#30456;&#20851;&#21644;&#21487;&#34892;&#30340;&#33021;&#28304;&#31649;&#29702;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2306.08318</link><description>&lt;p&gt;
&#20174;&#19968;&#32452;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#20013;&#35782;&#21035;&#33021;&#28304;&#31649;&#29702;&#37197;&#32622;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Identification of Energy Management Configuration Concepts from a Set of Pareto-optimal Solutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08318
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#24565;&#35782;&#21035;&#25216;&#26415;&#20174;&#24222;&#22823;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#25968;&#25454;&#38598;&#20013;&#25214;&#21040;&#30456;&#20851;&#21644;&#21487;&#34892;&#30340;&#33021;&#28304;&#31649;&#29702;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#26045;&#21644;&#24314;&#31569;&#29289;&#20013;&#23454;&#26045;&#36164;&#28304;&#39640;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#22312;&#21521;&#21487;&#25345;&#32493;&#31038;&#20250;&#36716;&#22411;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22810;&#20010;&#36890;&#24120;&#23384;&#22312;&#20914;&#31361;&#30446;&#26631;&#65288;&#22914;&#25104;&#26412;&#12289;&#23545;&#30005;&#32593;&#36816;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#31283;&#20581;&#24615;&#25110;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#65289;&#36873;&#25321;&#21512;&#36866;&#37197;&#32622;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#27010;&#24565;&#35782;&#21035;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#23558;&#37197;&#32622;&#36873;&#39033;&#20998;&#20026;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#32452;&#65288;&#27010;&#24565;&#65289;&#26469;&#24110;&#21161;&#20915;&#31574;&#32773;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#23558;&#30446;&#26631;&#21644;&#35774;&#35745;&#21442;&#25968;&#20998;&#25104;&#19981;&#21516;&#38598;&#21512;&#65288;&#31216;&#20026;&#25551;&#36848;&#31354;&#38388;&#65289;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#21033;&#29992;&#27010;&#24565;&#35782;&#21035;&#25216;&#26415;&#20174;&#19968;&#20010;&#38750;&#24120;&#24222;&#22823;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#25968;&#25454;&#38598;&#20013;&#25214;&#21040;&#30456;&#20851;&#21644;&#21487;&#34892;&#30340;&#33021;&#28304;&#31649;&#29702;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08318v2 Announce Type: replace  Abstract: Implementing resource efficient energy management systems in facilities and buildings becomes increasingly important in the transformation to a sustainable society. However, selecting a suitable configuration based on multiple, typically conflicting objectives, such as cost, robustness with respect to uncertainty of grid operation, or renewable energy utilization, is a difficult multi-criteria decision making problem. The recently developed concept identification technique can facilitate a decision maker by sorting configuration options into semantically meaningful groups (concepts). In this process, the partitioning of the objectives and design parameters into different sets (called description spaces) is a very important step. In this study we focus on utilizing the concept identification technique for finding relevant and viable energy management configurations from a very large data set of Pareto-optimal solutions. The data set c
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#35780;&#20272;&#39046;&#22495;&#27867;&#21270;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#25552;&#20986;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20351;&#29992;&#22810;&#20010;&#27979;&#35797;&#39046;&#22495;&#65292;&#20197;&#26356;&#20934;&#30830;&#35780;&#20272;OOD&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2305.15253</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39046;&#22495;&#27867;&#21270;&#30340;&#35780;&#20272;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluation Protocol of Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15253
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#39046;&#22495;&#27867;&#21270;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#25552;&#20986;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20351;&#29992;&#22810;&#20010;&#27979;&#35797;&#39046;&#22495;&#65292;&#20197;&#26356;&#20934;&#30830;&#35780;&#20272;OOD&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#30446;&#30340;&#26159;&#36890;&#36807;&#21033;&#29992;&#20174;&#22810;&#20010;&#35757;&#32451;&#39046;&#22495;&#23398;&#21040;&#30340;&#20849;&#21516;&#30693;&#35782;&#65292;&#35299;&#20915;&#38754;&#21521;&#26410;&#35265;&#27979;&#35797;&#39046;&#22495;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#25361;&#25112;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;OOD&#27867;&#21270;&#33021;&#21147;&#65292;&#38656;&#35201;&#27979;&#35797;&#25968;&#25454;&#20449;&#24687;&#19981;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39046;&#22495;&#27867;&#21270;&#21327;&#35758;&#20173;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#27979;&#35797;&#25968;&#25454;&#20449;&#24687;&#27844;&#28431;&#12290;&#26412;&#25991;&#20174;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#30340;&#20004;&#20010;&#26041;&#38754;&#65306;&#22312;ImageNet&#19978;&#36827;&#34892;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;oracle&#27169;&#22411;&#36873;&#25321;&#65292;&#25506;&#35752;&#27979;&#35797;&#25968;&#25454;&#20449;&#24687;&#27844;&#28431;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20462;&#25913;&#24403;&#21069;&#21327;&#35758;&#30340;&#24314;&#35758;&#65292;&#21363;&#24212;&#35813;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#24403;&#21069;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#24212;&#35813;&#20351;&#29992;&#22810;&#20010;&#27979;&#35797;&#39046;&#22495;&#12290;&#36825;&#23558;&#23548;&#33268;&#23545;OOD&#27867;&#21270;&#33021;&#21147;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#37325;&#26032;&#36816;&#34892;&#20102;&#24102;&#26377;&#20462;&#25913;&#21518;&#21327;&#35758;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15253v2 Announce Type: replace-cross  Abstract: Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is required that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the risks of test data information leakage from two aspects of the current evaluation protocol: supervised pretraining on ImageNet and oracle model selection. We propose modifications to the current protocol that we should employ self-supervised pretraining or train from scratch instead of employing the current supervised pretraining, and we should use multiple test domains. These would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the mod
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#33021;&#36829;&#21453;&#29256;&#26435;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#31639;&#27861;&#31283;&#23450;&#24615;&#25216;&#26415;&#20197;&#30830;&#20445;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2305.14822</link><description>&lt;p&gt;
&#29256;&#26435;&#26159;&#21542;&#21487;&#20197;&#31616;&#21270;&#20026;&#38544;&#31169;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Copyright be Reduced to Privacy?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14822
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#33021;&#36829;&#21453;&#29256;&#26435;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#31639;&#27861;&#31283;&#23450;&#24615;&#25216;&#26415;&#20197;&#30830;&#20445;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20154;&#36234;&#26469;&#36234;&#25285;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#20135;&#29983;&#19982;&#23427;&#20204;&#35757;&#32451;&#25152;&#20381;&#25454;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#38750;&#24120;&#30456;&#20284;&#30340;&#20135;&#20986;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#22823;&#24133;&#25552;&#39640;&#65292;&#24182;&#19988;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#26448;&#26009;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#19981;&#26029;&#25193;&#22823;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#25285;&#24551;&#21152;&#21095;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#32531;&#35299;&#29983;&#25104;&#20405;&#26435;&#26679;&#26412;&#39118;&#38505;&#30340;&#31574;&#30053;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24314;&#35758;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#31561;&#25216;&#26415;&#21644;&#20854;&#20182;&#24418;&#24335;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#26469;&#25552;&#20379;&#20851;&#20110;&#32570;&#20047;&#20405;&#26435;&#22797;&#21046;&#30340;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#31639;&#27861;&#31283;&#23450;&#24615;&#25216;&#26415;&#26159;&#21542;&#36866;&#21512;&#30830;&#20445;&#23545;&#29983;&#25104;&#24335;&#27169;&#22411;&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#65292;&#32780;&#19981;&#20250;&#26080;&#24847;&#20013;&#36829;&#21453;&#29256;&#26435;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#21487;&#35782;&#21035;&#20449;&#24687;&#30340;&#23384;&#22312;&#65292;&#22240;&#27492;&#26159;&#38754;&#21521;&#38544;&#31169;&#30340;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#20173;&#38656;&#35201;&#35299;&#20915;&#30340;&#21644;&#29256;&#26435;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14822v2 Announce Type: replace  Abstract: There is a growing concern that generative AI models will generate outputs closely resembling the copyrighted materials for which they are trained. This worry has intensified as the quality and complexity of generative models have immensely improved, and the availability of extensive datasets containing copyrighted material has expanded. Researchers are actively exploring strategies to mitigate the risk of generating infringing samples, with a recent line of work suggesting to employ techniques such as differential privacy and other forms of algorithmic stability to provide guarantees on the lack of infringing copying. In this work, we examine whether such algorithmic stability techniques are suitable to ensure the responsible use of generative models without inadvertently violating copyright laws. We argue that while these techniques aim to verify the presence of identifiable information in datasets, thus being privacy-oriented, cop
&lt;/p&gt;</description></item><item><title>LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2305.12519</link><description>&lt;p&gt;
LLM&#20146;&#23376;&#37492;&#23450;&#65306;LLM&#36951;&#20256;&#32487;&#25215;&#20013;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12519
&lt;/p&gt;
&lt;p&gt;
LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#25658;&#24102;&#21508;&#31181;&#28389;&#29992;&#39118;&#38505;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#25220;&#34989;&#12289;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#65292;&#25110;&#32773;&#21046;&#20316;&#24341;&#20154;&#27880;&#30446;&#30340;&#34394;&#20551;&#25512;&#25991;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20005;&#37325;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;LLM&#20146;&#23376;&#37492;&#23450;&#65288;LLM-Pat&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20219;&#20309;&#20505;&#36873;&#25991;&#26412;&#65288;"&#23376;&#31867;"&#65289;&#65292;LLM-Pat&#20351;&#29992;&#19968;&#20010;&#20013;&#38388;LLM&#65288;"&#29238;&#31867;"&#65289;&#37325;&#24314;&#19982;&#32473;&#23450;&#25991;&#26412;&#23545;&#24212;&#30340;"&#20804;&#24351;"&#25991;&#26412;&#65292;&#28982;&#21518;&#34913;&#37327;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;"&#20804;&#24351;"&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#39640;&#30456;&#20284;&#24615;&#34920;&#26126;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#31867;&#20284;&#20110;&#22522;&#22240;&#29305;&#24449;&#12290;&#25105;&#20204;&#24050;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12519v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to reconstruct a \textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encom
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25361;&#25112;&#20256;&#32479;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2303.05656</link><description>&lt;p&gt;
EHRDiff: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25506;&#32034;&#30495;&#23454;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25361;&#25112;&#20256;&#32479;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#21253;&#21547;&#20016;&#23500;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#65292;&#26159;&#21457;&#23637;&#31934;&#20934;&#21307;&#23398;&#31995;&#32479;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#25285;&#24551;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#33719;&#21462;&#39640;&#36136;&#37327;&#21644;&#22823;&#35268;&#27169;EHR&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#26041;&#27861;&#30740;&#21457;&#30340;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#21512;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#20854;&#21464;&#20307;&#29992;&#20110;EHR&#32508;&#21512;&#12290;&#23613;&#31649;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;EHR&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#35757;&#32451;&#19988;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#22349;&#22604;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#24314;&#27169;&#20013;&#26368;&#26032;&#24341;&#20837;&#30340;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#23574;&#31471;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;EHR&#25968;&#25454;&#32508;&#21512;&#26041;&#38754;&#30340;&#25928;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05656v2 Announce Type: replace  Abstract: Electronic health records (EHR) contain a wealth of biomedical information, serving as valuable resources for the development of precision medicine systems. However, privacy concerns have resulted in limited access to high-quality and large-scale EHR data for researchers, impeding progress in methodological development. Recent research has delved into synthesizing realistic EHR data through generative modeling techniques, where a majority of proposed methods relied on generative adversarial networks (GAN) and their variants for EHR synthesis. Despite GAN-based methods attaining state-of-the-art performance in generating EHR data, these approaches are difficult to train and prone to mode collapse. Recently introduced in generative modeling, diffusion models have established cutting-edge performance in image generation, but their efficacy in EHR data synthesis remains largely unexplored. In this study, we investigate the potential of d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#20108;&#36827;&#21046;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#20271;&#23572;&#26364;&#20998;&#24067;&#31934;&#30830;&#26144;&#23556;&#20026;&#33258;&#22238;&#24402;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;ARNN&#26550;&#26500;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#29289;&#29702;&#21547;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#25216;&#26415;&#25512;&#23548;&#20986;&#29305;&#23450;&#31995;&#32479;&#30340;&#26032;ARNN&#12290;</title><link>https://arxiv.org/abs/2302.08347</link><description>&lt;p&gt;
&#33258;&#20271;&#23572;&#26364;&#20998;&#24067;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#33258;&#26059;&#31995;&#32479;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
The autoregressive neural network architecture of the Boltzmann distribution of pairwise interacting spins systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.08347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#20108;&#36827;&#21046;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#20271;&#23572;&#26364;&#20998;&#24067;&#31934;&#30830;&#26144;&#23556;&#20026;&#33258;&#22238;&#24402;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;ARNN&#26550;&#26500;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#29289;&#29702;&#21547;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#25216;&#26415;&#25512;&#23548;&#20986;&#29305;&#23450;&#31995;&#32479;&#30340;&#26032;ARNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2302.08347v3 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328; Abstract: &#29983;&#25104;&#24335;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;ARNNs&#65289;&#26368;&#36817;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26480;&#20986;&#30340;&#32467;&#26524;&#65292;&#20419;&#25104;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#24212;&#29992;&#20013;&#26085;&#30410;&#27969;&#34892;&#12290;&#26412;&#24037;&#20316;&#23558;&#20108;&#36827;&#21046;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#20271;&#23572;&#26364;&#20998;&#24067;&#31934;&#30830;&#26144;&#23556;&#20026;&#33258;&#22238;&#24402;&#24418;&#24335;&#12290;&#32467;&#26524;&#30340;ARNN&#26550;&#26500;&#20855;&#26377;&#19982;&#21704;&#23494;&#39039;&#32806;&#21512;&#21644;&#22806;&#22330;&#23545;&#24212;&#30340;&#31532;&#19968;&#23618;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#20855;&#26377;&#35832;&#22914;&#27531;&#24046;&#36830;&#25509;&#21644;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#29289;&#29702;&#21547;&#20041;&#30340;&#36882;&#24402;&#26550;&#26500;&#31561;&#24191;&#27867;&#20351;&#29992;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20854;&#26550;&#26500;&#30340;&#26126;&#30830;&#34920;&#36848;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#25216;&#26415;&#25512;&#23548;&#29305;&#23450;&#31995;&#32479;&#30340;&#26032;ARNN&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#20004;&#20010;&#30693;&#21517;&#30340;&#24179;&#22343;&#22330;&#31995;&#32479;&#65292;&#23621;&#37324;-&#39759;&#26031;&#21644;Sherrington-Kirkpatrick&#27169;&#22411;&#65292;&#23548;&#20986;&#20102;&#26032;&#30340;&#26377;&#25928;ARNN&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.08347v3 Announce Type: replace-cross  Abstract: Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated exceptional results in image and language generation tasks, contributing to the growing popularity of generative models in both scientific and commercial applications. This work presents an exact mapping of the Boltzmann distribution of binary pairwise interacting systems into autoregressive form. The resulting ARNN architecture has weights and biases of its first layer corresponding to the Hamiltonian's couplings and external fields, featuring widely used structures such as the residual connections and a recurrent architecture with clear physical meanings. Moreover, its architecture's explicit formulation enables the use of statistical physics techniques to derive new ARNNs for specific systems. As examples, new effective ARNN architectures are derived from two well-known mean-field systems, the Curie-Weiss and Sherrington-Kirkpatrick models, showing 
&lt;/p&gt;</description></item><item><title>LMC&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2302.00924</link><description>&lt;p&gt;
LMC: &#22522;&#20110;&#23376;&#22270;&#25277;&#26679;&#30340;GNN&#24555;&#36895;&#35757;&#32451;&#19982;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00924
&lt;/p&gt;
&lt;p&gt;
LMC&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;GNNs&#23384;&#22312;&#30528;&#20247;&#25152;&#21608;&#30693;&#30340;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#21363;&#33410;&#28857;&#19982;&#28040;&#24687;&#20256;&#36882;&#23618;&#25968;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#26159;&#19968;&#31867;&#26377;&#21069;&#36884;&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#20013;&#20002;&#24323;&#23567;&#25209;&#37327;&#20043;&#22806;&#30340;&#28040;&#24687;&#26469;&#36991;&#20813;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#20294;&#36825;&#20250;&#29306;&#29298;&#26799;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#65292;&#21363;&#23616;&#37096;&#28040;&#24687;&#34917;&#20607;&#65288;LMC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00924v3 Announce Type: replace  Abstract: The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods -- a promising class of mini-batch training techniques -- discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the {\it first} subgraph-wise sampling method with provab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#21253;&#25324;&#28508;&#22312;&#36807;&#31243;&#22312;&#20869;&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26410;&#30693;&#37327;&#12290;</title><link>https://arxiv.org/abs/2301.12528</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#39034;&#24207;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sequential Estimation of Gaussian Process-based Deep State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#21253;&#25324;&#28508;&#22312;&#36807;&#31243;&#22312;&#20869;&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26410;&#30693;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#21253;&#25324;&#20989;&#25968;&#20272;&#35745;&#21644;&#27169;&#22411;&#30340;&#28508;&#22312;&#36807;&#31243;&#22312;&#20869;&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26410;&#30693;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#30340;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#30340;&#39640;&#26031;&#21644;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#26377;&#20004;&#32452;&#26410;&#30693;&#37327;&#65292;&#39640;&#24230;&#38750;&#32447;&#24615;&#26410;&#30693;&#37327;&#65288;&#28508;&#22312;&#36807;&#31243;&#30340;&#20540;&#65289;&#21644;&#26465;&#20214;&#32447;&#24615;&#26410;&#30693;&#37327;&#65288;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#30340;&#39640;&#26031;&#36807;&#31243;&#30340;&#24120;&#25968;&#21442;&#25968;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#30340;&#39640;&#26031;&#36807;&#31243;&#30340;&#21442;&#25968;&#22312;&#33719;&#24471;&#29366;&#24577;&#30340;&#39044;&#27979;&#23494;&#24230;&#26102;&#34987;&#25972;&#21512;&#22312;&#22806;&#65292;&#19981;&#38656;&#35201;&#31890;&#23376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#38598;&#21512;&#29256;&#26412;&#65292;&#20854;&#20013;&#38598;&#21512;&#30340;&#27599;&#20010;&#25104;&#21592;&#37117;&#26377;&#33258;&#24049;&#30340;&#29305;&#24449;&#38598;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#36319;&#36394;&#28508;&#22312;&#36807;&#31243;&#30452;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12528v2 Announce Type: replace  Abstract: We consider the problem of sequential estimation of the unknowns of state-space and deep state-space models that include estimation of functions and latent processes of the models. The proposed approach relies on Gaussian and deep Gaussian processes that are implemented via random feature-based Gaussian processes. In these models, we have two sets of unknowns, highly nonlinear unknowns (the values of the latent processes) and conditionally linear unknowns (the constant parameters of the random feature-based Gaussian processes). We present a method based on particle filtering where the parameters of the random feature-based Gaussian processes are integrated out in obtaining the predictive density of the states and do not need particles. We also propose an ensemble version of the method, with each member of the ensemble having its own set of features. With several experiments, we show that the method can track the latent processes up t
&lt;/p&gt;</description></item><item><title>FreshGNN&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;GNN&#23567;&#25209;&#37327;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#23450;&#30340;&#21382;&#21490;&#32531;&#23384;&#23384;&#20648;&#21644;&#37325;&#22797;&#20351;&#29992;GNN&#33410;&#28857;&#23884;&#20837;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;GNN&#27169;&#22411;&#26102;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26694;&#26550;&#23384;&#22312;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2301.07482</link><description>&lt;p&gt;
FreshGNN: &#36890;&#36807;&#31283;&#23450;&#30340;&#21382;&#21490;&#23884;&#20837;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.07482
&lt;/p&gt;
&lt;p&gt;
FreshGNN&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;GNN&#23567;&#25209;&#37327;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#23450;&#30340;&#21382;&#21490;&#32531;&#23384;&#23384;&#20648;&#21644;&#37325;&#22797;&#20351;&#29992;GNN&#33410;&#28857;&#23884;&#20837;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;GNN&#27169;&#22411;&#26102;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26694;&#26550;&#23384;&#22312;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#26102;&#30340;&#19968;&#20010;&#20851;&#38190;&#24615;&#33021;&#29942;&#39048;&#26159;&#23558;&#33410;&#28857;&#29305;&#24449;&#21152;&#36733;&#21040;GPU&#19978;&#12290;&#30001;&#20110;GPU&#20869;&#23384;&#26377;&#38480;&#65292;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#25968;&#25454;&#31227;&#21160;&#65292;&#20197;&#20415;&#22312;&#36895;&#24230;&#36739;&#24930;&#30340;&#22791;&#29992;&#35774;&#22791;&#19978;&#23384;&#20648;&#36825;&#20123;&#29305;&#24449;&#65288;&#20363;&#22914;CPU&#20869;&#23384;&#65289;&#12290;&#27492;&#22806;&#65292;&#22270;&#32467;&#26500;&#30340;&#19981;&#35268;&#21017;&#24615;&#23548;&#33268;&#25968;&#25454;&#23616;&#37096;&#24615;&#24046;&#65292;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#33021;&#22815;&#39640;&#25928;&#35757;&#32451;&#22823;&#22411;GNN&#27169;&#22411;&#30340;&#29616;&#26377;&#26694;&#26550;&#36890;&#24120;&#30001;&#20110;&#28041;&#21450;&#24403;&#21069;&#21487;&#29992;&#30340;&#24555;&#25463;&#26041;&#24335;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290; &#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreshGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;GNN&#23567;&#25209;&#37327;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21382;&#21490;&#32531;&#23384;&#26469;&#23384;&#20648;&#21644;&#37325;&#22797;&#20351;&#29992;GNN&#33410;&#28857;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#25552;&#21462;&#21407;&#22987;&#29305;&#24449;&#26469;&#37325;&#26032;&#35745;&#31639;&#23427;&#20204;&#12290; &#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#30456;&#24212;&#30340;&#32531;&#23384;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07482v3 Announce Type: replace  Abstract: A key performance bottleneck when training graph neural network (GNN) models on large, real-world graphs is loading node features onto a GPU. Due to limited GPU memory, expensive data movement is necessary to facilitate the storage of these features on alternative devices with slower access (e.g. CPU memory). Moreover, the irregularity of graph structures contributes to poor data locality which further exacerbates the problem. Consequently, existing frameworks capable of efficiently training large GNN models usually incur a significant accuracy degradation because of the currently-available shortcuts involved. To address these limitations, we instead propose FreshGNN, a general-purpose GNN mini-batch training framework that leverages a historical cache for storing and reusing GNN node embeddings instead of re-computing them through fetching raw features at every iteration. Critical to its success, the corresponding cache policy is de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Masked Vector Quantization&#65288;MVQ&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25513;&#30721;&#37197;&#32622;&#24182;&#20351;&#29992;Multiple Hypothese Dropout&#65288;MH-Dropout&#65289;&#35757;&#32451;&#21046;&#24230;&#65292;&#22686;&#21152;&#20102;&#27599;&#20010;&#20195;&#30721;&#21521;&#37327;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2301.06626</link><description>&lt;p&gt;
&#25513;&#30721;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Masked Vector Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.06626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Masked Vector Quantization&#65288;MVQ&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25513;&#30721;&#37197;&#32622;&#24182;&#20351;&#29992;Multiple Hypothese Dropout&#65288;MH-Dropout&#65289;&#35757;&#32451;&#21046;&#24230;&#65292;&#22686;&#21152;&#20102;&#27599;&#20010;&#20195;&#30721;&#21521;&#37327;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#30340;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#23637;&#31034;&#20102;&#23398;&#20064;&#22797;&#26434;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#27599;&#20010;&#23454;&#20363;&#30340;&#38271;&#24207;&#21015;&#26631;&#35760;&#21644;&#22823;&#37327;&#30721;&#20070;&#26465;&#30446;&#65292;&#23548;&#33268;&#38271;&#37319;&#26679;&#26102;&#38388;&#21644;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#26469;&#25311;&#21512;&#20998;&#31867;&#21518;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25513;&#30721;&#21521;&#37327;&#37327;&#21270;&#65288;MVQ&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25513;&#30721;&#37197;&#32622;&#65292;&#36890;&#36807;&#31216;&#20026;&#22810;&#20551;&#35774;&#20002;&#22833;&#65288;MH-Dropout&#65289;&#30340;&#38543;&#26426;&#32988;&#32773;&#36890;&#21507;&#35757;&#32451;&#21046;&#24230;&#65292;&#22686;&#21152;&#27599;&#20010;&#20195;&#30721;&#21521;&#37327;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;ImageNet 64&#215;64&#19978;&#65292;MVQ&#23558;&#29616;&#26377;&#21521;&#37327;&#37327;&#21270;&#26550;&#26500;&#20013;&#30340;FID&#38477;&#20302;&#20102;&#39640;&#36798;68%&#65288;&#27599;&#23454;&#20363;2&#20010;&#26631;&#35760;&#65289;&#21644;57%&#65288;&#27599;&#23454;&#20363;5&#20010;&#26631;&#35760;&#65289;&#12290;&#36825;&#20123;&#25913;&#36827;&#38543;&#30528;&#20943;&#23569;&#30721;&#20070;&#26465;&#30446;&#30340;&#20195;&#30721;&#26465;&#30446;&#32780;&#25193;&#22823;&#65292;&#24182;&#19988;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#21487;&#20197;&#23454;&#29616;7-45&#20493;&#30340;&#26631;&#35760;&#37319;&#26679;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.06626v2 Announce Type: replace  Abstract: Generative models with discrete latent representations have recently demonstrated an impressive ability to learn complex high-dimensional data distributions. However, their performance relies on a long sequence of tokens per instance and a large number of codebook entries, resulting in long sampling times and considerable computation to fit the categorical posterior. To address these issues, we propose the Masked Vector Quantization (MVQ) framework which increases the representational capacity of each code vector by learning mask configurations via a stochastic winner-takes-all training regime called Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\times$64, MVQ reduces FID in existing vector quantization architectures by up to $68\%$ at 2 tokens per instance and $57\%$ at 5 tokens. These improvements widen as codebook entries is reduced and allows for $7\textit{--}45\times$ speed-up in token sampling during inference. As an 
&lt;/p&gt;</description></item><item><title>&#20998;&#24067;&#40065;&#26834;&#24615;&#30028;&#23450;&#20102;&#27867;&#21270;&#38169;&#35823;&#65292;Bayesian&#26041;&#27861;&#22312;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24847;&#20041;&#19978;&#26159;&#20998;&#24067;&#40065;&#26834;&#30340;&#65292;&#21516;&#26102;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#20063;&#34987;&#35777;&#26126;&#26159;&#31561;&#20215;&#20110;Bayesian&#26041;&#27861;&#30340;&#12290;</title><link>https://arxiv.org/abs/2212.09962</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#24615;&#30028;&#23450;&#20102;&#27867;&#21270;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Distributional Robustness Bounds Generalization Errors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.09962
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#24615;&#30028;&#23450;&#20102;&#27867;&#21270;&#38169;&#35823;&#65292;Bayesian&#26041;&#27861;&#22312;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24847;&#20041;&#19978;&#26159;&#20998;&#24067;&#40065;&#26834;&#30340;&#65292;&#21516;&#26102;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#20063;&#34987;&#35777;&#26126;&#26159;&#31561;&#20215;&#20110;Bayesian&#26041;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bayesian methods, distributionally robust optimization methods, and regularization methods&#26159;&#20540;&#24471;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#29992;&#20110;&#25269;&#25239;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#65292;&#27604;&#22914;&#32463;&#39564;&#20998;&#24067;&#19982;&#30495;&#23454;&#22522;&#30784;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19977;&#31181;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#29305;&#21035;&#22320;&#25506;&#35752;&#20102;&#20026;&#20309;&#36825;&#20123;&#26694;&#26550;&#20542;&#21521;&#20110;&#20855;&#26377;&#26356;&#23567;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;&#20855;&#20307;&#22320;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20998;&#24067;&#40065;&#26834;&#24615;&#8221;&#30340;&#23450;&#37327;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#8220;&#40065;&#26834;&#24615;&#24230;&#37327;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#20013;&#30340;&#20960;&#20010;&#21746;&#23398;&#27010;&#24565;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#34920;&#26126;Bayesian&#26041;&#27861;&#22312;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24847;&#20041;&#19978;&#26159;&#20998;&#24067;&#40065;&#26834;&#30340;&#65307;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#36896;&#31867;&#20284;Dirichlet&#36807;&#31243;&#30340;&#20808;&#39564;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#35777;&#26126;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#31561;&#20215;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.09962v3 Announce Type: replace  Abstract: Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for "distributional robustness", propose the concept of "robustness measure", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equival
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Covariate-Assisted Ranking Estimation (CARE) &#27169;&#22411;&#65292;&#25193;&#23637;&#20102; Bradley-Terry-Luce (BTL) &#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21327;&#21464;&#37327;&#20449;&#24687;&#32467;&#21512;&#36827;&#25490;&#21517;&#20272;&#35745;&#20013;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#25490;&#21517;&#38382;&#39064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2212.09961</link><description>&lt;p&gt;
&#23454;&#20307;&#25490;&#21517;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#22806;&#29983;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification of MLE for Entity Ranking with Covariates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.09961
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Covariate-Assisted Ranking Estimation (CARE) &#27169;&#22411;&#65292;&#25193;&#23637;&#20102; Bradley-Terry-Luce (BTL) &#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21327;&#21464;&#37327;&#20449;&#24687;&#32467;&#21512;&#36827;&#25490;&#21517;&#20272;&#35745;&#20013;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#25490;&#21517;&#38382;&#39064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#21644;&#39069;&#22806;&#21327;&#21464;&#37327;&#20449;&#24687;&#65288;&#22914;&#25152;&#27604;&#36739;&#39033;&#30446;&#30340;&#23646;&#24615;&#65289;&#30340;&#25490;&#21517;&#38382;&#39064;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20808;&#21069;&#30340;&#25991;&#29486;&#20013;&#24456;&#23569;&#26377;&#20154;&#22312;&#21327;&#21464;&#37327;&#20449;&#24687;&#23384;&#22312;&#30340;&#26356;&#29616;&#23454;&#24773;&#22659;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21363; Covariate-Assisted Ranking Estimation (CARE) &#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#21327;&#21464;&#37327;&#20449;&#24687;&#25193;&#23637;&#20102;&#33879;&#21517;&#30340; Bradley-Terry-Luce (BTL) &#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#27604;&#36739;&#39033;&#30446;&#30340;&#28508;&#22312;&#20998;&#25968;&#19981;&#26159;&#22266;&#23450;&#30340; $\{\theta_i^*\}_{i=1}^n$&#65292;&#32780;&#26159;&#30001; $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$ &#32473;&#20986;&#65292;&#20854;&#20013; $\alpha_i^*$ &#21644; ${x}_i^\top\beta^*$ &#20998;&#21035;&#20195;&#34920;&#31532; $i$ &#20010;&#39033;&#30446;&#30340;&#28508;&#22312;&#22522;&#20934;&#20998;&#25968;&#21644;&#21327;&#21464;&#37327;&#20998;&#25968;&#12290;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#28982;&#30340;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#65292;&#24182;&#25512;&#23548;&#20102; $\ell_{\infty}$-
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.09961v2 Announce Type: replace-cross  Abstract: This paper concerns with statistical estimation and inference for the ranking problems based on pairwise comparisons with additional covariate information such as the attributes of the compared items. Despite extensive studies, few prior literatures investigate this problem under the more realistic setting where covariate information exists. To tackle this issue, we propose a novel model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate information. Specifically, instead of assuming every compared item has a fixed latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and ${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th item, respectively. We impose natural identifiability conditions and derive the $\ell_{\infty}$-
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#21644;&#20854;&#20182;&#30693;&#35782;&#26469;&#28304;&#22635;&#34917;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#22312;&#26085;&#24120;&#30693;&#35782;&#29702;&#35299;&#26041;&#38754;&#30340;&#24046;&#36317;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.12328</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#22810;&#27169;&#24577;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on knowledge-enhanced multimodal learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.12328
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#21644;&#20854;&#20182;&#30693;&#35782;&#26469;&#28304;&#22635;&#34917;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#22312;&#26085;&#24120;&#30693;&#35782;&#29702;&#35299;&#26041;&#38754;&#30340;&#24046;&#36317;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#23558;&#21508;&#31181;&#27169;&#24577;&#32467;&#21512;&#25104;&#19968;&#20010;&#32852;&#21512;&#34920;&#31034;&#12290;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#22810;&#31181;&#27169;&#22411;&#21644;&#25216;&#26415;&#65292;&#38024;&#23545;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;VL&#27169;&#22411;&#36890;&#36807;&#25193;&#23637;Transformer&#30340;&#24605;&#24819;&#65292;&#20351;&#24471;&#20004;&#31181;&#27169;&#24577;&#21487;&#20197;&#30456;&#20114;&#23398;&#20064;&#65292;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#20351;&#24471;VL&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#19968;&#23450;&#27700;&#24179;&#30340;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#65292;&#23613;&#31649;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#24046;&#36317;&#65306;&#23545;&#24120;&#35782;&#12289;&#20107;&#23454;&#12289;&#26102;&#38388;&#21644;&#20854;&#20182;&#26085;&#24120;&#30693;&#35782;&#26041;&#38754;&#30340;&#38480;&#21046;&#29702;&#35299;&#65292;&#23545;VL&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#30693;&#35782;&#22270;&#21644;&#20854;&#20182;&#30693;&#35782;&#26469;&#28304;&#21487;&#20197;&#36890;&#36807;&#26126;&#30830;&#25552;&#20379;&#32570;&#22833;&#20449;&#24687;&#26469;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#35299;&#38145;VL&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30693;&#35782;&#22270;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.12328v3 Announce Type: replace-cross  Abstract: Multimodal learning has been a field of increasing interest, aiming to combine various modalities in a single joint representation. Especially in the area of visiolinguistic (VL) learning multiple models and techniques have been developed, targeting a variety of tasks that involve images and text. VL models have reached unprecedented performances by extending the idea of Transformers, so that both modalities can learn from each other. Massive pre-training procedures enable VL models to acquire a certain level of real-world understanding, although many gaps can be identified: the limited comprehension of commonsense, factual, temporal and other everyday knowledge aspects questions the extendability of VL tasks. Knowledge graphs and other knowledge sources can fill those gaps by explicitly providing missing information, unlocking novel capabilities of VL models. In the same time, knowledge graphs enhance explainability, fairness 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32469;&#36807;&#20256;&#32479;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;&#21644;&#30913;&#20598;&#26497;&#27169;&#22411;&#30340;&#38480;&#21046;&#65292;&#30452;&#25509;&#25512;&#26029;&#30913;&#26631;&#35760;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#65292;&#25552;&#39640;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;</title><link>https://arxiv.org/abs/2211.07556</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;5&#33258;&#30001;&#24230;&#30913;&#26631;&#35760;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Utilizing Synthetic Data in Supervised Learning for Robust 5-DoF Magnetic Marker Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.07556
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32469;&#36807;&#20256;&#32479;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;&#21644;&#30913;&#20598;&#26497;&#27169;&#22411;&#30340;&#38480;&#21046;&#65292;&#30452;&#25509;&#25512;&#26029;&#30913;&#26631;&#35760;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#65292;&#25552;&#39640;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#36394;&#34987;&#21160;&#30913;&#26631;&#35760;&#22312;&#25512;&#21160;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26377;&#26395;&#26174;&#33879;&#25552;&#39640;&#31995;&#32479;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#30001;&#20110;&#38656;&#35201;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#65292;&#30913;&#26631;&#35760;&#30340;&#36319;&#36394;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#30913;&#20598;&#26497;&#27169;&#22411;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#30001;&#20110;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#29699;&#24418;&#30913;&#38081;&#21644;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30701;&#36317;&#31163;&#26102;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#32467;&#26524;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#35268;&#36991;&#36825;&#20123;&#38480;&#21046;&#65292;&#30452;&#25509;&#25512;&#26029;&#26631;&#35760;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.07556v2 Announce Type: replace  Abstract: Tracking passive magnetic markers plays a vital role in advancing healthcare and robotics, offering the potential to significantly improve the precision and efficiency of systems. This technology is key to developing smarter, more responsive tools and devices, such as enhanced surgical instruments, precise diagnostic tools, and robots with improved environmental interaction capabilities. However, traditionally, the tracking of magnetic markers is computationally expensive due to the requirement for iterative optimization procedures. Moreover, these methods depend on the magnetic dipole model for their optimization function, which can yield imprecise outcomes due to the model's significant inaccuracies when dealing with short distances between non-spherical magnet and sensor.Our paper introduces a novel approach that leverages neural networks to bypass these limitations, directly inferring the marker's position and orientation to accu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#34892;&#35786;&#26029;&#30340;&#30410;&#22788;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;&#24182;&#22312;&#32437;&#21521;&#30524;&#24213;&#29031;&#29255;&#20013;&#26816;&#27979;&#26089;&#26399;DR&#20005;&#37325;&#31243;&#24230;&#21464;&#21270;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;AUC&#12290;</title><link>https://arxiv.org/abs/2209.00915</link><description>&lt;p&gt;
&#20351;&#29992;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection of diabetic retinopathy using longitudinal self-supervised learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#34892;&#35786;&#26029;&#30340;&#30410;&#22788;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;&#24182;&#22312;&#32437;&#21521;&#30524;&#24213;&#29031;&#29255;&#20013;&#26816;&#27979;&#26089;&#26399;DR&#20005;&#37325;&#31243;&#24230;&#21464;&#21270;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#25104;&#20687;&#33021;&#22815;&#25429;&#25417;&#38745;&#24577;&#35299;&#21078;&#32467;&#26500;&#21644;&#30142;&#30149;&#36827;&#23637;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#23454;&#29616;&#23545;&#30142;&#30149;&#30340;&#26356;&#26089;&#21644;&#26356;&#22909;&#30340;&#20010;&#20307;&#21270;&#30149;&#29702;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26816;&#27979;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#30340;&#26041;&#27861;&#24456;&#23569;&#21033;&#29992;&#32437;&#21521;&#20449;&#24687;&#26469;&#25913;&#21892;DR&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#20855;&#26377;&#32437;&#21521;&#24615;&#36136;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#36827;&#34892;DR&#35786;&#26029;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;LSSL&#65289;&#26041;&#27861;&#65292;&#29992;&#26469;&#27169;&#25311;&#20174;&#32437;&#21521;&#35270;&#32593;&#33180;&#24425;&#33394;&#30524;&#24213;&#29031;&#29255;&#65288;CFP&#65289;&#20013;&#26816;&#27979;&#26089;&#26399;DR&#20005;&#37325;&#31243;&#24230;&#21464;&#21270;&#65292;&#20351;&#29992;&#19968;&#23545;&#36830;&#32493;&#26816;&#26597;&#12290;&#23454;&#39564;&#26159;&#22312;&#19968;&#20010;&#32437;&#21521;DR&#31579;&#26597;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#26377;&#25110;&#27809;&#26377;&#37027;&#20123;&#35757;&#32451;&#22909;&#30340;&#32534;&#30721;&#22120;&#65288;LSSL&#65289;&#20316;&#20026;&#32437;&#21521;&#20551;&#35774;&#20219;&#21153;&#12290;&#32467;&#26524;&#22312;&#22522;&#32447;&#65288;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#27169;&#22411;&#65289;&#19978;&#23454;&#29616;&#20102;0.875&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00915v3 Announce Type: replace-cross  Abstract: Longitudinal imaging is able to capture both static anatomical structures and dynamic changes in disease progression towards earlier and better patient-specific pathology management. However, conventional approaches for detecting diabetic retinopathy (DR) rarely take advantage of longitudinal information to improve DR analysis. In this work, we investigate the benefit of exploiting self-supervised learning with a longitudinal nature for DR diagnosis purposes. We compare different longitudinal self-supervised learning (LSSL) methods to model the disease progression from longitudinal retinal color fundus photographs (CFP) to detect early DR severity changes using a pair of consecutive exams. The experiments were conducted on a longitudinal DR screening dataset with or without those trained encoders (LSSL) acting as a longitudinal pretext task. Results achieve an AUC of 0.875 for the baseline (model trained from scratch) and an AU
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35760;&#24518;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20808;&#21069;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20135;&#29983;&#35823;&#23548;&#24615;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#26469;&#35780;&#20272;&#20010;&#20307;&#26679;&#26412;&#30340;&#35760;&#24518;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2208.08270</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#22686;&#24378;&#23545;&#38544;&#31169;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Privacy Effect of Data Enhancement via the Lens of Memorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.08270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35760;&#24518;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20808;&#21069;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20135;&#29983;&#35823;&#23548;&#24615;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#26469;&#35780;&#20272;&#20010;&#20307;&#26679;&#26412;&#30340;&#35760;&#24518;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#24050;&#32463;&#26174;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#25581;&#31034;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290; &#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65288;&#22312;&#35770;&#25991;&#20013;&#31216;&#20026;&#25968;&#25454;&#22686;&#24378;&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#12290; &#36825;&#31181;&#38544;&#31169;&#25928;&#24212;&#36890;&#24120;&#36890;&#36807;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#26469;&#34913;&#37327;&#65292;&#20854;&#30446;&#30340;&#26159;&#30830;&#23450;&#29305;&#23450;&#31034;&#20363;&#26159;&#21542;&#23646;&#20110;&#35757;&#32451;&#38598;&#12290; &#25105;&#20204;&#25552;&#20986;&#20174;&#31216;&#20026;&#35760;&#24518;&#30340;&#26032;&#35270;&#35282;&#26469;&#30740;&#31350;&#38544;&#31169;&#12290; &#36890;&#36807;&#35760;&#24518;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#37096;&#32626;&#30340;MIAs&#20135;&#29983;&#35823;&#23548;&#24615;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#22826;&#21487;&#33021;&#35782;&#21035;&#39640;&#38544;&#31169;&#39118;&#38505;&#26679;&#26412;&#26159;&#21542;&#20316;&#20026;&#25104;&#21592;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#35782;&#21035;&#20302;&#38544;&#31169;&#39118;&#38505;&#26679;&#26412;&#26356;&#23481;&#26131;&#12290; &#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#37096;&#32626;&#20102;&#19968;&#31181;&#26368;&#36817;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#25429;&#33719;&#20010;&#20307;&#26679;&#26412;&#30340;&#35760;&#24518;&#31243;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.08270v3 Announce Type: replace  Abstract: Machine learning poses severe privacy concerns as it has been shown that the learned models can reveal sensitive information about their training data. Many works have investigated the effect of widely adopted data augmentation and adversarial training techniques, termed data enhancement in the paper, on the privacy leakage of machine learning models. Such privacy effects are often measured by membership inference attacks (MIAs), which aim to identify whether a particular example belongs to the training set or not. We propose to investigate privacy from a new perspective called memorization. Through the lens of memorization, we find that previously deployed MIAs produce misleading results as they are less likely to identify samples with higher privacy risks as members compared to samples with low privacy risks. To solve this problem, we deploy a recent attack that can capture individual samples' memorization degrees for evaluation. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21305;&#37197;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21644;&#21160;&#24577;softassign&#31574;&#30053;&#65292;&#33021;&#22815;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2208.08233</link><description>&lt;p&gt;
&#22270;&#21305;&#37197;&#30340;&#21160;&#24577;softassign&#21644;&#33258;&#36866;&#24212;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Dynamical softassign and adaptive parameter tuning for graph matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.08233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21305;&#37197;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21644;&#21160;&#24577;softassign&#31574;&#30053;&#65292;&#33021;&#22815;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#30340;&#22270;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#30340;&#27969;&#34892;&#31639;&#27861;&#21253;&#25324;&#36882;&#24402;&#20998;&#37197;&#65288;GA&#65289;&#12289;&#25972;&#25968;&#25237;&#24433;&#22266;&#23450;&#28857;&#27861;&#65288;IPFP&#65289;&#21644;&#21452;&#38543;&#26426;&#25237;&#24433;&#22266;&#23450;&#28857;&#27861;&#65288;DSPFP&#65289;&#12290; &#36825;&#20123;&#31639;&#27861;&#22312;&#27493;&#38271;&#21442;&#25968;&#21644;&#32422;&#26463;&#31639;&#23376;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#22522;&#30784;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#22686;&#24378;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#21305;&#37197;&#20013;&#65292;&#26368;&#20248;&#27493;&#38271;&#21442;&#25968;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#20026;1&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31574;&#30053;&#26469;&#22788;&#29702;softassign&#36825;&#19968;&#27969;&#34892;&#32422;&#26463;&#31639;&#23376;&#22312;&#33410;&#28857;&#22522;&#25968;&#21644;&#28322;&#20986;&#39118;&#38505;&#26041;&#38754;&#30340;&#25935;&#24863;&#24615;&#12290; &#32467;&#21512;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21644;&#21160;&#24577;softassign&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21305;&#37197;&#31639;&#27861;&#65306;softas
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.08233v3 Announce Type: replace-cross  Abstract: This paper studies a unified framework for graph matching problems called the constrained gradient method. Popular algorithms within this framework include graduated assignment (GA), integer projected fixed-point method (IPFP), and doubly stochastic projected fixed-point method (DSPFP). These algorithms differ from the step size parameter and constrained operator. Our contributed adaptive step size parameter can guarantee the underlying algorithms' convergence and enhance their efficiency and accuracy. A preliminary analysis suggests that the optimal step size parameter has a high probability of being 1 in fully connected graph matching. Secondly, we propose a dynamic strategy for softassign, a popular constrained operator, to address its sensitivity concerning nodes' cardinality and risk of overflow. Combining the adaptive step size parameter and the dynamical softassign, we propose a novel graph matching algorithm: the softas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25277;&#26679;&#31639;&#27861;&#33719;&#21462;&#26465;&#20214;&#38750;&#32447;&#24615;&#26368;&#20248;&#25200;&#21160;&#65292;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#20013;&#26114;&#36149;&#30340;&#26799;&#24230;&#35745;&#31639;&#25104;&#26412;&#21644;&#26080;&#27861;&#20351;&#29992;&#30340;&#20276;&#38543;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2208.00956</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25277;&#26679;&#30340;&#26080;&#20276;&#38543;&#31639;&#27861;&#29992;&#20110;&#26465;&#20214;&#38750;&#32447;&#24615;&#26368;&#20248;&#25200;&#21160;&#65288;CNOPs&#65289;
&lt;/p&gt;
&lt;p&gt;
An adjoint-free algorithm for conditional nonlinear optimal perturbations (CNOPs) via sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.00956
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25277;&#26679;&#31639;&#27861;&#33719;&#21462;&#26465;&#20214;&#38750;&#32447;&#24615;&#26368;&#20248;&#25200;&#21160;&#65292;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#20013;&#26114;&#36149;&#30340;&#26799;&#24230;&#35745;&#31639;&#25104;&#26412;&#21644;&#26080;&#27861;&#20351;&#29992;&#30340;&#20276;&#38543;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#26465;&#20214;&#38750;&#32447;&#24615;&#26368;&#20248;&#25200;&#21160;&#65288;CNOPs&#65289;&#65292;&#19982;&#20256;&#32479;&#65288;&#30830;&#23450;&#24615;&#65289;&#20248;&#21270;&#26041;&#27861;&#19981;&#21516;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26080;&#27861;&#20351;&#29992;&#65292;&#38656;&#35201;&#25968;&#20540;&#35745;&#31639;&#26799;&#24230;&#65288;&#19968;&#38454;&#20449;&#24687;&#65289;&#65292;&#22240;&#27492;&#35745;&#31639;&#25104;&#26412;&#26114;&#36149;&#65292;&#38656;&#35201;&#22823;&#37327;&#36816;&#34892;&#25968;&#20540;&#27169;&#22411;&#30340;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;&#25277;&#26679;&#26041;&#27861;&#30452;&#25509;&#23558;&#26799;&#24230;&#38477;&#20302;&#21040;&#30446;&#26631;&#20989;&#25968;&#20540;&#65288;&#38646;&#38454;&#20449;&#24687;&#65289;&#65292;&#21516;&#26102;&#36991;&#20813;&#20351;&#29992;&#26080;&#27861;&#29992;&#20110;&#35768;&#22810;&#22823;&#27668;&#21644;&#28023;&#27915;&#27169;&#22411;&#30340;&#20276;&#38543;&#25216;&#26415;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12290;&#25105;&#20204;&#20174;&#22823;&#25968;&#23450;&#24459;&#20986;&#21457;&#23545;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#20102;&#30452;&#35266;&#20998;&#26512;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;Chernoff&#31867;&#22411;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#20197;&#20005;&#26684;&#21051;&#30011;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.00956v5 Announce Type: replace-cross  Abstract: In this paper, we propose a sampling algorithm based on state-of-the-art statistical machine learning techniques to obtain conditional nonlinear optimal perturbations (CNOPs), which is different from traditional (deterministic) optimization methods.1 Specifically, the traditional approach is unavailable in practice, which requires numerically computing the gradient (first-order information) such that the computation cost is expensive, since it needs a large number of times to run numerical models. However, the sampling approach directly reduces the gradient to the objective function value (zeroth-order information), which also avoids using the adjoint technique that is unusable for many atmosphere and ocean models and requires large amounts of storage. We show an intuitive analysis for the sampling algorithm from the law of large numbers and further present a Chernoff-type concentration inequality to rigorously characterize the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;WDRDG&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#29305;&#23450;&#30340;Wasserstein&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#22522;&#20110;&#26377;&#38480;&#28304;&#30693;&#35782;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2207.04913</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#38480;&#28304;&#30693;&#35782;&#19979;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#24615;&#23545;&#26410;&#30693;&#39046;&#22495;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing to Unseen Domains with Wasserstein Distributional Robustness under Limited Source Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.04913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;WDRDG&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#29305;&#23450;&#30340;Wasserstein&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#22522;&#20110;&#26377;&#38480;&#28304;&#30693;&#35782;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#27867;&#21270;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#32467;&#21512;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#21457;&#29983;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#30340;&#24773;&#24418;&#12290;&#24403;&#28304;&#22495;&#20013;&#26631;&#35760;&#26679;&#26412;&#26377;&#38480;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#19981;&#22815;&#20581;&#22766;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;{Wasserstein&#20998;&#24067;&#40065;&#26834;&#39046;&#22495;&#27867;&#21270;} (WDRDG)&#65292;&#28789;&#24863;&#26469;&#33258;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#40723;&#21169;&#22312;&#31867;&#21035;&#29305;&#23450;Wasserstein&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#30340;&#26465;&#20214;&#20998;&#24067;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20248;&#21270;&#20998;&#31867;&#22120;&#22312;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#27169;&#22359;&#65292;&#20197;&#37327;&#21270;&#26410;&#30693;&#30446;&#26631;&#20043;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.04913v2 Announce Type: replace  Abstract: Domain generalization aims at learning a universal model that performs well on unseen target domains, incorporating knowledge from multiple source domains. In this research, we consider the scenario where different domain shifts occur among conditional distributions of different classes across domains. When labeled samples in the source domains are limited, existing approaches are not sufficiently robust. To address this problem, we propose a novel domain generalization framework called {Wasserstein Distributionally Robust Domain Generalization} (WDRDG), inspired by the concept of distributionally robust optimization. We encourage robustness over conditional distributions within class-specific Wasserstein uncertainty sets and optimize the worst-case performance of a classifier over these uncertainty sets. We further develop a test-time adaptation module leveraging optimal transport to quantify the relationship between the unseen targ
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#26641;&#12289;&#29615;&#12289;&#21306;&#38388;&#22270;&#12289;&#29615;&#26641;&#21644;&#33258;&#30001;&#31435;&#26041;&#20307;&#20013;&#20301;&#22270;&#30340;&#29699;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#65292;&#20026;&#26410;&#35299;&#20915;&#30340;VC&#32500;&#24230;$d$&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2206.13254</link><description>&lt;p&gt;
&#22270;&#20013;&#29699;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Sample compression schemes for balls in graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.13254
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#26641;&#12289;&#29615;&#12289;&#21306;&#38388;&#22270;&#12289;&#29615;&#26641;&#21644;&#33258;&#30001;&#31435;&#26041;&#20307;&#20013;&#20301;&#22270;&#30340;&#29699;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#65292;&#20026;&#26410;&#35299;&#20915;&#30340;VC&#32500;&#24230;$d$&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26410;&#35299;&#38382;&#39064;&#26159;&#20219;&#20309;VC&#32500;&#24230;$d$&#30340;&#38598;&#21512;&#26063;&#26159;&#21542;&#25509;&#21463;&#22823;&#23567;&#20026;$O(d)$&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#29699;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;&#22270;$G=(V,E)$&#20013;&#30340;&#19968;&#20010;&#29699;$B=B_r(x)$&#65292;&#29699;$B$&#30340;&#19968;&#20010;&#21487;&#23454;&#29616;&#26679;&#26412;&#26159;$V$&#30340;&#19968;&#20010;&#26377;&#31526;&#21495;&#23376;&#38598;$X=(X^+,X^-)$&#65292;&#20351;&#24471;$B$&#21253;&#21547;$X^+$&#19988;&#19982;$X^-$&#19981;&#30456;&#20132;&#12290;&#22823;&#23567;&#20026;$k$&#30340;&#36866;&#24403;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#21253;&#25324;&#19968;&#20010;&#21387;&#32553;&#22120;&#21644;&#19968;&#20010;&#37325;&#26500;&#22120;&#12290;&#21387;&#32553;&#22120;&#23558;&#20219;&#20309;&#21487;&#23454;&#29616;&#26679;&#26412;$X$&#26144;&#23556;&#20026;&#22823;&#23567;&#26368;&#22810;&#20026;$k$&#30340;&#23376;&#26679;&#26412;$X'$&#12290;&#37325;&#26500;&#22120;&#23558;&#36825;&#26679;&#30340;&#23376;&#26679;&#26412;$X'$&#26144;&#23556;&#20026;$G$&#30340;&#29699;$B'$&#65292;&#20351;&#24471;$B'$&#21253;&#25324;$X^+$&#24182;&#19988;&#19982;$X^-$&#19981;&#30456;&#20132;&#12290;&#23545;&#20110;&#20219;&#24847;&#21322;&#24452;$r$&#30340;&#29699;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26641;&#30340;&#22823;&#23567;&#20026;$2$&#30340;&#36866;&#24403;&#26631;&#35760;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#29615;&#30340;&#22823;&#23567;&#20026;$3$&#65292;&#21306;&#38388;&#22270;&#30340;&#22823;&#23567;&#20026;$4$&#65292;&#29615;&#26641;&#30340;&#22823;&#23567;&#20026;$6$&#65292;&#20197;&#21450;&#33258;&#30001;&#31435;&#26041;&#20307;&#20013;&#20301;&#22270;&#30340;&#22823;&#23567;&#20026;$22$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.13254v2 Announce Type: replace-cross  Abstract: One of the open problems in machine learning is whether any set-family of VC-dimension $d$ admits a sample compression scheme of size $O(d)$. In this paper, we study this problem for balls in graphs. For a ball $B=B_r(x)$ of a graph $G=(V,E)$, a realizable sample for $B$ is a signed subset $X=(X^+,X^-)$ of $V$ such that $B$ contains $X^+$ and is disjoint from $X^-$. A proper sample compression scheme of size $k$ consists of a compressor and a reconstructor. The compressor maps any realizable sample $X$ to a subsample $X'$ of size at most $k$. The reconstructor maps each such subsample $X'$ to a ball $B'$ of $G$ such that $B'$ includes $X^+$ and is disjoint from $X^-$.   For balls of arbitrary radius $r$, we design proper labeled sample compression schemes of size $2$ for trees, of size $3$ for cycles, of size $4$ for interval graphs, of size $6$ for trees of cycles, and of size $22$ for cube-free median graphs. For balls of a g
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#22788;&#29702;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2205.07250</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#30340;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A cGAN Ensemble-based Uncertainty-aware Surrogate Model for Offline Model-based Optimization in Industrial Control Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.07250
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#22788;&#29702;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#23558;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#26102;&#36935;&#21040;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#22914;&#20309;&#21019;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20934;&#30830;&#25429;&#25417;&#22024;&#26434;&#24037;&#19994;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#22914;&#20309;&#22312;&#19981;&#20027;&#21160;&#25910;&#38598;&#24037;&#19994;&#31995;&#32479;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#20248;&#21270;&#25511;&#21046;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#22320;&#22788;&#29702;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26696;&#20363;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#31163;&#25955;&#25511;&#21046;&#26696;&#20363;&#21644;&#36830;&#32493;&#25511;&#21046;&#26696;&#20363;&#12290;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24037;&#19994;&#25511;&#21046;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#39046;&#22495;&#20013;&#32988;&#36807;&#20102;&#20960;&#20010;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.07250v2 Announce Type: replace-cross  Abstract: This study focuses on two important problems related to applying offline model-based optimization to real-world industrial control problems. The first problem is how to create a reliable probabilistic model that accurately captures the dynamics present in noisy industrial data. The second problem is how to reliably optimize control parameters without actively collecting feedback from industrial systems. Specifically, we introduce a novel cGAN ensemble-based uncertainty-aware surrogate model for reliable offline model-based optimization in industrial control problems. The effectiveness of the proposed method is demonstrated through extensive experiments conducted on two representative cases, namely a discrete control case and a continuous control case. The results of these experiments show that our method outperforms several competitive baselines in the field of offline model-based optimization for industrial control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#19981;&#21516;&#32570;&#38519;&#39044;&#27979;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#36328;&#29256;&#26412;&#21644;&#36328;&#39033;&#30446;&#30340;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2202.12074</link><description>&lt;p&gt;
&#23545;&#20110;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#19981;&#21516;&#32570;&#38519;&#39044;&#27979;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Effectiveness of One-Class Support Vector Machine in Different Defect Prediction Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.12074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#19981;&#21516;&#32570;&#38519;&#39044;&#27979;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#36328;&#29256;&#26412;&#21644;&#36328;&#39033;&#30446;&#30340;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#39044;&#27979;&#26088;&#22312;&#35782;&#21035;&#22312;&#36719;&#20214;&#25552;&#20379;&#32473;&#26368;&#32456;&#29992;&#25143;&#20043;&#21069;&#21487;&#33021;&#24341;&#36215;&#25925;&#38556;&#30340;&#36719;&#20214;&#32452;&#20214;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#19968;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#21452;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#65292;&#28982;&#32780;&#20854;&#26412;&#36136;&#20063;&#20801;&#35768;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#19968;&#31867;&#20998;&#31867;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;OCSVM&#65289;&#22312;&#39033;&#30446;&#20869;&#32570;&#38519;&#39044;&#27979;&#26041;&#38754;&#21487;&#20197;&#32988;&#36807;&#21452;&#31867;&#21035;&#20998;&#31867;&#22120;&#65292;&#28982;&#32780;&#24403;&#24212;&#29992;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#24773;&#20917;&#65288;&#21363;&#22522;&#20110;&#25552;&#20132;&#32423;&#21035;&#30340;&#32570;&#38519;&#39044;&#27979;&#65289;&#26102;&#24182;&#19981;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20165;&#20174;&#19968;&#31867;&#36827;&#34892;&#23398;&#20064;&#26159;&#21542;&#36275;&#20197;&#29983;&#25104;&#22312;&#20004;&#31181;&#20854;&#20182;&#19981;&#21516;&#22330;&#26223;&#65288;&#21363;&#31890;&#24230;&#65289;&#20013;&#20135;&#29983;&#26377;&#25928;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#65292;&#21363;&#36328;&#29256;&#26412;&#21644;&#36328;&#39033;&#30446;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20026;&#20102;&#23436;&#25972;&#24615;&#22797;&#21046;&#22312;&#39033;&#30446;&#20869;&#31890;&#24230;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#23454;&#20102;OCSVM&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.12074v2 Announce Type: replace-cross  Abstract: Defect prediction aims at identifying software components that are likely to cause faults before a software is made available to the end-user. To date, this task has been modeled as a two-class classification problem, however its nature also allows it to be formulated as a one-class classification task. Previous studies show that One-Class Support Vector Machine (OCSVM) can outperform two-class classifiers for within-project defect prediction, however it is not effective when employed at a finer granularity (i.e., commit-level defect prediction). In this paper, we further investigate whether learning from one class only is sufficient to produce effective defect prediction model in two other different scenarios (i.e., granularity), namely cross-version and cross-project defect prediction models, as well as replicate the previous work at within-project granularity for completeness. Our empirical results confirm that OCSVM perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#20559;&#24046;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35770;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#26694;&#26550;&#25551;&#36848;GMRL&#31639;&#27861;&#30340;&#21464;&#21270;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#38543;&#26426;&#20803;&#26799;&#24230;&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#26377;&#20559;&#30340;&#12290;</title><link>https://arxiv.org/abs/2112.15400</link><description>&lt;p&gt;
Meta-Reinforcement Learning&#20013;&#26799;&#24230;&#20559;&#24046;&#30340;&#29702;&#35770;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.15400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#20559;&#24046;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35770;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#26694;&#26550;&#25551;&#36848;GMRL&#31639;&#27861;&#30340;&#21464;&#21270;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#38543;&#26426;&#20803;&#26799;&#24230;&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#26377;&#20559;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;GMRL&#65289;&#26159;&#25351;&#20445;&#25345;&#20004;&#32423;&#20248;&#21270;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22806;&#23618;&#20803;&#23398;&#20064;&#32773;&#25351;&#23548;&#20869;&#23618;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#32773;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;GMRL&#31639;&#27861;&#30340;&#21464;&#21270;&#65292;&#24182;&#25351;&#20986;GMRL&#37319;&#29992;&#30340;&#29616;&#26377;&#38543;&#26426;&#20803;&#26799;&#24230;&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#26377;&#20559;&#30340;&#12290;&#36825;&#31181;&#20803;&#26799;&#24230;&#20559;&#24046;&#26469;&#33258;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#30001;&#20004;&#32423;&#38382;&#39064;&#32467;&#26500;&#24341;&#36215;&#30340;&#21512;&#25104;&#20559;&#24046;&#65292;&#23545;&#20869;&#37096;&#26356;&#26032;&#27493;&#39588;$K$&#12289;&#23398;&#20064;&#29575;$\alpha$&#12289;&#20272;&#35745;&#26041;&#24046;$\hat{\sigma}^{2}_{\text{In}}$&#21644;&#26679;&#26412;&#22823;&#23567;$|\tau|$&#26377;&#19968;&#20010;&#19978;&#38480;&#20026;$\mathcal{O}(K\alpha^{K}\hat{\sigma}_{\text{In}}|\tau|^{-0.5}$&#65307;2&#65289;&#30001;&#20110;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#32780;&#23548;&#33268;&#30340;&#22810;&#27493;Hessian&#20272;&#35745;&#20559;&#24046;$\hat{\Delta}_{H}$&#65292;&#20854;&#20855;&#26377;&#22810;&#39033;&#24335;&#24433;&#21709;$\mathcal{O}((K-1)(\hat{\Delta}_...
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.15400v4 Announce Type: replace-cross  Abstract: Gradient-based Meta-RL (GMRL) refers to methods that maintain two-level optimisation procedures wherein the outer-loop meta-learner guides the inner-loop gradient-based reinforcement learner to achieve fast adaptations. In this paper, we develop a unified framework that describes variations of GMRL algorithms and points out that existing stochastic meta-gradient estimators adopted by GMRL are actually \textbf{biased}. Such meta-gradient bias comes from two sources: 1) the compositional bias incurred by the two-level problem structure, which has an upper bound of $\mathcal{O}\big(K\alpha^{K}\hat{\sigma}_{\text{In}}|\tau|^{-0.5}\big)$ \emph{w.r.t.} inner-loop update step $K$, learning rate $\alpha$, estimate variance $\hat{\sigma}^{2}_{\text{In}}$ and sample size $|\tau|$, and 2) the multi-step Hessian estimation bias $\hat{\Delta}_{H}$ due to the use of autodiff, which has a polynomial impact $\mathcal{O}\big((K-1)(\hat{\Delta}_
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;$\epsilon_t$-&#36138;&#24515;&#21551;&#21457;&#24335;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#24773;&#22659;&#20013;&#65292;&#37319;&#29992;&#20445;&#23432;&#23548;&#21521;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#23454;&#29992;&#24212;&#29992;&#20013;&#23545;&#26032;&#22855;&#24615;&#30340;&#37325;&#35270;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#37319;&#32435;&#19981;&#23547;&#24120;&#21160;&#20316;&#65292;&#26377;&#25928;&#25511;&#21046;&#20102;&#32047;&#31215;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2009.13961</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#30340;&#22312;&#32447;&#21160;&#20316;&#23398;&#20064;&#65306;&#19968;&#20010;&#20445;&#23432;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Online Action Learning in High Dimensions: A Conservative Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2009.13961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;$\epsilon_t$-&#36138;&#24515;&#21551;&#21457;&#24335;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#24773;&#22659;&#20013;&#65292;&#37319;&#29992;&#20445;&#23432;&#23548;&#21521;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#23454;&#29992;&#24212;&#29992;&#20013;&#23545;&#26032;&#22855;&#24615;&#30340;&#37325;&#35270;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#37319;&#32435;&#19981;&#23547;&#24120;&#21160;&#20316;&#65292;&#26377;&#25928;&#25511;&#21046;&#20102;&#32047;&#31215;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#27969;&#34892;&#30340;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;$\epsilon_t$-&#36138;&#24515;&#21551;&#21457;&#24335;&#65292;&#25193;&#23637;&#21040;&#32771;&#34385;&#20445;&#23432;&#23548;&#21521;&#30340;&#39640;&#32500;&#24773;&#22659;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21407;&#22987;&#35268;&#21017;&#29992;&#20110;&#37319;&#32435;&#20840;&#26032;&#21160;&#20316;&#30340;&#26102;&#38388;&#30340;&#19968;&#37096;&#20998;&#65292;&#20998;&#37197;&#32473;&#22312;&#19968;&#32452;&#26377;&#21069;&#36884;&#30340;&#21160;&#20316;&#20013;&#36827;&#34892;&#26356;&#21152;&#19987;&#27880;&#30340;&#25628;&#32034;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25152;&#24471;&#35268;&#21017;&#21487;&#33021;&#23545;&#20173;&#28982;&#37325;&#35270;&#24778;&#21916;&#20294;&#38480;&#21046;&#37319;&#32435;&#19981;&#23547;&#24120;&#21160;&#20316;&#30340;&#23454;&#38469;&#24212;&#29992;&#26377;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#20110;&#20445;&#23432;&#39640;&#32500;&#24230;&#34928;&#20943;$\epsilon_t$-&#36138;&#24515;&#35268;&#21017;&#30340;&#32047;&#31215;&#36951;&#25022;&#25552;&#20379;&#20102;&#21512;&#29702;&#36793;&#30028;&#30340;&#27010;&#29575;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2009.13961v4 Announce Type: replace-cross  Abstract: Sequential learning problems are common in several fields of research and practical applications. Examples include dynamic pricing and assortment, design of auctions and incentives and permeate a large number of sequential treatment experiments. In this paper, we extend one of the most popular learning solutions, the $\epsilon_t$-greedy heuristics, to high-dimensional contexts considering a conservative directive. We do this by allocating part of the time the original rule uses to adopt completely new actions to a more focused search in a restrictive set of promising actions. The resulting rule might be useful for practical applications that still values surprises, although at a decreasing rate, while also has restrictions on the adoption of unusual actions. With high probability, we find reasonable bounds for the cumulative regret of a conservative high-dimensional decaying $\epsilon_t$-greedy rule. Also, we provide a lower bo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/1909.03820</link><description>&lt;p&gt;
&#29992;&#35745;&#25968;&#31526;&#21495;&#30340;&#19968;&#38454;&#36923;&#36753;&#23450;&#20041;&#30340;&#27010;&#24565;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Concepts Definable in First-Order Logic with Counting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1909.03820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Grohe&#21644;Tur\'an&#24341;&#20837;&#30340;&#36923;&#36753;&#26694;&#26550;&#19979;&#30340;&#20851;&#31995;&#32972;&#26223;&#32467;&#26500;&#19978;&#30340;&#24067;&#23572;&#20998;&#31867;&#38382;&#39064;&#12290;&#20247;&#25152;&#21608;&#30693;(Grohe&#21644;Ritzert, LICS 2017)&#65292;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19978;&#30340;&#19968;&#38454;&#36923;&#36753;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23398;&#20064;&#65292;&#20854;&#20013;&#32467;&#26500;&#30340;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26159;&#20197;&#32467;&#26500;&#30340;&#22823;&#23567;&#20026;&#21333;&#20301;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#30001;Kuske&#21644;Schweikardt(LICS 2017)&#24341;&#20837;&#30340;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;FOCN&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#24191;&#27867;&#25512;&#24191;&#21508;&#31181;&#35745;&#25968;&#36923;&#36753;&#30340;&#34920;&#29616;&#36923;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#31867;&#19978;&#23450;&#20041;&#30340;FOCN&#20013;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#19968;&#33268;&#22320;&#23398;&#20064;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#20197;&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#20540;&#26041;&#38754;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26080;&#35270;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#28145;&#24230;&#23376;&#31354;&#38388;&#32858;&#31867;&#32593;&#32476;&#65288;MvDSCN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#22810;&#35270;&#35282;&#33258;&#34920;&#31034;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#22810;&#35270;&#35282;&#20851;&#31995;&#26410;&#23884;&#20837;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#23398;&#20064;&#19981;&#36866;&#29992;&#20110;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/1908.01978</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#28145;&#24230;&#23376;&#31354;&#38388;&#32858;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-view Deep Subspace Clustering Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1908.01978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#28145;&#24230;&#23376;&#31354;&#38388;&#32858;&#31867;&#32593;&#32476;&#65288;MvDSCN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#22810;&#35270;&#35282;&#33258;&#34920;&#31034;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#22810;&#35270;&#35282;&#20851;&#31995;&#26410;&#23884;&#20837;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#23398;&#20064;&#19981;&#36866;&#29992;&#20110;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20114;&#34917;&#20449;&#24687;&#35270;&#35282;&#26469;&#21457;&#29616;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#39318;&#20808;&#25552;&#21462;&#22810;&#31181;&#25163;&#24037;&#35774;&#35745;&#29305;&#24449;&#65292;&#28982;&#21518;&#23398;&#20064;&#29992;&#20110;&#32858;&#31867;&#30340;&#32852;&#21512;&#30456;&#20284;&#30697;&#38453;&#12290;&#35813;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#22810;&#35270;&#35282;&#20851;&#31995;&#26410;&#23884;&#20837;&#29305;&#24449;&#23398;&#20064;&#20013;&#65292;2&#65289;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#24335;&#19981;&#36866;&#29992;&#20110;&#22810;&#35270;&#35282;&#32858;&#31867;&#12290;&#21363;&#20351;&#25552;&#21462;&#20102;&#28145;&#24230;&#29305;&#24449;&#65292;&#20063;&#24456;&#38590;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36873;&#25321;&#36866;&#24403;&#30340;&#39592;&#24178;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#28145;&#24230;&#23376;&#31354;&#38388;&#32858;&#31867;&#32593;&#32476;&#65288;MvDSCN&#65289;&#65292;&#23427;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#22810;&#35270;&#35282;&#33258;&#34920;&#31034;&#30697;&#38453;&#12290;MvDSCN&#30001;&#20004;&#20010;&#23376;&#32593;&#32476;&#32452;&#25104;&#65292;&#21363;&#22810;&#26679;&#24615;&#32593;&#32476;&#65288;Dnet&#65289;&#21644;&#26222;&#36866;&#24615;&#32593;&#32476;&#65288;Unet&#65289;&#12290;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#26500;&#24314;&#28508;&#22312;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1908.01978v2 Announce Type: replace-cross  Abstract: Multi-view subspace clustering aims to discover the inherent structure of data by fusing multiple views of complementary information. Most existing methods first extract multiple types of handcrafted features and then learn a joint affinity matrix for clustering. The disadvantage of this approach lies in two aspects: 1) multi-view relations are not embedded into feature learning, and 2) the end-to-end learning manner of deep learning is not suitable for multi-view clustering. Even when deep features have been extracted, it is a nontrivial problem to choose a proper backbone for clustering on different datasets. To address these issues, we propose the Multi-view Deep Subspace Clustering Networks (MvDSCN), which learns a multi-view self-representation matrix in an end-to-end manner. The MvDSCN consists of two sub-networks, \ie, a diversity network (Dnet) and a universality network (Unet). A latent space is built using deep convol
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29699;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#35299;&#30721;&#36229;&#29699;&#20307;&#30340;&#21322;&#24452;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#25509;&#36817;&#26368;&#20248;&#35299;&#30721;&#30340;&#25928;&#26524;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/1807.03162</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29699;&#35299;&#30721;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Sphere Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1807.03162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29699;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#35299;&#30721;&#36229;&#29699;&#20307;&#30340;&#21322;&#24452;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#25509;&#36817;&#26368;&#20248;&#35299;&#30721;&#30340;&#25928;&#26524;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#29699;&#35299;&#30721;&#31639;&#27861;&#65292;&#20854;&#20013;&#35299;&#30721;&#36229;&#29699;&#20307;&#30340;&#21322;&#24452;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#19979;&#23454;&#29616;&#30340;&#24615;&#33021;&#38750;&#24120;&#25509;&#36817;&#20110;&#26368;&#20248;&#26368;&#22823;&#20284;&#28982;&#35299;&#30721;&#65288;MLD&#65289;&#65292;&#19982;&#29616;&#26377;&#30340;&#29699;&#35299;&#30721;&#21464;&#20307;&#30456;&#27604;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#31181;&#25913;&#36827;&#24402;&#22240;&#20110;DNN&#26234;&#33021;&#23398;&#20064;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#36229;&#29699;&#20307;&#30340;&#21322;&#24452;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;DL&#30340;&#31639;&#27861;&#30340;&#39044;&#26399;&#22797;&#26434;&#24230;&#34987;&#36890;&#36807;&#20998;&#26512;&#25512;&#23548;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;DL&#31639;&#27861;&#20013;&#65292;&#35299;&#30721;&#36229;&#29699;&#20307;&#20869;&#30340;&#26230;&#26684;&#28857;&#30340;&#25968;&#37327;&#22312;&#24179;&#22343;&#24773;&#20917;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#22343;&#22823;&#22823;&#20943;&#23569;&#12290;&#36890;&#36807;&#39640;&#32500;&#27169;&#25311;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1807.03162v2 Announce Type: replace-cross  Abstract: In this paper, a deep learning (DL)-based sphere decoding algorithm is proposed, where the radius of the decoding hypersphere is learned by a deep neural network (DNN). The performance achieved by the proposed algorithm is very close to the optimal maximum likelihood decoding (MLD) over a wide range of signal-to-noise ratios (SNRs), while the computational complexity, compared to existing sphere decoding variants, is significantly reduced. This improvement is attributed to DNN's ability of intelligently learning the radius of the hypersphere used in decoding. The expected complexity of the proposed DL-based algorithm is analytically derived and compared with existing ones. It is shown that the number of lattice points inside the decoding hypersphere drastically reduces in the DL-based algorithm in both the average and worst-case senses. The effectiveness of the proposed algorithm is shown through simulation for high-dimensional
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#38899;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#36172;&#33218;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Thompson&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#65292;&#24182;&#25193;&#23637;&#20102;&#38382;&#39064;&#21040;&#24310;&#36831;&#35266;&#23519;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#24182;&#23454;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11565</link><description>&lt;p&gt;
Thompson&#37319;&#26679;&#29992;&#20110;&#20855;&#26377;&#22122;&#38899;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#36172;&#33218;&#38382;&#39064;&#30340;&#20449;&#24687;&#35770;&#24615;&#21518;&#24724;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis. (arXiv:2401.11565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#38899;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#36172;&#33218;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Thompson&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#65292;&#24182;&#25193;&#23637;&#20102;&#38382;&#39064;&#21040;&#24310;&#36831;&#35266;&#23519;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#24182;&#23454;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#19978;&#19979;&#25991;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19968;&#20010;&#26410;&#30693;&#22122;&#22768;&#21442;&#25968;&#30340;&#22122;&#22768;&#20449;&#36947;&#35266;&#23519;&#21040;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#21160;&#20316;&#31574;&#30053;&#65292;&#21487;&#20197;&#36817;&#20284;&#20110;&#20855;&#26377;&#22870;&#21169;&#27169;&#22411;&#12289;&#22122;&#22768;&#21442;&#25968;&#21644;&#20174;&#35266;&#23519;&#21040;&#30340;&#22122;&#22768;&#19978;&#19979;&#25991;&#20013;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;oracle&#30340;&#21160;&#20316;&#31574;&#30053;&#12290;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#19978;&#19979;&#25991;&#22122;&#22768;&#30340;&#39640;&#26031;&#36172;&#33218;&#30340;Thompson&#37319;&#26679;&#31639;&#27861;&#12290;&#37319;&#29992;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30456;&#23545;&#20110;oracle&#30340;&#21160;&#20316;&#31574;&#30053;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20010;&#38382;&#39064;&#25193;&#23637;&#21040;&#20102;&#20195;&#29702;&#22312;&#25509;&#25910;&#21040;&#22870;&#21169;&#21518;&#24310;&#36831;&#35266;&#23519;&#21040;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24310;&#36831;&#30495;&#23454;&#19978;&#19979;&#25991;&#23548;&#33268;&#26356;&#20302;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#22522;&#32447;&#31639;&#27861;&#30340;&#27604;&#36739;&#23454;&#35777;&#22320;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2401.05224</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#21542;&#20197;&#30456;&#20284;&#26041;&#24335;&#34920;&#31034;&#19990;&#30028;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20107;&#23454;&#19978;&#30340;&#27169;&#22411;&#30340;&#23545;&#40784;&#30340;&#25991;&#26412;-&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#22312;&#21508;&#33258;&#39046;&#22495;&#20013;&#20063;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#30001;&#20110;&#23427;&#20204;&#22522;&#26412;&#19978;&#34920;&#31034;&#21516;&#19968;&#20010;&#29289;&#29702;&#19990;&#30028;&#65292;&#21333;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#23545;&#40784;&#65311;&#36890;&#36807;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20998;&#26512;&#22270;&#20687;-&#26631;&#39064;&#22522;&#20934;&#19978;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#22312;&#20687;CLIP&#36825;&#26679;&#30340;&#23545;&#40784;&#32534;&#30721;&#22120;&#20013;&#32570;&#20047;&#32479;&#35745;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;&#21487;&#33021;&#23384;&#22312;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#30340;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#21033;&#29992;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#31181;&#23376;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861; - &#24555;&#36895;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#20248;&#21270;&#21644;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;CKA&#24230;&#37327;&#30340;&#21305;&#37197;/&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.16430</link><description>&lt;p&gt;
&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20174;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#23398;&#20064;&#65292;&#39318;&#20808;&#25311;&#21512;&#20559;&#22909;&#20998;&#25968;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;RLHF&#30340;&#22788;&#29702;&#36807;&#31243;&#22797;&#26434;&#12289;&#32791;&#26102;&#19988;&#19981;&#31283;&#23450;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#20351;&#29992;&#31163;&#31574;&#30053;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;DPO&#20351;&#29992;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#21644;&#23545;&#25968;&#25439;&#22833;&#65292;&#23548;&#33268;&#22312;&#20559;&#22909;&#25509;&#36817;&#30830;&#23450;&#24615;&#26102;&#24573;&#30053;&#20102;KL&#27491;&#21017;&#21270;&#39033;&#32780;&#36807;&#24230;&#25311;&#21512;&#20559;&#22909;&#25968;&#25454;&#12290;IPO&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26681;&#26597;&#25214;&#30340;&#25104;&#23545;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#26469;&#35299;&#20915;&#24573;&#30053;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#65292;&#24182;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#26159;IPO&#30340;&#25104;&#23545;&#25439;&#22833;&#20173;&#28982;&#26080;&#27861;&#20351;KL&#27491;&#21017;&#21270;&#29983;&#25928;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20559;&#22909;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#30340;&#20984;&#20989;&#25968;&#36861;&#36394;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21516;&#26102;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#36798;&#21040;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#38543;&#26426;&#26694;&#26550;&#30740;&#31350;&#35813;&#38382;&#39064;&#30340;&#24037;&#20316;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20004;&#31181;&#24773;&#22659;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00181</link><description>&lt;p&gt;
&#26368;&#20339;&#32467;&#21512;: &#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#20984;&#20989;&#25968;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing. (arXiv:2311.00181v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00181
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#30340;&#20984;&#20989;&#25968;&#36861;&#36394;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21516;&#26102;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#36798;&#21040;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#38543;&#26426;&#26694;&#26550;&#30740;&#31350;&#35813;&#38382;&#39064;&#30340;&#24037;&#20316;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20004;&#31181;&#24773;&#22659;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20984;&#20989;&#25968;&#36861;&#36394;(CFC)&#26159;&#19968;&#20010;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#27599;&#19968;&#36718;$t$&#65292;&#29609;&#23478;&#26681;&#25454;&#25439;&#22833;&#20989;&#25968;$f_t(x_t)$&#21644;&#20999;&#25442;&#21160;&#20316;&#30340;&#39069;&#22806;&#25104;&#26412;$c(x_t,x_{t-1})$&#36873;&#25321;&#21160;&#20316;$x_t$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#30340;CFC&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#21516;&#26102;&#33719;&#24471;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24179;&#26041;$\ell_2$&#33539;&#25968;&#30340;&#20999;&#25442;&#25104;&#26412;&#21644;&#19968;&#31867;&#24191;&#27867;&#30340;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#36825;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#26497;&#23567;&#21270;&#24207;&#21015;&#35201;&#20040;&#24418;&#25104;&#38789;&#65292;&#35201;&#20040;&#30001;&#23545;&#25163;&#36873;&#25321;&#12290;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#38543;&#26426;&#26694;&#26550;&#30740;&#31350;CFC&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#26368;&#20339;&#38543;&#26426;&#22312;&#32447;&#31639;&#27861;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#24773;&#26223;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#22312;&#38543;&#26426;&#24773;&#22659;&#19979;&#65292;&#23545;&#25239;&#24615;&#26368;&#20248;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20102;&#20004;&#31181;&#24773;&#22659;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convex function chasing (CFC) is an online optimization problem in which during each round $t$, a player plays an action $x_t$ in response to a hitting cost $f_t(x_t)$ and an additional cost of $c(x_t,x_{t-1})$ for switching actions. We study the CFC problem in stochastic and adversarial environments, giving algorithms that achieve performance guarantees simultaneously in both settings. Specifically, we consider the squared $\ell_2$-norm switching costs and a broad class of quadratic hitting costs for which the sequence of minimizers either forms a martingale or is chosen adversarially. This is the first work that studies the CFC problem using a stochastic framework. We provide a characterization of the optimal stochastic online algorithm and, drawing a comparison between the stochastic and adversarial scenarios, we demonstrate that the adversarial-optimal algorithm exhibits suboptimal performance in the stochastic context. Motivated by this, we provide a best-of-both-worlds algorithm 
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>http://arxiv.org/abs/2310.15168</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ghost on the Shell: An Expressive Representation of General 3D Shapes. (arXiv:2310.15168v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15168
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
The creation of photorealistic virtual worlds requires the accurate modeling of 3D surface geometry for a wide range of objects. For this, meshes are appealing since they 1) enable fast physics-based rendering with realistic material and lighting, 2) support physical simulation, and 3) are memory-efficient for modern graphics pipelines. Recent work on reconstructing and statistically modeling 3D shape, however, has critiqued meshes as being topologically inflexible. To capture a wide range of object shapes, any 3D representation must be able to model solid, watertight, shapes as well as thin, open, surfaces. Recent work has focused on the former, and methods for reconstructing open surfaces do not support fast reconstruction with material and lighting or unconditional generative modelling. Inspired by the observation that open surfaces can be seen as islands floating on watertight surfaces, we parameterize open surfaces by defining a manifold signed distance field on watertight templat
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.11959</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer
&lt;/p&gt;
&lt;p&gt;
A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis. (arXiv:2310.11959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11959
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#29420;&#29305;&#30340;&#32452;&#25104;&#21644;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#21464;&#21270;&#65292;&#38656;&#35201;&#22312;&#20854;&#20998;&#26512;&#20013;&#29305;&#21035;&#32771;&#34385;&#20998;&#35299;&#21644;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#23545;&#23376;&#24207;&#21015;&#32423;&#21035;&#30340;&#24314;&#27169;&#21644;&#20998;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSD-Mixer&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#20998;&#35299;&#30340;MLP-Mixer&#65292;&#23427;&#23398;&#20250;&#20102;&#23558;&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#26126;&#30830;&#22320;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20026;&#22810;&#23610;&#24230;&#23376;&#24207;&#21015;&#65292;&#21363;patches&#65292;&#24182;&#20351;&#29992;MLPs&#26469;&#32452;&#21512;patches&#20869;&#37096;&#21644;patches&#38388;&#30340;&#21464;&#21270;&#20197;&#21450;&#36890;&#36947;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#32422;&#26463;&#20998;&#35299;&#27531;&#24046;&#30340;&#24133;&#24230;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#23436;&#25972;&#30340;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition 
&lt;/p&gt;</description></item><item><title>SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11667</link><description>&lt;p&gt;
SOTOPIA: &#20132;&#20114;&#24335;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11667
&lt;/p&gt;
&lt;p&gt;
SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#31038;&#20132;&#30340;&#23384;&#22312;&#65307;&#25105;&#20204;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#36861;&#27714;&#31038;&#20132;&#30446;&#26631;&#65292;&#36825;&#26159;&#31038;&#20132;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SOTOPIA&#65292;&#19968;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#31038;&#20132;&#20114;&#21160;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#20154;&#25198;&#28436;&#35282;&#33394;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30456;&#20114;&#21327;&#20316;&#12289;&#21512;&#20316;&#12289;&#20132;&#27969;&#21644;&#31454;&#20105;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#31038;&#20132;&#30446;&#26631;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;LLM-based&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#36825;&#20010;&#20219;&#21153;&#31354;&#38388;&#20869;&#30340;&#35282;&#33394;&#25198;&#28436;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;SOTOPIA-Eval&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;SOTOPIA&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#20102;SOTOPIA&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#21363;SOTOPIA-hard&#65292;&#23545;&#25152;&#26377;&#27169;&#22411;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#26174;&#33879;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09484</link><description>&lt;p&gt;
&#25506;&#32034;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09484
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#21019;&#24314;&#30340;&#20154;&#33080;&#21464;&#24418;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#21019;&#26032;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;1&#65289;&#37319;&#26679;&#31639;&#27861;&#65292;2&#65289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#65292;&#20197;&#21450;3&#65289;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#22122;&#22768;&#36827;&#34892;&#37096;&#20998;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
&lt;/p&gt;</description></item><item><title>PhyloGFN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#65292;&#24182;&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08774</link><description>&lt;p&gt;
PhyloGFN: &#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PhyloGFN: Phylogenetic inference with generative flow networks. (arXiv:2310.08774v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08774
&lt;/p&gt;
&lt;p&gt;
PhyloGFN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#65292;&#24182;&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21457;&#32946;&#23398;&#26159;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#29983;&#29289;&#23454;&#20307;&#20043;&#38388;&#30340;&#36827;&#21270;&#20851;&#31995;&#12290;&#23613;&#31649;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#21644;&#20247;&#22810;&#24212;&#29992;&#65292;&#20294;&#20174;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#31995;&#32479;&#21457;&#32946;&#26641;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#26641;&#31354;&#38388;&#30340;&#39640;&#22797;&#26434;&#24615;&#23545;&#24403;&#21069;&#30340;&#32452;&#21512;&#21644;&#27010;&#29575;&#25216;&#26415;&#26500;&#25104;&#20102;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31995;&#32479;&#21457;&#32946;&#23398;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#22522;&#20110;&#26368;&#31616;&#21407;&#21017;&#30340;&#21644;&#36125;&#21494;&#26031;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#12290;&#30001;&#20110;GFlowNets&#36866;&#29992;&#20110;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;&#21644;&#37319;&#26679;&#26641;&#25299;&#25169;&#21644;&#36827;&#21270;&#36317;&#31163;&#30340;&#22810;&#27169;&#24577;&#21518;&#39564;&#20998;&#24067;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25674;&#36824;&#21518;&#39564;&#37319;&#26679;&#22120;PhyloGFN&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#12290;PhyloGFN&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimat
&lt;/p&gt;</description></item><item><title>&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;KAE&#65289;&#26159;&#19968;&#31181;&#22312;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#22686;&#24378;&#24615;&#33021;&#30340;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#12290;KAE&#23454;&#29616;&#20102;&#26377;&#25928;&#29983;&#25104;&#21644;&#20934;&#30830;&#37325;&#26500;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#22810;&#26679;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;KAE&#36824;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#29983;&#25104;&#20998;&#23376;&#65292;&#24182;&#22312;&#20998;&#23376;&#23545;&#25509;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08685</link><description>&lt;p&gt;
&#20998;&#23376;&#35774;&#35745;&#30340;&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Kernel-Elastic Autoencoder for Molecular Design. (arXiv:2310.08685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08685
&lt;/p&gt;
&lt;p&gt;
&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;KAE&#65289;&#26159;&#19968;&#31181;&#22312;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#22686;&#24378;&#24615;&#33021;&#30340;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#12290;KAE&#23454;&#29616;&#20102;&#26377;&#25928;&#29983;&#25104;&#21644;&#20934;&#30830;&#37325;&#26500;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#22810;&#26679;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;KAE&#36824;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#29983;&#25104;&#20998;&#23376;&#65292;&#24182;&#22312;&#20998;&#23376;&#23545;&#25509;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;KAE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#20998;&#23376;&#35774;&#35745;&#24615;&#33021;&#12290;KAE&#22522;&#20110;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65306;&#20462;&#25913;&#30340;&#26368;&#22823;&#22343;&#21248;&#20301;&#31227;&#21644;&#21152;&#26435;&#37325;&#26500;&#12290;KAE&#35299;&#20915;&#20102;&#21516;&#26102;&#23454;&#29616;&#26377;&#25928;&#29983;&#25104;&#21644;&#20934;&#30830;&#37325;&#26500;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;KAE&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#29420;&#31435;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#37325;&#26500;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#12290;KAE&#23454;&#29616;&#20102;&#26465;&#20214;&#29983;&#25104;&#65292;&#24182;&#20801;&#35768;&#22522;&#20110;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#22312;&#21463;&#38480;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;KAE&#21487;&#20197;&#26681;&#25454;&#20998;&#23376;&#23545;&#25509;&#24212;&#29992;&#20013;&#30340;&#26377;&#21033;&#32467;&#21512;&#20146;&#21644;&#21147;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;AutoDock Vina&#21644;Glide&#24471;&#20998;&#36827;&#34892;&#30830;&#35748;&#65292;&#32988;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#29616;&#26377;&#20505;&#36873;&#20998;&#23376;&#12290;&#38500;&#20102;&#20998;&#23376;&#35774;&#35745;&#65292;&#25105;&#20204;&#39044;&#26399;KAE&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Kernel-Elastic Autoencoder (KAE), a self-supervised generative model based on the transformer architecture with enhanced performance for molecular design. KAE is formulated based on two novel loss functions: modified maximum mean discrepancy and weighted reconstruction. KAE addresses the long-standing challenge of achieving valid generation and accurate reconstruction at the same time. KAE achieves remarkable diversity in molecule generation while maintaining near-perfect reconstructions on the independent testing dataset, surpassing previous molecule-generating models. KAE enables conditional generation and allows for decoding based on beam search resulting in state-of-the-art performance in constrained optimizations. Furthermore, KAE can generate molecules conditional to favorable binding affinities in docking applications as confirmed by AutoDock Vina and Glide scores, outperforming all existing candidates from the training dataset. Beyond molecular design, we antic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#65307;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#21021;&#27493;&#27979;&#35797;&#34920;&#26126;&#65292;&#26041;&#27861;&#23545;Ising&#27169;&#22411;&#30340;&#20020;&#30028;&#25351;&#25968;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08282</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#33258;&#30456;&#20284;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Data driven modeling of self-similar dynamics. (arXiv:2310.08282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#65307;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#21021;&#27493;&#27979;&#35797;&#34920;&#26126;&#65292;&#26041;&#27861;&#23545;Ising&#27169;&#22411;&#30340;&#20020;&#30028;&#25351;&#25968;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#23545;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25968;&#25454;&#39537;&#21160;&#22810;&#23610;&#24230;&#24314;&#27169;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#31995;&#32479;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#30456;&#20284;&#24615;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#34920;&#26126;&#22823;&#35268;&#27169;&#22797;&#26434;&#31995;&#32479;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#12290;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#27604;&#36739;&#21644;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#20174;&#21160;&#21147;&#23398;&#20013;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#23545;Ising&#27169;&#22411;&#30340;&#21021;&#27493;&#27979;&#35797;&#20135;&#29983;&#20102;&#19982;&#29702;&#35770;&#19968;&#33268;&#30340;&#20020;&#30028;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theo
&lt;/p&gt;</description></item><item><title>CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.07240</link><description>&lt;p&gt;
CacheGen&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24555;&#36895;&#19978;&#19979;&#25991;&#21152;&#36733;
&lt;/p&gt;
&lt;p&gt;
CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07240
&lt;/p&gt;
&lt;p&gt;
CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25215;&#25285;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20854;&#36755;&#20837;&#23558;&#25972;&#21512;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#24212;&#23545;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#25110;&#29992;&#25143;&#29305;&#23450;&#30340;&#23545;&#35805;&#21382;&#21490;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#21709;&#24212;&#24335;&#30340;LLM&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#25152;&#26377;&#19978;&#19979;&#25991;&#34987;&#33719;&#21462;&#21644;LLM&#22788;&#29702;&#20043;&#21069;&#65292;&#26080;&#27861;&#29983;&#25104;&#20219;&#20309;&#20869;&#23481;&#12290;&#29616;&#26377;&#31995;&#32479;&#20165;&#36890;&#36807;&#20248;&#21270;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#35745;&#31639;&#24310;&#36831;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#32531;&#23384;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#20013;&#38388;&#38190;&#20540;&#29305;&#24449;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#19978;&#19979;&#25991;&#33719;&#21462;&#30340;&#32593;&#32476;&#24310;&#36831;&#26356;&#38271;&#65288;&#20363;&#22914;&#65292;&#38190;&#20540;&#29305;&#24449;&#28040;&#32791;&#30340;&#24102;&#23485;&#27604;&#25991;&#26412;&#19978;&#19979;&#25991;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CacheGen&#65292;&#20197;&#26368;&#23567;&#21270;LLM&#19978;&#19979;&#25991;&#33719;&#21462;&#21644;&#22788;&#29702;&#30340;&#24310;&#36831;&#12290;CacheGen&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#29305;&#24449;&#21387;&#32553;&#20026;&#26356;&#32039;&#20945;&#30340;&#27604;&#29305;&#27969;&#34920;&#31034;&#65292;&#20943;&#23569;&#20102;&#20256;&#36755;&#25152;&#38656;&#30340;&#24102;&#23485;&#12290;&#32534;&#30721;&#22120;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#37327;&#21270;&#21644;......
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04870</link><description>&lt;p&gt;
Lemur&#65306;&#22312;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#23637;&#31034;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#39564;&#35777;&#24037;&#20855;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#35770;&#65292;&#23558;&#20854;&#20316;&#20026;&#25512;&#23548;&#35268;&#21017;&#30340;&#38598;&#21512;&#36827;&#34892;&#35770;&#35777;&#20854;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#25512;&#29702;&#24418;&#25104;&#20026;&#19968;&#20010;&#23436;&#22791;&#30340;&#33258;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#36825;&#22312;&#19968;&#32452;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#24102;&#26469;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#65292;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.02710</link><description>&lt;p&gt;
&#26412;&#22320;&#25628;&#32034;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Local Search GFlowNets. (arXiv:2310.02710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#65292;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#26159;&#19968;&#31181;&#23398;&#20064;&#19982;&#22870;&#21169;&#25104;&#27604;&#20363;&#30340;&#31163;&#25955;&#23545;&#35937;&#20998;&#24067;&#30340;&#25674;&#36824;&#37319;&#26679;&#26041;&#27861;&#12290;GFlowNets&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#26679;&#26412;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#24191;&#27867;&#26679;&#26412;&#31354;&#38388;&#19978;&#30340;&#36807;&#24230;&#25506;&#32034;&#65292;&#26377;&#26102;&#38590;&#20197;&#19968;&#33268;&#22320;&#29983;&#25104;&#39640;&#22870;&#21169;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#12290;&#36825;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20256;&#32479;&#30340;GFlowNet&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#26041;&#26696;&#21017;&#20351;&#29992;&#27491;&#21521;&#31574;&#30053;&#20174;&#22836;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#22312;&#20960;&#20010;&#29983;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space. This paper proposes to train GFlowNets with local search which focuses on exploiting high rewarded sample space to resolve this issue. Our main idea is to explore the local neighborhood via destruction and reconstruction guided by backward and forward policies, respectively. This allows biasing the samples toward high-reward solutions, which is not possible for a typical GFlowNet solution generation scheme which uses the forward policy to generate the solution from scratch. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks. Source code is available: \url{https://github.com/dbsxodud-11/ls_gfn}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01777</link><description>&lt;p&gt;
&#37319;&#29992;&#20272;&#35745;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#65288;SEA&#65289;
&lt;/p&gt;
&lt;p&gt;
SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;transformer&#26550;&#26500;&#22312;&#38656;&#35201;&#23545;&#24207;&#21015;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#24314;&#27169;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#32447;&#24615;&#36924;&#36817;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#20174;&#25945;&#24072;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#31232;&#30095;&#21644;&#32447;&#24615;&#26041;&#27861;&#22914;&#26524;&#19981;&#33021;&#20135;&#29983;&#23436;&#20840;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#36824;&#21487;&#33021;&#22833;&#21435;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEA&#65306;&#37319;&#29992;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;SEA&#36890;&#36807;&#22522;&#20110;&#26680;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#20272;&#35745;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#23545;&#23436;&#25972;&#27880;&#24847;&#21147;&#30697;&#38453;&#36827;&#34892;&#31232;&#30095;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture has made breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, transformers struggle with long sequences due to the quadratic complexity of the attention operation, and previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches may also lose interpretability if they do not produce full quadratic attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k se
&lt;/p&gt;</description></item><item><title>TACTiS-2&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#32447;&#24615;&#21442;&#25968;&#25968;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01327</link><description>&lt;p&gt;
TACTiS-2&#65306;&#26356;&#22909;&#12289;&#26356;&#24555;&#12289;&#26356;&#31616;&#21333;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series. (arXiv:2310.01327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01327
&lt;/p&gt;
&lt;p&gt;
TACTiS-2&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#32447;&#24615;&#21442;&#25968;&#25968;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#29992;&#20110;&#22810;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#26088;&#22312;&#28789;&#27963;&#22320;&#22788;&#29702;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#22522;&#20110;&#32852;&#21512;&#20998;&#24067;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#22522;&#20110;Transformer&#30340;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65288;TACTiS&#65289;&#12290;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20998;&#24067;&#21442;&#25968;&#25968;&#37327;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#32447;&#24615;&#32780;&#38750;&#38454;&#20056;&#20851;&#31995;&#12290;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#38656;&#35201;&#24341;&#20837;&#19968;&#31181;&#35757;&#32451;&#35838;&#31243;&#65292;&#24182;&#19988;&#38656;&#35201;&#23545;&#21407;&#22987;&#26550;&#26500;&#36827;&#34892;&#24517;&#35201;&#30340;&#25913;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24471;&#21040;&#30340;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#25913;&#21892;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#22312;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#28789;&#27963;&#24615;&#65292;&#22914;&#26080;&#32541;&#22788;&#29702;&#19981;&#23545;&#40784;&#21644;&#37319;&#26679;&#19981;&#22343;&#21248;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.00290</link><description>&lt;p&gt;
&#23436;&#32654;&#39044;&#27979;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data. (arXiv:2310.00290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#26159;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#27627;&#26080;&#30097;&#38382;&#65292;RC&#23558;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#26469;&#39044;&#27979;&#27169;&#22411;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#12289;&#39640;&#36895;&#24230;&#21644;&#39640;&#35745;&#31639;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#30452;&#21040;&#26368;&#36817;&#25165;&#24320;&#22987;&#12290;Bollt&#65288;2021&#65289;&#38416;&#26126;&#20102;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;Wold&#20998;&#35299;&#23450;&#29702;&#26159;&#29702;&#35299;&#36825;&#20123;&#32467;&#26500;&#30340;&#37324;&#31243;&#30865;&#12290;&#22312;&#38125;&#35760;&#36825;&#19968;&#33879;&#21517;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;RC&#31070;&#32463;&#32593;&#32476;&#20013;&#36755;&#20837;&#21644;&#24490;&#29615;&#26435;&#37325;&#30697;&#38453;&#30340;&#38544;&#34255;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26500;&#23545;&#20110;AR&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#20102;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15531</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36817;&#26399;&#23637;&#31034;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#25928;&#22320;&#20026;LLMs&#25552;&#20379;&#26381;&#21153;&#26041;&#38754;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#22823;&#20869;&#23384;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#25209;&#37327;&#25512;&#29702;&#35774;&#32622;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#20013;&#12290;&#20165;&#23545;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#23384;&#22312;&#22823;&#24133;&#24230;&#28608;&#27963;&#24322;&#24120;&#20540;&#65292;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21487;&#21462;&#30340;&#24322;&#24120;&#25928;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#65288;IC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#30340;per-IC&#37327;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#27599;&#20010;&#36755;&#20986;&#36890;&#36947;&#65288;OC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21160;&#26426;&#26159;&#35266;&#23519;&#21040;&#28608;&#27963;&#24322;&#24120;&#20540;&#24433;&#21709;&#26435;&#37325;&#30697;&#38453;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#22240;&#27492;&#22312;IC&#26041;&#21521;&#19978;&#23545;&#26435;&#37325;&#36827;&#34892;&#31867;&#20284;&#20998;&#32452;&#21487;&#20197;&#23558;&#24322;&#24120;&#20540;&#38548;&#31163;&#21040;&#19968;&#20010;&#20998;&#32452;&#20869;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#28608;&#27963;&#30340;&#24322;&#24120;&#20540;&#24182;&#19981;&#20915;&#23450;&#37327;&#21270;&#30340;&#38590;&#24230;&#65292;&#20854;&#22266;&#26377;&#30340;&#26435;&#37325;&#25935;&#24863;&#24615;&#20063;&#23384;&#22312;&#12290;&#36890;&#36807;per-IC&#37327;&#21270;&#20316;&#20026;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.15366</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#65306;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences. (arXiv:2309.15366v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15366
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#21183;&#26159;&#20854;&#20801;&#35768;&#23545;&#26681;&#25454;&#24191;&#27867;&#27010;&#29575;&#27979;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#32479;&#19968;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#30740;&#31350;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#27979;&#24230;&#20256;&#36882;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#19977;&#35282;&#20256;&#36882;&#26144;&#23556;&#30340;&#20351;&#29992;&#65292;&#20316;&#20026;&#25903;&#25345;&#29983;&#29289;&#31185;&#23398;&#30740;&#31350;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#31232;&#30095;&#25968;&#25454;&#22330;&#26223;&#22312;&#36752;&#23556;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#31995;&#21015;&#65288;&#31232;&#30095;&#30340;&#65289;&#33258;&#36866;&#24212;&#20256;&#36882;&#26144;&#23556;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#36825;&#20123;&#26144;&#23556;&#26159;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#31995;&#21015;&#21487;&#29992;&#25968;&#25454;&#26679;&#26412;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#32771;&#34385;&#30340;&#36752;&#23556;&#29983;&#29289;&#23398;&#24212;&#29992;&#20013;&#65292;&#27492;&#26041;&#27861;&#20026;&#29983;&#25104;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14552</link><description>&lt;p&gt;
&#31283;&#23450;&#25918;&#32622;&#30340;&#22806;&#37096;&#25509;&#35302;&#22359;&#30340;&#35302;&#35273;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Tactile Estimation of Extrinsic Contact Patch for Stable Placement. (arXiv:2309.14552v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#20316;&#25216;&#33021;&#26469;&#35828;&#65292;&#20934;&#30830;&#24863;&#30693;&#25509;&#35302;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#26426;&#22120;&#20154;&#35774;&#35745;&#21453;&#39304;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#35813;&#26426;&#22120;&#20154;&#24517;&#39035;&#23398;&#20064;&#23558;&#22797;&#26434;&#24418;&#29366;&#30340;&#29289;&#20307;&#22534;&#21472;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#38750;&#24120;&#36731;&#24494;&#30340;&#25509;&#35302;&#20132;&#20114;&#26469;&#25512;&#29702;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26681;&#25454;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#30340;&#35302;&#35273;&#35835;&#25968;&#26469;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21147;&#21644;&#35302;&#35273;&#35266;&#27979;&#26469;&#20272;&#35745;&#25235;&#21462;&#29289;&#20307;&#21644;&#20854;&#29615;&#22659;&#20043;&#38388;&#30340;&#25509;&#35302;&#21306;&#22495;&#65292;&#20174;&#32780;&#20272;&#35745;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#25509;&#35302;&#21306;&#22495;&#21487;&#20197;&#29992;&#26469;&#20272;&#35745;&#37322;&#25918;&#25235;&#21462;&#21518;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#27454;&#38750;&#24120;&#27969;&#34892;&#30340;&#26827;&#30424;&#28216;&#25103;&#20013;&#20351;&#29992;&#20102;&#22810;&#31181;&#29289;&#20307;&#23545;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#20351;&#29992;&#20462;&#25913;&#21453;&#39304;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#31867;&#21035;&#21306;&#20998;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07289</link><description>&lt;p&gt;
&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#20351;&#29992;&#20462;&#25913;&#21453;&#39304;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#31867;&#21035;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#24182;&#27979;&#35797;&#20102;&#19968;&#20010;&#23454;&#26102;&#25511;&#21046;&#29992;&#25143;&#30028;&#38754;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#33109;&#24102;&#37197;&#32622;&#30340;&#20843;&#20010;&#30005;&#26497;&#20013;&#25552;&#21462;&#34920;&#38754;&#32908;&#30005;&#27963;&#21160;&#65288;sEMG&#65289;&#12290;sEMG&#25968;&#25454;&#34987;&#23454;&#26102;&#27969;&#20837;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25163;&#21183;&#30340;&#23454;&#26102;&#20998;&#31867;&#12290;&#22312;&#21021;&#22987;&#27169;&#22411;&#26657;&#20934;&#21518;&#65292;&#21442;&#19982;&#32773;&#22312;&#20154;&#31867;&#23398;&#20064;&#38454;&#27573;&#20013;&#34987;&#25552;&#20379;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#21453;&#39304;&#65306;&#30495;&#23454;&#21453;&#39304;&#65292;&#22312;&#20854;&#20013;&#39044;&#27979;&#30340;&#25163;&#21183;&#20998;&#31867;&#31639;&#27861;&#30340;&#27010;&#29575;&#34987;&#26174;&#31034;&#32780;&#19981;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#65307;&#20462;&#25913;&#21453;&#39304;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#36825;&#20123;&#27010;&#29575;&#36827;&#34892;&#20102;&#38169;&#35823;&#30340;&#38544;&#34255;&#22686;&#24378;&#22788;&#29702;&#65307;&#21644;&#26080;&#21453;&#39304;&#12290;&#28982;&#21518;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36855;&#20320;&#28216;&#25103;&#35780;&#20272;&#20102;&#29992;&#25143;&#30340;&#34920;&#29616;&#65292;&#35201;&#27714;&#34987;&#35797;&#20351;&#29992;&#20843;&#20010;&#25163;&#21183;&#26469;&#25805;&#20316;&#28216;&#25103;&#35282;&#33394;&#23436;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#20462;&#25913;&#21453;&#39304;&#26465;&#20214;&#19979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25163;&#21183;&#31867;&#21035;&#21306;&#20998;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We designed and tested a system for real-time control of a user interface by extracting surface electromyographic (sEMG) activity from eight electrodes in a wrist-band configuration. sEMG data were streamed into a machine-learning algorithm that classified hand gestures in real-time. After an initial model calibration, participants were presented with one of three types of feedback during a human-learning stage: veridical feedback, in which predicted probabilities from the gesture classification algorithm were displayed without alteration, modified feedback, in which we applied a hidden augmentation of error to these probabilities, and no feedback. User performance was then evaluated in a series of minigames, in which subjects were required to use eight gestures to manipulate their game avatar to complete a task. Experimental results indicated that, relative to baseline, the modified feedback condition led to significantly improved accuracy and improved gesture class separation. These 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32769;&#24180;&#20154;&#27491;&#24120;&#29983;&#27963;&#26465;&#20214;&#19979;&#36830;&#32493;&#30417;&#27979;&#20854;&#35748;&#30693;&#27700;&#24179;&#65292;&#24182;&#33021;&#22815;&#39044;&#27979;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#24773;&#20917;&#65292;&#20026;&#26089;&#26399;&#24178;&#39044;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07133</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#35782;&#21035;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#32769;&#24180;&#20154;
&lt;/p&gt;
&lt;p&gt;
Using wearable device-based machine learning models to autonomously identify older adults with poor cognition. (arXiv:2309.07133v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07133
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32769;&#24180;&#20154;&#27491;&#24120;&#29983;&#27963;&#26465;&#20214;&#19979;&#36830;&#32493;&#30417;&#27979;&#20854;&#35748;&#30693;&#27700;&#24179;&#65292;&#24182;&#33021;&#22815;&#39044;&#27979;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#24773;&#20917;&#65292;&#20026;&#26089;&#26399;&#24178;&#39044;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#35748;&#30693;&#27979;&#35797;&#23545;&#24739;&#32773;&#21644;&#20020;&#24202;&#21307;&#29983;&#26469;&#35828;&#38750;&#24120;&#32791;&#26102;&#12290;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#22312;&#27491;&#24120;&#29983;&#27963;&#26465;&#20214;&#19979;&#36827;&#34892;&#25345;&#32493;&#30340;&#20581;&#24247;&#30417;&#27979;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#26089;&#26399;&#24178;&#39044;&#35748;&#30693;&#38556;&#30861;&#32769;&#24180;&#20154;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#21462;&#20102;&#19982;&#29983;&#29289;&#38047;&#33410;&#24459;&#12289;&#29615;&#22659;&#20809;&#29031;&#26292;&#38706;&#12289;&#36523;&#20307;&#27963;&#21160;&#27700;&#24179;&#12289;&#30561;&#30496;&#21644;&#20449;&#21495;&#22788;&#29702;&#30456;&#20851;&#30340;&#26032;&#39062;&#21487;&#31359;&#25140;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26681;&#25454;&#25968;&#23383;&#31526;&#21495;&#26367;&#20195;&#27979;&#35797;&#65288;DSST&#65289;&#12289;&#24314;&#31435;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30331;&#35760;&#31807;&#23398;&#20064;&#23376;&#27979;&#39564;&#65288;CERAD-WL&#65289;&#21644;&#21160;&#29289;&#27969;&#21033;&#24615;&#27979;&#35797;&#65288;AFT&#65289;&#30340;&#32467;&#26524;&#26469;&#39044;&#27979;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#21253;&#21547;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#25945;&#32946;&#27700;&#24179;&#12289;&#23130;&#23035;&#29366;&#20917;&#12289;&#23478;&#24237;&#25910;&#20837;&#12289;&#31958;&#23615;&#30149;&#29366;&#24577;&#21644;&#25233;&#37057;&#29366;&#20917;&#31561;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#19977;&#20010;&#35748;&#30693;&#32467;&#26524;&#26102;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;
Conducting cognitive tests is time-consuming for patients and clinicians. Wearable device-based prediction models allow for continuous health monitoring under normal living conditions and could offer an alternative to identifying older adults with cognitive impairments for early interventions. In this study, we first derived novel wearable-based features related to circadian rhythms, ambient light exposure, physical activity levels, sleep, and signal processing. Then, we quantified the ability of wearable-based machine-learning models to predict poor cognition based on outcomes from the Digit Symbol Substitution Test (DSST), the Consortium to Establish a Registry for Alzheimers Disease Word-Learning subtest (CERAD-WL), and the Animal Fluency Test (AFT). We found that the wearable-based models had significantly higher AUCs when predicting all three cognitive outcomes compared to benchmark models containing age, sex, education, marital status, household income, diabetic status, depressio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InstaFlow&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36716;&#21270;&#20026;&#19968;&#27493;&#27169;&#22411;&#65292;&#36890;&#36807;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#25552;&#39640;&#20102;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#12289;&#39640;&#36895;&#24230;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.06380</link><description>&lt;p&gt;
InstaFlow: &#19968;&#27493;&#21363;&#21487;&#23454;&#29616;&#39640;&#36136;&#37327;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. (arXiv:2309.06380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InstaFlow&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36716;&#21270;&#20026;&#19968;&#27493;&#27169;&#22411;&#65292;&#36890;&#36807;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#25552;&#39640;&#20102;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#12289;&#39640;&#36895;&#24230;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#36136;&#37327;&#21644;&#21019;&#36896;&#21147;&#24443;&#24213;&#25913;&#21464;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20854;&#22810;&#27493;&#37319;&#26679;&#36807;&#31243;&#34987;&#35748;&#20026;&#24456;&#24930;&#65292;&#36890;&#24120;&#38656;&#35201;&#21313;&#20960;&#27493;&#25512;&#26029;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20197;&#24448;&#35797;&#22270;&#36890;&#36807;&#33976;&#39311;&#26469;&#25552;&#39640;&#37319;&#26679;&#36895;&#24230;&#21644;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#23581;&#35797;&#37117;&#26410;&#33021;&#23454;&#29616;&#21151;&#33021;&#40784;&#20840;&#30340;&#19968;&#27493;&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#21363;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21040;&#30446;&#21069;&#20026;&#27490;&#21482;&#24212;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#20854;&#37325;&#26032;&#27969;&#21160;&#30340;&#36807;&#31243;&#65292;&#23427;&#23558;&#27010;&#29575;&#27969;&#30340;&#36712;&#36857;&#21464;&#24471;&#30452;&#32447;&#65292;&#25913;&#36827;&#20102;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#32806;&#21512;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23398;&#29983;&#27169;&#22411;&#20415;&#20110;&#33976;&#39311;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#26465;&#20214;&#30340;&#27969;&#31243;&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SD&#65289;&#36716;&#21270;&#20026;&#36229;&#24555;&#36895;&#30340;&#19968;&#27493;&#27169;&#22411;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21457;&#29616;&#37325;&#26032;&#27969;&#21160;&#22312;&#25913;&#21892;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20973;&#20511;&#25105;&#20204;&#30340;&#26032;&#27969;&#31243;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#36739;&#24555;&#30340;&#36895;&#24230;&#30452;&#25509;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.05153</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#33021;&#37327;&#22522;&#20934;&#27169;&#22411;&#65288;EBMs&#65289;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#36739;&#38271;&#12290;&#22240;&#27492;&#65292;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#65288;&#22914;GANs&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#21463;&#26368;&#36817;&#36890;&#36807;&#26368;&#22823;&#21270;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;DRL&#65289;&#26469;&#23398;&#20064;EBMs&#30340;&#21162;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#21487;&#34892;&#22320;&#23398;&#20064;&#21644;&#20174;&#19968;&#31995;&#21015;EBMs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20123;EBMs&#23450;&#20041;&#22312;&#36234;&#26469;&#36234;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#65292;&#24182;&#19982;&#27599;&#20010;EBM&#30340;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#12290;&#22312;&#27599;&#20010;&#22122;&#22768;&#27700;&#24179;&#19978;&#65292;&#21021;&#22987;&#21270;&#27169;&#22411;&#23398;&#20064;&#22312;EBM&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#20998;&#25674;&#65292;&#32780;&#20004;&#20010;&#27169;&#22411;&#22312;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#20869;&#20849;&#21516;&#20272;&#35745;&#12290;&#21021;&#22987;&#21270;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20316;&#20026;&#36215;&#22987;&#28857;&#65292;&#32463;&#36807;EBM&#30340;&#20960;&#20010;&#37319;&#26679;&#27493;&#39588;&#36827;&#34892;&#25913;&#36827;&#12290;&#36890;&#36807;&#25913;&#36827;&#21518;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24674;&#22797;&#20284;&#28982;&#26469;&#20248;&#21270;EBM&#12290;
&lt;/p&gt;
&lt;p&gt;
Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
&lt;/p&gt;</description></item><item><title>ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03103</link><description>&lt;p&gt;
ContrastWSD: &#20351;&#29992;&#35789;&#20041;&#28040;&#23696;&#21152;&#24378;&#38544;&#21947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure. (arXiv:2309.03103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03103
&lt;/p&gt;
&lt;p&gt;
ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ContrastWSD&#65292;&#19968;&#31181;&#22522;&#20110;RoBERTa&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#38598;&#25104;&#20102;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;(MIP)&#21644;&#35789;&#20041;&#28040;&#23696;(WSD)&#26469;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#30830;&#23450;&#23427;&#22312;&#21477;&#23376;&#20013;&#26159;&#21542;&#20197;&#38544;&#21947;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;WSD&#27169;&#22411;&#24471;&#20986;&#30340;&#21333;&#35789;&#35789;&#20041;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22686;&#24378;&#20102;&#38544;&#21947;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#36229;&#36807;&#20102;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#20165;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#20854;&#20182;&#22806;&#37096;&#30693;&#35782;&#30340;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#24378;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#25512;&#36827;&#38544;&#21947;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#20462;&#25913;&#26368;&#23567;&#20108;&#20056;GAN&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;SAN&#21487;&#20197;&#25913;&#21892;&#22768;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02836</link><description>&lt;p&gt;
BigVSAN: &#21033;&#29992;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#22686;&#24378;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network. (arXiv:2309.02836v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#20462;&#25913;&#26368;&#23567;&#20108;&#20056;GAN&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;SAN&#21487;&#20197;&#25913;&#21892;&#22768;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#22768;&#30721;&#22120;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#27604;&#23454;&#26102;&#26356;&#24555;&#22320;&#21512;&#25104;&#39640;&#20445;&#30495;&#38899;&#39057;&#27874;&#24418;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30740;&#31350;&#25253;&#21578;&#21457;&#29616;&#22823;&#22810;&#25968;GAN&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26080;&#27861;&#33719;&#24471;&#21306;&#20998;&#30495;&#20551;&#25968;&#25454;&#30340;&#26368;&#20339;&#25237;&#24433;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#30340;GAN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SAN&#22312;&#22768;&#30721;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#20462;&#25913;&#20102;&#22823;&#22810;&#25968;&#22522;&#20110;GAN&#30340;&#22768;&#30721;&#22120;&#25152;&#37319;&#29992;&#30340;&#26368;&#23567;&#20108;&#20056;GAN&#65292;&#20351;&#20854;&#25439;&#22833;&#20989;&#25968;&#28385;&#36275;SAN&#30340;&#35201;&#27714;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SAN&#21487;&#20197;&#36890;&#36807;&#23567;&#30340;&#20462;&#25913;&#25913;&#21892;&#21253;&#25324;BigVGAN&#22312;&#20869;&#30340;&#22522;&#20110;GAN&#30340;&#22768;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/sony/bigvsan&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial network (GAN)-based vocoders have been intensively studied because they can synthesize high-fidelity audio waveforms faster than real-time. However, it has been reported that most GANs fail to obtain the optimal projection for discriminating between real and fake data in the feature space. In the literature, it has been demonstrated that slicing adversarial network (SAN), an improved GAN training framework that can find the optimal projection, is effective in the image generation task. In this paper, we investigate the effectiveness of SAN in the vocoding task. For this purpose, we propose a scheme to modify least-squares GAN, which most GAN-based vocoders adopt, so that their loss functions satisfy the requirements of SAN. Through our experiments, we demonstrate that SAN can improve the performance of GAN-based vocoders, including BigVGAN, with small modifications. Our code is available at https://github.com/sony/bigvsan.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.01069</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#24314;&#27169;&#21160;&#21147;&#31995;&#32479;&#26159;&#29616;&#20195;&#31185;&#23398;&#21644;&#24037;&#31243;&#25968;&#25454;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290; &#21704;&#23494;&#39039;&#31995;&#32479;&#26159;&#19968;&#31867;&#22522;&#26412;&#19988;&#24191;&#27867;&#23384;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290; &#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27721;&#23494;&#23572;&#39039;&#26041;&#31243;&#30340;&#23398;&#20064;&#20559;&#24046;&#19979;&#65292;&#20174;&#31163;&#25955;&#35266;&#27979;&#30340;&#21521;&#37327;&#22330;&#20013;&#26080;&#30417;&#30563;&#22320;&#22238;&#24402;&#21160;&#21147;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#37327;&#12290;&#28982;&#32780;&#65292;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#30456;&#23545;&#20110;&#26679;&#26412;&#25968;&#37327;&#26159;&#24456;&#22823;&#30340;&#12290; &#26368;&#36817;&#21457;&#29616;&#30340;&#19968;&#31181;&#32531;&#35299;&#29366;&#24577;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#65292;&#24182;&#23558;&#35813;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#23884;&#20837;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26681;&#25454;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26415;&#35821;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20998;&#31163;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#23884;&#20837;&#20102;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.12581</link><description>&lt;p&gt;
&#19968;&#31181;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#26041;&#27861;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d&#65289;&#20551;&#35774;&#19979;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#20110;&#34987;&#25915;&#20987;&#23458;&#25143;&#31471;&#27604;&#29575;$\epsilon$&#20855;&#26377;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;$\epsilon$&#26377;&#31934;&#30830;&#30340;&#30693;&#35782;&#12290;&#31532;&#19977;&#65292;&#23427;&#20801;&#35768;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#22343;&#31561;&#30340;&#25968;&#25454;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#21253;&#25324;&#38750;i.i.d&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#23458;&#25143;&#31471;&#20855;&#26377;&#30053;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#35780;&#20272;&#30446;&#26631;&#21644;&#25351;&#26631;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;FedEval&#65292;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.11841</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;&#35780;&#20272;&#65306;&#30446;&#26631;&#21644;&#25351;&#26631;&#30340;&#35843;&#26597;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey for Federated Learning Evaluations: Goals and Measures. (arXiv:2308.11841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#35780;&#20272;&#30446;&#26631;&#21644;&#25351;&#26631;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;FedEval&#65292;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26159;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;&#19968;&#20010;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#20854;&#39044;&#26399;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26032;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;FL&#20855;&#26377;&#36328;&#23398;&#31185;&#24615;&#21644;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#65288;&#22914;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#65289;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#20027;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#28982;&#21518;&#25506;&#35752;&#20102;&#27599;&#20010;&#30446;&#26631;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;FedEval&#65292;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#21644;&#20840;&#38754;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#35780;&#20272;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#20854;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#35780;&#20272;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
&lt;/p&gt;</description></item><item><title>ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.10457</link><description>&lt;p&gt;
ALI-DPFL: &#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10457
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20849;&#20139;&#35757;&#32451;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#20801;&#35768;&#22810;&#20010;&#35774;&#22791;&#25110;&#32452;&#32455;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#23545;&#36825;&#20123;&#35757;&#32451;&#21442;&#25968;&#30340;&#25512;&#29702;&#25915;&#20987;&#65288;&#20363;&#22914;&#24046;&#20998;&#25915;&#20987;&#65289;&#26469;&#25512;&#26029;&#20010;&#20307;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#24046;&#20998;&#38544;&#31169;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#20197;&#38450;&#27490;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#32771;&#34385;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65292;&#20854;&#20013;&#26082;&#26377;&#38544;&#31169;&#39044;&#31639;&#21463;&#38480;&#65292;&#21448;&#26377;&#36890;&#20449;&#36718;&#27425;&#21463;&#38480;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#25214;&#21040;&#22312;&#20219;&#24847;&#20004;&#20010;&#39034;&#24207;&#20840;&#23616;&#26356;&#26032;&#20043;&#38388;&#30340;&#23458;&#25143;&#26426;&#20043;&#38388;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#26412;&#22320;&#36845;&#20195;&#27425;&#25968;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ALI-DPFL&#65289;&#12290;&#25105;&#20204;&#22312;FashionMNIST&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.07942</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis. (arXiv:2308.07942v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07942
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#20174;&#35757;&#32451;&#22270;&#35889;&#20013;&#23398;&#20064;&#25512;&#29702;&#27169;&#24335;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#22312;&#20998;&#31163;&#30340;&#27979;&#35797;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#20284;&#20046;&#24456;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#26126;&#26174;&#19981;&#22914;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22914;NBFNet&#12290;&#25105;&#20204;&#20551;&#35774;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#26159;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#65288;i&#65289;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#26681;&#26412;&#27809;&#26377;&#25490;&#21517;&#65292;&#65288;ii&#65289;&#22312;&#30830;&#23450;&#32473;&#23450;&#38142;&#25509;&#39044;&#27979;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#26102;&#65292;&#21482;&#32771;&#34385;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20123;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#25509;&#36817;NBFNet&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#32771;&#34385;&#21040;&#30340;&#21464;&#20307;&#21482;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#22686;&#24378;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#26080;&#32447;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#25910;&#25947;&#36895;&#24230;&#30340;&#26368;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.01562</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65306;&#21098;&#26525;&#35299;&#20915;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#22686;&#24378;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#26080;&#32447;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#25910;&#25947;&#36895;&#24230;&#30340;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26080;&#32447;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#32456;&#31471;&#29992;&#25143;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20043;&#38388;&#23384;&#22312;&#22810;&#20010;&#23618;&#32423;&#65292;&#29992;&#25143;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#30005;&#27744;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;&#26381;&#21153;&#22522;&#31449;&#20855;&#26377;&#22266;&#23450;&#30340;&#24102;&#23485;&#12290;&#37492;&#20110;&#36825;&#20123;&#23454;&#38469;&#32422;&#26463;&#21644;&#31995;&#32479;&#27169;&#22411;&#65292;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#21098;&#26525;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#32593;&#32476;&#30340;&#21098;&#26525;&#22686;&#24378;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#27169;&#22411;&#21098;&#26525;&#21644;&#23458;&#25143;&#31471;&#19982;&#20851;&#32852;&#22522;&#31449;&#20043;&#38388;&#30340;&#26080;&#32447;&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21098;&#26525;&#27604;&#29575;&#12289;&#23458;&#25143;&#31471;&#30340;&#20013;&#22830;&#22788;&#29702;&#22120;&#65288;CPU&#65289;&#39057;&#29575;&#21644;&#20256;&#36755;&#21151;&#29575;&#65292;&#20197;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#25910;&#25947;&#30028;&#30340;&#21487;&#25511;&#39033;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21407;&#22987;&#38382;&#39064;&#19981;&#26159;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#36880;&#27493;&#20984;&#36924;&#36817;&#65288;SCA&#65289;&#26041;&#27861;&#65292;&#32852;&#21512;&#20248;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21453;&#24605;&#35780;&#20272;&#24180;&#40836;&#20272;&#35745;&#23454;&#36341;&#30340;&#21628;&#21505;&#65292;&#23545;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#24494;&#19981;&#36275;&#36947;&#65292;&#32780;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26356;&#22823;&#12290;&#30740;&#31350;&#21033;&#29992;&#24471;&#21040;&#30340;&#35265;&#35299;&#25552;&#20986;&#20351;&#29992;FaRL&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04570</link><description>&lt;p&gt;
&#35780;&#20272;&#24180;&#40836;&#20272;&#35745;&#23454;&#36341;&#30340;&#21453;&#24605;&#21628;&#21505;&#65306;&#29616;&#26377;&#25216;&#26415;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#32479;&#19968;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark. (arXiv:2307.04570v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21453;&#24605;&#35780;&#20272;&#24180;&#40836;&#20272;&#35745;&#23454;&#36341;&#30340;&#21628;&#21505;&#65292;&#23545;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#24494;&#19981;&#36275;&#36947;&#65292;&#32780;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26356;&#22823;&#12290;&#30740;&#31350;&#21033;&#29992;&#24471;&#21040;&#30340;&#35265;&#35299;&#25552;&#20986;&#20351;&#29992;FaRL&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22522;&#20934;&#27969;&#31243;&#30340;&#19981;&#19968;&#33268;&#24615;&#23548;&#33268;&#30340;&#21457;&#24067;&#32467;&#26524;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#27604;&#36739;&#19981;&#21516;&#30340;&#24180;&#40836;&#20272;&#35745;&#26041;&#27861;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#25351;&#20986;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#26041;&#27861;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#25345;&#32493;&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#28982;&#32780;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#36825;&#20123;&#22768;&#26126;&#25552;&#20986;&#36136;&#30097;&#12290;&#26412;&#25991;&#35782;&#21035;&#20986;&#24403;&#21069;&#20351;&#29992;&#30340;&#35780;&#20272;&#21327;&#35758;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#29712;&#30862;&#20294;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#35299;&#20915;&#23427;&#20204;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#35813;&#21327;&#35758;&#30340;&#20855;&#20307;&#31034;&#20363;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#21327;&#35758;&#23545;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#24180;&#40836;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#19982;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;&#38754;&#37096;&#23545;&#40784;&#12289;&#38754;&#37096;&#35206;&#30422;&#12289;&#22270;&#20687;&#20998;&#36776;&#29575;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#37327;&#65289;&#30456;&#27604;&#24494;&#19981;&#36275;&#36947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20351;&#29992;FaRL&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We describe our evaluation protocol in detail and provide specific examples of how the protocol should be used. We utilize the protocol to offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16788</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#27748;&#65306;&#36890;&#36807;&#27169;&#22411;&#24179;&#22343;&#25913;&#36827;&#20462;&#21098;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21098;&#26525;&#26174;&#33879;&#21387;&#32553;&#65292;&#20174;&#32780;&#24471;&#21040;&#31232;&#30095;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#28014;&#28857;&#36816;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#27748;&#65288;Wortsman&#31561;&#20154;&#65292;2022&#24180;&#65289;&#36890;&#36807;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#25913;&#21892;&#27867;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#22788;&#20110;&#30456;&#21516;&#25439;&#22833;&#21306;&#22495;&#30340;&#27169;&#22411;&#20197;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23545;&#20219;&#24847;&#31232;&#30095;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#20250;&#38477;&#20302;&#25972;&#20307;&#31232;&#30095;&#24230;&#65292;&#21407;&#22240;&#26159;&#19981;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#21333;&#27425;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#20013;&#25506;&#32034;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#65288;&#20363;&#22914;&#25209;&#27425;&#25490;&#24207;&#25110;&#26435;&#37325;&#34928;&#20943;&#65289;&#20135;&#29983;&#30340;&#27169;&#22411;&#36866;&#21512;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#19988;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#24179;&#22343;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10180</link><description>&lt;p&gt;
&#22522;&#20110;Samplet&#22522; Pursuit &#30340;&#26680;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;l1&#27491;&#21017;&#21270;&#30340;Samplet&#22352;&#26631;&#19979;&#30340;&#26680;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;Samplet&#22522;&#30340;&#31995;&#25968;&#19978;&#65292;&#24212;&#29992;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#24378;&#21046;&#22686;&#21152;&#31232;&#30095;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;Samplet&#22522; Pursuit&#12290;Samplet&#22522;&#26159;&#27874;&#24418;&#31867;&#22411;&#30340;&#26377;&#31526;&#21495;&#27979;&#24230;&#65292;&#19987;&#38376;&#29992;&#20110;&#25955;&#20081;&#25968;&#25454;&#12290;&#23427;&#20204;&#20855;&#26377;&#19982;&#23567;&#27874;&#30456;&#20284;&#30340;&#26412;&#22320;&#21270;&#12289;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#25968;&#25454;&#21387;&#32553;&#24615;&#36136;&#12290;&#21487;&#20197;&#22312;Samplet&#22522;&#19978;&#31232;&#30095;&#22320;&#34920;&#31034;&#30340;&#20449;&#21495;&#31867;&#27604;&#21333;&#23610;&#24230;&#22522;&#19978;&#33021;&#22815;&#34920;&#31034;&#31232;&#30095;&#30340;&#20449;&#21495;&#31867;&#21035;&#35201;&#22823;&#24471;&#22810;&#12290;&#29305;&#21035;&#22320;&#65292;&#20165;&#29992;&#22522;&#20989;&#25968;&#26144;&#23556;&#30340;&#20960;&#20010;&#29305;&#24449;&#21472;&#21152;&#21363;&#21487;&#34920;&#31034;&#30340;&#25152;&#26377;&#20449;&#21495;&#20063;&#21487;&#20197;&#22312;Samplet&#22352;&#26631;&#19979;&#23454;&#29616;&#31232;&#30095;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#19982;&#24555;&#36895;&#36845;&#20195;&#25910;&#32553;&#38408;&#20540;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#20540;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.06721</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Conditional Independence Testing. (arXiv:2306.06721v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#20540;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#26816;&#39564;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#26159;&#35768;&#22810;&#22240;&#26524;&#22270;&#21457;&#29616;&#31639;&#27861;&#30340;&#26500;&#24314;&#22359;&#12290;CI&#27979;&#35797;&#26088;&#22312;&#25509;&#21463;&#25110;&#25298;&#32477;$X \perp \!\!\! \perp Y \mid Z$&#30340;&#38646;&#20551;&#35774;&#65292;&#20854;&#20013;$X \in \mathbb{R}&#65292;Y \in \mathbb{R}&#65292;Z \in \mathbb{R}^d$&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;Shah&#21644;Peters&#65288;2020&#65289;&#30340;&#19968;&#33324;&#21270;&#21327;&#26041;&#24046;&#27979;&#37327;&#21644;&#22522;&#20110;Cand\`es&#31561;&#20154;&#30340;&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#30340;&#20004;&#31181;&#31169;&#20154;CI&#27979;&#35797;&#36807;&#31243;&#65288;&#22312;&#27169;&#22411;-X&#20551;&#35774;&#19979;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#27979;&#35797;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#23427;&#20204;&#12290;&#36825;&#20123;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#30340;&#19968;&#33324;&#24773;&#20917;&#30340;&#31169;&#20154;CI&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional independence (CI) tests are widely used in statistical data analysis, e.g., they are the building block of many algorithms for causal graph discovery. The goal of a CI test is to accept or reject the null hypothesis that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in \mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional independence testing under the constraint of differential privacy. We design two private CI testing procedures: one based on the generalized covariance measure of Shah and Peters (2020) and another based on the conditional randomization test of Cand\`es et al. (2016) (under the model-X assumption). We provide theoretical guarantees on the performance of our tests and validate them empirically. These are the first private CI tests that work for the general case when $Z$ is continuous.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairMigration&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#21160;&#24577;&#36801;&#31227;&#26063;&#32676;&#65292;&#32780;&#19981;&#26159;&#29992;&#21407;&#22987;&#30340;&#25935;&#24863;&#23646;&#24615;&#26469;&#22266;&#23450;&#26063;&#32676;&#65292;&#20197;&#35757;&#32451;&#20844;&#24179;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2306.04212</link><description>&lt;p&gt;
&#36801;&#31227;&#26063;&#32676;&#20197;&#23454;&#29616;&#20844;&#24179;GNN
&lt;/p&gt;
&lt;p&gt;
Migrate Demographic Group For Fair GNNs. (arXiv:2306.04212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairMigration&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#21160;&#24577;&#36801;&#31227;&#26063;&#32676;&#65292;&#32780;&#19981;&#26159;&#29992;&#21407;&#22987;&#30340;&#25935;&#24863;&#23646;&#24615;&#26469;&#22266;&#23450;&#26063;&#32676;&#65292;&#20197;&#35757;&#32451;&#20844;&#24179;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;&#35774;&#35745;GNN&#26102;&#24120;&#24120;&#24573;&#30053;&#20844;&#24179;&#24615;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26377;&#20559;&#20449;&#24687;&#24456;&#23481;&#26131;&#24433;&#21709;&#26222;&#36890;&#30340;GNN&#65292;&#23548;&#33268;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#65288;&#26681;&#25454;&#25935;&#24863;&#23646;&#24615;&#65292;&#22914;&#31181;&#26063;&#21644;&#24180;&#40836;&#21010;&#20998;&#65289;&#30340;&#20559;&#35265;&#32467;&#26524;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#21162;&#21147;&#26469;&#35299;&#20915;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#25216;&#26415;&#36890;&#24120;&#36890;&#36807;&#21407;&#22987;&#25935;&#24863;&#23646;&#24615;&#23558;&#26063;&#32676;&#36827;&#34892;&#21010;&#20998;&#65292;&#24182;&#20551;&#23450;&#23427;&#20204;&#26159;&#22266;&#23450;&#30340;&#12290;&#19982;&#21407;&#22987;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#26377;&#20559;&#20449;&#24687;&#23558;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#65292;&#26080;&#35770;&#23454;&#26045;&#20844;&#24179;&#25216;&#26415;&#19982;&#21542;&#12290;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#35757;&#32451;&#20844;&#24179;&#30340;GNN&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;FairMigration&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#36801;&#31227;&#26063;&#32676;&#65292;&#32780;&#19981;&#26159;&#29992;&#21407;&#22987;&#30340;&#25935;&#24863;&#23646;&#24615;&#22266;&#23450;&#23427;&#20204;&#12290;FairMigration&#30001;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural networks (GNNs) have been applied in many scenarios due to the superior performance of graph learning. However, fairness is always ignored when designing GNNs. As a consequence, biased information in training data can easily affect vanilla GNNs, causing biased results toward particular demographic groups (divided by sensitive attributes, such as race and age). There have been efforts to address the fairness issue. However, existing fair techniques generally divide the demographic groups by raw sensitive attributes and assume that are fixed. The biased information correlated with raw sensitive attributes will run through the training process regardless of the implemented fair techniques. It is urgent to resolve this problem for training fair GNNs. To tackle this problem, we propose a brand new framework, FairMigration, which can dynamically migrate the demographic groups instead of keeping that fixed with raw sensitive attributes. FairMigration is composed of two training s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BootGen&#31639;&#27861;&#65292;&#20351;&#29992;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#22686;&#24378;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65292;&#21462;&#24471;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03111</link><description>&lt;p&gt;
&#38024;&#23545;&#31163;&#32447;&#35774;&#35745;&#29983;&#29289;&#24207;&#21015;&#30340;&#24471;&#20998;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#33258;&#21161;&#22686;&#24378;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences. (arXiv:2306.03111v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BootGen&#31639;&#27861;&#65292;&#20351;&#29992;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#22686;&#24378;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65292;&#21462;&#24471;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65288;&#22914;&#34507;&#30333;&#36136;&#12289;DNA&#21644;RNA&#65289;&#20197;&#26368;&#22823;&#21270;&#20165;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35780;&#20272;&#30340;&#40657;&#21283;&#23376;&#24471;&#20998;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#24471;&#20998;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#33258;&#21161;&#22686;&#24378;&#35757;&#32451;&#65288;BootGen&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37325;&#22797;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#25490;&#21517;&#21152;&#26435;&#27861;&#35757;&#32451;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#39640;&#20998;&#25968;&#30340;&#24207;&#21015;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;&#25509;&#19979;&#26469;&#30340;&#38454;&#27573;&#28041;&#21450;&#21040;&#33258;&#21161;&#22686;&#24378;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#24182;&#26631;&#35760;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#65292;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#19982;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#23545;&#40784;&#65292;&#23558;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#29983;&#25104;&#22120;&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#33258;&#21161;&#22686;&#24378;&#29983;&#25104;&#22120;&#21644;&#20195;&#29702;&#30340;&#26679;&#26412;&#65292;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#29289;&#24207;&#21015;&#20248;&#21270;&#26041;&#38754;&#32988;&#36807;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential
&lt;/p&gt;</description></item><item><title>DiffusionNAG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#31070;&#32463;&#32467;&#26500;&#30340;&#26377;&#21521;&#22270;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#22120;&#30340;&#25351;&#23548;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#20219;&#21153;&#26368;&#20248;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.16943</link><description>&lt;p&gt;
DiffusionNAG: &#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39044;&#27979;&#24341;&#23548;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models. (arXiv:2305.16943v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16943
&lt;/p&gt;
&lt;p&gt;
DiffusionNAG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#31070;&#32463;&#32467;&#26500;&#30340;&#26377;&#21521;&#22270;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#22120;&#30340;&#25351;&#23548;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#20219;&#21153;&#26368;&#20248;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#26041;&#27861;&#23384;&#22312;&#30528;&#23545;&#35768;&#22810;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#26550;&#26500;&#36827;&#34892;&#37325;&#22797;&#37319;&#26679;&#21644;&#35757;&#32451;&#25152;&#38656;&#30340;&#36807;&#38271;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;NAS&#36716;&#21521;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#26465;&#20214;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;(NAG)&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;DiffusionNAG&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#32467;&#26500;&#35270;&#20026;&#26377;&#21521;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32467;&#26500;&#30340;&#22270;&#25193;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#22120;&#30340;&#25351;&#23548;&#19979;&#65292;DiffusionNAG&#21487;&#20197;&#36890;&#36807;&#20174;&#26356;&#26377;&#21487;&#33021;&#28385;&#36275;&#25152;&#38656;&#29305;&#24615;&#30340;&#21306;&#22495;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#28789;&#27963;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#20219;&#21153;&#26368;&#20248;&#32467;&#26500;&#12290;&#19982;&#20351;&#29992;&#23646;&#24615;&#39044;&#27979;&#22120;&#23545;&#26550;&#26500;&#36827;&#34892;&#37319;&#26679;&#21644;&#36807;&#28388;&#30340;&#20808;&#21069;NAS&#26041;&#26696;&#30456;&#27604;&#65292;&#36825;&#31181;&#26465;&#20214;NAG&#26041;&#26696;&#26174;&#33879;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20110;&#39044;&#27979;&#22120;&#30340;NAS&#22330;&#26223;&#19979;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DiffusionNAG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#38142;&#36335;&#39044;&#27979;&#31561;&#19981;&#21516;&#24212;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.01391</link><description>&lt;p&gt;
&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Learning on Graphs: A Survey. (arXiv:2304.01391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#38142;&#36335;&#39044;&#27979;&#31561;&#19981;&#21516;&#24212;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#22914;&#31038;&#20132;&#32593;&#32476;&#12289;&#20998;&#23376;&#22270;&#21644;&#20132;&#26131;&#32593;&#32476;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;GNN&#20855;&#26377;&#19968;&#20123;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#23481;&#26131;&#32487;&#25215;&#35757;&#32451;&#25968;&#25454;&#30340;&#20559;&#35265;&#65292;&#19981;&#33021;&#24314;&#27169;&#22240;&#26524;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#32531;&#35299;&#36825;&#20123;&#32570;&#28857;&#26041;&#38754;&#20855;&#26377;&#24456;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#30340;&#21457;&#23637;&#65292;&#26412;&#32508;&#36848;&#23558;&#20998;&#31867;&#21644;&#20840;&#38754;&#22320;&#35780;&#20272;&#21453;&#20107;&#23454;&#22270;&#23398;&#20064;&#35770;&#25991;&#65292;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#32972;&#26223;&#21644;&#28608;&#21169;&#24615;&#20363;&#23376;&#12289;&#19968;&#33324;&#26694;&#26550;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of the training data and cannot model the casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various graph counterfactual learning approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on research problems studied. For each category, we provide background and motivating examples, a general f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17245</link><description>&lt;p&gt;
&#30740;&#31350;&#21644;&#20943;&#36731;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios. (arXiv:2303.17245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;MvC&#65289;&#26088;&#22312;&#25506;&#32034;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31867;&#21035;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#26631;&#31614;&#30417;&#30563;&#12290;&#22810;&#35270;&#22270;&#27604;&#21333;&#35270;&#22270;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22914;&#26524;&#35270;&#22270;&#22024;&#26434;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20005;&#37325;&#36864;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#30740;&#31350;&#20102;&#22024;&#26434;&#35270;&#22270;&#30340;&#32570;&#28857;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;&#31216;&#20026;MvCAN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MvC&#30446;&#26631;&#65292;&#20351;&#24471;&#19981;&#20849;&#20139;&#21442;&#25968;&#21644;&#19981;&#19968;&#33268;&#30340;&#32858;&#31867;&#39044;&#27979;&#21487;&#20197;&#36328;&#36234;&#22810;&#20010;&#35270;&#22270;&#65292;&#20197;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#36845;&#20195;&#36807;&#31243;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#25366;&#25496;&#22810;&#20010;&#35270;&#22270;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MvCAN&#30340;&#24037;&#20316;&#26159;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#65292;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#23454;&#29616;&#30340;&#12290;&#26368;&#21518;&#65292;&#23545;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26032;&#25910;&#38598;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MvCAN&#22312;&#22788;&#29702;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#22024;&#26434;&#35270;&#22270;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on publ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16971</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#20998;&#31867;&#20013;&#30340;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#65288;SJS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#25972;&#20307;&#20559;&#31227;&#30340;&#21487;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36793;&#38469;&#20998;&#24067;&#20197;&#21450;&#21518;&#39564;&#27010;&#29575;&#21644;&#31867;&#26465;&#20214;&#29305;&#24449;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#22312;&#27809;&#26377;&#26631;&#31614;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#30446;&#26631;&#25968;&#25454;&#38598;&#25311;&#21512;SJS&#21487;&#33021;&#20250;&#20135;&#29983;&#26631;&#31614;&#30340;&#26377;&#25928;&#39044;&#27979;&#21644;&#31867;&#20808;&#39564;&#27010;&#29575;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#38598;&#20043;&#38388;&#20256;&#36882;SJS&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#31867;&#21518;&#39564;&#27010;&#29575;&#30340;&#26465;&#20214;&#20462;&#27491;&#20844;&#24335;&#65292;&#30830;&#23450;&#24615;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#20197;&#21450;SJS&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;SJS&#29305;&#24449;&#30340;&#31639;&#27861;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#20250;&#22952;&#30861;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.10681</link><description>&lt;p&gt;
FrankenSplit:&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AI&#21152;&#36895;&#22120;&#30340;&#23835;&#36215;&#20351;&#24471;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#22823;&#27169;&#22411;&#30340;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#35831;&#27714;&#19979;&#25918;&#65292;&#32780;&#39640;&#32500;&#25968;&#25454;&#23558;&#20105;&#22842;&#26377;&#38480;&#30340;&#24102;&#23485;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#24182;&#22312;&#21453;&#26144;&#36793;&#32536;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#19981;&#23545;&#31216;&#36164;&#28304;&#20998;&#37197;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#32858;&#21512;&#35268;&#21017;-&#32852;&#37030;&#26799;&#24230;&#25237;&#24433;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21152;&#26435;&#26041;&#26696;&#65292;&#29992;&#20110;&#26368;&#20248;&#22320;&#32467;&#21512;&#28304;&#21644;&#30446;&#26631;&#26799;&#24230;&#65292;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#39046;&#22495;&#36716;&#31227;&#26102;&#24120;&#35265;&#30340;&#25216;&#26415;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.05049</link><description>&lt;p&gt;
&#32852;&#37030;&#33258;&#21152;&#26435;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Federated Auto-weighted Domain Adaptation. (arXiv:2302.05049v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#32858;&#21512;&#35268;&#21017;-&#32852;&#37030;&#26799;&#24230;&#25237;&#24433;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21152;&#26435;&#26041;&#26696;&#65292;&#29992;&#20110;&#26368;&#20248;&#22320;&#32467;&#21512;&#28304;&#21644;&#30446;&#26631;&#26799;&#24230;&#65292;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#39046;&#22495;&#36716;&#31227;&#26102;&#24120;&#35265;&#30340;&#25216;&#26415;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;FDA&#65289;&#26159;&#25551;&#36848;&#22810;&#20010;&#28304;&#23458;&#25143;&#31471;&#21327;&#20316;&#25913;&#21892;&#30446;&#26631;&#23458;&#25143;&#31471;&#24615;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#26377;&#38480;&#12290;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#65292;&#21152;&#19978;&#30446;&#26631;&#39046;&#22495;&#30340;&#31232;&#30095;&#25968;&#25454;&#65292;&#20351;&#24471;FDA&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#24120;&#35265;&#30340;&#25216;&#26415;&#65288;&#22914;FedAvg&#21644;&#24494;&#35843;&#65289;&#22312;&#23384;&#22312;&#26174;&#33879;&#39046;&#22495;&#36716;&#31227;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#26102;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#34920;&#24449;FDA&#35774;&#32622;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20998;&#26512;&#32858;&#21512;&#35268;&#21017;&#24615;&#33021;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;FDA&#30340;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#35268;&#21017;&#65292;&#31216;&#20026;&#32852;&#37030;&#26799;&#24230;&#25237;&#24433;&#65288;$\texttt{FedGP}$&#65289;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#26399;&#38388;&#32858;&#21512;&#28304;&#26799;&#24230;&#21644;&#30446;&#26631;&#26799;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#24471;&#24320;&#21457;&#19968;&#20010;&#33258;&#21160;&#21152;&#26435;&#26041;&#26696;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20010;&#26041;&#26696;&#33021;&#22815;&#26368;&#20248;&#22320;&#32467;&#21512;&#28304;&#21644;&#30446;&#26631;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Domain Adaptation (FDA) describes the federated learning setting where a set of source clients work collaboratively to improve the performance of a target client where limited data is available. The domain shift between the source and target domains, coupled with sparse data in the target domain, makes FDA a challenging problem, e.g., common techniques such as FedAvg and fine-tuning, often fail with the presence of significant domain shift and data scarcity. To comprehensively understand the problem, we introduce metrics that characterize the FDA setting and put forth a theoretical framework for analyzing the performance of aggregation rules. We also propose a novel aggregation rule for FDA, Federated Gradient Projection ($\texttt{FedGP}$), used to aggregate the source gradients and target gradient during training. Importantly, our framework enables the development of an $\textit{auto-weighting scheme}$ that optimally combines the source and target gradients. This scheme impr
&lt;/p&gt;</description></item><item><title>Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11118</link><description>&lt;p&gt;
Box$^2$EL: EL++&#25551;&#36848;&#36923;&#36753;&#20013;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#26041;&#27861;&#21450;&#20854;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11118
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#35770;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#19982;&#27010;&#24565;&#20449;&#24687;&#21644;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#26412;&#20307;&#35770;&#30340;&#24402;&#32435;&#25512;&#29702;&#25216;&#26415;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#25216;&#26415;&#26377;&#26395;&#34917;&#20805;&#20256;&#32479;&#30340;&#28436;&#32462;&#25512;&#29702;&#31639;&#27861;&#12290;&#31867;&#20284;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23436;&#21892;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#26412;&#20307;&#35770;&#23884;&#20837;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#24213;&#23618;&#25551;&#36848;&#36923;&#36753;&#30340;&#36923;&#36753;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21463;&#38480;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Box$^2$EL&#26041;&#27861;&#65292;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#37117;&#34920;&#31034;&#20026;&#30418;&#23376;&#65288;&#21363;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20811;&#26381;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#29305;&#24449;&#30340;&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.10956</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20165;&#20165;&#20174;&#22270;&#32467;&#26500;&#20013;&#24674;&#22797;&#20986;&#38544;&#34255;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure. (arXiv:2301.10956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#29305;&#24449;&#30340;&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#22788;&#29702;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#23427;&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#38416;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#35282;&#24230;&#30740;&#31350;&#20102;GNN&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21463;&#38544;&#34255;&#65288;&#25110;&#28508;&#22312;&#65289;&#33410;&#28857;&#29305;&#24449;&#25511;&#21046;&#30340;&#22270;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20123;&#29305;&#24449;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#36825;&#31181;&#26694;&#26550;&#30340;&#20856;&#22411;&#31034;&#20363;&#26159;&#20174;&#38544;&#34255;&#29305;&#24449;&#26500;&#24314;&#30340;kNN&#22270;&#12290;&#22312;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;GNN&#21487;&#20197;&#20165;&#20174;&#36755;&#20837;&#22270;&#20013;&#24674;&#22797;&#20986;&#38544;&#34255;&#33410;&#28857;&#29305;&#24449;&#65292;&#21363;&#20351;&#25152;&#26377;&#33410;&#28857;&#29305;&#24449;&#65292;&#21253;&#25324;&#38544;&#34255;&#29305;&#24449;&#26412;&#36523;&#21644;&#20219;&#20309;&#38388;&#25509;&#25552;&#31034;&#37117;&#19981;&#21487;&#29992;&#12290;GNN&#21487;&#20197;&#36827;&#19968;&#27493;&#21033;&#29992;&#24674;&#22797;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#22270;&#32467;&#26500;&#33258;&#36523;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are popular models for graph learning problems. GNNs show strong empirical performance in many practical tasks. However, the theoretical properties have not been completely elucidated. In this paper, we investigate whether GNNs can exploit the graph structure from the perspective of the expressive power of GNNs. In our analysis, we consider graph generation processes that are controlled by hidden (or latent) node features, which contain all information about the graph structure. A typical example of this framework is kNN graphs constructed from the hidden features. In our main results, we show that GNNs can recover the hidden node features from the input graph alone, even when all node features, including the hidden features themselves and any indirect hints, are unavailable. GNNs can further use the recovered node features for downstream tasks. These results show that GNNs can fully exploit the graph structure by themselves, and in effect, GNNs can use bot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#65292;&#22312;&#22810;&#20010;&#29992;&#25143;&#30340;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#20854;&#24863;&#30693;&#24615;&#33021;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#36136;&#22320;&#21644;&#29992;&#25143;&#20132;&#20114;&#30340;&#21464;&#21270;&#65292;&#24182;&#20351;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2212.13332</link><description>&lt;p&gt;
&#23454;&#26102;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering. (arXiv:2212.13332v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#65292;&#22312;&#22810;&#20010;&#29992;&#25143;&#30340;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#20854;&#24863;&#30693;&#24615;&#33021;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#36136;&#22320;&#21644;&#29992;&#25143;&#20132;&#20114;&#30340;&#21464;&#21270;&#65292;&#24182;&#20351;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#32570;&#20047;&#20154;&#31867;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20132;&#20114;&#20013;&#32463;&#21382;&#21040;&#30340;&#20016;&#23500;&#35302;&#35273;&#20449;&#21495;&#65292;&#20363;&#22914;&#22312;&#34920;&#38754;&#19978;&#30340;&#27178;&#21521;&#31227;&#21160;&#20013;&#24863;&#21463;&#21040;&#30340;&#36136;&#22320;&#24863;&#12290;&#22312;VR&#29615;&#22659;&#20013;&#28155;&#21152;&#36924;&#30495;&#30340;&#35302;&#35273;&#36136;&#22320;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#29992;&#25143;&#20132;&#20114;&#30340;&#21464;&#21270;&#21644;&#19990;&#30028;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#36136;&#22320;&#30340;&#27169;&#22411;&#12290;&#30446;&#21069;&#23384;&#22312;&#29992;&#20110;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#30340;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#20026;&#27599;&#31181;&#36136;&#22320;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#20316;&#26465;&#20214;&#27169;&#22411;&#65292;&#29992;&#20110;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#29992;&#25143;&#30340;&#24863;&#30693;&#24615;&#33021;&#35780;&#20272;&#26469;&#21576;&#29616;&#36924;&#30495;&#30340;&#36136;&#22320;&#25391;&#21160;&#12290;&#35813;&#27169;&#22411;&#32479;&#19968;&#36866;&#29992;&#20110;&#25152;&#26377;&#26448;&#26009;&#65292;&#20351;&#29992;&#26469;&#33258;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#65288;GelSight&#65289;&#30340;&#25968;&#25454;&#65292;&#22312;&#23454;&#26102;&#26465;&#20214;&#19979;&#21576;&#29616;&#36866;&#24403;&#30340;&#34920;&#38754;&#12290;&#22312;&#36136;&#22320;&#28210;&#26579;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#39640;&#24102;&#23485;&#30340;&#25391;&#35302;&#35273;&#20256;&#24863;&#22120;&#36830;&#25509;&#21040;&#19968;&#20010;3D&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Virtual Reality (VR) environments lack the rich haptic signals that humans experience during real-life interactions, such as the sensation of texture during lateral movement on a surface. Adding realistic haptic textures to VR environments requires a model that generalizes to variations of a user's interaction and to the wide variety of existing textures in the world. Current methodologies for haptic texture rendering exist, but they usually develop one model per texture, resulting in low scalability. We present a deep learning-based action-conditional model for haptic texture rendering and evaluate its perceptual performance in rendering realistic texture vibrations through a multi part human user study. This model is unified over all materials and uses data from a vision-based tactile sensor (GelSight) to render the appropriate surface conditioned on the user's action in real time. For rendering texture, we use a high-bandwidth vibrotactile transducer attached to a 3D Systems
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.06370</link><description>&lt;p&gt;
&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#29983;&#25104;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation. (arXiv:2212.06370v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#65292;&#24212;&#35813;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#20043;&#22806;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;(PIs)&#12290;&#21482;&#35201;Pis&#36275;&#22815;&#31364;&#32780;&#19988;&#25429;&#33719;&#20102;&#22823;&#37096;&#20998;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#36825;&#20123;Pis&#23601;&#26159;&#26377;&#29992;&#30340;&#25110;"&#39640;&#36136;&#37327;"&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#20026;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#39044;&#27979;&#21306;&#38388;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#30446;&#26631;&#39044;&#27979;&#20043;&#22806;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20004;&#20010;&#20276;&#20387;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#20351;&#29992;&#19968;&#20010;&#36755;&#20986;&#65292;&#30446;&#26631;&#20272;&#35745;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#20004;&#20010;&#36755;&#20986;&#65292;&#30456;&#24212;PI&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#29983;&#25104;PI&#30340;&#32593;&#32476;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#32771;&#34385;&#20102;&#30446;&#26631;&#20272;&#35745;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#20004;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#20943;&#23567;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#21644;&#25552;&#39640;Pis&#30340;&#36136;&#37327;(&#36890;&#36807;&#20854;&#35206;&#30422;&#27010;&#29575;&#36827;&#34892;&#27979;&#37327;)&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#19988;&#36136;&#37327;&#26356;&#39640;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#21516;&#26102;&#21448;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or "high-quality" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2208.07243</link><description>&lt;p&gt;
&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#30340;&#25351;&#25968;&#38598;&#20013;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient. (arXiv:2208.07243v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#20854;&#20013;&#27599;&#19968;&#27493;&#36845;&#20195;&#65292;&#26399;&#26395;&#20013;&#21521;&#30446;&#26631;&#21462;&#24471;&#36827;&#23637;&#12290;&#24403;&#36827;&#23637;&#19982;&#31639;&#27861;&#30340;&#27493;&#38271;&#25104;&#27604;&#20363;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#12290;&#36825;&#20123;&#23614;&#37096;&#30028;&#38480;&#19982;&#26356;&#24120;&#35265;&#30340;&#38543;&#26426;&#36924;&#36817;&#30340;&#28176;&#36817;&#27491;&#24577;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20960;&#20309;&#38459;&#23612;&#24615;&#35777;&#26126;&#12290;&#36825;&#25193;&#23637;&#20102;Hajek&#65288;1982&#65289;&#23545;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#32467;&#26524;&#21040;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#20855;&#26377;&#38750;&#38646;&#26799;&#24230;&#30340;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#29992;&#26469;&#35777;&#26126;$O(1/t)$&#21644;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the behavior of stochastic approximation algorithms where iterates, in expectation, make progress towards an objective at each step. When progress is proportional to the step size of the algorithm, we prove exponential concentration bounds. These tail-bounds contrast asymptotic normality results which are more frequently associated with stochastic approximation. The methods that we develop rely on a geometric ergodicity proof. This extends a result on Markov chains due to Hajek (1982) to the area of stochastic approximation algorithms. For Projected Stochastic Gradient Descent with a non-vanishing gradient, our results can be used to prove $O(1/t)$ and linear convergence rates.
&lt;/p&gt;</description></item><item><title>EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2204.10438</link><description>&lt;p&gt;
EVOTER&#65306;&#36879;&#26126;&#21487;&#35299;&#37322;&#35268;&#21017;&#38598;&#30340;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
EVOTER: Evolution of Transparent Explainable Rule-sets. (arXiv:2204.10438v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10438
&lt;/p&gt;
&lt;p&gt;
EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;AI&#31995;&#32479;&#26159;&#40657;&#30418;&#23376;&#65292;&#20026;&#32473;&#23450;&#30340;&#36755;&#20837;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#39046;&#22495;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#21644;&#20449;&#20219;&#24230;&#35201;&#27714;&#65292;&#36825;&#20123;&#35201;&#27714;&#19981;&#33021;&#30452;&#25509;&#28385;&#36275;&#36825;&#20123;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#24320;&#22987;&#26102;&#27169;&#22411;&#23601;&#26159;&#36879;&#26126;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#35268;&#21017;&#38598;&#65292;&#31216;&#20026;EVOTER&#12290;EVOTER&#22312;&#22810;&#20010;&#39044;&#27979;/&#20998;&#31867;&#21644;&#22788;&#26041;/&#25919;&#31574;&#25628;&#32034;&#39046;&#22495;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26377;&#21644;&#27809;&#26377;&#20195;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#33021;&#22815;&#21457;&#29616;&#21644;&#40657;&#30418;&#27169;&#22411;&#30456;&#20284;&#30340;&#26377;&#24847;&#20041;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20123;&#35268;&#21017;&#21487;&#20197;&#25552;&#20379;&#39046;&#22495;&#30340;&#35265;&#35299;&#65292;&#24182;&#20351;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#26174;&#24615;&#21270;&#12290;&#20063;&#21487;&#20197;&#30452;&#25509;&#23545;&#23427;&#20204;&#36827;&#34892;&#32534;&#36753;&#65292;&#20197;&#28040;&#38500;&#20559;&#35265;&#24182;&#28155;&#21152;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;EVOTER&#20026;&#26410;&#26469;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#30340;&#21487;&#38752;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most AI systems are black boxes generating reasonable outputs for given inputs. Some domains, however, have explainability and trustworthiness requirements that cannot be directly met by these approaches. Various methods have therefore been developed to interpret black-box models after training. This paper advocates an alternative approach where the models are transparent and explainable to begin with. This approach, EVOTER, evolves rule-sets based on simple logical expressions. The approach is evaluated in several prediction/classification and prescription/policy search domains with and without a surrogate. It is shown to discover meaningful rule sets that perform similarly to black-box models. The rules can provide insight into the domain, and make biases hidden in the data explicit. It may also be possible to edit them directly to remove biases and add constraints. EVOTER thus forms a promising foundation for building trustworthy AI systems for real-world applications in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#26465;&#20214;&#65292;&#29992;&#20110;&#25552;&#20379;&#20855;&#26377;&#24130;&#24459;&#20248;&#21270;&#36712;&#36857;&#30340;&#38382;&#39064;&#30340;&#26356;&#32039;&#23494;&#19978;&#30028;&#65292;&#28436;&#31034;&#20102;&#22914;&#20309;&#32479;&#19968;&#33719;&#24471;&#26368;&#20248;&#21152;&#36895;&#26041;&#27861;&#21450;&#20854;&#35745;&#21010;&#21644;&#25910;&#25947;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2202.00992</link><description>&lt;p&gt;
&#22522;&#20110;&#24130;&#24459;&#35889;&#26465;&#20214;&#19979;&#30340;&#20248;&#21270;&#25910;&#25947;&#29575;&#32039;&#23494;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions. (arXiv:2202.00992v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#26465;&#20214;&#65292;&#29992;&#20110;&#25552;&#20379;&#20855;&#26377;&#24130;&#24459;&#20248;&#21270;&#36712;&#36857;&#30340;&#38382;&#39064;&#30340;&#26356;&#32039;&#23494;&#19978;&#30028;&#65292;&#28436;&#31034;&#20102;&#22914;&#20309;&#32479;&#19968;&#33719;&#24471;&#26368;&#20248;&#21152;&#36895;&#26041;&#27861;&#21450;&#20854;&#35745;&#21010;&#21644;&#25910;&#25947;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20108;&#27425;&#38382;&#39064;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;&#21462;&#20915;&#20110;&#35889;&#30340;&#20302;&#33021;&#37096;&#20998;&#12290;&#23545;&#20110;&#22823;&#22411;&#65288;&#26377;&#25928;&#26080;&#38480;&#32500;&#65289;&#38382;&#39064;&#65292;&#36825;&#37096;&#20998;&#35889;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#24130;&#24459;&#20998;&#24067;&#33258;&#28982;&#34920;&#31034;&#25110;&#36817;&#20284;&#65292;&#23548;&#33268;&#26799;&#24230;&#31639;&#27861;&#30340;&#36845;&#20195;&#35299;&#34920;&#29616;&#20986;&#24130;&#24459;&#25910;&#25947;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#26465;&#20214;&#65292;&#29992;&#20110;&#25552;&#20379;&#20855;&#26377;&#24130;&#24459;&#20248;&#21270;&#36712;&#36857;&#30340;&#38382;&#39064;&#30340;&#26356;&#32039;&#23494;&#19978;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26465;&#20214;&#26469;&#24314;&#31435;&#19968;&#24352;&#24191;&#27867;&#20248;&#21270;&#31639;&#27861;&#30340;&#19978;&#19979;&#30028;&#23436;&#25972;&#22270;&#20687;&#8212;&#8212;&#26799;&#24230;&#19979;&#38477;&#12289;&#26368;&#38497;&#19979;&#38477;&#12289;&#37325;&#29699;&#12289;&#20849;&#36717;&#26799;&#24230;&#8212;&#8212;&#24182;&#24378;&#35843;&#20102;&#23398;&#20064;&#29575;&#21644;&#21160;&#37327;&#30340;&#22522;&#26412;&#35745;&#21010;&#12290;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#32479;&#19968;&#33719;&#24471;&#26368;&#20248;&#21152;&#36895;&#26041;&#27861;&#21450;&#20854;&#35745;&#21010;&#21644;&#25910;&#25947;&#19978;&#30028;&#65292;&#23545;&#20110;&#32473;&#23450;&#35889;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#20110;&#39318;&#20010;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2111.10933</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#21487;&#20197;&#36229;&#36234;&#38598;&#20013;&#24335;&#19978;&#32622;&#20449;&#30028;&#38480;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms. (arXiv:2111.10933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#20551;&#35774;N&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#38754;&#23545;&#30528;&#19968;&#32452;&#20849;&#21516;&#30340;M&#20010;&#33218;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#33218;&#22870;&#21169;&#20998;&#24067;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#20174;&#37051;&#23621;&#22788;&#25509;&#25910;&#20449;&#24687;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#30001;&#19968;&#20010;&#26080;&#21521;&#22270;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#38480;&#65288;UCB&#65289;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;KL-UCB&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#23454;&#29616;&#27604;&#20854;&#21333;&#19968;&#26234;&#33021;&#20307;&#30456;&#24212;&#31639;&#27861;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#21069;&#25552;&#26159;&#26234;&#33021;&#20307;&#33267;&#23569;&#26377;&#19968;&#20010;&#37051;&#23621;&#65292;&#32780;&#19988;&#26234;&#33021;&#20307;&#26377;&#36234;&#22810;&#30340;&#37051;&#23621;&#65292;&#21518;&#24724;&#20540;&#20250;&#36234;&#22909;&#65292;&#36825;&#24847;&#21619;&#30528;&#25972;&#20307;&#30340;&#21644;&#22823;&#20110;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a decentralized multi-armed bandit problem in a multi-agent network. The problem is simultaneously solved by N agents assuming they face a common set of M arms and share the same arms' reward distributions. Each agent can receive information only from its neighbors, where the neighbor relationships among the agents are described by an undirected graph. Two fully decentralized multi-armed bandit algorithms are proposed, respectively based on the classic upper confidence bound (UCB) algorithm and the state-of-the-art KL-UCB algorithm. The proposed decentralized algorithms permit each agent in the network to achieve a better logarithmic asymptotic regret than their single-agent counterparts, provided that the agent has at least one neighbor, and the more neighbors an agent has, the better regret it will have, meaning that the sum is more than its component parts.
&lt;/p&gt;</description></item><item><title>CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/1902.05605</link><description>&lt;p&gt;
CrossQ: &#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#30340;&#25209;&#24402;&#19968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.05605
&lt;/p&gt;
&lt;p&gt;
CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#25928;&#29575;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#65292;&#22914;REDQ&#21644;DroQ&#65292;&#36890;&#36807;&#23558;&#25209;&#27425;&#26631;&#20934;&#21270;&#30340;&#26356;&#26032;&#25968;&#25454;&#65288;UTD&#65289;&#27604;&#29575;&#22686;&#21152;&#21040;&#27599;&#20010;&#29615;&#22659;&#26679;&#26412;&#19978;&#30340;20&#20010;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#25913;&#21892;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#24102;&#26469;&#22823;&#24133;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CrossQ&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#23427;&#24039;&#22937;&#22320;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#65292;&#24182;&#21435;&#38500;&#20102;&#30446;&#26631;&#32593;&#32476;&#65292;&#20197;&#22312;&#20445;&#25345;&#20302;UTD&#27604;&#29575;&#20026;1&#30340;&#21516;&#26102;&#36229;&#36234;&#30446;&#21069;&#30340;&#26368;&#26032;&#26679;&#26412;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CrossQ&#19981;&#20381;&#36182;&#20110;&#24403;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#39640;&#32423;&#20559;&#24046;&#32553;&#20943;&#26041;&#26696;&#12290;CrossQ&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#65288;2&#65289;&#19982;REDQ&#21644;DroQ&#30456;&#27604;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#65288;3&#65289;&#23454;&#26045;&#31616;&#21333;&#65292;&#20165;&#38656;&#35201;&#22312;SAC&#20043;&#19978;&#28155;&#21152;&#20960;&#34892;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.
&lt;/p&gt;</description></item></channel></rss>