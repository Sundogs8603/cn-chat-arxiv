<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#20004;&#28857;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30340;&#39640;&#25928;&#24615;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01621</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#27169;&#22411;&#38646;&#38454;&#20248;&#21270;&#30340;&#38543;&#26426;&#20004;&#28857;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Two Points Method for Deep Model Zeroth-order Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#20004;&#28857;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30340;&#39640;&#25928;&#24615;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#30001;&#20110;&#30828;&#20214;&#39044;&#31639;&#25110;&#32570;&#20047;&#21453;&#21521;&#20256;&#25773;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#26500;&#24314;&#25110;&#23436;&#20840;&#24494;&#35843;&#36825;&#26679;&#30340;&#22823;&#27169;&#22411;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#38646;&#38454;&#26041;&#27861;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#23427;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#26412;&#25991;&#22312;&#26080;&#26799;&#24230;&#24773;&#24418;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38543;&#26426;&#20004;&#28857;&#65288;S2P&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#21644;&#25918;&#26494;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#25552;&#20986;&#20102;S2P&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#36136;&#12290;&#29702;&#35770;&#24615;&#36136;&#36824;&#25581;&#31034;&#20102;&#26356;&#24555;&#12289;&#26356;&#31283;&#23450;&#30340;S2P&#21464;&#20307;&#8212;&#8212;&#21152;&#36895;S2P&#65288;AS2P&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#25910;&#25947;&#24615;&#36136;&#65292;&#26356;&#22909;&#22320;&#34920;&#31034;&#20102;&#28145;&#24230;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;AS2P&#22312;&#20248;&#21270;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#65288;&#21253;&#25324;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#30446;&#26631;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;</title><link>https://arxiv.org/abs/2404.02113</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#30693;&#36827;&#34892;&#35843;&#25972;&#65306;&#37325;&#26032;&#23457;&#35270;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20272;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32487;&#32493;&#25110;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#29615;&#22659;&#30340;&#35775;&#38382;&#24212;&#35813;&#26159;&#26377;&#38480;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#30340;&#31639;&#27861;&#33021;&#22815;&#38271;&#26102;&#38388;&#36816;&#34892;&#65292;&#24182;&#19981;&#26029;&#36866;&#24212;&#26032;&#30340;&#12289;&#24847;&#24819;&#19981;&#21040;&#30340;&#24773;&#20917;&#65292;&#37027;&#20040;&#25105;&#20204;&#24517;&#39035;&#24895;&#24847;&#22312;&#25972;&#20010;&#20195;&#29702;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#37096;&#32626;&#25105;&#20204;&#30340;&#20195;&#29702;&#32780;&#19981;&#35843;&#25972;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#29978;&#33267;&#32487;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#20855;&#22791;&#23545;&#20195;&#29702;&#30340;&#37096;&#32626;&#29615;&#22659;&#20855;&#26377;&#26080;&#38480;&#21046;&#35775;&#38382;&#26435;&#30340;&#26631;&#20934;&#20570;&#27861;&#21487;&#33021;&#24050;&#32463;&#38459;&#30861;&#20102;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20854;&#20013;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#30334;&#20998;&#20043;&#19968;&#21487;&#20197;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;DQN&#21644;Soft Actor Critic&#22312;&#21508;&#31181;&#25345;&#32493;&#21644;&#38750;&#31283;&#23450;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02113v1 Announce Type: new  Abstract: In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#30340;R2D2&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.18052</link><description>&lt;p&gt;
&#22312;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;R2D2&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
R2D2 image reconstruction with model uncertainty quantification in radio astronomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#30340;R2D2&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#29992;&#20110;&#22825;&#25991;&#23398;&#20013;&#23556;&#30005;&#24178;&#28041;(RI)&#25104;&#20687;&#30340;&#8220;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#21040;&#27531;&#24046;DNN&#31995;&#21015;&#8221;(R2D2)&#26041;&#27861;&#12290;R2D2&#30340;&#37325;&#24314;&#24418;&#25104;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#36845;&#20195;&#22320;&#20272;&#35745;&#20026;&#20197;&#21069;&#36845;&#20195;&#30340;&#22270;&#20687;&#20272;&#35745;&#21644;&#30456;&#20851;&#25968;&#25454;&#27531;&#24046;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20854;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#30740;&#31350;R2D2&#22270;&#20687;&#20272;&#35745;&#36807;&#31243;&#30340;&#31283;&#20581;&#24615;&#12290;&#37319;&#29992;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#31995;&#21015;&#65292;&#26469;&#33258;&#22312;&#27599;&#27425;&#36845;&#20195;&#36807;&#31243;&#20013;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#19981;&#21516;&#38543;&#26426;DNN&#21021;&#22987;&#21270;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#20010;R2D2&#23454;&#20363;&#20063;&#21487;&#29992;&#20110;&#20135;&#29983;&#8220;R2D2&#26679;&#26412;&#8221;&#65292;&#20174;&#20013;&#32463;&#39564;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#36171;&#20104;&#31639;&#27861;&#32852;&#21512;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21151;&#33021;&#12290;&#37325;&#28857;&#25918;&#22312;RI imag
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18052v1 Announce Type: cross  Abstract: The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models. Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration. The resulting multiple R2D2 instances can also be leveraged to generate ``R2D2 samples'', from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality. Focusing on RI imag
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>GPFL&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#30340;&#23458;&#25143;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#26412;&#22320;&#21644;&#20840;&#23616;&#19979;&#38477;&#26041;&#21521;&#26469;&#34913;&#37327;&#23458;&#25143;&#20215;&#20540;&#65292;&#24182;&#37319;&#29992;&#21033;&#29992;-&#25506;&#32034;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;FEMINST&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#36229;&#36807;9%&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2403.17833</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#30340;&#23458;&#25143;&#36873;&#25321;&#26694;&#26550;&#29992;&#20110;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17833
&lt;/p&gt;
&lt;p&gt;
GPFL&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#30340;&#23458;&#25143;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#26412;&#22320;&#21644;&#20840;&#23616;&#19979;&#38477;&#26041;&#21521;&#26469;&#34913;&#37327;&#23458;&#25143;&#20215;&#20540;&#65292;&#24182;&#37319;&#29992;&#21033;&#29992;-&#25506;&#32034;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;FEMINST&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#36229;&#36807;9%&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#36873;&#25321;&#23545;&#30830;&#23450;&#21442;&#19982;&#32773;&#23458;&#25143;&#20197;&#24179;&#34913;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#35745;&#31639;&#36127;&#25285;&#21644;&#29420;&#31435;&#23458;&#25143;&#22788;&#29702;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPFL&#65292;&#36890;&#36807;&#27604;&#36739;&#26412;&#22320;&#21644;&#20840;&#23616;&#19979;&#38477;&#26041;&#21521;&#26469;&#34913;&#37327;&#23458;&#25143;&#20215;&#20540;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#21033;&#29992;-&#25506;&#32034;&#26426;&#21046;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23545;FEMINST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPFL&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;FEMINST&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#36229;&#36807;9%&#30340;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;GPFL&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39044;&#36873;&#21644;&#21442;&#25968;&#37325;&#29992;&#23637;&#31034;&#20986;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17833v1 Announce Type: new  Abstract: Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency. Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment. To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions. We also employ an Exploit-Explore mechanism to enhance performance. Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16843</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#26159;&#21542;&#20250;&#24863;&#21040;&#21518;&#24724;&#65311;&#22312;&#32447;&#23398;&#20064;&#21644;&#28216;&#25103;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLM Agents Have Regret? A Case Study in Online Learning and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16843
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;(&#20132;&#20114;&#24335;)&#20915;&#31574;&#21046;&#23450;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#19981;&#26029;&#30340;&#25104;&#21151;&#65292;&#20294;LLM&#20195;&#29702;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#36827;&#34892;&#20805;&#20998;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#26102;&#30340;&#22810;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20856;&#22411;&#22330;&#26223;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLM&#20195;&#29702;&#22312;&#36825;&#20123;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#23427;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;\emph{&#21518;&#24724;}&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32463;&#20856;(&#38750;&#24179;&#31283;)&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;LLMs&#30340;&#26080;&#21518;&#24724;&#34892;&#20026;&#65292;&#20197;&#21450;&#24403;LLM&#20195;&#29702;&#36890;&#36807;&#36827;&#34892;&#37325;&#22797;&#28216;&#25103;&#36827;&#34892;&#20132;&#20114;&#26102;&#22343;&#34913;&#30340;&#20986;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#26080;&#21518;&#24724;&#34892;&#20026;&#25552;&#20379;&#19968;&#20123;&#29702;&#35770;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21345;&#25176;&#22270;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#39044;&#27979;&#26410;&#23433;&#35013;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#24182;&#23454;&#29616;&#38271;&#26399;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.16049</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#21345;&#25176;&#22270;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#31995;&#32479;&#30340;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21345;&#25176;&#22270;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#39044;&#27979;&#26410;&#23433;&#35013;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#24182;&#23454;&#29616;&#38271;&#26399;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#39044;&#27979;&#26102;&#38388;&#27169;&#24335;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#36712;&#36857;&#32454;&#24494;&#19988;&#32463;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#39044;&#27979;&#26694;&#26550;&#19981;&#26029;&#34987;&#23436;&#21892;&#65292;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#26041;&#27861;&#12289;&#25968;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31995;&#32479;&#20043;&#19968;&#65292;&#20849;&#20139;&#20132;&#36890;&#31995;&#32479;&#22914;&#20849;&#20139;&#21333;&#36710;&#30001;&#20110;&#22478;&#24066;&#32422;&#26463;&#21644;&#29615;&#22659;&#38382;&#39064;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#30001;&#20110;&#31995;&#32479;&#30340;&#24320;&#25918;&#24615;&#21644;&#21508;&#31449;&#28857;&#20043;&#38388;&#30340;&#20351;&#29992;&#27169;&#24335;&#19981;&#24179;&#34913;&#65292;&#39044;&#27979;&#21333;&#36710;&#31449;&#28857;&#30340;&#31199;&#20511;&#21644;&#24402;&#36824;&#27169;&#24335;&#20173;&#28982;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21345;&#25176;&#22270;&#26041;&#27861;&#26469;&#39044;&#27979;&#31199;&#20511;&#21644;&#24402;&#36824;&#27169;&#24335;&#12290;&#21345;&#25176;&#22270;&#26041;&#27861;&#26377;&#21161;&#20110;&#23545;&#26032;&#23433;&#35013;&#31449;&#28857;&#30340;&#38656;&#27714;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#20123;&#26032;&#31449;&#28857;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#38271;&#26399;&#39044;&#27979;&#65292;&#36825;&#26159;&#20197;&#21069;&#23578;&#26410;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16049v1 Announce Type: new  Abstract: Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14623</link><description>&lt;p&gt;
&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Simplified Diffusion Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14623
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#20415;&#20110;&#23558;&#20854;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#32479;&#19968;&#36215;&#26469;&#65292;&#35299;&#20915;&#20102;DSB&#22312;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;SGM&#20316;&#20026;DSB&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#30830;&#20445;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#20102;SGM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#23384;&#22312;&#29702;&#35770;&#36817;&#20284;&#65292;&#20294;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#35777;&#23454;&#20102;&#31616;&#21270;&#30340;DSB&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#20026;&#20808;&#36827;&#30340;&#29983;&#25104;&#24314;&#27169;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#23454;&#29616;&#20108;&#27425;&#25104;&#26412;&#19979;&#30340;&#30452;&#32447; OT &#20301;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.13117</link><description>&lt;p&gt;
&#26368;&#20248;&#27969;&#21305;&#37197;&#65306;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#30452;&#32447;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Optimal Flow Matching: Learning Straight Trajectories in Just One Step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#23454;&#29616;&#20108;&#27425;&#25104;&#26412;&#19979;&#30340;&#30452;&#32447; OT &#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#27969;&#21305;&#37197;&#26041;&#27861;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24471;&#21040;&#20102;&#34028;&#21187;&#21457;&#23637;&#12290;&#31038;&#21306;&#36861;&#27714;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#23646;&#24615;&#26159;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#30452;&#32447;&#36712;&#36857;&#30340;&#27969;&#65292;&#36825;&#20123;&#36712;&#36857;&#23454;&#29616;&#20102;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#32622;&#25442;&#12290;&#30452;&#32447;&#24615;&#23545;&#20110;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#27969;&#30340;&#36335;&#24452;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27969;&#30452;&#32447;&#21270;&#26041;&#27861;&#37117;&#22522;&#20110;&#38750;&#24179;&#20961;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31215;&#32047;&#35823;&#24046;&#25110;&#21033;&#29992;&#21551;&#21457;&#24335;&#23567;&#25209;&#37327;OT&#36817;&#20284;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#20165;&#36890;&#36807;&#19968;&#27425;&#27969;&#21305;&#37197;&#27493;&#39588;&#21363;&#21487;&#20026;&#20108;&#27425;&#25104;&#26412;&#24674;&#22797;&#30452;&#32447;OT&#32622;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13117v1 Announce Type: cross  Abstract: Over the several recent years, there has been a boom in development of flow matching methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the optimal transport (OT) displacements. Straightness is crucial for fast integration of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative procedures which accumulate the error during training or exploit heuristic minibatch OT approximations. To address this issue, we develop a novel optimal flow matching approach which recovers the straight OT displacement for the quadratic cost in just one flow matching step.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#65292;&#30740;&#31350;&#20102;SGD&#22312;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#27604;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#21644;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#65292;&#20026;&#25913;&#21892;SGD&#29702;&#35299;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.08585</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#25913;&#21892;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08585
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#65292;&#30740;&#31350;&#20102;SGD&#22312;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#27604;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#21644;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#65292;&#20026;&#25913;&#21892;SGD&#29702;&#35299;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31639;&#27861;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#27867;&#21270;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#26377;&#26102;&#20250;&#27604;&#23725;&#22238;&#24402;&#24046;&#65292;&#36825;&#26159;&#30001;&#20110;&#27839;&#19981;&#21516;&#32500;&#24230;&#30340;&#20248;&#21270;&#19981;&#22343;&#21248;&#36896;&#25104;&#30340;&#12290;&#39044;&#26465;&#20214;&#21270;&#36890;&#36807;&#37325;&#26032;&#24179;&#34913;&#27839;&#19981;&#21516;&#26041;&#21521;&#30340;&#20248;&#21270;&#26469;&#25552;&#20379;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#33258;&#28982;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#26465;&#20214;&#21270;&#33021;&#22815;&#25552;&#21319;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#31243;&#24230;&#20197;&#21450;&#23427;&#26159;&#21542;&#33021;&#22815;&#22635;&#34917;&#29616;&#26377;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#19981;&#30830;&#23450;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#26465;&#20214;&#21270;&#23545;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#19982;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20102;&#35299;&#21644;&#25913;&#21892;SGD&#20570;&#20986;&#20102;&#20960;&#39033;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \&amp; preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#22312;&#25511;&#21046;&#29109;&#25439;&#22833;&#30340;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.08362</link><description>&lt;p&gt;
&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Microcanonical Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08362
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#22312;&#25511;&#21046;&#29109;&#25439;&#22833;&#30340;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26159;&#19968;&#31181;&#33021;&#37327;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#21487;&#23454;&#29616;&#39640;&#32500;&#20998;&#24067;&#30340;&#39640;&#25928;&#37319;&#26679;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23558;&#26679;&#26412;&#20174;&#39640;&#29109;&#20998;&#24067;&#65288;&#22914;&#39640;&#26031;&#30333;&#22122;&#22768;&#65289;&#36716;&#36816;&#33267;&#20302;&#33021;&#21306;&#22495;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#27169;&#22411;&#32622;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#31034;&#23427;&#36890;&#24120;&#20250;&#30001;&#20110;&#22312;&#19979;&#38477;&#36807;&#31243;&#20013;&#22833;&#21435;&#19981;&#24517;&#35201;&#30340;&#29109;&#32780;&#36807;&#24230;&#25311;&#21512;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#20801;&#35768;&#26356;&#22909;&#22320;&#25511;&#21046;&#29109;&#25439;&#22833;&#65292;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#20184;&#20986;&#36739;&#23567;&#20195;&#20215;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#32972;&#26223;&#20013;&#65292;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08362v1 Announce Type: cross  Abstract: Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05966</link><description>&lt;p&gt;
&#33021;&#29983;&#25104;&#27169;&#22411;&#25913;&#36827;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Models Improve Self-Supervised Representation Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#31361;&#26174;&#20102;&#20854;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#24378;&#22823;&#35270;&#35273;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21033;&#29992;&#21516;&#19968;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#38480;&#21046;&#20102;&#21464;&#25442;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#23548;&#33268;&#34920;&#31034;&#19981;&#22815;&#20248;&#21270;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#30452;&#25509;&#22312;&#28304;&#22270;&#20687;&#34920;&#31034;&#19978;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#22686;&#24378;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#30340;&#35821;&#20041;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05966v1 Announce Type: cross  Abstract: The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.05571</link><description>&lt;p&gt;
&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36712;&#36857;&#20248;&#21270;&#38754;&#20020;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#35774;&#32622;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#26694;&#26550;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#65292;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#39069;&#22806;&#32422;&#26463;&#36829;&#21453;&#25439;&#22833;&#30340;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#26088;&#22312;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#36817;&#20284;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32422;&#26463;&#36829;&#21453;&#12290;&#28982;&#21518;&#29992;&#26679;&#26412;&#20316;&#20026;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#26469;&#20248;&#21270;&#24182;&#24471;&#20986;&#26368;&#32456;&#35299;&#65292;&#24182;&#39564;&#35777;&#21487;&#34892;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05571v1 Announce Type: cross  Abstract: Trajectory optimization in robotics poses a challenging non-convex problem due to complex dynamics and environmental settings. Traditional numerical optimization methods are time-consuming in finding feasible solutions, whereas data-driven approaches lack safety guarantees for the output trajectories. In this paper, we introduce a general and fully parallelizable framework that combines diffusion models and numerical solvers for non-convex trajectory optimization, ensuring both computational efficiency and constraint satisfaction. A novel constrained diffusion model is proposed with an additional constraint violation loss for training. It aims to approximate the distribution of locally optimal solutions while minimizing constraint violations during sampling. The samples are then used as initial guesses for a numerical solver to refine and derive final solutions with formal verification of feasibility and optimality. Experimental evalua
&lt;/p&gt;</description></item><item><title>UniTable &#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#34920;&#32467;&#26500;&#35782;&#21035;&#65292;&#23558;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#30446;&#26631;&#32479;&#19968;&#21040;&#35821;&#35328;&#24314;&#27169;&#20013;&#65292;&#22312;&#22235;&#20010;&#26368;&#22823;&#30340;TSR&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04822</link><description>&lt;p&gt;
UniTable: &#26397;&#21521;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#34920;&#32467;&#26500;&#35782;&#21035;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04822
&lt;/p&gt;
&lt;p&gt;
UniTable &#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#34920;&#32467;&#26500;&#35782;&#21035;&#65292;&#23558;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#30446;&#26631;&#32479;&#19968;&#21040;&#35821;&#35328;&#24314;&#27169;&#20013;&#65292;&#22312;&#22235;&#20010;&#26368;&#22823;&#30340;TSR&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#20256;&#36798;&#30001;&#20154;&#31867;&#21019;&#24314;&#30340;&#38544;&#24335;&#32422;&#23450;&#30340;&#20107;&#23454;&#21644;&#25968;&#37327;&#25968;&#25454;&#65292;&#36825;&#24448;&#24448;&#26159;&#26426;&#22120;&#38590;&#20197;&#35299;&#26512;&#30340;&#12290;&#20197;&#24448;&#20851;&#20110;&#34920;&#32467;&#26500;&#35782;&#21035;&#65288;TSR&#65289;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#21487;&#29992;&#36755;&#20837;&#21644;&#24037;&#20855;&#30340;&#22797;&#26434;&#29305;&#23450;&#20219;&#21153;&#32452;&#21512;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UniTable&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;TSR&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#35757;&#32451;&#33539;&#24335;&#32467;&#21512;&#20102;&#32431;&#31929;&#20687;&#32032;&#32423;&#36755;&#20837;&#30340;&#31616;&#21333;&#24615;&#20197;&#21450;&#26469;&#33258;&#21508;&#31181;&#26410;&#27880;&#37322;&#34920;&#26684;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;SSP&#65289;&#36171;&#20104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25152;&#26377;&#19977;&#20010;TSR&#20219;&#21153;&#30340;&#35757;&#32451;&#30446;&#26631; - &#25552;&#21462;&#34920;&#32467;&#26500;&#65292;&#21333;&#20803;&#26684;&#20869;&#23481;&#21644;&#21333;&#20803;&#26684;&#36793;&#30028;&#26694;&#65288;bbox&#65289; - &#32479;&#19968;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#35757;&#32451;&#30446;&#26631;&#65306;&#35821;&#35328;&#24314;&#27169;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#31361;&#26174;&#20102;UniTable&#22312;&#22235;&#20010;&#26368;&#22823;&#30340;TSR&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04822v1 Announce Type: cross  Abstract: Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable's state-of-the-art (SOTA) performance on four of the largest TSR datasets. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.04720</link><description>&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#28909;&#21551;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04720
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#34920;&#31034;&#24322;&#36136;&#24615;&#34920;&#26684;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#20803;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20803;&#29305;&#24449;&#65292;&#20363;&#22914;&#65292;&#32479;&#35745;&#37327;&#25110;&#26631;&#24535;&#28857;&#12290;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22914;Dataset2Vec&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26080;&#20154;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#23454;&#29616;&#22312;liltab&#21253;&#20013;&#65292;&#35813;&#21253;&#21487;&#22312;GitHub&#19978;&#25214;&#21040;https://github.com/azoz01/liltab&#12290;&#25105;&#20204;&#30340;&#21253;&#22522;&#20110;[Iwata and Kumagai, 2020]&#25552;&#20986;&#30340;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#34920;&#26684;&#25968;&#25454;&#30340;&#24050;&#24314;&#31435;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#22914;Dataset2Vec &#30340;&#32534;&#30721;&#29305;&#24449;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#29983;&#25104;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26367;&#20195;&#34920;&#31034;&#12290;&#23427;&#20204;&#37117;&#21033;&#29992;&#20102;&#25968;&#25454;&#38598;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#35780;&#20215;&#20102;Dataset2Vec&#21644;liltab
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04720v1 Announce Type: new  Abstract: Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#27969;&#20307;&#21160;&#21147;&#23398;&#25511;&#21046;&#19979;&#30340;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#31181;&#31574;&#30053;&#20197;&#20445;&#35777;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04329</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#26426;&#32764;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A mechanism-informed reinforcement learning framework for shape optimization of airfoils
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04329
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#27969;&#20307;&#21160;&#21147;&#23398;&#25511;&#21046;&#19979;&#30340;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#31181;&#31574;&#30053;&#20197;&#20445;&#35777;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#32764;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#26174;&#33879;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#21463;&#27969;&#20307;&#21160;&#21147;&#23398;&#25511;&#21046;&#30340;&#22806;&#22411;&#20248;&#21270;&#30340;&#22797;&#26434;&#24615;&#12290;&#37319;&#29992;&#22522;&#20110;PDEs&#30340;&#27714;&#35299;&#22120;&#20197;&#20445;&#35777;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#37197;&#32622;&#21644;&#20960;&#20309;&#24418;&#29366;&#21457;&#29983;&#20102;&#26497;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#21452;&#21152;&#26435;&#27531;&#24046;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#30446;&#26631;&#20989;&#25968;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#20026;&#20102;&#31616;&#21270;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#24182;&#22788;&#29702;&#20960;&#20309;&#21464;&#24418;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#25289;&#26222;&#25289;&#26031;&#24179;&#28369;&#12289;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;B\'ezier&#25311;&#21512;&#31574;&#30053;&#12290;&#36825;&#31181;&#32452;&#21512;&#19981;&#20165;&#28040;&#38500;&#20102;&#32593;&#26684;&#32416;&#32544;&#65292;&#36824;&#20445;&#35777;&#20102;&#23545;&#26426;&#32764;&#20960;&#20309;&#30340;&#31934;&#30830;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21033;&#29992;B\'ezier&#26354;&#32447;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04329v1 Announce Type: cross  Abstract: In this study, we present the mechanism-informed reinforcement learning framework for airfoil shape optimization. By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics. The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration. Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals. To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a B\'ezier fitting strategy. This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil geometry. Our neural network architecture leverages B\'ezier curves for efficient dime
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02187</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mutual Information Estimation via Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20114;&#20449;&#24687;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20272;&#35745;&#38382;&#39064;&#65292;&#21363;&#24341;&#20837;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#20855;&#26377;&#24050;&#30693;&#20114;&#20449;&#24687;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#36890;&#36807;&#39640;&#32500;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#20272;&#35745;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#8220;&#25209;&#21028;&#24615;&#31383;&#21475;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#38024;&#23545;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#28151;&#21512;&#25968;&#25454;&#65292;&#36825;&#20123;&#31383;&#21475;&#26159;&#21487;&#20197;&#26126;&#30830;&#22320;&#21463;&#21040;&#19968;&#23450;&#30340;&#20998;&#31163;&#24230;&#37327;&#32422;&#26463;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.01633</link><description>&lt;p&gt;
&#25209;&#21028;&#24615;&#31383;&#21475;&#65306;&#25193;&#25955;&#27169;&#22411;&#20013;&#29305;&#24449;&#20986;&#29616;&#30340;&#38750;&#28176;&#36827;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Critical windows: non-asymptotic theory for feature emergence in diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01633
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#8220;&#25209;&#21028;&#24615;&#31383;&#21475;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#38024;&#23545;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#28151;&#21512;&#25968;&#25454;&#65292;&#36825;&#20123;&#31383;&#21475;&#26159;&#21487;&#20197;&#26126;&#30830;&#22320;&#21463;&#21040;&#19968;&#23450;&#30340;&#20998;&#31163;&#24230;&#37327;&#32422;&#26463;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#23637;&#29702;&#35770;&#26469;&#29702;&#35299;&#22270;&#20687;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#20013;&#19968;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25209;&#21028;&#24615;&#31383;&#21475;&#12290;&#23454;&#35777;&#19978;&#35266;&#23519;&#21040;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#23384;&#22312;&#29421;&#31364;&#30340;&#26102;&#38388;&#38388;&#38548;&#65292;&#22312;&#27492;&#26399;&#38388;&#20250;&#20986;&#29616;&#26368;&#32456;&#22270;&#20687;&#30340;&#29305;&#23450;&#29305;&#24449;&#65292;&#20363;&#22914;&#22270;&#20687;&#31867;&#21035;&#25110;&#32972;&#26223;&#39068;&#33394;&#12290;&#32780;&#36825;&#31181;&#29305;&#24615;&#23545;&#20110;&#35299;&#37322;&#24615;&#26159;&#26377;&#21033;&#30340;&#65292;&#22240;&#20026;&#24847;&#21619;&#30528;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#29305;&#24615;&#23450;&#20301;&#21040;&#36712;&#36857;&#30340;&#19968;&#20010;&#23567;&#29255;&#27573;&#65292;&#20294;&#36825;&#20284;&#20046;&#19982;&#25193;&#25955;&#30340;&#36830;&#32493;&#24615;&#36136;&#30456;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#31383;&#21475;&#65292;&#24182;&#34920;&#26126;&#23545;&#20110;&#26469;&#33258;&#28151;&#21512;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#31383;&#21475;&#21487;&#20197;&#29992;&#19968;&#23450;&#30340;&#36328;&#32452;&#21644;&#32452;&#20869;&#20998;&#31163;&#24230;&#37327;&#26469;&#26174;&#24335;&#22320;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#20026;&#35832;&#22914;&#33391;&#26465;&#20214;G&#30340;&#20855;&#20307;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#36825;&#20123;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01633v1 Announce Type: new  Abstract: We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya &amp; Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#28857;&#20851;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;OOD&#25968;&#25454;&#24212;&#20855;&#26377;&#26356;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#31616;&#21333;&#30452;&#35273;&#65292;&#36890;&#36807;&#36817;&#20284;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#23454;&#29616;&#35813;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.01485</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#36827;&#34892;&#36817;&#20284;&#29992;&#20110;&#26816;&#27979;&#31163;&#32676;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#28857;&#20851;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;OOD&#25968;&#25454;&#24212;&#20855;&#26377;&#26356;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#31616;&#21333;&#30452;&#35273;&#65292;&#36890;&#36807;&#36817;&#20284;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#23454;&#29616;&#35813;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20284;&#28982;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#26159;&#36817;&#24180;&#26469;&#29992;&#20110;&#25311;&#21512;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#38899;&#39057;&#65289;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#12290;&#23427;&#20204;&#21487;&#20197;&#33258;&#28982;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20043;&#19968;&#65292;&#21363;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;Nalisnick&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#34920;&#26126;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22987;&#32456;&#20026;OOD&#25968;&#25454;&#25512;&#26029;&#20986;&#27604;&#23427;&#20204;&#35757;&#32451;&#36807;&#30340;&#25968;&#25454;&#26356;&#39640;&#30340;&#23545;&#25968;&#20284;&#28982;&#65292;&#26631;&#24535;&#30528;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#25968;&#25454;&#28857;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21442;&#25968;&#26799;&#24230;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#31616;&#21333;&#30452;&#35273;&#65292;&#21363;OOD&#25968;&#25454;&#30340;&#26799;&#24230;&#33539;&#25968;&#24212;&#35813;&#22823;&#20110;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#22320;&#23558;&#26799;&#24230;&#22823;&#23567;&#30340;&#24230;&#37327;&#37327;&#21270;&#20026;&#36817;&#20284;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#20855;&#26377;&#36739;&#22823;&#30340;&#32477;&#23545;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18839</link><description>&lt;p&gt;
&#25193;&#23637;&#27969;&#21305;&#37197;&#65306;&#20855;&#26377;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#33879;&#21517;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20197;&#22522;&#20110;&#24341;&#23548;&#30340;&#26080;&#20998;&#31867;&#22120;&#26041;&#27861;&#20026;&#39318;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24341;&#23548;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#19981;&#20165;&#35201;&#27714;&#29992;&#25143;&#24494;&#35843;&#8220;&#24341;&#23548;&#24378;&#24230;&#8221;&#65292;&#32780;&#19988;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#19981;&#19968;&#23450;&#23545;&#24212;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#26412;&#25991;&#22522;&#20110;&#27969;&#21305;&#37197;&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#27969;&#21305;&#37197;&#26159;&#25193;&#25955;&#26041;&#27861;&#30340;&#24403;&#21069;&#24378;&#22823;&#31454;&#20105;&#32773;&#20043;&#19968;&#12290;&#21463;&#23558;&#27010;&#29575;&#36335;&#24452;&#35299;&#37322;&#20026;&#36335;&#24452;&#31354;&#38388;&#19978;&#30340;&#20998;&#24067;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#19981;&#26159;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#12290;&#36825;&#19968;&#29702;&#35770;&#33258;&#28982;&#22320;&#25512;&#23548;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18839v1 Announce Type: new  Abstract: The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the "guidance strength," but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.17886</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;&#38646;&#38454;&#37319;&#26679;&#26041;&#27861;&#65306;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#32531;&#35299;&#20122;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#20854;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#26410;&#24402;&#19968;&#21270;&#23494;&#24230;&#26597;&#35810;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#39318;&#20808;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#25311;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#21363;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;DMC&#65289;&#65292;&#20854;&#24471;&#20998;&#20989;&#25968;&#36890;&#36807;&#36890;&#29992;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#36924;&#36817;&#12290;DMC&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#35861;&#30340;&#20803;&#31639;&#27861;&#65292;&#20854;&#20013;&#31070;&#35861;&#26159;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;&#20998;&#25968;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#30340;&#35775;&#38382;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;&#36825;&#20010;&#31070;&#35861;&#30340;&#23454;&#29616;&#65292;&#36825;&#23558;DMC&#36716;&#21270;&#20026;&#19968;&#20010;&#30495;&#27491;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;ZOD-MC&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;DMC&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#32780;&#19981;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#20026;&#23545;&#25968;&#20985;&#25110;&#28385;&#36275;&#20219;&#20309;&#31561;&#21608;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;ZOD-MC&#23545;&#25152;&#38656;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#65292;&#23613;&#31649;&#20173;&#28982;&#21463;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
&lt;/p&gt;</description></item><item><title>&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17501</link><description>&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#20316;&#20026;&#19968;&#20010;&#22823;&#22411;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Intensive Care as One Big Sequence Modeling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17501
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#28041;&#21450;&#29421;&#31364;&#30340;&#33258;&#21253;&#21547;&#20219;&#21153;&#65292;&#22914;&#33043;&#27602;&#30151;&#39044;&#27979;&#25110;&#40635;&#37257;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#65288;&#20027;&#35201;&#31034;&#20363;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20855;&#26377;&#36229;&#36234;&#29305;&#23450;&#20219;&#21153;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#38544;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#20445;&#20581;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#20581;&#20316;&#20026;&#24207;&#21015;&#24314;&#27169;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#24739;&#32773;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34987;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#31561;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#23545;&#27969;&#20013;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22312;&#23454;&#39564;&#20013;&#25506;&#32034;&#36825;&#19968;&#33539;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MIMIC-SEQ&#65292;&#36825;&#26159;&#19968;&#20010;&#24207;&#21015;&#24314;&#27169;&#22522;&#20934;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;MIMIC-IV&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#19968;&#31181;&#32479;&#19968;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17501v1 Announce Type: cross  Abstract: Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.17042</link><description>&lt;p&gt;
&#36890;&#21521;&#20174;&#35797;&#39564;&#25512;&#24191;&#25512;&#29702;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizing Inferences from Trials to Target Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#22312;&#20135;&#29983;&#20869;&#37096;&#26377;&#25928;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23545;&#25193;&#23637;&#36825;&#20123;&#21457;&#29616;&#20197;&#33719;&#24471;&#22806;&#37096;&#26377;&#25928;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#25506;&#31350;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24212;&#23545;&#36825;&#20123;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#30340;&#21069;&#27839;&#65292;&#27010;&#25324;&#20102;2023&#24180;&#31179;&#23395;&#22312;&#24067;&#26391;&#22823;&#23398;&#35745;&#31639;&#19982;&#23454;&#39564;&#25968;&#23398;&#30740;&#31350;&#25152;&#65288;ICERM&#65289;&#20030;&#34892;&#30340;&#19968;&#27425;&#36328;&#23398;&#31185;&#30740;&#35752;&#20250;&#30340;&#31934;&#21326;&#12290;&#35813;&#30740;&#35752;&#20250;&#27719;&#38598;&#20102;&#26469;&#33258;&#31038;&#20250;&#31185;&#23398;&#12289;&#21307;&#23398;&#12289;&#20844;&#20849;&#21355;&#29983;&#12289;&#32479;&#35745;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#23398;&#31185;&#22312;&#25512;&#26029;&#23454;&#39564;&#32467;&#26524;&#26041;&#38754;&#38754;&#20020;&#30340;&#29420;&#29305;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#25105;&#20204;&#25972;&#21512;&#27491;&#22312;&#36827;&#34892;&#30340;&#21162;&#21147;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17042v1 Announce Type: cross  Abstract: Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting me
&lt;/p&gt;</description></item><item><title>PRoLoRA&#26159;&#19968;&#20010;&#26032;&#30340;&#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#25773;&#20943;&#23569;&#12289;&#26059;&#36716;&#22686;&#24378;&#12289;&#37096;&#20998;&#20849;&#20139;&#32454;&#21270;&#21644;&#20462;&#27491;&#21021;&#22987;&#21270;&#31574;&#30053;&#31561;&#22235;&#20010;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#23545;LoRA&#30340;&#20248;&#21183;&#25552;&#21319;&#65292;&#36991;&#20813;&#20102;&#20854;&#20182;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16902</link><description>&lt;p&gt;
PRoLoRA: &#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#26356;&#39640;&#25928;LoRA
&lt;/p&gt;
&lt;p&gt;
PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16902
&lt;/p&gt;
&lt;p&gt;
PRoLoRA&#26159;&#19968;&#20010;&#26032;&#30340;&#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#25773;&#20943;&#23569;&#12289;&#26059;&#36716;&#22686;&#24378;&#12289;&#37096;&#20998;&#20849;&#20139;&#32454;&#21270;&#21644;&#20462;&#27491;&#21021;&#22987;&#21270;&#31574;&#30053;&#31561;&#22235;&#20010;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#23545;LoRA&#30340;&#20248;&#21183;&#25552;&#21319;&#65292;&#36991;&#20813;&#20102;&#20854;&#20182;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#25193;&#23637;&#65292;&#21516;&#26102;&#26381;&#21153;&#22810;&#20010;LoRAs&#21464;&#24471;&#26085;&#30410;&#19981;&#20999;&#23454;&#38469;&#65292;&#23548;&#33268;&#25104;&#26412;&#19981;&#21487;&#25215;&#21463;&#65292;&#38656;&#35201;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#65288;PRoLoRA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#30340;&#23618;&#20869;&#20849;&#20139;&#26426;&#21046;&#65306;&#24191;&#25773;&#20943;&#23569;&#12289;&#26059;&#36716;&#22686;&#24378;&#12289;&#37096;&#20998;&#20849;&#20139;&#32454;&#21270;&#21644;&#20462;&#27491;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#12290;&#20316;&#20026;LoRA&#30340;&#36229;&#38598;&#65292;PRoLoRA&#20445;&#30041;&#20102;&#20854;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#20855;&#26377;&#21331;&#36234;&#30340;&#27169;&#22411;&#23481;&#37327;&#12289;&#23454;&#29992;&#21487;&#34892;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#26377;&#25928;&#22320;&#36991;&#24320;&#20102;&#20854;&#20182;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;PRoLoRA&#22312;&#29305;&#23450;&#21442;&#25968;&#39044;&#31639;&#21644;&#24615;&#33021;&#30446;&#26631;&#24773;&#26223;&#19979;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;LLMs&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PRoLoRA&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;&#19968;&#20493;&#30340;&#24773;&#20917;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16902v1 Announce Type: new  Abstract: With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16354</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-guided Skill Learning with Temporal Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#21457;&#29616;&#25216;&#33021;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#20986;&#36712;&#36857;&#30340;&#21021;&#22987;&#20998;&#21106;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20998;&#23618;&#21464;&#20998;&#25512;&#26029;&#26694;&#26550;&#23558;LLM&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#36712;&#36857;&#27573;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25511;&#21046;&#21387;&#32553;&#21644;&#21487;&#37325;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#30340;&#26032;&#36741;&#21161;&#30446;&#26631;&#65292;&#24110;&#21161;&#24341;&#23548;&#36825;&#31181;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;Agent&#33021;&#22815;&#21457;&#29616;&#26377;&#21161;&#20110;&#21152;&#36895;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#22312;BabyAI&#65288;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#23548;&#33322;&#29615;&#22659;&#65289;&#20197;&#21450;ALFRED&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#29615;&#22659;&#65289;&#30340;&#26032;&#38271;&#26399;&#20219;&#21153;&#20013;&#32988;&#36807;&#22522;&#32447;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16300</link><description>&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#24635;&#26159;&#35201;&#25552;&#20379;&#39044;&#27979;&#65311;&#22312;&#36861;&#27714;&#26368;&#22823;&#39044;&#27979;&#24615;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;&#36873;&#25321;&#24615;&#22238;&#24402;&#65292;&#20063;&#31216;&#20026;&#8220;&#25298;&#32477;&#36873;&#39033;&#8221;&#65292;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#25918;&#24323;&#39044;&#27979;&#12290;&#23613;&#31649;7&#21313;&#24180;&#21069;&#23601;&#26368;&#21021;&#25552;&#20986;&#20102;&#36873;&#25321;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#29992;&#20110;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#20195;&#29702;&#65292;&#23588;&#20854;&#26159;&#26465;&#20214;&#26041;&#24046;&#12290;&#20294;&#36825;&#31181;&#20851;&#27880;&#24573;&#35270;&#20102;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20026;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#26377;&#26681;&#25454;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20415;&#36827;&#34892;&#24688;&#24403;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16300v1 Announce Type: new  Abstract: Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the "reject option," allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper compar
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.16187</link><description>&lt;p&gt;
&#21033;&#29992;&#20854;&#20248;&#21183;&#25915;&#20987;LLM&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Attacking LLM Watermarks by Exploiting Their Strengths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16187
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#22270;&#29255;&#33021;&#22815;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#23558;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20197;&#39564;&#35777;&#20854;&#26469;&#28304;&#65292;&#23545;&#20110;&#20943;&#23569;&#23545;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28389;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#26696;&#20173;&#28982;&#20196;&#20154;&#24847;&#22806;&#22320;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#20849;&#20139;&#30340;&#21487;&#21462;&#29305;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#65292;&#21453;&#36807;&#26469;&#21364;&#20351;&#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#36973;&#21463;&#21508;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#27700;&#21360;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#20005;&#26684;&#30740;&#31350;&#28508;&#22312;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25915;&#20987;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#38450;&#24481;&#25514;&#26045;&#8212;&#8212;&#24314;&#31435;&#20102;&#19968;&#22871;&#23884;&#20837;&#21644;&#26816;&#27979;LLM&#27700;&#21360;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.15567</link><description>&lt;p&gt;
&#20855;&#26377;&#24076;&#23572;&#20271;&#29305;&#34920;&#31034;&#30340;&#22522;&#30784;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Foundation Policies with Hilbert Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 &#36827;&#34892;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#20363;&#22914;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#65292;&#24050;&#32463;&#20351;&#24471;&#21487;&#20197;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#19968;&#20010;&#30495;&#27491;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#20197;&#33719;&#21462;&#36890;&#29992;&#25919;&#31574;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;RL&#65292;&#22522;&#20110;&#35832;&#22914;&#22522;&#20110;&#30446;&#26631;&#30340;RL&#12289;&#34892;&#20026;&#20811;&#38534;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#23398;&#20064;&#31561;&#21407;&#21017;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#21457;&#29616;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#12289;&#38656;&#35201;&#39640;&#36136;&#37327;&#28436;&#31034;&#25968;&#25454;&#25110;&#32570;&#20047;&#26126;&#30830;&#30340;&#25552;&#31034;&#25110;&#36866;&#24212;&#26426;&#21046;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#30340;&#36890;&#29992;&#25919;&#31574;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#33719;&#21462;&#36825;&#20123;&#34892;&#20026;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 Announce Type: cross  Abstract: Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear prompting or adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly ada
&lt;/p&gt;</description></item><item><title>&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.15390</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#20462;&#22797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explorations of Self-Repair in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15390
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#29421;&#31364;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#21457;&#29616;&#20102;&#33258;&#20462;&#22797;&#29616;&#35937;&#65292;&#21363;&#22914;&#26524;&#21093;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#20214;&#65292;&#21518;&#32493;&#32452;&#20214;&#20250;&#25913;&#21464;&#20854;&#34892;&#20026;&#20197;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#36807;&#21435;&#30340;&#25991;&#29486;&#65292;&#23637;&#31034;&#20102;&#24403;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#21093;&#31163;&#21333;&#20010;&#27880;&#24847;&#21147;&#22836;&#26102;&#65292;&#33258;&#20462;&#22797;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#65292;&#33258;&#20462;&#22797;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#22240;&#20026;&#22836;&#37096;&#30340;&#21407;&#22987;&#30452;&#25509;&#25928;&#26524;&#24182;&#26410;&#23436;&#20840;&#24674;&#22797;&#65292;&#24182;&#19988;&#26159;&#22024;&#26434;&#30340;&#65292;&#22240;&#20026;&#33258;&#20462;&#22797;&#31243;&#24230;&#22312;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#26174;&#33879;&#21464;&#21270;&#65288;&#26377;&#26102;&#36229;&#36807;&#21407;&#22987;&#25928;&#26524;&#65289;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20419;&#25104;&#33258;&#20462;&#22797;&#30340;&#20004;&#31181;&#19981;&#21516;&#26426;&#21046;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#65288;&#21487;&#20462;&#22797;&#30452;&#25509;&#25928;&#26524;&#30340;30%&#65289;&#20197;&#21450;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2402.15259</link><description>&lt;p&gt;
&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Open Ad Hoc Teamwork with Cooperative Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#38754;&#20020;&#30528;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#19982;&#38431;&#21451;&#21327;&#20316;&#20294;&#27809;&#26377;&#20808;&#21069;&#21327;&#35843;&#25110;&#32852;&#21512;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#12290;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#26029;&#21464;&#21270;&#30340;&#38431;&#21451;&#25968;&#37327;&#30340;&#29615;&#22659;&#65292;&#21363;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;&#29616;&#26377;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#23398;&#20064;&#65288;GPL&#65289;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#22788;&#29702;&#26080;&#38480;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#65292;&#26377;&#25928;&#24212;&#23545;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;GPL&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20294;&#20854;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#23545;&#35299;&#37322;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20174;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#20026;GPL&#20013;&#37319;&#29992;&#30340;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14992</link><description>&lt;p&gt;
&#23567;&#22411;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
tinyBenchmarks: evaluating LLMs with fewer examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#23548;&#33268;&#21019;&#24314;&#20102;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24443;&#24213;&#27979;&#35797;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#31034;&#20363;&#65292;&#20351;&#24471;&#35780;&#20272;LLMs&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;MMLU&#19978;&#30340;&#24615;&#33021;&#65288;&#19968;&#20010;&#21253;&#21547;14K&#20010;&#31034;&#20363;&#30340;&#27969;&#34892;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#21482;&#38656;&#35201;&#22312;100&#20010;&#31934;&#24515;&#25361;&#36873;&#30340;&#31034;&#20363;&#19978;&#35780;&#20272;&#36825;&#20010;LLMs&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#24037;&#20855;&#21644;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22411;&#29256;&#26412;&#65306;Open LLM Leaderboard&#12289;MMLU&#12289;HELM&#21644;AlpacaEval 2.0&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#24494;&#22411;&#22522;&#20934;&#27979;&#35797;&#36275;&#20197;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#37325;&#29616;&#21407;&#22987;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.14807</link><description>&lt;p&gt;
&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#20013;&#21160;&#24577;&#19981;&#23433;&#38745;&#22810;&#33218;&#32769;&#34382;&#26426;&#20219;&#21153;&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65288;DLM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#38477;&#20302;&#23381;&#20135;&#22919;&#27515;&#20129;&#29575;&#30340;&#21162;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39044;&#38450;&#20445;&#20581;&#35745;&#21010;&#65292;&#21521;&#39640;&#39118;&#38505;&#20154;&#32676;&#20256;&#25773;&#37325;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DLM&#65306;&#19968;&#31181;&#29992;&#20110;RMAB&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14045</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#25512;&#36827;&#20302;&#31209;&#21644;&#23616;&#37096;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#30340;&#22823;&#23481;&#37327;&#21644;&#22797;&#26434;&#24615;&#26159;&#23384;&#20648;&#12289;&#20256;&#36755;&#21644;&#22788;&#29702;&#30340;&#29942;&#39048;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21450;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#24212;&#29992;&#24050;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#23637;&#31034;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;LRMA&#21644;LLRMA&#30340;&#20316;&#21697;&#12290;&#25991;&#29486;&#30340;&#35814;&#32454;&#20998;&#26512;&#30830;&#35748;&#20102;&#24212;&#29992;&#20110;&#21508;&#31181;&#25104;&#20687;&#27169;&#24577;&#30340;LRMA&#21644;LLRMA&#26041;&#27861;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;LRMA&#21644;LLRMA&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#33258;2015&#24180;&#20197;&#26469;&#65292;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26126;&#26174;&#20559;&#21521;&#20110;LLRMA&#65292;&#26174;&#31034;&#20102;&#30456;&#23545;&#20110;LRMA&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;LLRMA&#25152;&#20351;&#29992;&#30340;&#27973;&#23618;&#30456;&#20284;&#24615;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20808;&#36827;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#26469;&#22788;&#29702;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14045v1 Announce Type: cross  Abstract: The large volume and complexity of medical imaging datasets are bottlenecks for storage, transmission, and processing. To tackle these challenges, the application of low-rank matrix approximation (LRMA) and its derivative, local LRMA (LLRMA) has demonstrated potential.   This paper conducts a systematic literature review to showcase works applying LRMA and LLRMA in medical imaging. A detailed analysis of the literature identifies LRMA and LLRMA methods applied to various imaging modalities. This paper addresses the challenges and limitations associated with existing LRMA and LLRMA methods.   We note a significant shift towards a preference for LLRMA in the medical imaging field since 2015, demonstrating its potential and effectiveness in capturing complex structures in medical data compared to LRMA. Acknowledging the limitations of shallow similarity methods used with LLRMA, we suggest advanced semantic image segmentation for similarit
&lt;/p&gt;</description></item><item><title>GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14009</link><description>&lt;p&gt;
&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometry-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14009
&lt;/p&gt;
&lt;p&gt;
GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#30340;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#22312;&#20960;&#20309;&#32422;&#26463;&#19979;&#23398;&#20064;&#65292;&#65288;ii&#65289;&#31070;&#32463;&#22330;&#20316;&#20026;&#21512;&#36866;&#30340;&#34920;&#31034;&#65292;&#65288;iii&#65289;&#29983;&#25104;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#27424;&#23450;&#31995;&#32479;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GINN&#30340;&#26500;&#24314;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#34987;&#32431;&#32422;&#26463;&#39537;&#21160;&#22320;&#35270;&#20026;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#26174;&#24335;&#30340;&#22810;&#26679;&#24615;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32422;&#26463;&#65292;&#29305;&#21035;&#26159;&#32452;&#20214;&#30340;&#36830;&#36890;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#33707;&#23572;&#26031;&#29702;&#35770;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#24494;&#25439;&#22833;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#22330;&#26223;&#20013;&#65292;GINN&#23398;&#20064;&#33539;&#24335;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12847</link><description>&lt;p&gt;
&#35843;&#25972;&#36807;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#30693;&#35782;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models are Better Knowledge Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21161;&#25163;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24517;&#39035;&#33021;&#22815;&#36890;&#36807;&#25345;&#32493;&#22312;&#26032;&#25968;&#25454;&#19978;&#35757;&#32451;&#26469;&#26356;&#26032;&#23427;&#20204;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20256;&#32479;&#20570;&#27861;&#28041;&#21450;&#22312;&#26032;&#25991;&#26723;&#19978;&#25345;&#32493;&#39044;&#22521;&#35757;&#65292;&#28982;&#21518;&#26681;&#25454;&#38382;&#39064;-&#31572;&#26696;&#65288;QA&#65289;&#23545;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35889;&#32858;&#31867;&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#28176;&#36817;&#39640;&#26031;&#27874;&#21160;&#29616;&#35937;&#65292;&#20026;&#31934;&#30830;&#39044;&#27979;&#35889;&#32858;&#31867;&#30340;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.12302</link><description>&lt;p&gt;
&#35889;&#32858;&#31867;&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#28176;&#36817;&#39640;&#26031;&#27874;&#21160;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35889;&#32858;&#31867;&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#28176;&#36817;&#39640;&#26031;&#27874;&#21160;&#29616;&#35937;&#65292;&#20026;&#31934;&#30830;&#39044;&#27979;&#35889;&#32858;&#31867;&#30340;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#30456;&#20284;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#26465;&#30446;&#27874;&#21160;&#65292;&#35813;&#27874;&#21160;&#30452;&#21040;&#29616;&#22312;&#20173;&#26410;&#24471;&#21040;&#25551;&#36848;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19968;&#33324;&#23574;&#23792;&#38543;&#26426;&#30697;&#38453;&#27169;&#22411;&#30340;&#20449;&#21495;+&#22122;&#22768;&#32467;&#26500;&#34987;&#36716;&#31227;&#21040;&#30456;&#24212;&#30340;&#26684;&#25289;&#22982;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#26465;&#30446;&#27874;&#21160;&#22312;&#22823;&#32500;&#24230;&#21306;&#22495;&#21576;&#39640;&#26031;&#20998;&#24067;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#30340;&#32467;&#26524;&#26159;&#20934;&#30830;&#39044;&#27979;&#35889;&#32858;&#31867;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#26368;&#21518;&#19968;&#22359;&#32570;&#22833;&#30340;&#25340;&#22270;&#12290;&#25552;&#20986;&#30340;&#35777;&#26126;&#38750;&#24120;&#36890;&#29992;&#65292;&#20165;&#20381;&#36182;&#20110;&#22122;&#22768;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#20010;&#29616;&#35937;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12302v1 Announce Type: cross  Abstract: The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering. The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#28040;&#38500;&#20102;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#26377;&#21161;&#20110;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#20302;&#31209;&#35299;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11867</link><description>&lt;p&gt;
LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#27809;&#26377;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
LoRA Training in the NTK Regime has No Spurious Local Minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11867
&lt;/p&gt;
&lt;p&gt;
LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#28040;&#38500;&#20102;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#26377;&#21161;&#20110;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#20302;&#31209;&#35299;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24050;&#25104;&#20026;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;LoRA&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#27169;&#24335;&#19979;&#20351;&#29992;LoRA&#24494;&#35843;&#65292;&#20854;&#20013;&#21253;&#21547;$N$&#20010;&#25968;&#25454;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#65306;(i) &#20840;&#38754;&#24494;&#35843;&#65288;&#19981;&#20351;&#29992;LoRA&#65289;&#20801;&#35768;&#31209;&#20026;$r\lesssim \sqrt{N}$&#30340;&#20302;&#31209;&#35299;; (ii) &#20351;&#29992;&#31209;&#20026;$r\gtrsim \sqrt{N}$&#30340;LoRA&#28040;&#38500;&#20102;&#34394;&#20551;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20351;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25214;&#21040;&#20302;&#31209;&#35299;; (iii) &#20351;&#29992;LoRA&#25214;&#21040;&#30340;&#20302;&#31209;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11867v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>AdAdaGrad&#21644;AdAdaGradNorm&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#21152;&#25209;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#35777;&#26126;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#22312;$O(1/K)$&#36895;&#24230;&#19979;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.11215</link><description>&lt;p&gt;
AdAdaGrad&#65306;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11215
&lt;/p&gt;
&lt;p&gt;
AdAdaGrad&#21644;AdAdaGradNorm&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#21152;&#25209;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#35777;&#26126;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#22312;$O(1/K)$&#36895;&#24230;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#22120;&#20013;&#25209;&#37327;&#22823;&#23567;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21464;&#21270;&#25209;&#22823;&#23567;&#30340;&#23454;&#36341;&#30456;&#23545;&#20854;&#20182;&#36229;&#21442;&#25968;&#36739;&#23569;&#25506;&#35752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#20013;&#23548;&#20986;&#30340;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#20256;&#32479;&#19978;&#20165;&#24212;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#32771;&#34385;&#21040;&#23398;&#20064;&#36895;&#29575;&#21644;&#25209;&#22823;&#23567;&#20043;&#38388;&#30340;&#26174;&#33879;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#38656;&#35201;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;AdAdaGrad&#21450;&#20854;&#26631;&#37327;&#21464;&#20307;AdAdaGradNorm&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#22686;&#21152;&#25209;&#22823;&#23567;&#65292;&#21516;&#26102;&#20351;&#29992;AdaGrad&#21644;AdaGradNorm&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#20197;$O(1/K)$&#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#29992;&#20110;&#25214;&#21040;&#20809;&#28369;&#38750;&#20984;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#22312;$K$&#27425;&#36845;&#20195;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11215v1 Announce Type: new  Abstract: The choice of batch sizes in stochastic gradient optimizers is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10447</link><description>&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#65306;&#20004;&#31181;&#36716;&#21464;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Incremental Sequence Labeling: A Tale of Two Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#28041;&#21450;&#22312;&#20445;&#30041;&#23545;&#20808;&#21069;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#38543;&#26102;&#38388;&#19981;&#26029;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65306;E2O&#65288;&#27169;&#22411;&#23558;&#26087;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#38750;&#23454;&#20307;&#65289;&#21644;O2E&#65288;&#27169;&#22411;&#23558;&#38750;&#23454;&#20307;&#25110;&#26087;&#23454;&#20307;&#26631;&#35760;&#20026;&#26032;&#23454;&#20307;&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;E2O&#38382;&#39064;&#19978;&#65292;&#24573;&#35270;&#20102;O2E&#38382;&#39064;&#12290;&#36825;&#31181;&#24573;&#30053;&#23548;&#33268;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23545;&#26032;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#26102;&#23384;&#22312;&#20559;&#35265;&#65292;&#35748;&#20026;&#23427;&#20204;&#23646;&#20110;&#26032;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26080;&#35821;&#20041;&#36716;&#21464;&#30340;&#22686;&#37327;&#39034;&#24207;&#26631;&#35760;&#65288;IS3&#65289;&#12290;&#21463;&#21040;&#24050;&#30830;&#23450;&#30340;&#35821;&#20041;&#36716;&#21464;&#65288;E2O&#21644;O2E&#65289;&#30340;&#21551;&#21457;&#65292;IS3&#26088;&#22312;&#32531;&#35299;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#33267;&#20110;E2O&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#27169;&#22411;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10447v1 Announce Type: new  Abstract: The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10403</link><description>&lt;p&gt;
&#20174;&#20998;&#27573;&#19977;&#32447;&#24615;&#32593;&#32476;&#20013;&#23548;&#20986;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Polyhedral Complex Derivation from Piecewise Trilinear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#65288;CPWA&#65289;&#20989;&#25968;&#20013;&#25552;&#21462;&#32593;&#26684;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#21253;&#25324;&#38750;&#32447;&#24615;&#20301;&#32622;&#32534;&#30721;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#35889;&#20559;&#24046;&#20043;&#31867;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#22312;&#24212;&#29992;&#22522;&#20110;CPWA&#20989;&#25968;&#30340;&#32593;&#26684;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#30340;&#32593;&#26684;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#22312;&#22855;&#25343;&#23572;&#32422;&#26463;&#19979;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#19977;&#32447;&#24615;&#21306;&#22495;&#20869;&#30340;&#24179;&#38754;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19977;&#20010;&#39640;&#32500;&#26354;&#38754;&#20043;&#38388;&#30340;&#20132;&#28857;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27721;&#26126;&#36317;&#31163;&#21644;&#25928;&#29575;&#20197;&#21450;&#35282;&#36317;&#31163;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#27491;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#20102;t&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21270;&#33258;&#22238;&#24402;&#36807;&#31243;&#26469;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;ParaTAA&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#24182;&#34892;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2402.09970</link><description>&lt;p&gt;
&#21152;&#36895;&#24182;&#34892;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Accelerating Parallel Sampling of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21270;&#33258;&#22238;&#24402;&#36807;&#31243;&#26469;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;ParaTAA&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#24182;&#34892;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#37319;&#26679;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#36890;&#24120;&#32791;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#33258;&#22238;&#24402;&#36807;&#31243;&#26469;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#37319;&#26679;&#36807;&#31243;&#37325;&#26032;&#26500;&#24314;&#20026;&#36890;&#36807;&#22266;&#23450;&#28857;&#36845;&#20195;&#35299;&#20915;&#19977;&#35282;&#38750;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#21019;&#26032;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#31995;&#32479;&#21270;&#30340;&#25216;&#26415;&#65292;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#27714;&#35299;&#36807;&#31243;&#25152;&#38656;&#30340;&#36845;&#20195;&#27493;&#39588;&#12290;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ParaTAA&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#24182;&#34892;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#26469;&#22686;&#21152;&#37319;&#26679;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ParaTAA&#21487;&#20197;&#20943;&#23569;&#24120;&#35265;&#30340;&#39034;&#24207;&#37319;&#26679;&#25152;&#38656;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09970v1 Announce Type: new  Abstract: Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.09963</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#22256;&#38590;?
&lt;/p&gt;
&lt;p&gt;
Why are Sensitive Functions Hard for Transformers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#23384;&#22312;&#19968;&#31995;&#21015;&#30340;&#23398;&#20064;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#22914;&#22312;&#23398;&#20064;&#35745;&#31639;&#31616;&#21333;&#24418;&#24335;&#35821;&#35328;&#65288;&#22914;PARITY&#65289;&#26102;&#30340;&#25345;&#20037;&#22256;&#38590;&#65292;&#20197;&#21450;&#23545;&#20302;&#38454;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#29702;&#35770;&#35201;&#20040;&#36807;&#24230;&#39044;&#27979;&#65292;&#35201;&#20040;&#20302;&#20272;&#20102;&#23454;&#38469;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65306;&#36755;&#20986;&#23545;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#22810;&#20010;&#37096;&#20998;&#25935;&#24863;&#30340;Transformer&#23384;&#22312;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#23396;&#31435;&#28857;&#65292;&#23548;&#33268;&#27867;&#21270;&#20013;&#30340;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#65292;&#22914;&#23427;&#20204;&#23545;&#20302;&#25935;&#24863;&#24615;&#21644;&#20302;&#38454;&#30340;&#27867;&#21270;&#20559;&#24046;&#65292;&#20197;&#21450;&#22312;&#38271;&#24230;&#27867;&#21270;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09963v1 Announce Type: new  Abstract: Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalizati
&lt;/p&gt;</description></item><item><title>&#26080;&#38656;&#20154;&#24037;&#31574;&#21010;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#27491;&#30830;&#24615;&#65288;RTC&#65289;&#20316;&#20026;&#35780;&#20272;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;RTC&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#23545;&#20195;&#30721;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#22522;&#20934;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08699</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35780;&#20272;&#20855;&#26377;&#24448;&#36820;&#27491;&#30830;&#24615;&#30340;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Unsupervised Evaluation of Code LLMs with Round-Trip Correctness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08699
&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#20154;&#24037;&#31574;&#21010;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#27491;&#30830;&#24615;&#65288;RTC&#65289;&#20316;&#20026;&#35780;&#20272;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;RTC&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#23545;&#20195;&#30721;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#22522;&#20934;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30740;&#31350;&#19968;&#30452;&#20381;&#36182;&#20110;&#19968;&#20123;&#23567;&#30340;&#25163;&#21160;&#31574;&#21010;&#30340;&#22522;&#20934;&#65292;&#22914;HumanEval&#21644;MBPP&#65292;&#36825;&#20123;&#22522;&#20934;&#21482;&#20195;&#34920;&#20102;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#30340;&#19968;&#20010;&#29421;&#31364;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24448;&#36820;&#27491;&#30830;&#24615;&#65288;RTC&#65289;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#35780;&#20272;&#26041;&#27861;&#12290;RTC&#20801;&#35768;&#22312;&#26356;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#36719;&#20214;&#39046;&#22495;&#23545;&#20195;&#30721;LLM&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#20154;&#24037;&#31574;&#21010;&#12290;RTC&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#25105;&#20204;&#21487;&#20197;&#35201;&#27714;&#27169;&#22411;&#20570;&#20986;&#39044;&#27979;&#65288;&#20363;&#22914;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19968;&#20123;&#20195;&#30721;&#65289;&#65292;&#23558;&#35813;&#39044;&#27979;&#36820;&#22238;&#65288;&#20363;&#22914;&#20174;&#39044;&#27979;&#30340;&#25551;&#36848;&#20013;&#21512;&#25104;&#20195;&#30721;&#65289;&#65292;&#24182;&#26816;&#26597;&#36825;&#20010;&#24448;&#36820;&#36807;&#31243;&#26159;&#21542;&#23548;&#33268;&#19982;&#21407;&#22987;&#36755;&#20837;&#35821;&#20041;&#31561;&#25928;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;RTC&#26469;&#35780;&#20272;&#20195;&#30721;&#21512;&#25104;&#21644;&#32534;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;RTC&#19982;&#29616;&#26377;&#29421;&#31364;&#39046;&#22495;&#20195;&#30721;&#21512;&#25104;&#22522;&#20934;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#21516;&#26102;&#20063;&#20801;&#35768;&#25105;&#20204;&#25193;&#23637;&#21040;&#26356;&#24191;&#38420;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08699v1 Announce Type: cross Abstract: To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader se
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G-Retriever&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#29992;&#25143;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;&#22238;&#22797;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.07630</link><description>&lt;p&gt;
G-Retriever: &#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G-Retriever&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#29992;&#25143;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;&#22238;&#22797;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#20855;&#26377;&#25991;&#26412;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#23545;&#35805;&#30028;&#38754;&#21521;&#22270;&#24418;&#25552;&#20986;&#38382;&#39064;&#12290;&#38024;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#25991;&#26412;&#22238;&#22797;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20197;&#21508;&#31181;&#26041;&#24335;&#25972;&#21512;&#36215;&#26469;&#19981;&#21516;&#65292;&#23427;&#20204;&#22823;&#22810;&#38598;&#20013;&#22312;&#20256;&#32479;&#22270;&#20219;&#21153;(&#22914;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#20998;&#31867;)&#25110;&#32773;&#22312;&#23567;&#22411;&#25110;&#21512;&#25104;&#22270;&#19978;&#22238;&#31572;&#31616;&#21333;&#30340;&#22270;&#26597;&#35810;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#20026;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#22270;&#38382;&#39064;&#22238;&#31572;(GraphQA)&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;G-Retriever&#26041;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;GNN&#21644;LLM&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#21487;&#20197;&#22312;&#25932;&#20154;&#36873;&#25321;&#30340;&#20108;&#36827;&#21046;&#24207;&#21015;&#19978;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#36890;&#36807;&#36739;&#20302;&#30340;&#26631;&#23450;&#36317;&#31163;&#36827;&#34892;&#20934;&#30830;&#36817;&#20284;&#12290;</title><link>https://arxiv.org/abs/2402.07458</link><description>&lt;p&gt;
&#20851;&#20110;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#26631;&#23450;&#36317;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Distance from Calibration in Sequential Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#21487;&#20197;&#22312;&#25932;&#20154;&#36873;&#25321;&#30340;&#20108;&#36827;&#21046;&#24207;&#21015;&#19978;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#36890;&#36807;&#36739;&#20302;&#30340;&#26631;&#23450;&#36317;&#31163;&#36827;&#34892;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#39034;&#24207;&#20108;&#36827;&#21046;&#39044;&#27979;&#22330;&#26223;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#22120;&#30340;&#35780;&#20272;&#26159;&#20197;&#26631;&#23450;&#36317;&#31163;&#20026;&#22522;&#20934;&#30340;&#65292;&#26631;&#23450;&#36317;&#31163;&#23450;&#20041;&#20026;&#39044;&#27979;&#20540;&#19982;&#20107;&#21518;&#23436;&#20840;&#26631;&#23450;&#30340;&#39044;&#27979;&#38598;&#20043;&#38388;&#30340;$L_1$&#36317;&#31163;&#12290;&#36825;&#31867;&#20284;&#20110;&#26368;&#36817;&#30001;B{\l}asiok&#12289;Gopalan&#12289;Hu&#21644;Nakkiran&#65288;STOC 2023&#65289;&#25552;&#20986;&#30340;&#31163;&#32447;&#22330;&#26223;&#20013;&#30340;&#26631;&#23450;&#24230;&#37327;&#12290;&#26631;&#23450;&#36317;&#31163;&#26159;&#19968;&#31181;&#33258;&#28982;&#19988;&#30452;&#35266;&#30340;&#20559;&#31163;&#23436;&#32654;&#26631;&#23450;&#30340;&#24230;&#37327;&#65292;&#24182;&#19988;&#28385;&#36275;&#19981;&#21516;&#20110;&#35768;&#22810;&#24120;&#35265;&#30340;&#26631;&#23450;&#24230;&#37327;&#65288;&#22914;$L_1$&#26631;&#23450;&#35823;&#24046;&#21450;&#20854;&#21464;&#31181;&#65289;&#30340;Lipschitz&#36830;&#32493;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23545;&#25932;&#20154;&#36873;&#25321;&#30340;&#38271;&#24230;&#20026;$T$&#30340;&#20108;&#36827;&#21046;&#24207;&#21015;&#19978;&#65292;&#20197;&#26399;&#26395;$O(\sqrt{T})$&#30340;&#26631;&#23450;&#36317;&#31163;&#23454;&#29616;&#12290;&#22312;&#36825;&#20010;&#19978;&#30028;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32467;&#26500;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#26631;&#23450;&#36317;&#31163;&#21487;&#20197;&#36890;&#36807;&#36739;&#20302;&#30340;&#26631;&#23450;&#36317;&#31163;&#36827;&#34892;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. This is analogous to a calibration measure recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting. The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants.   We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes. At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;</title><link>https://arxiv.org/abs/2402.06855</link><description>&lt;p&gt;
&#26356;&#22909;&#36824;&#26159;&#26356;&#24046;&#65311;&#36890;&#36807;&#26631;&#31614;&#22686;&#24378;&#23398;&#20064;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
For Better or For Worse? Learning Minimum Variance Features With Label Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#25104;&#21151;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#31867;-&#21253;&#25324;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup-&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20837;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27492;&#31867;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#65288;&#21253;&#25324;&#26435;&#37325;&#34928;&#20943;&#65289;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#37325;&#35201;&#21518;&#26524;&#26159;&#28040;&#26497;&#30340;&#65306;&#19982;&#26631;&#20934;&#35757;&#32451;&#30456;&#27604;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06282</link><description>&lt;p&gt;
&#33719;&#21462;&#12289;&#21512;&#24182;&#12289;&#39044;&#27979;&#65306;&#36890;&#36807;&#25968;&#25454;&#28246;&#22686;&#24378;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
Retrieve, Merge, Predict: Augmenting Tables with Data Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#26159;&#32473;&#23450;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#26684;&#22686;&#24378;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26816;&#32034;&#21487;&#36830;&#25509;&#30340;&#34920;&#26684;&#12289;&#21512;&#24182;&#20449;&#24687;&#21644;&#39044;&#27979;&#32467;&#26524;&#34920;&#26684;&#12290;&#20316;&#20026;&#25968;&#25454;&#28246;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;YADL&#65288;&#21478;&#19968;&#20010;&#25968;&#25454;&#28246;&#65289;-&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#27492;&#25968;&#25454;&#21457;&#29616;&#20219;&#21153;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;-&#21644;Open Data US&#65292;&#19968;&#20010;&#34987;&#24341;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#28246;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#25968;&#25454;&#28246;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27010;&#36848;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#65292;&#26088;&#22312;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an in-depth analysis of data discovery in data lakes, focusing on table augmentation for given machine learning tasks. We analyze alternative methods used in the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. As data lakes, the paper uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for benchmarking this data discovery task -- and Open Data US, a well-referenced real data lake. Through systematic exploration on both lakes, our study outlines the importance of accurately retrieving join candidates and the efficiency of simple merging methods. We report new insights on the benefits of existing solutions and on their limitations, aiming at guiding future research in this space.
&lt;/p&gt;</description></item><item><title>Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences.</title><link>https://arxiv.org/abs/2402.05982</link><description>&lt;p&gt;
Anfinsen Goes Neural: &#19968;&#31181;&#29992;&#20110;&#26465;&#20214;&#25239;&#20307;&#35774;&#35745;&#30340;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05982
&lt;/p&gt;
&lt;p&gt;
Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#22312;&#25512;&#21160;&#27835;&#30103;&#23398;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23545;&#19968;&#33324;&#34507;&#30333;&#36136;&#30693;&#35782;&#30340;&#21033;&#29992;&#26377;&#38480;&#65292;&#24182;&#20551;&#35774;&#22270;&#27169;&#22411;&#36829;&#21453;&#34507;&#30333;&#36136;&#30340;&#32463;&#39564;&#21457;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Anfinsen Goes Neural (AGN)&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;(pLM)&#24182;&#32534;&#30721;&#20102;&#19968;&#31181;&#20851;&#20110;&#34507;&#30333;&#36136;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#21363;Anfinsen's dogma&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36981;&#24490;&#24207;&#21015;&#29983;&#25104;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#32467;&#26500;&#39044;&#27979;&#30340;&#20004;&#27493;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#23454;&#39564;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#36807;&#22810;&#37325;&#22797;&#26631;&#35760;&#30340;&#19981;&#29616;&#23454;&#24207;&#21015;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#32452;&#21512;&#30340;&#27491;&#21017;&#21270;&#39033;&#21040;&#20132;&#21449;&#29109;&#30446;&#26631;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody design plays a pivotal role in advancing therapeutics. Although deep learning has made rapid progress in this field, existing methods make limited use of general protein knowledge and assume a graphical model (GM) that violates empirical findings on proteins. To address these limitations, we present Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained protein language model (pLM) and encodes a seminal finding on proteins called Anfinsen's dogma. Our framework follows a two-step process of sequence generation with pLM and structure prediction with graph neural network (GNN). Experiments show that our approach outperforms state-of-the-art results on benchmark experiments. We also address a critical limitation of non-autoregressive models -- namely, that they tend to generate unrealistic sequences with overly repeating tokens. To resolve this, we introduce a composition-based regularization term to the cross-entropy objective that allows an efficient trade-off be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05961</link><description>&lt;p&gt;
&#22522;&#22240;&#24341;&#23548;GFlowNets&#65306;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GFlowNet&#21464;&#20307;&#65292;&#21363;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN)&#65292;&#23427;&#23558;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#38598;&#25104;&#21040;GFlowNet&#20013;&#12290;&#36951;&#20256;&#25628;&#32034;&#26377;&#25928;&#22320;&#24341;&#23548;GFlowNet&#36827;&#20837;&#39640;&#22238;&#25253;&#21306;&#22495;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#36807;&#24230;&#25506;&#32034;&#23548;&#33268;&#30340;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#21644;&#25506;&#32034;&#26377;&#38480;&#21306;&#22495;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#25918;&#35757;&#32451;&#21644;&#26080;&#30417;&#30563;&#26368;&#22823;&#20284;&#28982;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#22522;&#22240;&#24341;&#23548;GFlowNet&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270; (PMO) &#39046;&#22495;&#30340;&#23448;&#26041;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27979;&#35797;&#20013;&#25253;&#21578;&#30340;&#26368;&#20339;&#24471;&#20998;15.185&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;23&#20010;&#20219;&#21153;&#20013;&#30340;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;GFlowNets&#21644;&#36951;&#20256;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and unsupervised maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including reinforcement learning, Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2402.05626</link><description>&lt;p&gt;
&#27973;&#23618;ReLU-like&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65306;&#31283;&#23450;&#28857;&#12289;&#38797;&#28857;&#36867;&#36920;&#21644;&#32593;&#32476;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;&#30001;&#20110;&#28608;&#27963;&#20989;&#25968;&#26159;&#19981;&#21487;&#24494;&#30340;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22914;&#20309;&#23436;&#20840;&#25551;&#36848;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#21487;&#24494;&#21644;&#21487;&#24494;&#24773;&#20917;&#30340;&#31283;&#23450;&#28857;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#31283;&#23450;&#28857;&#19981;&#21253;&#21547;&#8220;&#36867;&#36920;&#31070;&#32463;&#20803;&#8221;&#65288;&#36890;&#36807;&#19968;&#38454;&#26465;&#20214;&#23450;&#20041;&#65289;&#65292;&#37027;&#20040;&#23427;&#24517;&#23450;&#26159;&#19968;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#37327;&#36755;&#20986;&#24773;&#20917;&#19979;&#65292;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#20445;&#35777;&#20102;&#31283;&#23450;&#28857;&#19981;&#26159;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#25551;&#36848;&#20102;&#20174;&#26080;&#31351;&#23567;&#65288;&#28040;&#22833;&#65289;&#21021;&#22987;&#21270;&#24320;&#22987;&#30340;&#27973;&#23618;ReLU-like&#32593;&#32476;&#30340;&#38797;&#28857;&#21040;&#38797;&#28857;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#30452;&#25509;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23436;&#20840;&#35752;&#35770;&#20102;&#32593;&#32476;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain "escape neurons", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network emb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05445</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;LLMs&#30340;LoRA-Finetuning&#37327;&#21270;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Accurate LoRA-Finetuning Quantization of LLMs via Information Retention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;LLMs&#30340;LoRA-finetuning&#37327;&#21270;&#30740;&#31350;&#24471;&#21040;&#20934;&#30830;&#20294;&#32039;&#20945;&#30340;LLMs&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23548;&#33268;&#37327;&#21270;&#30340;LLMs&#20005;&#37325;&#36864;&#21270;&#65292;&#29978;&#33267;&#26080;&#27861;&#20174;LoRA&#30340;&#35843;&#20248;&#20013;&#33719;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IR-QLoRA&#65292;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#24102;&#26377;LoRA&#30340;&#37327;&#21270;LLMs&#21464;&#24471;&#39640;&#24230;&#20934;&#30830;&#12290;&#25152;&#25552;&#20986;&#30340;IR-QLoRA&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#31181;&#20174;&#32479;&#19968;&#20449;&#24687;&#35270;&#35282;&#27966;&#29983;&#30340;&#25216;&#26415;&#65306;&#65288;1&#65289;&#22522;&#20110;&#32479;&#35745;&#30340;&#20449;&#24687;&#26657;&#20934;&#37327;&#21270;&#20801;&#35768;LLMs&#30340;&#37327;&#21270;&#21442;&#25968;&#31934;&#30830;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#65307;&#65288;2&#65289;&#22522;&#20110;&#35843;&#20248;&#30340;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#20351;LoRA&#21033;&#29992;&#20855;&#26377;&#22810;&#26679;&#20449;&#24687;&#30340;&#24377;&#24615;&#34920;&#31034;&#36716;&#25442;&#12290;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;2-4&#20301;&#23485;&#19979;&#65292;IR-QLoRA&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLaMA&#21644;LLaMA2&#31995;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#65292;4&#20301;LLaMA-7B&#30456;&#27604;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>https://arxiv.org/abs/2402.05098</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20934;&#27979;&#35797;&#21644;&#25913;&#36827;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#30340;&#38750;&#26631;&#20934;&#21270;&#23494;&#24230;&#25110;&#33021;&#37327;&#20989;&#25968;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#25193;&#25955;&#32467;&#26500;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#22522;&#20110;&#27169;&#25311;&#30340;&#21464;&#20998;&#26041;&#27861;&#21644;&#31163;&#31574;&#30053;&#26041;&#27861;&#65288;&#36830;&#32493;&#29983;&#25104;&#27969;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#30456;&#23545;&#20248;&#21183;&#65292;&#21516;&#26102;&#23545;&#36807;&#21435;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#36136;&#30097;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#25506;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#25628;&#32034;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#25913;&#21892;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#22312;https://github.com/GFNOrg/gfn-diffusion&#65292;&#20316;&#20026;&#26410;&#26469;&#22312;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#19978;&#24037;&#20316;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04114</link><description>&lt;p&gt;
SCAFFLSA&#65306;&#37327;&#21270;&#21644;&#28040;&#38500;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#21644;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65288;FedLSA&#65289;&#36827;&#34892;&#20102;&#38750;&#28176;&#36827;&#20998;&#26512;&#12290;&#25105;&#20204;&#26126;&#30830;&#37327;&#21270;&#20102;&#24322;&#36136;&#20195;&#29702;&#26412;&#22320;&#35757;&#32451;&#24341;&#20837;&#30340;&#20559;&#24046;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FedLSA&#30340;&#36890;&#20449;&#22797;&#26434;&#24615;&#19982;&#25152;&#38656;&#31934;&#24230; $\epsilon$ &#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#36825;&#38480;&#21046;&#20102;&#32852;&#37030;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCAFFLSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;FedLSA&#21464;&#20307;&#65292;&#23427;&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#26469;&#26657;&#27491;&#26412;&#22320;&#35757;&#32451;&#30340;&#20559;&#24046;&#65292;&#24182;&#22312;&#19981;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#24212;&#30340;&#22797;&#26434;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.03923</link><description>&lt;p&gt;
&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Return-Aligned Decision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65288;&#21363;&#22238;&#25253;&#65289;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24212;&#29992;&#33539;&#22260;&#30340;&#25193;&#22823;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#20351;&#23454;&#38469;&#22238;&#25253;&#19982;&#25351;&#23450;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#30340;&#26234;&#33021;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20174;&#32780;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#20915;&#31574;Transformer&#65288;DT&#65289;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#20248;&#21270;&#29983;&#25104;&#20197;&#30446;&#26631;&#22238;&#25253;&#20026;&#26465;&#20214;&#30340;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#24182;&#37197;&#22791;&#20102;&#20351;&#29992;&#30446;&#26631;&#22238;&#25253;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;DT&#26088;&#22312;&#23545;&#40784;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#65292;&#20294;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#20102;DT&#20013;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22238;&#25253;&#20174;&#20256;&#32479;&#30340;&#36755;&#20837;&#24207;&#21015;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#21253;&#21547;&#22238;&#25253;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25506;&#32034;&#26410;&#30693;&#30340;&#21363;&#26102;&#22870;&#21169;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03771</link><description>&lt;p&gt;
&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#20363;&#32423;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03771
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25506;&#32034;&#26410;&#30693;&#30340;&#21363;&#26102;&#22870;&#21169;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#21160;&#20316;&#30340;&#21363;&#26102;&#22870;&#21169;&#20449;&#21495;&#23558;&#20026;&#20195;&#29702;&#29983;&#25104;&#65292;&#20197;&#20415;&#20195;&#29702;&#23398;&#20064;&#22914;&#20309;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#20197;&#33719;&#21462;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#26080;&#27861;&#33719;&#21462;&#21363;&#26102;&#22870;&#21169;&#20449;&#21495;&#12290;&#30456;&#21453;&#65292;&#23398;&#20064;&#22120;&#21482;&#22312;&#36335;&#24452;&#30340;&#32467;&#26463;&#22788;&#33719;&#21462;&#22870;&#21169;&#65292;&#20854;&#20013;&#36335;&#24452;&#30340;&#37096;&#20998;&#24207;&#21015;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#22120;&#24517;&#39035;&#38754;&#23545;&#25506;&#32034;&#21253;&#20013;&#26410;&#30693;&#21363;&#26102;&#22870;&#21169;&#30340;&#26174;&#33879;&#22256;&#38590;&#65292;&#36825;&#19981;&#33021;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#35299;&#20915;&#65292;&#21253;&#25324;&#20165;&#32771;&#34385;&#23436;&#25972;&#36335;&#24452;&#24182;&#24573;&#30053;&#20869;&#37096;&#22870;&#21169;&#20998;&#24067;&#30340;&#36712;&#36857;&#26041;&#27861;&#12290;&#20026;&#20102;&#27491;&#24335;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#31216;&#20026;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;Reinforcement Learning from Bagged Rewards&#65292;RLBR&#65289;&#65292;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;RLBR&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#24847;&#31934;&#24230;&#19979;&#36924;&#36817;Lipschitz&#20989;&#25968;&#65292;&#22312;&#21442;&#25968;&#37327;&#21644;&#21069;&#21521;&#20256;&#25773;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03460</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31361;&#30772;&#32500;&#24230;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
Breaking the Curse of Dimensionality with Distributed Neural Computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03460
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#24847;&#31934;&#24230;&#19979;&#36924;&#36817;Lipschitz&#20989;&#25968;&#65292;&#22312;&#21442;&#25968;&#37327;&#21644;&#21069;&#21521;&#20256;&#25773;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31639;&#27861;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#8220;&#31070;&#32463;&#36884;&#24452;&#8221;&#65292;&#21487;&#20197;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#23454;&#29616;&#20219;&#24847;&#31934;&#24230;&#65292;&#21516;&#26102;&#20165;&#21152;&#36733;&#23569;&#37327;&#21442;&#25968;&#21040;GPU VRAM&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27599;&#20010;&#35823;&#24046;&#27700;&#24179;$\varepsilon&gt;0$&#21644;&#27599;&#20010;Lipschitz&#20989;&#25968;$f:[0,1]^n\to \mathbb{R}$&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#36884;&#24452;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;$[0,1]^n$&#19978;&#20197;$\varepsilon$&#31934;&#24230;&#22343;&#21248;&#36924;&#36817;$f$&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#22312;&#20869;&#23384;&#20013;&#21152;&#36733;$\mathcal{O}(\varepsilon^{-1})$&#20010;&#32593;&#32476;&#21442;&#25968;&#20197;&#21450;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#21152;&#36733;$\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$&#20010;&#32593;&#32476;&#21442;&#25968;&#12290;&#36825;&#25913;&#36827;&#20102;&#20256;&#32479;&#38750;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#21363;ReLU&#22810;&#23618;&#24863;&#30693;&#26426;&#65289;&#30340;&#26368;&#20248;&#30028;&#38480;&#65292;&#21518;&#32773;&#38656;&#35201;$\mathcal{O}(\varepsilon^{-n/2})$&#20010;&#21442;&#25968;&#26469;&#36798;&#21040;&#30456;&#21516;&#30340;&#31934;&#24230;&#12290;&#30446;&#21069;&#21807;&#19968;&#30340;&#20854;&#20182;&#21487;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\varepsilon&gt;0$ and every Lipschitz function $f:[0,1]^n\to \mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\mathcal{O}(\varepsilon^{-1})$ parameters to be loaded in memory and $\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\mathcal{O}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03312</link><description>&lt;p&gt;
&#28145;&#24230;&#34917;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Depth Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#22312;&#19968;&#20123;&#65288;&#28304;&#65289;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#30446;&#26631;&#27979;&#35797;&#25968;&#25454;&#26102;&#65292;&#24120;&#24120;&#20250;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#24046;&#36317;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22914;&#39046;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#21487;&#33021;&#38656;&#35201;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#28304;&#25968;&#25454;&#65288;&#36890;&#24120;&#19981;&#21487;&#29992;&#65289;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;&#26080;&#28304;DA&#65292;&#21017;&#38656;&#35201;&#22810;&#27425;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#65292;&#21363;&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#31232;&#30095;&#28145;&#24230;&#22270;&#25512;&#26029;&#20986;&#23494;&#38598;&#28145;&#24230;&#22270;&#30340;&#20219;&#21153;&#65292;&#20197;&#22312;&#19968;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#27599;&#31181;&#25968;&#25454;&#27169;&#24577;&#20013;&#30340;&#39046;&#22495;&#36716;&#31227;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#31232;&#30095;&#28145;&#24230;&#27169;&#24577;&#23637;&#29616;&#20986;&#27604;&#22270;&#20687;&#26356;&#23567;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#28304;&#39046;&#22495;&#20013;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22359;&#65292;&#23427;&#20445;&#30041;&#20102;&#20174;&#20165;&#32534;&#30721;&#31232;&#30095;&#28145;&#24230;&#29305;&#24449;&#21040;&#32534;&#30721;&#22270;&#20687;&#21644;&#31232;&#30095;&#28145;&#24230;&#30340;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#23884;&#20837;&#27169;&#22359;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.03282</link><description>&lt;p&gt;
&#19968;&#20010;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#22312;RLHF&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Partially Observed Reward-States in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#30740;&#31350;&#22240;&#20854;&#22312;LLMs&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#21050;&#28608;&#30340;&#21453;&#24212;&#24050;&#30693;&#20381;&#36182;&#20110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#8220;&#20869;&#37096;&#29366;&#24577;&#8221;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#20013;&#38388;&#21453;&#39304;&#65292;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#23558;RLHF&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PORRL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;RLHF&#20013;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#30340;&#20154;&#31867;&#21453;&#39304; - &#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#21040;PORRL&#30340;&#32553;&#20943;&#12290;&#23545;&#20110;&#22522;&#25968;&#21453;&#39304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30340;&#32479;&#35745;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#23454;&#20363;&#21270;&#20026;POR-UCRL&#21644;POR-UCBVI&#12290;&#23545;&#20110;&#20915;&#26007;&#21453;&#39304;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#25968;&#21453;&#39304;&#32553;&#20943;&#19981;&#33021;&#36798;&#21040;&#20122;&#32447;&#24615;&#30340;&#20915;&#26007;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;</title><link>https://arxiv.org/abs/2402.03201</link><description>&lt;p&gt;
&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#36827;&#34892;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guidance with Spherical Gaussian Constraint for Conditional Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#21487;&#24494;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#25351;&#23548;&#26469;&#22788;&#29702;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#26679;&#26412;&#36136;&#37327;&#19978;&#20570;&#20986;&#22949;&#21327;&#65292;&#24182;&#38656;&#35201;&#36739;&#23567;&#30340;&#24341;&#23548;&#27493;&#38271;&#65292;&#23548;&#33268;&#37319;&#26679;&#36807;&#31243;&#21464;&#38271;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22312;&#24341;&#23548;&#25439;&#22833;&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#27969;&#24418;&#20559;&#31163;&#30340;&#26681;&#26412;&#38382;&#39064;&#25152;&#22312;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#25439;&#22833;&#24341;&#23548;&#30340;&#20272;&#35745;&#35823;&#24046;&#30340;&#29305;&#23450;&#19979;&#30028;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27969;&#24418;&#20559;&#31163;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#65288;DSG&#65289;&#30340;&#25193;&#25955;&#65292;&#20174;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#38598;&#20013;&#29616;&#35937;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;DSG&#36890;&#36807;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#24341;&#23548;&#27493;&#39588;&#32422;&#26463;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form 
&lt;/p&gt;</description></item><item><title>&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03191</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#65292;&#32858;&#31867;&#21644;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Isotropy, Clusters, and Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03191
&lt;/p&gt;
&lt;p&gt;
&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23884;&#20837;&#31354;&#38388;&#26159;&#21542;&#22343;&#21248;&#21033;&#29992;&#25152;&#26377;&#32500;&#24230;&#65288;&#21363;&#26159;&#21542;&#20855;&#26377;&#21516;&#24615;&#36136;&#65289;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#35752;&#35770;&#12290;&#26377;&#35777;&#25454;&#25903;&#25345;&#21644;&#21453;&#23545;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#26045;&#21516;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#21516;&#24615;&#36136;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#35201;&#27714;&#19982;&#32858;&#31867;&#30340;&#23384;&#22312;&#19981;&#20860;&#23481;&#65292;&#36825;&#20063;&#23545;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#20107;&#23454;&#65292;&#24182;&#29992;&#23427;&#26469;&#38416;&#26126;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03008</link><description>&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusive Gibbs Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03008
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#28151;&#21512;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#65288;DiGS&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#37319;&#26679;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;DiGS&#38598;&#25104;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21033;&#29992;&#39640;&#26031;&#21367;&#31215;&#21019;&#24314;&#19968;&#20010;&#36741;&#21161;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#36830;&#25509;&#23396;&#31435;&#30340;&#27169;&#24577;&#65292;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#20174;&#20004;&#20010;&#31354;&#38388;&#20013;&#20132;&#26367;&#25277;&#21462;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37319;&#26679;&#22810;&#27169;&#24577;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#24182;&#34892;&#28201;&#24230;&#27861;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35270;&#35273;&#29702;&#35299;&#22522;&#30784;&#27169;&#22411;&#22312;&#36947;&#36335;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#21508;&#31181;&#39550;&#39542;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#65292;&#20026;&#23454;&#29616;&#23545;&#21608;&#22260;&#22330;&#26223;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.02968</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#29702;&#35299;&#65306;&#20174;&#23398;&#20064;&#35270;&#35282;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35270;&#35273;&#29702;&#35299;&#22522;&#30784;&#27169;&#22411;&#22312;&#36947;&#36335;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#21508;&#31181;&#39550;&#39542;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#65292;&#20026;&#23454;&#29616;&#23545;&#21608;&#22260;&#22330;&#26223;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#30830;&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#25104;&#20026;&#26174;&#33879;&#22609;&#36896;&#26234;&#33021;&#31995;&#32479;&#33021;&#21147;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#22312;&#26234;&#33021;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21147;&#37327;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#21464;&#38761;&#24615;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#35270;&#35273;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35270;&#35273;&#29702;&#35299;&#22522;&#30784;&#27169;&#22411;(MM-VUFMs)&#20855;&#22791;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#24182;&#21516;&#26102;&#22788;&#29702;&#21508;&#31181;&#19982;&#39550;&#39542;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#65292;&#20026;&#23545;&#21608;&#22260;&#22330;&#26223;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36947;&#36335;&#22330;&#26223;&#30340;MM-VUFMs&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#20165;&#26159;&#25552;&#20379;&#23545;&#24120;&#35265;&#23454;&#36341;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#28041;&#21450;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12289;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12289;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#20419;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;Koopman&#31639;&#23376;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#36229;&#26799;&#24230;&#30340;&#36712;&#36857;&#26469;&#39640;&#25928;&#22320;&#36817;&#20284;&#20840;&#23616;&#36229;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#36229;&#21442;&#25968;&#30340;&#36138;&#23146;&#20248;&#21270;&#65292;&#20860;&#20855;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02741</link><description>&lt;p&gt;
&#20855;&#26377;Koopman&#31639;&#23376;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Glocal Hypergradient Estimation with Koopman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;Koopman&#31639;&#23376;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#36229;&#26799;&#24230;&#30340;&#36712;&#36857;&#26469;&#39640;&#25928;&#22320;&#36817;&#20284;&#20840;&#23616;&#36229;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#36229;&#21442;&#25968;&#30340;&#36138;&#23146;&#20248;&#21270;&#65292;&#20860;&#20855;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#20351;&#29992;&#36229;&#26799;&#24230;&#26469;&#26356;&#26032;&#36229;&#21442;&#25968;&#65292;&#21363;&#20803;&#26631;&#20934;&#30340;&#26799;&#24230;&#19982;&#36229;&#21442;&#25968;&#30340;&#20851;&#31995;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26356;&#26032;&#31574;&#30053;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992;&#27169;&#22411;&#35757;&#32451;&#23436;&#25104;&#21518;&#24471;&#21040;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#26469;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#20043;&#21518;&#24471;&#21040;&#30340;&#23616;&#37096;&#36229;&#26799;&#24230;&#12290;&#34429;&#28982;&#20840;&#23616;&#36229;&#26799;&#24230;&#20855;&#26377;&#21487;&#38752;&#24615;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#26174;&#33879;&#65307;&#30456;&#21453;&#65292;&#23616;&#37096;&#36229;&#26799;&#24230;&#36895;&#24230;&#24555;&#20294;&#24120;&#24120;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;glocal&#36229;&#26799;&#24230;&#20272;&#35745;&#65292;&#23558;&#8220;&#20840;&#23616;&#8221;&#30340;&#36136;&#37327;&#19982;&#8220;&#23616;&#37096;&#8221;&#30340;&#25928;&#29575;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;Koopman&#31639;&#23376;&#29702;&#35770;&#26469;&#32447;&#24615;&#21270;&#36229;&#26799;&#24230;&#30340;&#21160;&#24577;&#65292;&#20197;&#20415;&#21487;&#20197;&#20165;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#36229;&#26799;&#24230;&#30340;&#36712;&#36857;&#26469;&#39640;&#25928;&#22320;&#36817;&#20284;&#20840;&#23616;&#36229;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20272;&#35745;&#30340;&#20840;&#23616;&#36229;&#26799;&#24230;&#36138;&#23146;&#22320;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending "global" quality with "local" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02672</link><description>&lt;p&gt;
&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65306;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23545;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATEs&#65289;&#30340;&#20272;&#35745;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#22914;&#26524;&#20998;&#24067;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#21487;&#20197;&#38598;&#20013;&#65292;&#21487;&#20197;&#23545;CATEs&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#65292;&#21017;&#24456;&#38590;&#36827;&#34892;&#25968;&#25454;&#32858;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;CATE&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#24635;&#32467;&#22914;&#19979;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#36827;&#34892;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#12290;&#21322;&#21442;&#25968;&#25110;&#38750;&#21442;&#25968;&#30340;CATE&#27169;&#22411;&#33021;&#22815;&#27604;&#21442;&#25968;&#27169;&#22411;&#26356;&#31283;&#20581;&#22320;&#36827;&#34892;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#23545;&#20110;&#27169;&#22411;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#26356;&#24378;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#25552;&#20986;&#26377;&#25928;&#30340;&#36890;&#20449;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02644</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#30340;&#26041;&#27861;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Variational DAG Estimation via State Augmentation With Stochastic Permutations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02644
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#21363;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#65292;&#26159;&#19968;&#20010;&#22312;&#32479;&#35745;&#21644;&#35745;&#31639;&#19978;&#37117;&#24456;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#31561;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#24212;&#29992;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#26041;&#38754;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#22788;&#29702;&#20247;&#25152;&#21608;&#30693;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#12290;&#20174;&#27010;&#29575;&#25512;&#26029;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#65288;i&#65289;&#34920;&#31034;&#28385;&#36275;DAG&#32422;&#26463;&#30340;&#22270;&#30340;&#20998;&#24067;&#21644;&#65288;ii&#65289;&#20272;&#35745;&#24213;&#23618;&#32452;&#21512;&#31354;&#38388;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;DAG&#21644;&#25490;&#21015;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26500;&#24314;&#32852;&#21512;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#21518;&#39564;&#20272;&#35745;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;&#31163;&#25955;&#20998;&#24067;&#30340;&#36830;&#32493;&#26494;&#24347;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#33021;&#22815;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and re
&lt;/p&gt;</description></item><item><title>PoCo&#26159;&#19968;&#31181;&#31574;&#30053;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#20219;&#21153;&#32423;&#32452;&#21512;&#21644;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.02511</link><description>&lt;p&gt;
PoCo: &#26469;&#33258;&#21644;&#20026;&#24322;&#26500;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#31574;&#30053;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
PoCo: Policy Composition from and for Heterogeneous Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02511
&lt;/p&gt;
&lt;p&gt;
PoCo&#26159;&#19968;&#31181;&#31574;&#30053;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#20219;&#21153;&#32423;&#32452;&#21512;&#21644;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#20197;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#22312;&#39068;&#33394;&#12289;&#28145;&#24230;&#12289;&#35302;&#35273;&#21644;&#23039;&#24577;&#24863;&#31561;&#19981;&#21516;&#27169;&#24577;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#22312;&#27169;&#25311;&#12289;&#30495;&#23454;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#35270;&#39057;&#31561;&#19981;&#21516;&#39046;&#22495;&#25910;&#38598;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#25910;&#38598;&#24182;&#27719;&#38598;&#19968;&#20010;&#39046;&#22495;&#30340;&#25152;&#26377;&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#31574;&#30053;&#26469;&#22788;&#29702;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#24322;&#26500;&#24615;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#21644;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#31574;&#30053;&#32452;&#21512;&#65292;&#36890;&#36807;&#32452;&#21512;&#29992;&#25193;&#25955;&#27169;&#22411;&#34920;&#31034;&#30340;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#65292;&#23558;&#36328;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23398;&#20064;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#25805;&#20316;&#20013;&#20351;&#29992;&#20219;&#21153;&#32423;&#32452;&#21512;&#65292;&#24182;&#19982;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#32452;&#21512;&#65292;&#20197;&#22312;&#25512;&#29702;&#26102;&#35843;&#25972;&#31574;&#30053;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#12289;&#20154;&#31867;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35270;&#35273;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02382</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35270;&#35273;&#35843;&#25972;&#20013;&#25552;&#31034;&#35789;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Power of Prompt for Visual Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35270;&#35273;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#26159;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#35789;&#26469;&#23450;&#21046;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;VPT&#21450;&#20854;&#21464;&#31181;&#32463;&#24120;&#36935;&#21040;&#35832;&#22914;&#25552;&#31034;&#21021;&#22987;&#21270;&#12289;&#25552;&#31034;&#38271;&#24230;&#21644;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#24615;&#33021;&#19981;&#20339;&#31561;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#25104;&#21151;&#30340;&#19978;&#19979;&#25991;&#36866;&#24212;&#12290;&#26412;&#30740;&#31350;&#20174;&#25506;&#32034;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#31034;&#35789;&#19982;&#34917;&#19969;&#20196;&#29260;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#28436;&#21464;&#24320;&#22987;&#12290;&#21463;&#21040;&#25552;&#31034;&#20196;&#29260;&#19982;&#34917;&#19969;&#20196;&#29260;&#20043;&#38388;&#24448;&#24448;&#20855;&#26377;&#39640;&#20114;&#20449;&#24687;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#35813;&#31574;&#30053;&#24615;&#21021;&#22987;&#21270;&#26126;&#26174;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;VPT&#65292;&#26080;&#38656;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#27969;&#31243;&#20445;&#25345;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#35814;&#23613;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02325</link><description>&lt;p&gt;
&#21160;&#37327;&#22312;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#20013;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#20316;&#29992;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23545;&#27492;&#32570;&#20047;&#29702;&#35770;&#35299;&#37322;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;SGD&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#31243;&#24230;&#30001;&#23398;&#20064;&#29575;&#12289;&#25209;&#22823;&#23567;&#12289;&#21160;&#37327;&#22240;&#23376;&#12289;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#21450;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#30830;&#23450;&#12290;&#36825;&#19968;&#29702;&#35770;&#21457;&#29616;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#21160;&#37327;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21160;&#37327;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#20316;&#29992;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SGD&#21160;&#37327;&#24179;&#28369;&#29305;&#24615;&#30340;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;SGD&#21160;&#37327;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.
&lt;/p&gt;</description></item><item><title>INViT&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#23884;&#22871;&#35774;&#35745;&#21644;&#19981;&#21464;&#30340;&#35270;&#22270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#25552;&#39640;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02317</link><description>&lt;p&gt;
INViT:&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#21487;&#27867;&#21270;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02317
&lt;/p&gt;
&lt;p&gt;
INViT&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#23884;&#22871;&#35774;&#35745;&#21644;&#19981;&#21464;&#30340;&#35270;&#22270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#25552;&#39640;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23398;&#20064;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#24555;&#36895;&#21551;&#21457;&#24335;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22810;&#25968;&#27714;&#35299;&#22120;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#25110;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#30340;&#20998;&#24067;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#65288;INViT&#65289;&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#24378;&#21046;&#19968;&#20010;&#23884;&#22871;&#35774;&#35745;&#20197;&#21450;&#19981;&#21464;&#30340;&#35270;&#22270;&#26469;&#20419;&#36827;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#24212;&#29992;&#20102;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;INViT&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;(CTRA)&#65292;&#29992;&#20110;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;CTRA&#36890;&#36807;&#23545;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#35299;&#20915;&#20102;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02190</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;(CTRA)&#65292;&#29992;&#20110;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;CTRA&#36890;&#36807;&#23545;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#35299;&#20915;&#20102;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#23547;&#25214;&#26368;&#20339;&#35299;&#26159;&#26368;&#24120;&#35265;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#30446;&#26631;&#20989;&#25968;&#21644;&#32422;&#26463;&#26465;&#20214;&#21482;&#26159;&#21407;&#22987;&#29616;&#23454;&#19990;&#30028;&#24773;&#20917;&#30340;&#36817;&#20284;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23547;&#25214;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#21644;&#32422;&#26463;&#20005;&#37325;&#24615;&#30340;&#21464;&#21270;&#25104;&#20026;&#33258;&#28982;&#30340;&#26041;&#21521;&#12290;&#36825;&#31181;&#31574;&#30053;&#25552;&#20379;&#20102;&#22312;&#21518;&#22788;&#29702;&#36807;&#31243;&#20013;&#36873;&#25321;&#21512;&#36866;&#35299;&#20915;&#26041;&#26696;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#36825;&#20123;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#27604;&#30830;&#23450;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#36830;&#32493;&#24352;&#37327;&#26494;&#24347;&#36864;&#28779; (CTRA) &#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;CTRA&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#26494;&#24347;&#26041;&#27861;&#65292;&#23558;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36716;&#25442;&#20026;&#36830;&#32493;&#24352;&#37327;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22810;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#25214;&#21040;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#21644;&#32422;&#26463;&#20005;&#37325;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the best solution is the most common objective in combinatorial optimization (CO) problems. However, a single solution may not be suitable in practical scenarios, as the objective functions and constraints are only approximations of original real-world situations. To tackle this, finding (i) "heterogeneous solutions", diverse solutions with distinct characteristics, and (ii) "penalty-diversified solutions", variations in constraint severity, are natural directions. This strategy provides the flexibility to select a suitable solution during post-processing. However, discovering these diverse solutions is more challenging than identifying a single solution. To overcome this challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning-based CO solvers. CTRA addresses various problems simultaneously by extending the continual relaxation approach, which transforms discrete decision variables into continual tensors. This method finds heterog
&lt;/p&gt;</description></item><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.17177</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Discovery of PDEs via the Adjoint Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#26469;&#21457;&#29616;&#32473;&#23450;&#25968;&#25454;&#30340;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20197;&#19968;&#33324;&#24418;&#24335;&#32771;&#34385;&#21442;&#25968;&#21270;&#30340;PDE&#65292;&#24182;&#21046;&#23450;&#26368;&#23567;&#21270;PDE&#35299;&#19982;&#25968;&#25454;&#35823;&#24046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#21464;&#20998;&#35745;&#31639;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65288;&#20276;&#38543;&#26041;&#31243;&#65289;&#30340;&#28436;&#21270;&#26041;&#31243;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#20986;&#19982;PDE&#21442;&#25968;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#26063;&#21442;&#25968;&#21270;&#21644;&#38750;&#32447;&#24615;PDEs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20276;&#38543;&#26041;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#20276;&#38543;&#26041;&#27861;&#21487;&#20197;&#20197;&#26426;&#22120;&#31934;&#24230;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20276;&#38543;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;&#33879;&#21517;&#30340;PDE-FIND&#65288;Rudy et al., 2017&#65289;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived. Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy. However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017). Even th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKSeed&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#21644;&#26377;&#38480;&#30340;&#38543;&#26426;&#31181;&#23376;&#38598;&#21512;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25104;&#26412;&#36739;&#20302;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.06353</link><description>&lt;p&gt;
&#20351;&#29992;&#36890;&#20449;&#25104;&#26412;&#20302;&#20110;18&#21315;&#23383;&#33410;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKSeed&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#21644;&#26377;&#38480;&#30340;&#38543;&#26426;&#31181;&#23376;&#38598;&#21512;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25104;&#26412;&#36739;&#20302;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38656;&#35201;&#36890;&#36807;&#32454;&#21270;&#35843;&#25972;&#26469;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#19981;&#29306;&#29298;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#32456;&#31471;&#35774;&#22791;&#19978;&#20016;&#23500;&#30340;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#32454;&#21270;&#35843;&#25972;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;LLM&#32852;&#37030;&#32454;&#21270;&#35843;&#25972;&#26041;&#27861;&#20381;&#36182;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#32454;&#21270;&#35843;&#25972;&#25216;&#26415;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#20840;&#21442;&#25968;&#35843;&#25972;&#21487;&#33021;&#36798;&#21040;&#30340;&#24615;&#33021;&#39640;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;LLM&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;FedKSeed&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#31181;&#23376;&#30340;&#26377;&#38480;&#38598;&#21512;&#36827;&#34892;&#38646;&#38454;&#20248;&#21270;&#12290;&#23427;&#26174;&#33879;&#38477;&#20302;&#20102;&#26381;&#21153;&#22120;&#21644;&#32456;&#31471;&#20043;&#38388;&#30340;&#20256;&#36755;&#35201;&#27714;&#65292;&#20165;&#38656;&#20256;&#36755;&#20960;&#20010;&#38543;&#26426;&#31181;&#23376;&#21644;&#26631;&#37327;&#26799;&#24230;&#65292;&#20165;&#21344;&#29992;&#20960;&#21315;&#23383;&#33410;&#30340;&#31354;&#38388;&#65292;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#33021;&#22815;&#36827;&#34892;&#20159;&#32423;LLM&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#27010;&#29575;&#24046;&#24322;&#21270;&#31181;&#23376;&#37319;&#26679;&#65292;&#20248;&#20808;&#32771;&#34385;&#19968;&#20123;&#31181;&#23376;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioriti
&lt;/p&gt;</description></item><item><title>SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.18376</link><description>&lt;p&gt;
SQLformer&#65306;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18376
&lt;/p&gt;
&lt;p&gt;
SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#25552;&#21462;&#27665;&#20027;&#21270;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#20027;&#35201;&#38556;&#30861;&#21253;&#25324;&#39046;&#22495;&#27867;&#21270;&#65292;&#21363;&#36866;&#24212;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25968;&#25454;&#24211;&#65292;&#24182;&#19988;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#30456;&#24212;&#30340;SQL&#26597;&#35810;&#23545;&#40784;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SQLformer&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25191;&#34892;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#39044;&#27979;SQL&#26597;&#35810;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23618;&#20013;&#32467;&#21512;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#24211;&#34920;&#21644;&#21015;&#36873;&#25321;&#24341;&#23548;&#30340;&#65292;&#26377;&#21161;&#20110;&#35299;&#30721;&#22120;&#20197;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#35268;&#33539;&#39034;&#24207;&#29983;&#25104;SQL&#26597;&#35810;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#29616;&#38454;&#27573;&#30340;&#25216;&#26415;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.02124</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#21327;&#20316;&#26426;&#21046;&#65306;&#31038;&#20250;&#24515;&#29702;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#31038;&#20250;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#20123;NLP&#31995;&#32479;&#33021;&#21542;&#27169;&#20223;&#31867;&#20154;&#31867;&#30340;&#21327;&#20316;&#26234;&#33021;&#65292;&#22312;&#30001;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#22810;&#20195;&#29702;&#31038;&#20250;&#20013;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#23454;&#36341;&#23454;&#39564;&#19982;&#29702;&#35770;&#35266;&#28857;&#30456;&#32467;&#21512;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#30001;LLM&#20195;&#29702;&#32452;&#25104;&#30340;&#29420;&#29305;&#8220;&#31038;&#20250;&#8221;&#65292;&#27599;&#20010;&#20195;&#29702;&#20197;&#29305;&#23450;&#30340;&#8220;&#29305;&#36136;&#8221;&#65288;&#38543;&#21644;&#25110;&#36807;&#20110;&#33258;&#20449;&#65289;&#20026;&#29305;&#24449;&#65292;&#24182;&#19982;&#19981;&#21516;&#30340;&#8220;&#24605;&#32500;&#27169;&#24335;&#8221;&#65288;&#36777;&#35770;&#25110;&#21453;&#24605;&#65289;&#23637;&#24320;&#21327;&#20316;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#22810;&#20195;&#29702;&#31038;&#20250;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#19981;&#20165;&#32988;&#36807;&#20808;&#21069;&#39030;&#23574;&#26041;&#27861;&#65292;&#32780;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;API&#20196;&#29260;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35828;&#26126;LLM&#20195;&#29702;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
&lt;/p&gt;</description></item><item><title>Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16452</link><description>&lt;p&gt;
Context-Former&#65306;&#22522;&#20110;&#28508;&#22312;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#25340;&#25509;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16452
&lt;/p&gt;
&lt;p&gt;
Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#20351;RL&#33021;&#22815;&#23398;&#20064;&#20248;&#20110;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Decision Transformer&#65288;DT&#65289;&#23558;&#20915;&#31574;&#24314;&#27169;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#22312;&#31163;&#32447;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;DT&#32570;&#20047;&#25340;&#25509;&#33021;&#21147;&#65292;&#22240;&#27492;&#25552;&#39640;DT&#24615;&#33021;&#38656;&#35201;&#21033;&#29992;&#25340;&#25509;&#33021;&#21147;&#12290;&#20026;&#20102;&#36171;&#20104;DT&#25340;&#25509;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36712;&#36857;&#25340;&#25509;&#25277;&#35937;&#20026;&#19987;&#23478;&#21305;&#37197;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;ContextFormer&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#34920;&#31034;&#26469;&#38598;&#25104;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#12290;&#36890;&#36807;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#26469;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.13544</link><description>&lt;p&gt;
&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#22914;&#20309;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#65311;
&lt;/p&gt;
&lt;p&gt;
Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?. (arXiv:2401.13544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#12290;&#36890;&#36807;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#26469;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#25506;&#32034;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#65292;&#21253;&#25324;&#20174;&#21407;&#22987;&#29305;&#24449;&#20013;&#36880;&#27493;&#39044;&#27979;&#39640;&#32423;&#27010;&#24565;&#21644;&#20174;&#39044;&#27979;&#30340;&#27010;&#24565;&#20013;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#12290;&#36825;&#20010;&#27169;&#22411;&#31867;&#21035;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#20248;&#21183;&#26159;&#29992;&#25143;&#33021;&#22815;&#23545;&#39044;&#27979;&#30340;&#27010;&#24565;&#20540;&#36827;&#34892;&#24178;&#39044;&#65292;&#20174;&#32780;&#24433;&#21709;&#27169;&#22411;&#30340;&#19979;&#28216;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#24050;&#32463;&#35757;&#32451;&#22909;&#20294;&#26412;&#36136;&#19978;&#19981;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#65292;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#39564;&#35777;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#21487;&#24178;&#39044;&#24615;&#23450;&#20041;&#20026;&#22522;&#20110;&#27010;&#24565;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#23450;&#20041;&#26469;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#21644;&#33258;&#28982;&#22270;&#20687;&#22522;&#20934;&#19978;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#24178;&#39044;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24494;&#35843;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#32463;&#24120;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the 
&lt;/p&gt;</description></item><item><title>SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.05821</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#27010;&#24565;&#29942;&#39048;&#29992;&#20110;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05821
&lt;/p&gt;
&lt;p&gt;
SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#38590;&#20197;&#24402;&#22240;&#30340;&#38382;&#39064;&#20197;&#21450;&#19981;&#23545;&#40784;&#31561;&#31561;&#37117;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#21442;&#19982;&#65292;&#36825;&#20123;&#19987;&#23478;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24182;&#32416;&#27491;&#38169;&#35823;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65288;SCoBots&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#30340;&#27010;&#24565;&#29942;&#39048;&#23618;&#65292;&#20351;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#36879;&#26126;&#21270;&#12290;SCoBots&#19981;&#20165;&#21033;&#29992;&#30456;&#20851;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#36824;&#21033;&#29992;&#20851;&#31995;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#24378;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;SCoBots&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#26377;&#25928;&#29702;&#35299;&#21644;&#35268;&#33539;&#20182;&#20204;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SCoBots&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#26368;&#31616;&#21333;&#19988;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;Pong&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#21152;&#20197;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.17972</link><description>&lt;p&gt;
&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Infilling Code Generation. (arXiv:2311.17972v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#23558;&#34917;&#20805;&#25805;&#20316;&#34701;&#20837;&#33258;&#22238;&#24402;&#35299;&#30721;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#33021;&#22815;&#36827;&#34892;&#22635;&#20805;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#22635;&#20805;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#34917;&#20805;&#25805;&#20316;&#26088;&#22312;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#21069;&#32512;&#21644;&#21518;&#32512;&#22635;&#20805;&#20013;&#38388;&#20869;&#23481;&#65292;&#32780;&#33258;&#34917;&#26426;&#21046;&#39034;&#24207;&#29983;&#25104;&#36825;&#20123;&#21608;&#22260;&#19978;&#19979;&#25991;&#21644;&#34987;&#22635;&#20805;&#20869;&#23481;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#22312;&#20256;&#32479;&#35299;&#30721;&#20013;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20854;&#36827;&#21270;&#20026;&#38750;&#21333;&#35843;&#36807;&#31243;&#12290;&#20013;&#26029;&#26426;&#21046;&#20801;&#35768;&#25512;&#36831;&#29983;&#25104;&#29305;&#23450;&#30340;&#20195;&#30721;&#65292;&#30452;&#21040;&#30830;&#23450;&#30340;&#21518;&#32512;&#24314;&#31435;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;&#24490;&#29615;&#26426;&#21046;&#21033;&#29992;&#33258;&#34917;&#21644;&#20174;&#24038;&#21040;&#21491;&#35299;&#30721;&#30340;&#20114;&#34917;&#24615;&#65292;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#27599;&#20010;&#29983;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.01953</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimistic Multi-Agent Policy Gradient for Cooperative Tasks. (arXiv:2311.01953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27425;&#20248;&#34892;&#20026;&#65292;&#23548;&#33268;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#27425;&#20248;&#32852;&#21512;&#31574;&#30053;&#65292;&#20986;&#29616;&#20102;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#65288;RO&#65289;&#38382;&#39064;&#12290;&#26089;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#20048;&#35266;&#20027;&#20041;&#21487;&#20197;&#32531;&#35299;&#20351;&#29992;&#34920;&#26684;&#21270;Q&#23398;&#20064;&#26102;&#30340;RO&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#26469;&#35828;&#65292;&#21033;&#29992;&#20989;&#25968;&#36924;&#36817;&#20048;&#35266;&#20027;&#20041;&#21487;&#33021;&#21152;&#21095;&#36807;&#20272;&#35745;&#65292;&#20174;&#32780;&#22833;&#36133;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#20005;&#37325;&#30340;RO&#24773;&#20917;&#19979;&#21487;&#33021;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;MAPG&#26041;&#27861;&#20013;&#23454;&#29616;&#20048;&#35266;&#26356;&#26032;&#24182;&#32531;&#35299;RO&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#65292;&#20854;&#20013;&#19968;&#20010;&#36229;&#21442;&#25968;&#36873;&#25321;&#20048;&#35266;&#31243;&#24230;&#20197;&#22312;&#26356;&#26032;&#31574;&#30053;&#26102;&#37325;&#26032;&#22609;&#36896;&#20248;&#21183;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21487;&#33021;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#22238;&#25253;&#36739;&#20302;&#30340;&#20010;&#21035;&#21160;&#20316;&#20445;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Relative overgeneralization} (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. In early work, optimism has been shown to mitigate the \textit{RO} problem when using tabular Q-learning. However, with function approximation optimism can amplify overestimation and thus fail on complex tasks. On the other hand, recent deep multi-agent policy gradient (MAPG) methods have succeeded in many complex tasks but may fail with severe \textit{RO}. We propose a general, yet simple, framework to enable optimistic updates in MAPG methods and alleviate the RO problem. Specifically, we employ a \textit{Leaky ReLU} function where a single hyperparameter selects the degree of optimism to reshape the advantages when updating the policy. Intuitively, our method remains optimistic toward individual actions with lower returns which are potentially caused by other agents' sub-optimal be
&lt;/p&gt;</description></item><item><title>&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.01412</link><description>&lt;p&gt;
Castor: &#22240;&#26524;&#26102;&#24207;&#21306;&#22495;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01412
&lt;/p&gt;
&lt;p&gt;
&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#28041;&#21450;&#21040;&#20174;&#27668;&#20505;&#31185;&#23398;&#21040;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#23398;&#31185;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#24120;&#36981;&#24490;&#22810;&#20010;&#20808;&#39564;&#26410;&#30693;&#30340;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21487;&#20197;&#20174;&#20855;&#26377;&#24050;&#30693;&#21306;&#22495;&#30340;&#24322;&#26500;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#20294;&#22312;&#20840;&#38754;&#23398;&#20064;&#21306;&#22495;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#22270;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CASTOR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#30001;&#19981;&#21516;&#22240;&#26524;&#22270;&#32479;&#27835;&#30340;&#21508;&#31181;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;EM&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;CASTOR&#25512;&#26029;&#20986;&#21306;&#22495;&#30340;&#25968;&#37327;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CASTOR&#30340;&#31283;&#20581;&#25910;&#25947;&#24615;&#36136;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its profic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00094</link><description>&lt;p&gt;
&#34920;&#36798;&#24314;&#27169;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19981;&#36275;&#65306;&#21487;&#22788;&#29702;&#30340;&#25512;&#29702;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective. (arXiv:2311.00094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#20363;&#26159;&#20808;&#23558;&#31163;&#32447;&#36712;&#36857;&#25311;&#21512;&#21040;&#19968;&#20010;&#24207;&#21015;&#27169;&#22411;&#20013;&#65292;&#28982;&#21518;&#36890;&#36807;&#35813;&#27169;&#22411;&#25552;&#31034;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#26222;&#36941;&#35748;&#20026;&#34920;&#36798;&#24615;&#26356;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#20102;&#21487;&#22788;&#29702;&#24615;&#65292;&#21363;&#31934;&#30830;&#32780;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#21516;&#26679;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#24102;&#26469;&#30340;&#22522;&#26412;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#39640;&#24230;&#38750;&#24179;&#20961;&#30340;&#26465;&#20214;/&#32422;&#26463;&#29983;&#25104;&#65292;&#20197;&#24341;&#20986;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#20173;&#28982;&#21487;&#20197;&#36817;&#20284;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#31895;&#31961;&#30340;&#20272;&#35745;&#26174;&#33879;&#21066;&#24369;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#65288;TPM&#65289;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#24615;&#32463;&#36807;&#23454;&#35777;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>http://arxiv.org/abs/2310.16705</link><description>&lt;p&gt;
&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#27969;&#22312;&#21464;&#20998;&#25512;&#26029;&#30340;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference. (arXiv:2310.16705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#24615;&#32463;&#36807;&#23454;&#35777;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21464;&#20998;&#21442;&#25968;&#34987;&#35843;&#25972;&#20197;&#20351;&#21464;&#20998;&#20998;&#24067;&#19982;&#30495;&#23454;&#21518;&#39564;&#23613;&#21487;&#33021;&#25509;&#36817;&#12290;&#21487;&#20197;&#36890;&#36807;&#40657;&#31665;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#26222;&#36890;&#26799;&#24230;&#19979;&#38477;&#25110;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26469;&#35299;&#20915;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#19968;&#20010;&#8220;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#8221;&#20013;&#23450;&#20041;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#20248;&#21270;&#25216;&#26415;&#65292;&#21363;&#40657;&#31665;&#21464;&#20998;&#25512;&#26029;&#21644;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#65292;&#21487;&#20197;&#37325;&#26032;&#35299;&#37322;&#20026;&#25152;&#25552;&#20986;&#30340;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#30340;&#29305;&#23450;&#23454;&#20363;&#12290;&#20026;&#20102;&#25552;&#39640;&#20248;&#21270;&#25928;&#29575;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#26469;&#25968;&#20540;&#27714;&#35299;&#31163;&#25955;&#26799;&#24230;&#27969;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;</title><link>http://arxiv.org/abs/2310.16152</link><description>&lt;p&gt;
FLTrojan: &#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#23545;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#27844;&#38706;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#27491;&#25104;&#20026;&#35768;&#22810;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#65292;&#20854;&#20013;&#20010;&#20307;FL&#21442;&#19982;&#32773;&#22312;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#24448;&#24448;&#20855;&#26377;&#25935;&#24863;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#31243;&#24230;&#24182;&#19981;&#31616;&#21333;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#26159;&#35797;&#22270;&#25552;&#21462;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#25110;&#22825;&#30495;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#30340;&#20004;&#20010;&#26032;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#27604;&#26368;&#32456;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#36896;&#25104;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#65292;&#36825;&#20123;&#26435;&#37325;&#29305;&#21035;&#36127;&#36131;&#35760;&#24518;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#22914;&#20309;&#22312;FL&#20013;&#27844;&#38706;&#20854;&#20182;&#29992;&#25143;&#30340;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.12274</link><description>&lt;p&gt;
&#19968;&#22270;&#25269;&#21315;&#35328;&#65306;&#20351;&#29992;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#23545;&#35937;&#32423;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21453;&#36716;&#26159;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#35789;&#8221;&#30340;&#23884;&#20837;&#34920;&#31034;&#22270;&#20687;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20351;&#20854;&#33021;&#22815;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21487;&#33719;&#24471;&#20010;&#21035;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#35782;&#21035;&#21644;&#25972;&#21512;&#19968;&#20010;&#22330;&#26223;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#20173;&#28982;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20063;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#30340;&#36827;&#19968;&#27493;&#35777;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#21477;&#23376;-&#22270;&#20687;&#23545;&#20013;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#12290;&#20026;&#20102;&#22686;&#24378;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65306;&#27880;&#24847;&#21147;&#25513;&#30721;&#65288;AttnMask&#65289;&#23558;&#23398;&#20064;&#38598;&#20013;&#22312;&#30456;&#20851;&#21306;&#22495;&#65307;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#65288;PromptCL&#65289;&#23558;&#19981;&#21516;&#27010;&#24565;&#30340;&#23884;&#20837;&#20998;&#31163;&#24320;&#26469;&#65307;&#20197;&#21450;&#32465;&#23450;&#24418;&#23481;&#35789;&#65288;Bind adj.&#65289;&#23558;&#26032;&#30340;&#8220;&#35789;&#8221;&#19982;&#24050;&#30693;&#35789;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09213</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Unseen Image Synthesis with Diffusion Models. (arXiv:2310.09213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29983;&#25104;&#39046;&#22495;&#30340;&#36235;&#21183;&#26159;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#21644;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#36890;&#29992;&#39046;&#22495;&#34920;&#31034;&#65292;&#32780;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#36873;&#25321;&#30456;&#21453;&#30340;&#26041;&#21521;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#20923;&#32467;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#21333;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28508;&#22312;&#37319;&#26679;&#21644;&#20960;&#20309;&#20248;&#21270;&#26469;&#21512;&#25104;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#21363;&#20351;&#26159;&#20165;&#22312;&#21333;&#39046;&#22495;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;DDPMs&#24050;&#32463;&#20855;&#22791;&#20102;&#36275;&#22815;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#36716;&#28508;&#22312;&#32534;&#30721;&#65292;&#24182;&#32463;&#36807;&#21452;&#21521;&#30830;&#23450;&#24615;&#25193;&#25955;&#21644;&#21435;&#22122;&#36712;&#36857;&#37325;&#26500;&#20219;&#24847;&#22270;&#20687;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#30740;&#31350;&#26410;&#35265;&#36807;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27839;&#21435;&#22122;&#38142;&#30340;OOD&#26679;&#26412;&#30340;&#32479;&#35745;&#21644;&#20960;&#20309;&#34892;&#20026;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#37117;&#34920;&#26126;&#65292;&#21453;&#36716;&#30340;OOD&#26679;&#26412;&#20063;&#24314;&#31435;&#20102;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the current trend in the generative field is scaling up towards larger models and more training data for generalized domain representations, we go the opposite direction in this work by synthesizing unseen domain images without additional training. We do so via latent sampling and geometric optimization using pre-trained and frozen Denoising Diffusion Probabilistic Models (DDPMs) on single-domain datasets. Our key observation is that DDPMs pre-trained even just on single-domain images are already equipped with sufficient representation abilities to reconstruct arbitrary images from the inverted latent encoding following bi-directional deterministic diffusion and denoising trajectories. This motivates us to investigate the statistical and geometric behaviors of the Out-Of-Distribution (OOD) samples from unseen image domains in the latent spaces along the denoising chain. Notably, we theoretically and empirically show that the inverted OOD samples also establish Gaussians that are 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2310.08566</link><description>&lt;p&gt;
&#20197;Transformer&#20026;&#20915;&#31574;&#32773;&#65306;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining. (arXiv:2310.08566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08566
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#33021;&#21147;&#65292;&#21363;&#24403;&#23427;&#20204;&#38754;&#23545;&#26469;&#33258;&#26410;&#30693;&#29615;&#22659;&#30340;&#20132;&#20114;&#36712;&#36857;&#26102;&#65292;&#23427;&#20204;&#33021;&#22815;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35757;&#32451;&#20197;&#25191;&#34892;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#23578;&#19981;&#28165;&#26970;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#21738;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#24050;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#23545;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#36825;&#21253;&#25324;&#20102;&#20004;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#31639;&#27861;&#33976;&#39311;&#21644;&#20915;&#31574;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#23558;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;&#24191;&#20041;&#35823;&#24046;&#30340;&#32553;&#25918;&#33539;&#22260;&#19982;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.05269</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#30340;&#21069;&#27839;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications. (arXiv:2310.05269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05269
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#23458;&#25143;&#21644;&#20027;&#26426;&#36830;&#25509;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#39046;&#22495;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#23433;&#20840;&#20998;&#24067;&#24335;ML&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38544;&#31169;&#23433;&#20840;&#24615;&#12290;FL&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#23558;ML&#27169;&#22411;&#20256;&#36755;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#65292;&#26377;&#25928;&#22320;&#23558;&#20113;&#22522;&#30784;&#35774;&#26045;&#19982;&#20013;&#24515;&#21270;&#21644;&#20998;&#25955;&#21270;&#31995;&#32479;&#30340;&#27969;&#31243;&#22788;&#29702;&#21644;&#25968;&#25454;&#23384;&#20648;&#38656;&#27714;&#30456;&#32467;&#21512;&#65292;&#27880;&#37325;&#21487;&#20280;&#32553;&#24615;&#12289;&#38544;&#31169;&#32771;&#34385;&#21644;&#32463;&#27982;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;&#22312;&#24403;&#21069;&#30340;FL&#23454;&#29616;&#20013;&#65292;&#25968;&#25454;&#25152;&#26377;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20197;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#21442;&#25968;&#30340;&#24418;&#24335;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#21019;&#26032;&#28040;&#38500;&#20102;&#19982;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#23458;&#25143;&#21644;&#21442;&#19982;&#32773;&#30452;&#25509;&#19982;&#20113;&#20013;&#24515;&#36890;&#20449;&#21407;&#22987;&#21644;&#28508;&#22312;&#26426;&#23494;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#19981;&#20165;&#38477;&#20302;&#20102;&#19982;&#19982;&#20256;&#32479;&#20113;&#20013;&#24515;&#36890;&#20449;&#30456;&#20851;&#30340;&#36153;&#29992;&#65292;&#36824;&#20445;&#25252;&#20102;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with 
&lt;/p&gt;</description></item><item><title>GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.03399</link><description>&lt;p&gt;
GRAPES: &#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03399
&lt;/p&gt;
&lt;p&gt;
GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#20197;&#19981;&#21516;&#26041;&#24335;&#32858;&#21512;&#21608;&#22260;&#20449;&#24687;&#26469;&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#36825;&#20123;&#32593;&#32476;&#30340;&#21152;&#28145;&#65292;&#30001;&#20110;&#37051;&#22495;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#24863;&#21463;&#37326;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#28040;&#32791;&#12290;&#22270;&#37319;&#26679;&#36890;&#36807;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;GNNs&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GNNs&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#12290;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#19987;&#27880;&#20110;&#22266;&#23450;&#30340;&#37319;&#26679;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32467;&#26500;&#25110;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GRAPES&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#35782;&#21035;&#29992;&#20110;&#35757;&#32451;GNN&#20998;&#31867;&#22120;&#30340;&#19968;&#32452;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12290;GRAPES&#20351;&#29992;GFlowNet&#26469;&#23398;&#20064;&#32473;&#23450;&#20998;&#31867;&#30446;&#26631;&#30340;&#33410;&#28857;&#37319;&#26679;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#22270;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;GRAPES&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;GRAPES&#21363;&#20351;&#22312;&#37319;&#26679;&#27604;&#20363;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#20173;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#26041;&#27861;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.01105</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#36830;&#32493;&#29109;&#24052;&#27663;&#20013;&#24515;&#20272;&#35745;&#26041;&#27861;&#21450;&#20854;&#22312;&#19968;&#33324;&#25104;&#26412;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy-Guided Continuous Entropic Barycenter Estimation for General Costs. (arXiv:2310.01105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#26041;&#27861;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#36755;&#36816;&#65288;OT&#65289;&#24052;&#27663;&#20013;&#24515;&#26159;&#19968;&#31181;&#22312;&#25429;&#25417;&#27010;&#29575;&#20998;&#24067;&#20960;&#20309;&#29305;&#24615;&#30340;&#21516;&#26102;&#23545;&#20854;&#36827;&#34892;&#24179;&#22343;&#30340;&#25968;&#23398;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#21463;&#21040;&#20851;&#27880;&#30340;&#22522;&#20110;&#24369;OT&#30340;&#36830;&#32493;&#29109;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#23545;&#20598;&#37325;&#26500;&#12290;&#38500;&#20102;&#21019;&#26032;&#24615;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20855;&#26377;&#20197;&#19979;&#33509;&#24178;&#20248;&#21183;&#29305;&#28857;&#65306;&#65288;i&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#24674;&#22797;&#35299;&#30340;&#36136;&#37327;&#30028;&#38480;&#65307;&#65288;ii&#65289;&#35813;&#26041;&#27861;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#65292;&#21487;&#20197;&#20351;&#29992;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#31639;&#27861;&#35299;&#20915;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#36991;&#20813;&#20351;&#29992;&#26497;&#23567;-&#26497;&#22823;&#12289;&#24378;&#21270;&#31561;&#22797;&#26434;&#25216;&#24039;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;s
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.00771</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;Decision Transformer&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#21482;&#33021;&#36890;&#36807;&#35821;&#35328;&#39044;&#35757;&#32451;&#23454;&#29616;&#65292;&#36824;&#26159;&#21487;&#20197;&#36890;&#36807;&#19981;&#28041;&#21450;&#35821;&#35328;&#30340;&#26356;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#35821;&#35328;&#23545;&#20110;&#25913;&#21892;&#24615;&#33021;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#23454;&#38469;&#19978;&#65292;&#20351;&#29992;&#21512;&#25104;&#30340;IID&#25968;&#25454;&#36827;&#34892;&#23569;&#37327;&#26356;&#26032;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#25552;&#21319;&#65307;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#39044;&#35757;&#32451;Conservative Q-Learning(CQL)&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;Q-learning&#65292;&#24182;&#36890;&#24120;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#39592;&#24178;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#22312;CQL&#31639;&#27861;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21464;&#25442;&#20013;&#30340;Clifford&#20960;&#20309;&#19981;&#21464;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;ADE Coxeter&#20803;&#32032;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#25366;&#25496;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#21464;&#37327;&#30340;&#36755;&#20986;&#23436;&#20840;&#21462;&#20915;&#20110;&#31616;&#21333;&#26681;&#30340;&#36873;&#25321;&#21644;&#23545;&#24212;&#21453;&#23556;&#30340;&#25490;&#21015;&#39034;&#24207;&#65292;&#23384;&#22312;&#24040;&#22823;&#30340;&#36864;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00041</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;ADE Coxeter&#20803;&#32032;&#30340;Clifford&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Clifford invariants of ADE Coxeter elements. (arXiv:2310.00041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21464;&#25442;&#20013;&#30340;Clifford&#20960;&#20309;&#19981;&#21464;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;ADE Coxeter&#20803;&#32032;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#25366;&#25496;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#21464;&#37327;&#30340;&#36755;&#20986;&#23436;&#20840;&#21462;&#20915;&#20110;&#31616;&#21333;&#26681;&#30340;&#36873;&#25321;&#21644;&#23545;&#24212;&#21453;&#23556;&#30340;&#25490;&#21015;&#39034;&#24207;&#65292;&#23384;&#22312;&#24040;&#22823;&#30340;&#36864;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32447;&#24615;&#21464;&#25442;&#30340;Clifford&#20960;&#20309;&#19981;&#21464;&#37327;&#19978;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#19968;&#31867;&#22312;&#26681;&#31995;&#12289;&#21453;&#23556;&#32676;&#12289;&#26446;&#32676;&#21644;&#26446;&#20195;&#25968;&#30340;&#32972;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#20960;&#20309;&#21464;&#25442;&#30340;&#19981;&#21464;&#37327;&#30340;&#30740;&#31350;: Coxeter&#21464;&#25442;&#12290;&#25105;&#20204;&#23545;$A_8$&#12289;$D_8$&#21644;$E_8$&#30340;&#25152;&#26377;Coxeter&#21464;&#25442;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#39640;&#24615;&#33021;&#35745;&#31639;&#26426;&#35745;&#31639;&#20102;&#23427;&#20204;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#31181;&#35745;&#31639;&#20195;&#25968;&#30340;&#33539;&#24335;&#29983;&#25104;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21033;&#29992;&#25968;&#25454;&#31185;&#23398;&#30340;&#25216;&#26415;&#65292;&#22914;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26469;&#25366;&#25496;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#12290;&#30001;&#20110;&#36755;&#20986; - &#19981;&#21464;&#37327; - &#23436;&#20840;&#30001;&#31616;&#21333;&#26681;&#30340;&#36873;&#25321;&#21644;Coxeter&#20803;&#32032;&#20013;&#23545;&#24212;&#21453;&#23556;&#30340;&#25490;&#21015;&#39034;&#24207;&#20915;&#23450;&#65292;&#25105;&#20204;&#26399;&#26395;&#22312;&#26144;&#23556;&#20013;&#23384;&#22312;&#24040;&#22823;&#30340;&#36864;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been recent interest in novel Clifford geometric invariants of linear transformations. This motivates the investigation of such invariants for a certain type of geometric transformation of interest in the context of root systems, reflection groups, Lie groups and Lie algebras: the Coxeter transformations. We perform exhaustive calculations of all Coxeter transformations for $A_8$, $D_8$ and $E_8$ for a choice of basis of simple roots and compute their invariants, using high-performance computing. This computational algebra paradigm generates a dataset that can then be mined using techniques from data science such as supervised and unsupervised machine learning. In this paper we focus on neural network classification and principal component analysis. Since the output -- the invariants -- is fully determined by the choice of simple roots and the permutation order of the corresponding reflections in the Coxeter element, we expect huge degeneracy in the mapping. This provides the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16965</link><description>&lt;p&gt;
&#25511;&#21046;&#32452;&#21512;&#20248;&#21270;&#30340;&#36830;&#32493;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25214;&#21040;&#36817;&#20284;&#35299;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;CO&#38382;&#39064;&#19978;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30456;&#23545;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#24615;&#33021;&#24694;&#21270;&#65292;&#20294;&#23545;&#20110;PI-GNN&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#21364;&#27809;&#26377;&#22826;&#22810;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PI-GNN&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#25918;&#26494;&#31574;&#30053;&#65292;&#23398;&#20064;&#21518;&#38656;&#35201;&#20174;&#36830;&#32493;&#31354;&#38388;&#20154;&#24037;&#36716;&#25442;&#22238;&#21407;&#22987;&#31163;&#25955;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#30340;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#35299;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#25152;&#26377;&#21464;&#37327;&#37117;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.16858</link><description>&lt;p&gt;
Transductive Learning&#30340;&#23574;&#38160;&#27867;&#21270;&#65306;&#19968;&#31181;Transductive Local Rademacher Complexity&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16858
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20256;&#32479;&#30340;local rademacher complexity (LRC)&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;transductive&#35774;&#32622;&#20013;&#65292;&#30456;&#23545;&#20110;&#20856;&#22411;&#30340;LRC&#26041;&#27861;&#22312;&#24402;&#32435;&#35774;&#32622;&#20013;&#30340;&#20998;&#26512;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rademacher complex&#30340;&#23616;&#37096;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;transductive learning&#38382;&#39064;&#65292;&#24182;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#24471;&#21040;&#20102;&#23574;&#38160;&#30340;&#30028;&#38480;&#12290;&#19982;LRC&#30340;&#21457;&#23637;&#31867;&#20284;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#29420;&#31435;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#24320;&#22987;&#26500;&#24314;TLRC&#65292;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16096</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#21487;&#20197;&#36991;&#20813;&#30340;&#65306;&#25968;&#25454;&#38598;&#20013;&#24615;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#25935;&#24863;&#24615;&#24341;&#21457;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26263;&#31034;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#33021;&#36807;&#20110;&#19968;&#33324;&#21270;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#25968;&#25454;&#20998;&#24067;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#22312;&#28041;&#21450;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26126;&#26174;&#30340;&#30683;&#30462;&#25512;&#21160;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#19968;&#20010;&#38382;&#39064;&#65306;&#23545;&#25239;&#26679;&#26412;&#26159;&#21542;&#30495;&#30340;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#8212;&#8212;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#23567;&#23481;&#31215;&#23376;&#38598;&#30340;&#38598;&#20013;&#31243;&#24230;&#65292;&#20915;&#23450;&#20102;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#33258;&#28982;&#22320;&#24471;&#21040;&#20139;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#22312;&#29305;&#23450;&#33539;&#22260;&#20869;&#21487;&#35777;&#26126;&#35748;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15278</link><description>&lt;p&gt;
&#30524;&#19981;&#35265;&#24515;&#19981;&#24565;&#65306;&#21033;&#29992;&#35270;&#39057;&#36319;&#36394;&#21551;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23545;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#26377;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#30340;&#35760;&#24518;&#65292;&#20197;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#38754;&#21521;&#23545;&#35937;&#30340;&#35760;&#24518;&#32534;&#30721;&#21040;&#22810;&#23545;&#35937;&#25805;&#32437;&#25512;&#29702;&#21644;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DOOM&#21644;LOOM&#65292;&#23427;&#20204;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#26469;&#32534;&#30721;&#32473;&#23450;&#37096;&#20998;&#35270;&#28857;&#20113;&#21644;&#23545;&#35937;&#21457;&#29616;&#19982;&#36319;&#36394;&#24341;&#25806;&#30340;&#36712;&#36857;&#21382;&#21490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#65292;&#26032;&#20986;&#29616;&#30340;&#23545;&#35937;&#65292;&#20197;&#21450;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#19981;&#21516;&#25968;&#37327;&#30340;&#24178;&#25200;&#21160;&#20316;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38544;&#24335;&#35760;&#24518;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;Klir&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#21644;&#35777;&#25454;&#23398;&#27966;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#22312;&#32771;&#34385;&#21040;&#26631;&#31614;&#20013;&#24050;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#35299;&#21644;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.12494</link><description>&lt;p&gt;
&#26377;&#20851;&#23500;&#26631;&#31614;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#35777;&#25454;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evidential uncertainties on rich labels for active learning. (arXiv:2309.12494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;Klir&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#21644;&#35777;&#25454;&#23398;&#27966;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#22312;&#32771;&#34385;&#21040;&#26631;&#31614;&#20013;&#24050;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#35299;&#21644;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#19981;&#30830;&#23450;&#37319;&#26679;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#21487;&#38477;&#20302;&#21644;&#19981;&#21487;&#38477;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#31616;&#21270;&#35745;&#31639;&#38454;&#27573;&#65292;&#24182;&#28040;&#38500;&#23545;&#35266;&#23519;&#32467;&#26524;&#30340;&#20381;&#36182;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#32771;&#34385;&#26631;&#31614;&#20013;&#24050;&#32463;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#31572;&#39064;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;Klir&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#21644;&#35777;&#25454;&#23398;&#27966;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#20449;&#20219;&#20989;&#25968;&#29702;&#35770;&#26469;&#35299;&#20915;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in active learning, and more precisely in uncertainty sampling, has focused on the decomposition of model uncertainty into reducible and irreducible uncertainties. In this paper, we propose to simplify the computational phase and remove the dependence on observations, but more importantly to take into account the uncertainty already present in the labels, \emph{i.e.} the uncertainty of the oracles. Two strategies are proposed, sampling by Klir uncertainty, which addresses the exploration-exploitation problem, and sampling by evidential epistemic uncertainty, which extends the reducible uncertainty to the evidential framework, both using the theory of belief functions.
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.10140</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#23398;&#20064;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#20960;&#20309;&#65292;&#23427;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#32467;&#26500;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#29305;&#24449;&#20960;&#20309;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#35299;&#20915;&#30001;&#23398;&#20064;&#35774;&#32622;&#25351;&#23450;&#30340;&#20381;&#36182;&#32452;&#20214;&#30340;&#26368;&#20339;&#29305;&#24449;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#25216;&#26415;&#26469;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#26368;&#20339;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#22120;&#12290;&#20026;&#20102;&#23637;&#31034;&#23884;&#22871;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;&#26465;&#20214;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#20339;&#29305;&#24449;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;DiffAug&#65292;&#36890;&#36807;&#25193;&#25955;&#27493;&#39588;&#23454;&#29616;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#24179;&#28369;&#30340;&#28508;&#31354;&#38388;&#65292;&#26088;&#22312;&#25552;&#39640;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07909</link><description>&lt;p&gt;
&#20174;&#22836;&#24320;&#22987;&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25552;&#21319;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;DiffAug&#65292;&#36890;&#36807;&#25193;&#25955;&#27493;&#39588;&#23454;&#29616;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#24179;&#28369;&#30340;&#28508;&#31354;&#38388;&#65292;&#26088;&#22312;&#25552;&#39640;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20135;&#29983;&#31283;&#20581;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#25110;&#22806;&#37096;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#22312;&#36716;&#25442;&#21040;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#26102;&#24212;&#29992;&#21463;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#31185;&#23398;&#30456;&#20851;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#38480;&#21046;&#26469;&#28304;&#20110;&#36825;&#20123;&#39046;&#22495;&#20013;&#20808;&#39564;&#30693;&#35782;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;DiffAug&#12290;DiffAug&#36890;&#36807;&#25193;&#25955;&#27493;&#39588;&#30830;&#20445;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#19968;&#20010;&#24179;&#28369;&#30340;&#28508;&#31354;&#38388;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;DiffAug&#39318;&#20808;&#23545;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#20351;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#22312;&#28508;&#31354;&#38388;&#19978;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised contrastive learning methods have recently seen significant improvements, particularly through data augmentation strategies that aim to produce robust and generalizable representations. However, prevailing data augmentation methods, whether hand designed or based on foundation models, tend to rely heavily on prior knowledge or external data. This dependence often compromises their effectiveness and efficiency. Furthermore, the applicability of most existing data augmentation strategies is limited when transitioning to other research domains, especially science-related data. This limitation stems from the paucity of prior knowledge and labeled data available in these domains. To address these challenges, we introduce DiffAug-a novel and efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure that the augmented and original data share a smoothed latent space, which is achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug first 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.03426</link><description>&lt;p&gt;
&#30456;&#31561;&#30340;&#38271;&#26399;&#25928;&#30410;&#29575;&#65306;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#27010;&#24565;&#24212;&#29992;&#21040;&#39034;&#24207;&#20915;&#31574;&#20013;
&lt;/p&gt;
&lt;p&gt;
Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03426
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#21487;&#33021;&#23545;&#26102;&#38388;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38271;&#26399;&#20844;&#24179;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24573;&#30053;&#38271;&#26399;&#24433;&#21709;&#26102;&#65292;&#31616;&#21333;&#22320;&#24212;&#29992;&#38745;&#24577;&#20844;&#24179;&#24615;&#20934;&#21017;&#23454;&#38469;&#19978;&#20250;&#21152;&#21095;&#20559;&#35265;&#12290;&#20026;&#20102;&#26126;&#30830;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#20559;&#35265;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26694;&#26550;&#20013;&#21046;&#23450;&#20102;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#20182;&#20204;&#23558;&#38271;&#26399;&#20559;&#35265;&#23450;&#20041;&#20026;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#38745;&#24577;&#20559;&#35265;&#30340;&#24635;&#21644;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#22320;&#23545;&#36880;&#27493;&#20559;&#35265;&#27714;&#21644;&#21487;&#33021;&#23548;&#33268;&#34394;&#20551;&#30340;&#20844;&#24179;&#24863;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#21040;&#36716;&#25442;&#36807;&#31243;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#37325;&#35201;&#24615;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#23427;&#26126;&#30830;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38271;&#26399;&#25910;&#30410;&#30340;&#31574;&#30053;&#26799;&#24230;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16245</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#26657;&#20934;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#24120;&#26159;&#29616;&#20195;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#22522;&#20110;AI&#30340;DSS&#20013;&#20351;&#29992;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#29702;&#30001;&#30340;AI&#31995;&#32479;&#12290;XAI&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20010;&#21035;&#39044;&#27979;&#30340;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#21363;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#26080;&#27861;&#37327;&#21270;&#19982;&#29305;&#24449;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;Calibrated Explanations&#65288;CE&#65289;&#30340;&#25193;&#23637;&#65292;&#20043;&#21069;&#21482;&#25903;&#25345;&#20998;&#31867;&#65292;&#29616;&#22312;&#25903;&#25345;&#26631;&#20934;&#22238;&#24402;&#21644;&#27010;&#29575;&#22238;&#24402;&#65292;&#21363;&#30446;&#26631;&#36229;&#36807;&#20219;&#24847;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#22238;&#24402;&#38382;&#39064;&#30340;&#25193;&#23637;&#20445;&#30041;&#20102;CE&#30340;&#25152;&#26377;&#20248;&#28857;&#65292;&#20363;&#22914;&#23558;&#24213;&#23618;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IPA&#30340;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12871</link><description>&lt;p&gt;
IPA&#65306;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency. (arXiv:2308.12871v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12871
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IPA&#30340;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#20135;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#22320;&#20248;&#21270;&#22810;&#27169;&#22411;&#25512;&#29702;&#31649;&#36947;&#20197;&#23454;&#29616;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#23545;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#31616;&#21270;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#24191;&#38420;&#32780;&#22797;&#26434;&#30340;&#26435;&#34913;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#20379;&#32773;&#36890;&#24120;&#36873;&#25321;&#32771;&#34385;&#20854;&#20013;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#21327;&#35843;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#31649;&#29702;&#25512;&#29702;&#31649;&#36947;&#20013;&#27169;&#22411;&#21464;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IPA&#65292;&#19968;&#31181;&#22312;&#32447;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#31649;&#36947;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#27599;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#27169;&#22411;&#21464;&#20307;&#26159;&#21516;&#19968;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21516;&#29256;&#26412;&#65292;&#20854;&#36164;&#28304;&#38656;&#27714;&#12289;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#26377;&#25152;&#19981;&#21516;&#12290;IPA&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25209;&#22788;&#29702;&#22823;&#23567;&#12289;&#22797;&#21046;&#21644;&#27169;&#22411;&#21464;&#20307;&#26469;&#20248;&#21270;&#20934;&#30830;&#24615;&#12289;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#24310;&#36831;SLA&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11842</link><description>&lt;p&gt;
&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#35782;&#21035;&#21644;&#20998;&#26512;&#23545;&#31216;&#27169;&#24335;&#24050;&#32463;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#29289;&#29702;&#23398;&#20013;&#30340;&#24341;&#21147;&#23450;&#24459;&#30340;&#21046;&#23450;&#21644;&#21270;&#23398;&#32467;&#26500;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#22312;&#26576;&#20123;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24418;&#24335;&#21270;&#22320;&#25551;&#36848;&#19968;&#31867;&#20855;&#26377;&#19968;&#33324;&#23545;&#31216;&#24615;&#27010;&#24565;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#23384;&#22312;&#23545;&#31216;&#30340;&#26368;&#20248;&#20540;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#36825;&#20123;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20316;&#20026;&#22810;&#26234;&#20307;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#22312;&#20855;&#26377;&#37325;&#22797;&#23545;&#31216;&#27169;&#24335;&#30340;&#26410;&#35265;&#22330;&#26223;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#31561;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00951</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#21040;&#36719;&#24615;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30340;&#19987;&#23478;&#27169;&#22411;(MoEs)&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#25512;&#29702;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;MoEs&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#20002;&#22833;&#26631;&#35760;&#12289;&#26080;&#27861;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#25110;&#26080;&#25928;&#30340;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Soft MoE&#65292;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;Transformer&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20445;&#25345;&#20102;MoEs&#30340;&#20248;&#28857;&#12290;Soft MoE&#36890;&#36807;&#21521;&#27599;&#20010;&#19987;&#23478;&#20256;&#36882;&#25152;&#26377;&#36755;&#20837;&#26631;&#35760;&#30340;&#19981;&#21516;&#21152;&#26435;&#32452;&#21512;&#26469;&#25191;&#34892;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#12290;&#19982;&#20854;&#20182;MoE&#20316;&#21697;&#19968;&#26679;&#65292;Soft MoE&#20013;&#30340;&#19987;&#23478;&#21482;&#22788;&#29702;&#19968;&#37096;&#20998;&#65288;&#32452;&#21512;&#30340;&#65289;&#26631;&#35760;&#65292;&#20197;&#22312;&#36739;&#20302;&#30340;&#25512;&#29702;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#23481;&#37327;&#12290;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#65292;Soft MoE&#22312;&#26631;&#20934;Transformer&#65288;ViTs&#65289;&#21644;&#27969;&#34892;&#30340;MoE&#21464;&#20307;&#65288;Tokens Choice&#21644;Experts Choice&#65289;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;Soft MoE-Base/16&#30340;&#25512;&#29702;&#25104;&#26412;&#27604;ViT-Huge/14&#20302;10.5&#20493;&#65288;&#22681;&#38047;&#26102;&#38388;&#38477;&#20302;&#20102;5.7&#20493;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#30340;&#20891;&#20107;&#23433;&#20840;&#35780;&#20272;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35299;&#20915;&#39640;&#39118;&#38505;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.08840</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65306;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#30340;&#20891;&#20107;&#23433;&#20840;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#23433;&#20840;&#31574;&#30053;&#23398;&#20064;&#30340;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#30340;&#20891;&#20107;&#23433;&#20840;&#35780;&#20272;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35299;&#20915;&#39640;&#39118;&#38505;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#65292;&#22914;&#21009;&#20107;&#21496;&#27861;&#12289;&#21307;&#23398;&#21644;&#20844;&#20849;&#25919;&#31574;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#31639;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#24314;&#35758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36234;&#21335;&#25112;&#20105;&#26399;&#38388;&#65292;&#26159;&#21542;&#26377;&#21487;&#33021;&#25913;&#36827;&#19968;&#31181;&#23433;&#20840;&#35780;&#20272;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#22312;&#20854;&#24341;&#20837;&#21518;&#31435;&#21363;&#27979;&#37327;&#21040;&#30340;&#32467;&#26524;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20010;&#23454;&#35777;&#24212;&#29992;&#25552;&#20986;&#20102;&#22312;&#39640;&#39118;&#38505;&#31639;&#27861;&#20915;&#31574;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#20960;&#20010;&#26041;&#27861;&#23398;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#22312;&#23454;&#26045;&#26032;&#31639;&#27861;&#20043;&#21069;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#23545;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#39118;&#38505;&#36827;&#34892;&#34920;&#24449;&#21644;&#25511;&#21046;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#31639;&#27861;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#26032;&#31639;&#27861;&#38656;&#35201;&#36879;&#26126;&#30340;&#22806;&#25512;&#12290;&#31532;&#19977;&#65292;&#29616;&#26377;&#31639;&#27861;&#28041;&#21450;&#24120;&#35265;&#20294;&#38590;&#20197;&#20248;&#21270;&#30340;&#31163;&#25955;&#20915;&#31574;&#34920;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#22343;&#26465;&#20214;&#39118;&#38505;&#65288;ACRisk&#65289;&#65292;&#39318;&#20808;&#37327;&#21270;&#20102;&#20135;&#29983;&#36739;&#24046;&#32467;&#26524;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.  To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#26144;&#23556;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#20998;&#32452;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#21644;&#27169;&#22359;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#37327;&#20219;&#21153;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.03374</link><description>&lt;p&gt;
STG-MTL: &#20351;&#29992;&#25968;&#25454;&#26144;&#23556;&#30340;&#21487;&#20280;&#32553;&#20219;&#21153;&#20998;&#32452;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map. (arXiv:2307.03374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#26144;&#23556;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#20998;&#32452;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#21644;&#27169;&#22359;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#37327;&#20219;&#21153;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#22240;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#20219;&#21153;&#23398;&#20064;&#32780;&#35328;&#20855;&#26377;&#24615;&#33021;&#25913;&#36827;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#24448;&#24448;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#33021;&#30340;&#20219;&#21153;&#20998;&#32452;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#36825;&#20351;&#24471;&#36873;&#25321;&#26368;&#20339;&#20219;&#21153;&#20998;&#32452;&#21464;&#24471;&#22256;&#38590;&#65292;&#24182;&#19988;&#19968;&#20123;&#20998;&#32452;&#21487;&#33021;&#20250;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#36127;&#38754;&#24178;&#25200;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20005;&#37325;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#25968;&#25454;&#26144;&#23556;&#30340;&#20998;&#31867;&#20219;&#21153;&#20998;&#32452;&#30340;&#21487;&#20280;&#32553;&#21644;&#27169;&#22359;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#20219;&#21153;&#25968;&#37327;&#19979;&#65288;&#39640;&#36798;100&#20010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) is a powerful technique that has gained popularity due to its performance improvement over traditional Single-Task Learning (STL). However, MTL is often challenging because there is an exponential number of possible task groupings, which can make it difficult to choose the best one, and some groupings might produce performance degradation due to negative interference between tasks. Furthermore, existing solutions are severely suffering from scalability issues, limiting any practical application. In our paper, we propose a new data-driven method that addresses these challenges and provides a scalable and modular solution for classification task grouping based on hand-crafted features, specifically Data Maps, which capture the training behavior for each classification task during the MTL training. We experiment with the method demonstrating its effectiveness, even on an unprecedented number of tasks (up to 100).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.10050</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpolating Item and User Fairness in Recommendation Systems. (arXiv:2306.10050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#36793;&#24179;&#21488;&#20013;&#65292;&#24179;&#21488;&#19982;&#21334;&#23478;&#65288;&#39033;&#30446;&#65289;&#21644;&#23458;&#25143;&#65288;&#29992;&#25143;&#65289;&#31561;&#21508;&#31181;&#21508;&#26679;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#20114;&#21160;&#65292;&#27599;&#20010;&#30456;&#20851;&#32773;&#37117;&#26377;&#33258;&#24049;&#30340;&#26399;&#26395;&#32467;&#26524;&#65292;&#23547;&#25214;&#21512;&#36866;&#30340;&#24179;&#34913;&#28857;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#8220;&#20844;&#24179;&#25104;&#26412;&#8221;&#65292;&#23427;&#25429;&#25417;&#20102;&#24179;&#21488;&#22312;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21033;&#30410;&#26102;&#21487;&#33021;&#20570;&#20986;&#30340;&#22949;&#21327;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#25512;&#33616;&#26694;&#26550;&#65292;&#20854;&#20013;&#24179;&#21488;&#22312;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#32422;&#26463;&#26102;&#26368;&#22823;&#21270;&#20854;&#25910;&#30410;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26356;&#29616;&#23454;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20844;&#24179;&#25512;&#33616;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24179;&#21488;&#32570;&#20047;&#20102;&#35299;&#29992;&#25143;&#20559;&#22909;&#30340;&#30693;&#35782;&#65292;&#21482;&#33021;&#35266;&#23519;&#20108;&#36827;&#21046;&#36141;&#20080;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#32500;&#25252;&#24179;&#21488;&#25910;&#30410;&#30340;&#21516;&#26102;&#31649;&#29702;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#21516;&#26102;&#20445;&#25345;&#39640;&#25910;&#30410;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms employ recommendation systems to enhance customer engagement and drive revenue. However, in a multi-sided platform where the platform interacts with diverse stakeholders such as sellers (items) and customers (users), each with their own desired outcomes, finding an appropriate middle ground becomes a complex operational challenge. In this work, we investigate the ``price of fairness'', which captures the platform's potential compromises when balancing the interests of different stakeholders. Motivated by this, we propose a fair recommendation framework where the platform maximizes its revenue while interpolating between item and user fairness constraints. We further examine the fair recommendation problem in a more realistic yet challenging online setting, where the platform lacks knowledge of user preferences and can only observe binary purchase decisions. To address this, we design a low-regret online optimization algorithm that preserves the platform's revenue while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25345;&#32493;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#23545;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;&#65292;&#26088;&#22312;&#40723;&#21169;&#20195;&#29702;&#22120;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01896</link><description>&lt;p&gt;
&#24212;&#23545;&#25345;&#32493;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning. (arXiv:2306.01896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25345;&#32493;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#23545;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;&#65292;&#26088;&#22312;&#40723;&#21169;&#20195;&#29702;&#22120;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22806;&#25512;&#19988;&#24378;&#28872;&#20381;&#36182;&#21608;&#26399;&#24615;&#37325;&#32622;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#20197;&#40723;&#21169;&#20195;&#29702;&#39318;&#20808;&#23398;&#20064;&#31283;&#23450;&#24615;&#65288;&#21363;&#23454;&#29616;&#26377;&#30028;&#25104;&#26412;&#65289;&#65292;&#28982;&#21518;&#20877;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#20943;&#23569;&#20102;&#20195;&#29702;&#22120;&#30340;&#21457;&#25955;&#29575;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning (RL) algorithms have been successfully applied to many tasks, their inability to extrapolate and strong reliance on episodic resets inhibits their applicability to many real-world settings. For instance, in stochastic queueing problems, the state space can be unbounded and the agent may have to learn online without the system ever being reset to states the agent has seen before. In such settings, we show that deep RL agents can diverge into unseen states from which they can never recover due to the lack of resets, especially in highly stochastic environments. Towards overcoming this divergence, we introduce a Lyapunov-inspired reward shaping approach that encourages the agent to first learn to be stable (i.e. to achieve bounded cost) and then to learn to be optimal. We theoretically show that our reward shaping technique reduces the rate of divergence of the agent and empirically find that it prevents it. We further combine our reward shaping approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01147</link><description>&lt;p&gt;
&#24179;&#28369;&#21333;&#35843;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#24615;&#32422;&#26463;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#24378;&#21147;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#25903;&#25345;&#20844;&#24179;&#24615;&#65292;&#24182;&#22686;&#21152;&#25968;&#25454;&#39537;&#21160;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#32463;&#20856;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#20102;&#21333;&#35843;&#24615;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#32780;&#24448;&#24448;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38519;&#20837;&#19981;&#33391;&#23616;&#37096;&#26368;&#20248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;MM&#32593;&#32476;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#20351;&#29992;&#20005;&#26684;&#36882;&#22686;&#30340;&#24179;&#28369;&#38750;&#32447;&#24615;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#24471;&#21040;&#30340;&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#27169;&#22359;&#32487;&#25215;&#20102;MM&#26550;&#26500;&#30340;&#28176;&#36817;&#36924;&#36817;&#24615;&#36136;&#12290;&#23427;&#21487;&#20197;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#21333;&#35843;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#65292;SMM&#27169;&#22359;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#35745;&#31639;&#38656;&#27714;&#20063;&#35201;&#23569;&#24471;&#22810;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#19982;&#26367;&#20195;&#31070;&#32463;&#21644;&#38750;&#31070;&#32463;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#24471;&#26356;&#20026;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#34920;&#31034;&#21518;&#39564;&#20998;&#24067;&#30340;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#30340;&#30446;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#27969;&#22330;&#37325;&#26500;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.20004</link><description>&lt;p&gt;
&#23398;&#20064;&#35299;&#20915;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#65306;&#19968;&#31181;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to solve Bayesian inverse problems: An amortized variational inference approach. (arXiv:2305.20004v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#34920;&#31034;&#21518;&#39564;&#20998;&#24067;&#30340;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#30340;&#30446;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#27969;&#22330;&#37325;&#26500;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#65292;&#21363;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#20272;&#35745;&#29289;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#36125;&#21494;&#26031;&#20844;&#24335;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#32531;&#35299;&#30149;&#24577;&#24615;&#38382;&#39064;&#24182;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#26512;&#21518;&#39564;&#19981;&#36890;&#24120;&#21487;&#29992;&#65292;&#20154;&#20204;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#25110;&#36817;&#20284;&#21464;&#20998;&#25512;&#29702;&#12290;&#20294;&#26159;&#65292;&#38656;&#35201;&#37325;&#26032;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#25512;&#29702;&#20197;&#36866;&#24212;&#27599;&#32452;&#26032;&#25968;&#25454;&#12290;&#36825;&#31181;&#32570;&#28857;&#38480;&#21046;&#20102;&#36125;&#21494;&#26031;&#20844;&#24335;&#22312;&#23454;&#26102;&#35774;&#32622;&#65292;&#20363;&#22914;&#24037;&#31243;&#31995;&#32479;&#30340;&#20581;&#24247;&#30417;&#27979;&#21644;&#21307;&#30103;&#35786;&#26029;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#25968;&#25454;&#21040;&#21518;&#39564;&#30340;&#36125;&#21494;&#26031;&#36870;&#26144;&#23556;&#65292;&#21363;&#20174;&#25968;&#25454;&#21040;&#21518;&#39564;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#34920;&#31034;&#21518;&#39564;&#20998;&#24067;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#25674;&#38144;&#21464;&#20998;&#25512;&#29702;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#65292;&#20854;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#39044;&#27979;&#20174;&#25968;&#25454;&#20013;&#39044;&#27979;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#19968;&#20123;&#36870;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#21442;&#25968;&#20272;&#35745;&#21644;&#27969;&#22330;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems, i.e., estimating parameters of physical models from experimental data, are ubiquitous in science and engineering. The Bayesian formulation is the gold standard because it alleviates ill-posedness issues and quantifies epistemic uncertainty. Since analytical posteriors are not typically available, one resorts to Markov chain Monte Carlo sampling or approximate variational inference. However, inference needs to be rerun from scratch for each new set of data. This drawback limits the applicability of the Bayesian formulation to real-time settings, e.g., health monitoring of engineered systems, and medical diagnosis. The objective of this paper is to develop a methodology that enables real-time inference by learning the Bayesian inverse map, i.e., the map from data to posteriors. Our approach is as follows. We represent the posterior distribution using a parameterization based on deep neural networks. Next, we learn the network parameters by amortized variational inferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.17193</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20998;&#26512;&#65306;&#22312;&#27809;&#26377;&#22522;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#29983;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth. (arXiv:2305.17193v1 [q-bio.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17193
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#30340;&#32435;&#31859;&#32423;&#20998;&#36776;&#29575;&#29616;&#24050;&#20351;&#33639;&#20809;&#20998;&#23376;&#23450;&#20301;&#24037;&#20855;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#25972;&#20010;&#32454;&#32990;&#32467;&#26500;&#29983;&#29289;&#23398;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#25968;&#25454;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26032;&#29983;&#29289;&#23398;&#26412;&#36523;&#27809;&#26377;&#34987;&#21457;&#29616;&#36807;&#65292;&#20063;&#27809;&#26377;&#22522;&#20934;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#22312;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#20854;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#23545;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#30340;&#21152;&#36895;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nanoscale resolution of super-resolution microscopy has now enabled the use of fluorescent based molecular localization tools to study whole cell structural biology. Machine learning based analysis of super-resolution data offers tremendous potential for discovery of new biology, that by definition is not known and lacks ground truth. Herein, we describe the application of weakly supervised learning paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the molecular architecture of subcellular macromolecules and organelles.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16338</link><description>&lt;p&gt;
&#28145;&#24605;&#29087;&#34385;&#65306;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#24050;&#32463;&#23637;&#31034;&#20102;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#20302;&#25928;&#24615;&#28304;&#20110;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#35760;&#24518;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22240;&#27492;&#65292;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#30340;&#38544;&#24335;&#35760;&#24518;&#26426;&#21046;&#19981;&#21516;&#65292;&#20154;&#33041;&#21033;&#29992;&#20998;&#24067;&#24335;&#23384;&#20648;&#22120;&#23384;&#20648;&#35760;&#24518;&#65292;&#20197;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#32452;&#32455;&#22810;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#26469;&#23384;&#20648;&#12289;&#34701;&#21512;&#21644;&#26816;&#32034;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;Atari&#28216;&#25103;&#21644;&#20803;&#19990;&#30028;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#24494;&#35843;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15659</link><description>&lt;p&gt;
&#22914;&#20309;&#36867;&#31163;&#38160;&#21270;&#30340;&#26497;&#23567;&#20540;&#28857;
&lt;/p&gt;
&lt;p&gt;
How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20123;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#21457;&#29616;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31639;&#27861;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#24230;&#37327;&#23427;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#27010;&#24565;&#12290;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;&#31639;&#27861;&#30340;&#20027;&#35201;&#32452;&#20214;&#26159;&#20351;&#29992;&#20174;&#38543;&#26426;&#25200;&#21160;&#36845;&#20195;&#20013;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20272;&#35745;&#23548;&#33268;&#26356;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#26041;&#21521;&#12290;&#23545;&#20110;&#25104;&#26412;&#20989;&#25968;&#26159;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23454;&#29992;&#31639;&#27861;&#8212;&#8212;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;</title><link>http://arxiv.org/abs/2305.14749</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;RNA&#35774;&#35745;&#22312;&#21512;&#25104;&#29983;&#29289;&#23398;&#21644;&#27835;&#30103;&#24320;&#21457;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;RNA&#22810;&#26679;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#30340;&#22522;&#30784;&#26159;&#23427;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#65292;&#20351;&#21333;&#19968;&#24207;&#21015;&#33021;&#22815;&#37319;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#19977;&#32500;&#32467;&#26500;&#29366;&#24577;&#12290;&#30446;&#21069;&#65292;&#35745;&#31639;&#29983;&#29289;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#32463;&#24120;&#34987;&#25552;&#20986;&#20026;&#36870;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#37319;&#29992;&#21333;&#19968;&#39044;&#26399;&#32467;&#26500;&#26500;&#35937;&#26469;&#35774;&#35745;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gRNAde&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#19968;&#32452;&#19977;&#32500;RNA&#39592;&#26550;&#32467;&#26500;&#25805;&#20316;&#30340;&#20960;&#20309;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;RNA&#35774;&#35745;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;gRNAde&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/chaitjo/geometric-rna-design&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (CMANPs) &#21450;&#20854;&#27880;&#24847;&#21147;&#22359; CMAB&#65292;&#33021;&#22312;&#24120;&#25968;&#20869;&#23384;&#19979;&#36827;&#34892;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#25805;&#20316;&#65292;&#24182;&#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14567</link><description>&lt;p&gt;
&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Constant Memory Attentive Neural Processes. (arXiv:2305.14567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (CMANPs) &#21450;&#20854;&#27880;&#24847;&#21147;&#22359; CMAB&#65292;&#33021;&#22312;&#24120;&#25968;&#20869;&#23384;&#19979;&#36827;&#34892;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#25805;&#20316;&#65292;&#24182;&#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243; (Neural Processes, NPs) &#26159;&#20272;&#35745;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;NPs &#21253;&#21547;&#19968;&#20010;&#32534;&#30721;&#26465;&#20214;&#25968;&#25454;&#38598;&#30340;&#26465;&#20214;&#38454;&#27573;&#12289;&#19968;&#20010;&#20351;&#29992;&#32534;&#30721;&#39044;&#27979;&#30340;&#26597;&#35810;&#38454;&#27573;&#21644;&#19968;&#20010;&#20351;&#29992;&#26032;&#25968;&#25454;&#28857;&#26356;&#26032;&#32534;&#30721;&#30340;&#26356;&#26032;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#65292;&#36825;&#20010;&#20869;&#23384;&#38543;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21576;&#32447;&#24615;&#25110;&#20108;&#27425;&#20989;&#25968;&#22686;&#38271;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (Constant Memory Attentive Neural Processes, CMANPs)&#65292;&#23427;&#30340;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#38454;&#27573;&#22343;&#21482;&#38656;&#35201;&#24120;&#25968;&#20869;&#23384;&#12290;&#22312;&#26500;&#24314; CMANPs &#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36890;&#29992;&#27880;&#24847;&#21147;&#22359;&#65292;&#31216;&#20026; Constant Memory Attention Block (CMAB)&#65292;&#23427;&#21487;&#20197;&#22312;&#24120;&#25968;&#20869;&#23384;&#20013;&#35745;&#31639;&#36755;&#20986;&#24182;&#36827;&#34892;&#24120;&#25968;&#30340;&#26356;&#26032;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CMANPs &#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#20445;&#25345;&#24120;&#25968;&#20869;&#23384;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Processes (NPs) are efficient methods for estimating predictive uncertainties. NPs comprise of a conditioning phase where a context dataset is encoded, a querying phase where the model makes predictions using the context dataset encoding, and an updating phase where the model updates its encoding with newly received datapoints. However, state-of-the-art methods require additional memory which scales linearly or quadratically with the size of the dataset, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires constant memory for the conditioning, querying, and updating phases. In building CMANPs, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that can compute its output in constant memory and perform updates in constant computation. Empirically, we show CMANPs achieve state-of-the-art results on meta-regression an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#65292;&#20855;&#26377;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13082</link><description>&lt;p&gt;
Sketch-and-Project Meets Newton Method: &#20855;&#26377;&#20302;&#31209;&#26356;&#26032;&#30340;&#20840;&#23616;$\mathcal O(k^{-2})$ &#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates. (arXiv:2305.13082v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#65292;&#20855;&#26377;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;$\mathcal O(k^{-2})$ &#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#19977;&#20010;&#26041;&#38754;&#26469;&#30475;&#24453;&#65306;i) &#20316;&#20026;&#19968;&#20010;&#33609;&#22270;&#21644;&#25237;&#24433;&#31639;&#27861;&#65292;&#23545;Newton&#26041;&#27861;&#30340;&#26356;&#26032;&#36827;&#34892;&#25237;&#24433;&#65292;ii) &#20316;&#20026;&#22312;&#34987;&#33609;&#22270;&#21270;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;Newton&#26041;&#27861;&#65292;&#21644; iii) &#20316;&#20026;&#22312;&#34987;&#33609;&#22270;&#21270;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#38459;&#23612;Newton&#26041;&#27861;&#12290;SGN&#32487;&#25215;&#20102;&#36825;&#19977;&#20010;&#26041;&#38754;&#30340;&#20248;&#28857;&#65306;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;$\mathcal O(k^{-2})$&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#19982;&#22522;&#20934;&#31639;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the first sketch-and-project Newton method with fast $\mathcal O(k^{-2})$ global convergence rate for self-concordant functions. Our method, SGN, can be viewed in three ways: i) as a sketch-and-project algorithm projecting updates of Newton method, ii) as a cubically regularized Newton ethod in sketched subspaces, and iii) as a damped Newton method in sketched subspaces. SGN inherits best of all three worlds: cheap iteration costs of sketch-and-project methods, state-of-the-art $\mathcal O(k^{-2})$ global convergence rate of full-rank Newton-like methods and the algorithm simplicity of damped Newton methods. Finally, we demonstrate its comparable empirical performance to baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08698</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08698
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;MKGC&#65289;&#28041;&#21450;&#20351;&#29992;&#22810;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#21019;&#24314;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MKGC&#27169;&#22411;&#22312;&#22788;&#29702;&#21160;&#24577;&#29616;&#23454;&#22330;&#26223;&#20013;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#35774;&#32622;&#20027;&#35201;&#20851;&#27880;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#22810;&#27169;&#24577;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25506;&#32034;&#36830;&#32493;MKGC&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#30830;&#20445;&#20445;&#30041;&#20174;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36807;&#21435;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24320;&#21457;&#32456;&#36523;MKGC&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#36825;&#20010;&#22797;&#26434;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#32463;&#39564;&#21457;&#29616;&#65292;&#24403;&#22810;&#23186;&#20307;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;&#19968;&#20123;&#20856;&#22411;&#30340;MKGC&#27169;&#22411;&#21487;&#33021;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#24847;&#22806;&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#37027;&#20123;&#20165;&#21033;&#29992;&#25991;&#26412;&#36164;&#28304;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#20197;&#23454;&#39564;&#35777;&#25454;&#20026;&#22522;&#30784;&#65292;&#24635;&#32467;&#20986;&#20197;&#19979;&#35770;&#28857;&#65306;&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#25968;&#25454;&#28304;&#21464;&#21270;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
&lt;/p&gt;</description></item><item><title>Speck&#26159;&#19968;&#27454;&#26234;&#33021;&#35270;&#35273;&#20256;&#24863;&#22120;SoC&#65292;&#35813;&#33455;&#29255;&#38598;&#25104;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#30456;&#26426;&#21644;&#20302;&#21151;&#32791;&#30340;sCNN&#35745;&#31639;&#26550;&#26500;&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#21333;&#20803;&#29983;&#20135;&#25104;&#26412;&#12290;&#36890;&#36807;&#20248;&#21270;&#39640;&#24230;&#31232;&#30095;&#30340;&#35745;&#31639;&#21644;&#26368;&#23567;&#21270;&#24310;&#36831;&#65292;Speck&#21487;&#20197;&#22788;&#29702;&#39640;&#36895;&#30340;&#31232;&#30095;&#25968;&#25454;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.06793</link><description>&lt;p&gt;
Speck: &#19968;&#27454;&#25317;&#26377;&#20302;&#24310;&#36831;327K&#31070;&#32463;&#20803;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#31649;&#36947;&#30340;&#26234;&#33021;&#20107;&#20214;&#22411;&#35270;&#35273;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline. (arXiv:2304.06793v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06793
&lt;/p&gt;
&lt;p&gt;
Speck&#26159;&#19968;&#27454;&#26234;&#33021;&#35270;&#35273;&#20256;&#24863;&#22120;SoC&#65292;&#35813;&#33455;&#29255;&#38598;&#25104;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#30456;&#26426;&#21644;&#20302;&#21151;&#32791;&#30340;sCNN&#35745;&#31639;&#26550;&#26500;&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#21333;&#20803;&#29983;&#20135;&#25104;&#26412;&#12290;&#36890;&#36807;&#20248;&#21270;&#39640;&#24230;&#31232;&#30095;&#30340;&#35745;&#31639;&#21644;&#26368;&#23567;&#21270;&#24310;&#36831;&#65292;Speck&#21487;&#20197;&#22788;&#29702;&#39640;&#36895;&#30340;&#31232;&#30095;&#25968;&#25454;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#20013;&#25552;&#21462;&#39640;&#32423;&#20449;&#24687;&#30340;&#36793;&#32536;&#35745;&#31639;&#26041;&#26696;&#27491;&#22312;&#26085;&#30410;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#35270;&#35273;&#20256;&#24863;&#22120;SoC&#65292;&#20855;&#26377;&#22522;&#20110;&#20107;&#20214;&#30340;&#30456;&#26426;&#21644;&#20302;&#21151;&#32791;&#30340;sCNN&#35745;&#31639;&#26550;&#26500;&#12290;&#35813;SoC&#30340;&#31471;&#21040;&#31471;&#35774;&#35745;&#31616;&#21333;&#65292;&#21487;&#20316;&#20026;&#29420;&#31435;&#24212;&#29992;&#25110;&#20316;&#20026;&#36739;&#22823;&#31995;&#32479;&#20013;&#30340;&#36793;&#32536;&#33410;&#28857;&#12290;&#32508;&#21512;&#20256;&#24863;&#22120;&#21644;&#22788;&#29702;&#20110;&#21333;&#20010;&#33455;&#29255;&#20013;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21333;&#20803;&#29983;&#20135;&#25104;&#26412;&#12290;&#22312;&#22788;&#29702;&#31649;&#36947;&#20013;&#65292;&#23545;&#39640;&#24230;&#31232;&#30095;&#30340;&#35745;&#31639;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#23558;&#24310;&#36831;&#26368;&#23567;&#21270;&#20026;$3.36\mu s$&#65292;&#20197;&#24212;&#23545;&#35270;&#35273;&#20256;&#24863;&#22120;&#30340;&#20107;&#20214;&#39537;&#21160;&#20449;&#21495;&#31561;&#39640;&#36895;&#20449;&#21495;&#30340;&#31232;&#30095;&#25968;&#25454;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge computing solutions that enable the extraction of high level information from a variety of sensors is in increasingly high demand. This is due to the increasing number of smart devices that require sensory processing for their application on the edge. To tackle this problem, we present a smart vision sensor System on Chip (Soc), featuring an event-based camera and a low power asynchronous spiking Convolutional Neuronal Network (sCNN) computing architecture embedded on a single chip. By combining both sensor and processing on a single die, we can lower unit production costs significantly. Moreover, the simple end-to-end nature of the SoC facilitates small stand-alone applications as well as functioning as an edge node in a larger systems. The event-driven nature of the vision sensor delivers high-speed signals in a sparse data stream. This is reflected in the processing pipeline, focuses on optimising highly sparse computation and minimising latency for 9 sCNN layers to $3.36\mu s$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.06701</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Decision Support Policies. (arXiv:2304.06701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#20915;&#31574;&#32773;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#26469;&#25552;&#39640;&#20915;&#31574;&#32467;&#26524;&#65292;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#65292;&#21738;&#31181;&#24418;&#24335;&#30340;&#25903;&#25345;&#20250;&#22312;&#20302;&#25104;&#26412;&#19979;&#23548;&#33268;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#32473;&#23450;&#36755;&#20837;&#26102;&#36873;&#25321;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#30340;&#20915;&#31574;&#32773;&#65292;&#24182;&#23558;&#23398;&#20064;&#21508;&#33258;&#30340;&#31574;&#30053;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20010;&#38382;&#39064;&#26435;&#34913;&#20102;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#12290;&#20351;&#29992;&#38543;&#26426;&#29615;&#22659;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; $\texttt{THREAD}$&#65292;&#36825;&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#26469;&#30830;&#23450;&#25104;&#26412;-&#24615;&#33021;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#35745;&#31639;&#23454;&#39564;&#26469;&#35777;&#26126; $\texttt{THREAD}$ &#30456;&#23545;&#20110;&#32447;&#19979;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855; $\texttt{Modiste}$&#65292;&#23427;&#20026;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#35786;&#26029;&#25552;&#20379;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#12290;$\texttt{Modiste}$ &#20351;&#29992; $\texttt{THREAD}$ &#20026;&#27599;&#20301;&#21307;&#29983;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#24182;&#25512;&#33616;&#20010;&#24615;&#21270;&#30740;&#31350;&#20197;&#20248;&#21270;&#24739;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#24182;&#23558;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#38477;&#33267;&#26368;&#20302;&#12290;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; $\texttt{Modiste}$ &#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual human decision-makers may benefit from different forms of support to improve decision outcomes. However, a key question is which form of support will lead to accurate decisions at a low cost. In this work, we propose learning a decision support policy that, for a given input, chooses which form of support, if any, to provide. We consider decision-makers for whom we have no prior information and formalize learning their respective policies as a multi-objective optimization problem that trades off accuracy and cost. Using techniques from stochastic contextual bandits, we propose $\texttt{THREAD}$, an online algorithm to personalize a decision support policy for each decision-maker, and devise a hyper-parameter tuning strategy to identify a cost-performance trade-off using simulated human behavior. We provide computational experiments to demonstrate the benefits of $\texttt{THREAD}$ compared to offline baselines. We then introduce $\texttt{Modiste}$, an interactive tool that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05749</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation. (arXiv:2304.05749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#65288;CTDGNs&#65289;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65288;LTF&#65289;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;CTDGN&#23545;&#20110;&#24314;&#27169;&#26102;&#38388;&#22270;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20294;&#30001;&#20110;&#23545;&#21382;&#21490;&#25968;&#25454;&#30340;&#23454;&#36136;&#35201;&#27714;&#65292;&#23427;&#20204;&#22312;LTF&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20063;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#24341;&#20837;CTDGN&#30340;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#65292;&#24182;&#36827;&#34892;&#25513;&#34109;&#28151;&#21512;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#26356;&#22810;&#24773;&#20917;&#65292;&#32780;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#21040;&#20219;&#24847;CTDGN&#20013;&#65292;&#32780;&#19981;&#22686;&#21152;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;UmmU&#22312;LTF&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \textbf{\underline{U}ncertainty \underline{M}asked \underline{M}ix\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.16668</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#25308;&#21344;&#24237;&#23481;&#38169;&#32858;&#21512;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;FLANDERS&#23558;&#27599;&#20010;FL&#36718;&#27425;&#20013;&#30001;&#23458;&#25143;&#31471;&#21457;&#36865;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#35270;&#20026;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#35266;&#27979;&#19982;&#30001;&#30697;&#38453;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#30340;&#35266;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#20316;&#20026;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#20540;&#12290;&#22312;&#19981;&#21516;FL&#35774;&#32622;&#19979;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FLANDERS&#22312;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#26041;&#38754;&#19982;&#26368;&#24378;&#22823;&#30340;&#22522;&#32447;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#30456;&#27604;&#65292; FLANDERS&#21363;&#20351;&#22312;&#26497;&#20854;&#20005;&#37325;&#30340;&#25915;&#20987;&#22330;&#26223;&#19979;&#20173;&#28982;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12012</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#30340;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#26816;&#26597;&#65292;&#35760;&#24405;&#22823;&#33041;&#30340;&#30005;&#27963;&#21160;&#12290;&#35813;&#26816;&#26597;&#29992;&#20110;&#24110;&#21161;&#35786;&#26029;&#21508;&#31181;&#33041;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#30315;&#30187;&#26816;&#27979;&#12290;&#22312;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20013;&#65292;&#20027;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;EEG&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#23545;&#20110;&#26816;&#27979;&#24433;&#21709;&#22823;&#33041;&#30340;&#30142;&#30149;&#24456;&#26377;&#29992;&#12290;&#26377;&#26102;&#65292;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20869;&#35782;&#21035;EEG&#30340;&#26368;&#23567;&#21464;&#21270;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;DWT&#21487;&#20197;&#22312;&#19981;&#21516;&#39057;&#24102;&#36827;&#34892;&#20449;&#21495;&#33391;&#22909;&#30340;&#20998;&#35299;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#38477;&#32500;&#31639;&#27861;&#65306;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#34701;&#21512;&#35268;&#21017;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11405</link><description>&lt;p&gt;
&#21028;&#21035;&#29109;&#32858;&#31867;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Discriminative Entropy Clustering and its Relation to K-means and SVM. (arXiv:2301.11405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#21035;&#27169;&#22411;&#20013;&#65292;&#26368;&#22823;&#21270;&#27169;&#22411;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#24418;&#24335;&#19978;&#19982; softmax &#39044;&#27979;&#30340;&#8220;&#20915;&#31574;&#24615;&#8221;&#21644;&#8220;&#20844;&#24179;&#24615;&#8221;&#26377;&#20851;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#22522;&#20110;&#29109;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#12290; &#26368;&#36817;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#25105;&#26631;&#35760;&#26041;&#27861;&#20195;&#34920;&#20102;&#28145;&#24230;&#32858;&#31867;&#30340;&#26368;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290; &#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#29109;&#32858;&#31867;&#26041;&#27861;&#30340;&#35768;&#22810;&#36890;&#29992;&#23646;&#24615;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982; K-means &#21644;&#26080;&#30417;&#30563; SVM &#25216;&#26415;&#30340;&#20851;&#31995;&#12290; &#25105;&#20204;&#35777;&#26126;&#19982; K-&#22343;&#20540;&#26377;&#30528;&#26681;&#26412;&#30340;&#21306;&#21035;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#19982;&#22522;&#20110; SVM &#30340;&#32858;&#31867;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26174;&#24335;&#30340;&#36793;&#38469;&#26368;&#22823;&#21270;&#19982;&#29109;&#32858;&#31867;&#32852;&#31995;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20132;&#21449;&#29109;&#30340;&#24120;&#35265;&#24418;&#24335;&#23545;&#20110;&#20266;&#26631;&#31614;&#38169;&#35823;&#19981;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340; EM &#31639;&#27861;&#65292;&#22312;&#35768;&#22810;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximization of mutual information between the model's input and output is formally related to "decisiveness" and "fairness" of the softmax predictions, motivating such unsupervised entropy-based losses for discriminative models. Recent self-labeling methods based on such losses represent the state of the art in deep clustering. First, we discuss a number of general properties of such entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques. Disproving some earlier published claims, we point out fundamental differences with K-means. On the other hand, we show similarity with SVM-based clustering allowing us to link explicit margin maximization to entropy clustering. Finally, we observe that the common form of cross-entropy is not robust to pseudo-label errors. Our new loss addresses the problem and leads to a new EM algorithm improving the state of the art on many standard benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#22312;&#26377;&#21644;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2211.11435</link><description>&lt;p&gt;
ZigZag: &#36890;&#36807;&#20004;&#27493;&#25512;&#29702;&#23454;&#29616;&#30340;&#36890;&#29992;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference. (arXiv:2211.11435v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#22312;&#26377;&#21644;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#32593;&#32476;&#20135;&#29983;&#26377;&#29992;&#30340;&#39044;&#27979;&#30340;&#33021;&#21147;&#24050;&#32463;&#34987;&#20805;&#20998;&#35777;&#26126;&#65292;&#20294;&#26159;&#20272;&#35745;&#36825;&#20123;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35832;&#22914;MC-Dropout&#21644;Deep Ensembles&#20043;&#31867;&#30340;&#37319;&#26679;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#26368;&#27969;&#34892;&#30340;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#26102;&#38656;&#35201;&#36827;&#34892;&#35768;&#22810;&#21069;&#21521;&#20256;&#36882;&#65292;&#36825;&#20250;&#20943;&#24930;&#36895;&#24230;&#12290;&#26080;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#26356;&#24555;&#65292;&#20294;&#23384;&#22312;&#20854;&#20182;&#32570;&#28857;&#65292;&#20363;&#22914;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#20449;&#24230;&#36739;&#20302;&#12289;&#20351;&#29992;&#22256;&#38590;&#20197;&#21450;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;&#26080;&#37319;&#26679;&#26041;&#27861;&#65292;&#23427;&#20197;&#26126;&#26174;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#12290;&#20854;&#22522;&#26412;&#21407;&#29702;&#26159;&#35757;&#32451;&#32593;&#32476;&#22312;&#26377;&#21644;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#24403;&#27809;&#26377;&#25552;&#20379;&#20808;&#39564;&#20449;&#24687;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#32593;&#32476;&#30340;&#33258;&#36523;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whereas the ability of deep networks to produce useful predictions has been amply demonstrated, estimating the reliability of these predictions remains challenging. Sampling approaches such as MC-Dropout and Deep Ensembles have emerged as the most popular ones for this purpose. Unfortunately, they require many forward passes at inference time, which slows them down. Sampling-free approaches can be faster but suffer from other drawbacks, such as lower reliability of uncertainty estimates, difficulty of use, and limited applicability to different types of tasks and data.  In this work, we introduce a sampling-free approach that is generic and easy to deploy, while producing reliable uncertainty estimates on par with state-of-the-art methods at a significantly lower computational cost. It is predicated on training the network to produce the same output with and without additional information about it. At inference time, when no prior information is given, we use the network's own predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.16205</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31363;&#21548;&#30446;&#26631;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#24182;&#37325;&#26500;&#21463;&#23475;&#32773;&#30340;&#26412;&#22320;/&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#32463;&#20856;&#25915;&#20987;&#12290;&#26412;&#22320;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#27844;&#28431;&#27604;&#26381;&#21153;&#22120;&#23398;&#20064;&#30340;&#20840;&#23616;&#27169;&#22411;&#26356;&#22810;&#30340;&#31169;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#21033;&#29992;&#20102;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#20998;&#26512;&#19979;&#30028;&#12290;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26412;&#22320;&#37325;&#26500;&#25915;&#20987;&#23545;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#37117;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#19982;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we initiate the study of local model reconstruction attacks for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between a targeted client and the server, and then reconstructs the local/personalized model of the victim. The local model reconstruction attack allows the adversary to trigger other classical attacks in a more effective way, since the local model only depends on the client's data and can leak more private information than the global model learned by the server. Additionally, we propose a novel model-based attribute inference attack in federated learning leveraging the local model reconstruction attack. We provide an analytical lower-bound for this attribute inference attack. Empirical results using real world datasets confirm that our local reconstruction attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in 
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20016;&#23500;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2202.12040</link><description>&lt;p&gt;
&#33258;&#20027;&#35757;&#32451;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Training: A Survey. (arXiv:2202.12040v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12040
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20016;&#23500;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#31639;&#27861;&#26088;&#22312;&#20174;&#23569;&#37327;&#26377;&#26631;&#31614;&#35266;&#27979;&#21644;&#22823;&#37327;&#26080;&#26631;&#31614;&#35266;&#27979;&#20013;&#23398;&#20064;&#39044;&#27979;&#20989;&#25968;&#12290;&#30001;&#20110;&#36825;&#20010;&#26694;&#26550;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#30456;&#20851;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#29616;&#26377;&#30340;&#25216;&#26415;&#20013;&#65292;&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#22312;&#36817;&#24180;&#26469;&#26080;&#30097;&#24341;&#36215;&#20102;&#26356;&#22823;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#32780;&#19981;&#23545;&#25968;&#25454;&#20998;&#24067;&#20316;&#20986;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26080;&#31526;&#21495;&#36755;&#20986;&#20998;&#25968;&#25110;&#20854;&#36793;&#30028;&#20316;&#20026;&#32622;&#20449;&#24230;&#30340;&#25351;&#26631;&#12290;&#33258;&#20027;&#35757;&#32451;&#31639;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#26159;&#36890;&#36807;&#32473;&#20855;&#26377;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#30340;&#36793;&#30028;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20266;&#26631;&#35760;&#30340;&#31034;&#20363;&#26469;&#22686;&#24378;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19982;&#26377;&#26631;&#31614;&#35757;&#32451;&#38598;&#19968;&#36215;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#30340;&#23574;&#38160;&#19978;&#30028;&#65292;&#20026;&#24191;&#27867;&#30340;&#32447;&#24615;&#27867;&#20989;&#22240;&#26524;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#20256;&#32479;&#30340;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#30446;&#26631;&#65292;&#24182;&#19988;&#20165;&#21462;&#20915;&#20110;&#28508;&#21464;&#37327;&#22312;&#32467;&#26524;&#21644;&#21442;&#25968;&#30340;Riesz&#34920;&#31034;&#22120;&#20013;&#25152;&#23548;&#33268;&#30340;&#39069;&#22806;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2112.13398</link><description>&lt;p&gt;
&#12298;&#38271;&#35805;&#30701;&#35828;&#65306;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Long Story Short: Omitted Variable Bias in Causal Machine Learning. (arXiv:2112.13398v4 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13398
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#30340;&#23574;&#38160;&#19978;&#30028;&#65292;&#20026;&#24191;&#27867;&#30340;&#32447;&#24615;&#27867;&#20989;&#22240;&#26524;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#20256;&#32479;&#30340;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#30446;&#26631;&#65292;&#24182;&#19988;&#20165;&#21462;&#20915;&#20110;&#28508;&#21464;&#37327;&#22312;&#32467;&#26524;&#21644;&#21442;&#25968;&#30340;Riesz&#34920;&#31034;&#22120;&#20013;&#25152;&#23548;&#33268;&#30340;&#39069;&#22806;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#22240;&#26524;&#21442;&#25968;&#30340;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#30340;&#19968;&#33324;&#20294;&#31616;&#21333;&#30340;&#23574;&#38160;&#19978;&#30028;&#65292;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#34987;&#35748;&#23450;&#20026;&#32467;&#26524;&#30340;&#26465;&#20214;&#26399;&#26395;&#20989;&#25968;&#30340;&#32447;&#24615;&#27867;&#20989;&#12290;&#36825;&#26679;&#30340;&#27867;&#20989;&#21253;&#25324;&#35768;&#22810;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#35843;&#26597;&#30446;&#26631;&#65292;&#20363;&#22914;&#65288;&#21152;&#26435;&#65289;&#28508;&#22312;&#32467;&#26524;&#30340;&#24179;&#22343;&#20540;&#12289;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;&#21253;&#25324;&#23376;&#32452;&#25928;&#24212;&#65292;&#22914;&#23545;&#24453;&#22788;&#29702;&#23545;&#35937;&#30340;&#24433;&#21709;&#65289;&#12289;&#65288;&#21152;&#26435;&#65289;&#24179;&#22343;&#23548;&#25968;&#21644;&#26469;&#33258;&#21327;&#21464;&#37327;&#20998;&#24067;&#21464;&#21270;&#30340;&#31574;&#30053;&#25928;&#24212; - &#20840;&#37096;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#38750;&#21442;&#25968;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#20381;&#36182;&#20110;&#30446;&#26631;&#27867;&#20989;&#30340;Riesz-Fr&#233;chet&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20559;&#24046;&#19978;&#30028;&#20165;&#21462;&#20915;&#20110;&#28508;&#21464;&#37327;&#22312;&#32467;&#26524;&#21644;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;Riesz&#34920;&#31034;&#22120;&#20013;&#25152;&#21019;&#24314;&#30340;&#38468;&#21152;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#21644;&#24179;&#22343;&#23548;&#25968;&#65289;
&lt;/p&gt;
&lt;p&gt;
We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example, (weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated), (weighted) average derivatives, and policy effects from shifts in covariate distribution -- all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (e.g, average treatment effects and avearage derivative
&lt;/p&gt;</description></item></channel></rss>