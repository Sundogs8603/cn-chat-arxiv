<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#34987;&#21160;&#25910;&#38598;&#25968;&#25454;&#31867;&#22411;&#23545;&#20110;&#39044;&#27979;&#21387;&#21147;&#21644;&#25233;&#37057;&#30340;&#36129;&#29486;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WiFi&#29305;&#24449;&#21644;&#30005;&#35805;&#26085;&#24535;&#29305;&#24449;&#23545;&#20110;&#21387;&#21147;&#21644;&#25233;&#37057;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.13607</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#21516;&#34987;&#21160;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;&#20110;&#39044;&#27979;&#21387;&#21147;&#21644;&#25233;&#37057;&#30340;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Analyzing the contribution of different passively collected data to predict Stress and Depression. (arXiv:2310.13607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#34987;&#21160;&#25910;&#38598;&#25968;&#25454;&#31867;&#22411;&#23545;&#20110;&#39044;&#27979;&#21387;&#21147;&#21644;&#25233;&#37057;&#30340;&#36129;&#29486;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WiFi&#29305;&#24449;&#21644;&#30005;&#35805;&#26085;&#24535;&#29305;&#24449;&#23545;&#20110;&#21387;&#21147;&#21644;&#25233;&#37057;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#34987;&#21160;&#25429;&#33719;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#20154;&#31867;&#34892;&#20026;&#21644;&#29615;&#22659;&#32972;&#26223;&#30340;&#21487;&#33021;&#24615;&#25512;&#21160;&#20102;&#20854;&#22312;&#24515;&#29702;&#20581;&#24247;&#35780;&#20272;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19981;&#21516;&#34987;&#21160;&#25910;&#38598;&#20256;&#24863;&#22120;&#25968;&#25454;&#31867;&#22411;&#65288;WiFi&#12289;GPS&#12289;&#31038;&#20132;&#20114;&#21160;&#12289;&#30005;&#35805;&#26085;&#24535;&#12289;&#20307;&#21147;&#27963;&#21160;&#12289;&#38899;&#39057;&#21644;&#23398;&#26415;&#29305;&#24449;&#65289;&#23545;&#20110;&#39044;&#27979;&#27599;&#26085;&#30340;&#33258;&#25105;&#25253;&#21578;&#21387;&#21147;&#21644;PHQ-9&#25233;&#37057;&#24471;&#20998;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#21407;&#22987;&#21407;&#22987;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;125&#20010;&#20013;&#32423;&#29305;&#24449;&#12290;&#36825;&#20123;125&#20010;&#29305;&#24449;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#31867;&#22411;&#30340;&#29305;&#24449;&#32452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19982;&#20351;&#29992;&#29305;&#23450;&#29305;&#24449;&#32452;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26469;&#35780;&#20272;&#27599;&#20010;&#29305;&#24449;&#31867;&#22411;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;WiFi&#29305;&#24449;&#65288;&#32534;&#30721;&#31227;&#21160;&#27169;&#24335;&#65289;&#21644;&#30005;&#35805;&#26085;&#24535;&#29305;&#24449;&#65288;&#32534;&#30721;&#19982;&#30561;&#30496;&#27169;&#24335;&#30456;&#20851;&#30340;&#20449;&#24687;&#65289;&#20026;&#21387;&#21147;&#21644;&#25233;&#37057;&#39044;&#27979;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The possibility of recognizing diverse aspects of human behavior and environmental context from passively captured data motivates its use for mental health assessment. In this paper, we analyze the contribution of different passively collected sensor data types (WiFi, GPS, Social interaction, Phone Log, Physical Activity, Audio, and Academic features) to predict daily selfreport stress and PHQ-9 depression score. First, we compute 125 mid-level features from the original raw data. These 125 features include groups of features from the different sensor data types. Then, we evaluate the contribution of each feature type by comparing the performance of Neural Network models trained with all features against Neural Network models trained with specific feature groups. Our results show that WiFi features (which encode mobility patterns) and Phone Log features (which encode information correlated with sleep patterns), provide significative information for stress and depression prediction.
&lt;/p&gt;</description></item><item><title>ReLM&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#36741;&#21161;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20174;&#32780;&#25552;&#39640;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26032;&#26694;&#26550;,&#22312;&#19981;&#21516;&#21270;&#23398;&#21453;&#24212;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2310.13590</link><description>&lt;p&gt;
ReLM:&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction. (arXiv:2310.13590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13590
&lt;/p&gt;
&lt;p&gt;
ReLM&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#36741;&#21161;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20174;&#32780;&#25552;&#39640;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26032;&#26694;&#26550;,&#22312;&#19981;&#21516;&#21270;&#23398;&#21453;&#24212;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21270;&#23398;&#21453;&#24212;&#26159;&#21270;&#23398;&#20013;&#19968;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#20174;&#32473;&#23450;&#30340;&#21453;&#24212;&#36807;&#31243;&#20013;&#39044;&#27979;&#20986;&#20135;&#29289;&#12290;&#20256;&#32479;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25216;&#26415;&#65292;&#36890;&#24120;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#21644;&#26080;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;ReLM&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#32534;&#30721;&#30340;&#21270;&#23398;&#30693;&#35782;&#26469;&#36741;&#21161;GNN&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23454;&#38469;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32622;&#20449;&#24230;&#35780;&#20998;&#31574;&#30053;&#65292;&#20351;LM&#33021;&#22815;&#33258;&#25105;&#35780;&#20272;&#20854;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReLM&#25913;&#36827;&#20102;&#21508;&#31181;&#21270;&#23398;&#21453;&#24212;&#25968;&#25454;&#38598;&#19978;&#20808;&#36827;GNN&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model's robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#26641;&#24863;&#30693;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#33021;&#21147;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35821;&#35328;&#29305;&#23450;&#35268;&#21017;&#12289;POS&#26631;&#31614;&#32423;&#21035;&#25110;&#21482;&#38024;&#23545;&#20027;&#21477;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.13583</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#26641;&#24863;&#30693;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#25552;&#21319;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering. (arXiv:2310.13583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13583
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#26641;&#24863;&#30693;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#33021;&#21147;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35821;&#35328;&#29305;&#23450;&#35268;&#21017;&#12289;POS&#26631;&#31614;&#32423;&#21035;&#25110;&#21482;&#38024;&#23545;&#20027;&#21477;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;XLM-R&#21644;mT5&#65289;&#30340;&#33021;&#21147;&#26377;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22686;&#38271;&#65292;&#20294;&#24050;&#32463;&#21457;&#29616;&#23427;&#20204;&#22312;&#22788;&#29702;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#21333;&#35789;&#39034;&#24207;&#27169;&#24335;&#30340;&#21487;&#21464;&#24615;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#28304;&#31471;&#25110;&#30446;&#26631;&#31471;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#26469;&#20943;&#36731;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#37325;&#25490;&#24207;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#29305;&#23450;&#35821;&#35328;&#30340;&#35268;&#21017;&#65292;&#24037;&#20316;&#22312;POS&#26631;&#31614;&#30340;&#32423;&#21035;&#19978;&#65292;&#25110;&#32773;&#21482;&#38024;&#23545;&#20027;&#21477;&#65292;&#32780;&#20445;&#25345;&#20174;&#23646;&#20174;&#21477;&#19981;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#22823;&#30340;&#37325;&#25490;&#24207;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#36890;&#29992;&#20381;&#36182;&#20851;&#31995;&#26469;&#23450;&#20041;&#65292;&#33021;&#22815;&#21033;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#20197;&#21477;&#27861;&#19978;&#19979;&#25991;&#20026;&#26465;&#20214;&#30340;&#32454;&#31890;&#24230;&#21333;&#35789;&#39034;&#24207;&#27169;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21477;&#27861;&#26641;&#30340;&#25152;&#26377;&#23618;&#32423;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via sourceor target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.13576</link><description>&lt;p&gt;
&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#25112;&#30053;&#20915;&#31574;&#21040;&#29983;&#29289;&#23398;&#21644;&#32463;&#27982;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36880;&#27493;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#25105;&#20204;&#36824;&#24418;&#24335;&#21270;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#25490;&#38500;&#20250;&#24341;&#20837;&#24490;&#29615;&#30340;&#36793;&#30340;&#39640;&#25928;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20351;&#24471;&#22312;DAG&#31354;&#38388;&#20013;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#31163;&#25955;&#25628;&#32034;&#21644;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#36825;&#26159;&#32452;&#21512;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#30340;&#26041;&#27861;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;&#24369;&#35821;&#20041;&#20808;&#39564;&#21644;&#36328;&#23610;&#24230;&#30456;&#20851;&#20808;&#39564;&#30693;&#35782;&#36880;&#27493;&#25913;&#36827;&#20998;&#21106;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#20998;&#21106;&#24615;&#33021;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.13574</link><description>&lt;p&gt;
&#22522;&#20110;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#30340;&#27867;&#21270;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Progressive Dual Priori Network for Generalized Breast Tumor Segmentation. (arXiv:2310.13574v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#30340;&#26041;&#27861;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;&#24369;&#35821;&#20041;&#20808;&#39564;&#21644;&#36328;&#23610;&#24230;&#30456;&#20851;&#20808;&#39564;&#30693;&#35782;&#36880;&#27493;&#25913;&#36827;&#20998;&#21106;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#20998;&#21106;&#24615;&#33021;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25913;&#36827;&#23545;&#23567;&#23610;&#23544;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#19981;&#35268;&#21017;&#24418;&#29366;&#20083;&#33146;&#32959;&#30244;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#65288;PDPNet&#65289;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#26426;&#26500;&#37319;&#38598;&#30340;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;DCE-MRI&#65289;&#20013;&#20998;&#21106;&#20083;&#33146;&#32959;&#30244;&#12290;PDPNet&#39318;&#20808;&#36890;&#36807;&#31895;&#20998;&#21106;&#23450;&#20301;&#27169;&#22359;&#35009;&#21098;&#32959;&#30244;&#21306;&#22495;&#65292;&#28982;&#21518;&#20351;&#29992;&#24369;&#35821;&#20041;&#20808;&#39564;&#21644;&#36328;&#23610;&#24230;&#30456;&#20851;&#20808;&#39564;&#30693;&#35782;&#36880;&#27493;&#25913;&#36827;&#20083;&#33146;&#32959;&#30244;&#25513;&#33180;&#12290;&#20026;&#20102;&#39564;&#35777;PDPNet&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20122;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;PDPNet&#30340;DSC&#12289;SEN&#12289;KAPPA&#21644;HD95&#20998;&#21035;&#25552;&#39640;&#20102;3.63&#65285;&#12289;8.19&#65285;&#12289;5.52&#65285;&#21644;3.66&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#23450;&#20301;&#27169;&#22359;&#21487;&#20197;&#20943;&#23569;&#27491;&#24120;&#32452;&#32455;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To promote the generalization ability of breast tumor segmentation models, as well as to improve the segmentation performance for breast tumors with smaller size, low-contrast amd irregular shape, we propose a progressive dual priori network (PDPNet) to segment breast tumors from dynamic enhanced magnetic resonance images (DCE-MRI) acquired at different sites. The PDPNet first cropped tumor regions with a coarse-segmentation based localization module, then the breast tumor mask was progressively refined by using the weak semantic priori and cross-scale correlation prior knowledge. To validate the effectiveness of PDPNet, we compared it with several state-of-the-art methods on multi-center datasets. The results showed that, comparing against the suboptimal method, the DSC, SEN, KAPPA and HD95 of PDPNet were improved 3.63\%, 8.19\%, 5.52\%, and 3.66\% respectively. In addition, through ablations, we demonstrated that the proposed localization module can decrease the influence of normal t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#21452;&#35895;&#29616;&#35937;&#26159;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#35895;&#29616;&#35937;&#30340;&#35299;&#37322;&#65306;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2310.13572</link><description>&lt;p&gt;
&#35299;&#24320;&#21452;&#35895;&#20043;&#35868;&#65306;&#36879;&#36807;&#23398;&#24471;&#29305;&#24449;&#31354;&#38388;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space. (arXiv:2310.13572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#21452;&#35895;&#29616;&#35937;&#26159;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#35895;&#29616;&#35937;&#30340;&#35299;&#37322;&#65306;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35895;&#29616;&#35937;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#21576;&#29616;&#20986;&#19968;&#31181;&#36870;&#30452;&#35273;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#20854;&#34920;&#29616;&#12290;&#34429;&#28982;&#23545;&#35813;&#29616;&#35937;&#22312;&#29305;&#23450;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#20123;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#20173;&#27809;&#26377;&#30830;&#31435;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#21452;&#35895;&#30340;&#20844;&#35748;&#29702;&#35770;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21452;&#35895;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20986;&#29616;&#21463;&#21040;&#22122;&#22768;&#25968;&#25454;&#24433;&#21709;&#30340;&#24378;&#28872;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20986;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20250;&#20986;&#29616;&#21452;&#35895;&#29616;&#35937;&#12290;&#25105;&#20204;&#35748;&#20026;&#21452;&#35895;&#26159;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#33719;&#21462;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#21452;&#35895;&#29616;&#35937;&#19981;&#24212;&#35813;&#22312;&#33391;&#22909;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#20013;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory to account for its occurrence in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that its occurrence is strongly influenced by the presence of noisy data. Through conducting a comprehensive analysis of the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that double descent is a consequence of the model first learning the noisy data until interpolation and then adding implicit regularization via over-parameterization acquiring therefore capability to separate the information from the noise. We postulate that double descent should never occur in well-regulari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20449;&#21495;&#30340;&#29305;&#28857;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13565</link><description>&lt;p&gt;
&#20026;&#26356;&#24555;&#20048;&#30340;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#20195;&#29702;&#35774;&#35745;&#30340;&#22870;&#21169;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Reward Shaping for Happier Autonomous Cyber Security Agents. (arXiv:2310.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20449;&#21495;&#30340;&#29305;&#28857;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#22686;&#21152;&#30340;&#28508;&#21147;&#12290;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#20219;&#21153;&#20013;&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#27492;&#20219;&#21153;&#26102;&#20026;&#20195;&#29702;&#25552;&#20379;&#30340;&#22870;&#21169;&#20449;&#21495;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#24615;&#36136;&#65292;&#22870;&#21169;&#20449;&#21495;&#36890;&#24120;&#37319;&#29992;&#20197;&#19979;&#24418;&#24335;&#65306;1&#65289;&#20197;&#24809;&#32602;&#30340;&#24418;&#24335;&#65288;&#20363;&#22914;&#21457;&#29983;&#20102;&#22949;&#21327;&#65289;&#65307;2&#65289;&#22312;&#27599;&#27425;&#38450;&#24481;&#20219;&#21153;&#20013;&#31232;&#30095;&#20998;&#24067;&#12290;&#36825;&#26679;&#30340;&#22870;&#21169;&#29305;&#24449;&#19981;&#21516;&#20110;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#20013;&#20195;&#29702;&#23450;&#26399;&#33719;&#24471;&#36827;&#23637;&#30340;&#22870;&#21169;&#65288;&#32780;&#19981;&#26159;&#20598;&#23572;&#22240;&#22833;&#36133;&#32780;&#21463;&#21040;&#24809;&#32602;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33021;&#22815;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#65292;&#20197;&#20351;&#20195;&#29702;&#26356;&#33021;&#26377;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#28508;&#22312;&#22320;&#25910;&#25947;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#22870;&#21169;&#24133;&#24230;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models become more capable, they have exhibited increased potential in solving complex tasks. One of the most promising directions uses deep reinforcement learning to train autonomous agents in computer network defense tasks. This work studies the impact of the reward signal that is provided to the agents when training for this task. Due to the nature of cybersecurity tasks, the reward signal is typically 1) in the form of penalties (e.g., when a compromise occurs), and 2) distributed sparsely across each defense episode. Such reward characteristics are atypical of classic reinforcement learning tasks where the agent is regularly rewarded for progress (cf. to getting occasionally penalized for failures). We investigate reward shaping techniques that could bridge this gap so as to enable agents to train more sample-efficiently and potentially converge to a better performance. We first show that deep reinforcement learning algorithms are sensitive to the magnitude of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#31070;&#32463;&#32531;&#23384;&#25216;&#26415;&#65292;&#20351;&#29992;&#36793;&#30028;&#25277;&#26679;&#21644;&#22996;&#21592;&#20250;&#26597;&#35810;&#20316;&#20026;&#20915;&#31574;&#31574;&#30053;&#65292;&#20248;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.13561</link><description>&lt;p&gt;
&#32531;&#23384;&#19982;&#31934;&#28860;&#65306;&#20248;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cache &amp; Distil: Optimising API Calls to Large Language Models. (arXiv:2310.13561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#31070;&#32463;&#32531;&#23384;&#25216;&#26415;&#65292;&#20351;&#29992;&#36793;&#30028;&#25277;&#26679;&#21644;&#22996;&#21592;&#20250;&#26597;&#35810;&#20316;&#20026;&#20915;&#31574;&#31574;&#30053;&#65292;&#20248;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#37096;&#32626;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#24448;&#24448;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;API&#35843;&#29992;&#26469;&#28385;&#36275;&#29992;&#25143;&#26597;&#35810;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#35843;&#29992;&#30340;&#39057;&#29575;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;--&#23398;&#29983;&#27169;&#22411;--&#19981;&#26029;&#22320;&#22312;LLM&#30340;&#21709;&#24212;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20010;&#23398;&#29983;&#27169;&#22411;&#36880;&#28176;&#29420;&#31435;&#22788;&#29702;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#35831;&#27714;&#65292;&#24182;&#36880;&#27493;&#25552;&#39640;&#20854;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#31070;&#32463;&#32531;&#23384;&#12290;&#31070;&#32463;&#32531;&#23384;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#19968;&#20010;&#20915;&#31574;&#31574;&#30053;&#65292;&#29992;&#20110;&#20915;&#23450;&#21738;&#20123;&#35831;&#27714;&#24212;&#30001;&#23398;&#29983;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#65292;&#21738;&#20123;&#35831;&#27714;&#24212;&#37325;&#23450;&#21521;&#32473;LLM&#65292;&#20174;&#32780;&#24110;&#21161;&#23398;&#29983;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#32463;&#20856;&#30340;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#36873;&#25321;&#20934;&#21017;&#20316;&#20026;&#35813;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36793;&#30028;&#25277;&#26679;&#21644;&#22996;&#21592;&#20250;&#26597;&#35810;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39044;&#31639;&#19979;&#24102;&#26469;&#19968;&#33268;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale deployment of generative AI tools often depends on costly API calls to a Large Language Model (LLM) to fulfil user queries. To curtail the frequency of these calls, one can employ a smaller language model -- a student -- which is continuously trained on the responses of the LLM. This student gradually gains proficiency in independently handling an increasing number of user requests, a process we term neural caching. The crucial element in neural caching is a policy that decides which requests should be processed by the student alone and which should be redirected to the LLM, subsequently aiding the student's learning. In this study, we focus on classification tasks, and we consider a range of classic active learning-based selection criteria as the policy. Our experiments suggest that Margin Sampling and Query by Committee bring consistent benefits across tasks and budgets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;Von Mises&#20272;&#35745;&#22120;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#20272;&#35745;&#22120;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13553</link><description>&lt;p&gt;
&#20851;&#20110;&#20197;Von Mises&#20272;&#35745;&#22120;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21450;&#20854;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery. (arXiv:2310.13553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;Von Mises&#20272;&#35745;&#22120;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#20272;&#35745;&#22120;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#22810;&#20803;&#20998;&#24067;&#30340;&#38750;&#21442;&#25968;Von Mises&#20272;&#35745;&#22120;&#65292;&#38024;&#23545;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#21160;&#26426;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#38598;&#20013;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35813;&#20272;&#35745;&#22120;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#31216;&#20026;VM-CI&#65292;&#35813;&#26041;&#27861;&#22312;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#26368;&#20248;&#21442;&#25968;&#36895;&#29575;&#12290;&#21033;&#29992;&#25351;&#25968;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;VM-CI&#30340;&#25972;&#20307;&#35823;&#24046;&#30340;&#32039;&#23494;&#19978;&#30028;&#12290;&#30001;&#27492;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#20351;&#29992;VM-CI&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#20219;&#20309;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36827;&#34892;&#21051;&#30011;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#20110;&#36830;&#32493;&#21464;&#37327;&#22240;&#26524;&#21457;&#29616;&#30340;&#39318;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;VM-CI&#22312;&#26102;&#38388;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#25110;&#20004;&#32773;&#65289;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#36716;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Motivated by conditional independence testing, an essential step in constraint-based causal discovery algorithms, we study the nonparametric Von Mises estimator for the entropy of multivariate distributions built on a kernel density estimator. We establish an exponential concentration inequality for this estimator. We design a test for conditional independence (CI) based on our estimator, called VM-CI, which achieves optimal parametric rates under smoothness assumptions. Leveraging the exponential concentration, we prove a tight upper bound for the overall error of VM-CI. This, in turn, allows us to characterize the sample complexity of any constraint-based causal discovery algorithm that uses VM-CI for CI tests. To the best of our knowledge, this is the first sample complexity guarantee for causal discovery for continuous variables. Furthermore, we empirically show that VM-CI outperforms other popular CI tests in terms of either time or sample complexity (or both), which translates to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19979;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#22810;&#20010;MDPs&#20043;&#38388;&#20849;&#20139;&#30340;&#28508;&#22312;&#32467;&#26500;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#23545;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;MDPs&#21644;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#31867;&#65292;&#24182;&#20351;&#29992;$\eta$-bracketing number&#26469;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#21644;&#20219;&#21153;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#20915;&#23450;&#20102;&#22810;&#20219;&#21153;&#30456;&#36739;&#20110;&#21333;&#20219;&#21153;RL&#30340;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.13550</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21487;&#35777;&#26126;&#21463;&#30410;
&lt;/p&gt;
&lt;p&gt;
Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes. (arXiv:2310.13550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#38750;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19979;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#22810;&#20010;MDPs&#20043;&#38388;&#20849;&#20139;&#30340;&#28508;&#22312;&#32467;&#26500;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#23545;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;MDPs&#21644;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#31867;&#65292;&#24182;&#20351;&#29992;$\eta$-bracketing number&#26469;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#21644;&#20219;&#21153;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#20915;&#23450;&#20102;&#22810;&#20219;&#21153;&#30456;&#36739;&#20110;&#21333;&#20219;&#21153;RL&#30340;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#19979;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#22810;&#20010;MDPs&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#30340;&#28508;&#22312;&#32467;&#26500;&#24050;&#34987;&#35777;&#26126;&#30456;&#23545;&#20110;&#21333;&#20219;&#21153;RL&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#31181;&#21463;&#30410;&#26159;&#21542;&#33021;&#22815;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#37096;&#20998;&#21487;&#35266;&#23519;MDPs&#65288;POMDPs&#65289;&#21644;&#26356;&#19968;&#33324;&#30340;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSRs&#65289;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#31354;&#38388;&#20351;&#24471;&#24456;&#38590;&#30830;&#23450;&#22810;&#20219;&#21153;PSRs&#30340;&#20849;&#21516;&#28508;&#22312;&#32467;&#26500;&#31867;&#22411;&#33021;&#22815;&#20943;&#23569;&#27169;&#22411;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20551;&#35774;&#20102;&#19968;&#20010;&#29992;&#20110;&#20219;&#21153;&#30340;&#32852;&#21512;&#27169;&#22411;&#31867;&#65292;&#24182;&#20351;&#29992;$\eta$-bracketing number&#26469;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#65307;&#36825;&#20010;&#25968;&#20063;&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#25351;&#26631;&#26469;&#25429;&#25417;&#20219;&#21153;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#20915;&#23450;&#20102;&#22810;&#20219;&#21153;&#30456;&#36739;&#20110;&#21333;&#20219;&#21153;RL&#30340;&#21463;&#30410;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19978;&#28216;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;PSRs&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems, such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency. To this end, we posit a joint model class for tasks and use the notion of $\eta$-bracketing number to quantify its complexity; this number also serves as a general metric to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study upstream multi-task learning over PSRs, i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#36827;&#34892;&#27491;-&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36317;&#31163;&#24863;&#30693;&#30340;PU&#25439;&#22833;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#19982;&#22270;&#32467;&#26500;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13538</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#24863;&#30693;&#22270;&#23398;&#20064;&#30340;&#27491;-&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Positive-Unlabeled Node Classification with Structure-aware Graph Learning. (arXiv:2310.13538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#36827;&#34892;&#27491;-&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36317;&#31163;&#24863;&#30693;&#30340;PU&#25439;&#22833;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#19982;&#22270;&#32467;&#26500;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#30740;&#31350;&#38382;&#39064;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#38598;&#21487;&#33021;&#24182;&#19981;&#22914;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#25152;&#20551;&#23450;&#30340;&#37027;&#26679;&#24179;&#34913;&#21644;&#20934;&#30830;&#12290;&#27491;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#33410;&#28857;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#33410;&#28857;&#34987;&#38480;&#21046;&#20026;&#27491;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#22810;&#26679;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#30123;&#24773;&#39044;&#27979;&#25110;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;PU&#33410;&#28857;&#20998;&#31867;&#24037;&#20316;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#36825;&#21487;&#33021;&#26159;&#20851;&#38190;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#32467;&#26500;&#36827;&#34892;PU&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36317;&#31163;&#24863;&#30693;&#30340;PU&#25439;&#22833;&#65292;&#21033;&#29992;&#22270;&#20013;&#30340;&#21516;&#36136;&#24615;&#24341;&#20837;&#26356;&#20934;&#30830;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#20351;&#27169;&#22411;&#19982;&#22270;&#32467;&#26500;&#23545;&#40784;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#20063;&#23548;&#33268;&#26368;&#23567;&#21270;&#24102;&#26377;&#27491;&#36127;&#26631;&#31614;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;&#23545;&#22810;&#31181;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification on graphs is an important research problem with many applications. Real-world graph data sets may not be balanced and accurate as assumed by most existing works. A challenging setting is positive-unlabeled (PU) node classification, where labeled nodes are restricted to positive nodes. It has diverse applications, e.g., pandemic prediction or network anomaly detection. Existing works on PU node classification overlook information in the graph structure, which can be critical. In this paper, we propose to better utilize graph structure for PU node classification. We first propose a distance-aware PU loss that uses homophily in graphs to introduce more accurate supervision. We also propose a regularizer to align the model with graph structure. Theoretical analysis shows that minimizing the proposed loss also leads to minimizing the expected loss with both positive and negative labels. Extensive empirical evaluation on diverse graph data sets demonstrates its superior p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#35270;&#39057;&#24207;&#21015;&#20013;&#36880;&#28176;&#21464;&#21270;&#30340;&#39046;&#22495;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#22270;&#20687;&#24207;&#21015;&#20013;&#21333;&#29420;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#25913;&#21464;&#22825;&#27668;&#26465;&#20214;&#21644;&#26102;&#27573;&#26469;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13533</link><description>&lt;p&gt;
ICCV 2023&#35270;&#35273;&#36830;&#32493;&#23398;&#20064;&#25361;&#25112;&#30340;&#25216;&#26415;&#25253;&#21578;&#65306;&#35821;&#20041;&#20998;&#21106;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation. (arXiv:2310.13533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#35270;&#39057;&#24207;&#21015;&#20013;&#36880;&#28176;&#21464;&#21270;&#30340;&#39046;&#22495;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#22270;&#20687;&#24207;&#21015;&#20013;&#21333;&#29420;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#25913;&#21464;&#22825;&#27668;&#26465;&#20214;&#21644;&#26102;&#27573;&#26469;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65288;TTA&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#35270;&#39057;&#24207;&#21015;&#20013;&#36880;&#28176;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#21512;&#25104;&#39550;&#39542;&#35270;&#39057;&#25968;&#25454;&#38598;- SHIFT&#12290;&#28304;&#27169;&#22411;&#26159;&#22312;&#22825;&#27668;&#26228;&#26391;&#30340;&#30333;&#22825;&#25293;&#25668;&#30340;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#12290;&#27979;&#35797;&#26102;&#38388;&#30340;&#39046;&#22495;&#21464;&#21270;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#21644;&#19981;&#21516;&#26102;&#27573;&#12290;TTA&#26041;&#27861;&#22312;&#27599;&#20010;&#22270;&#20687;&#24207;&#21015;&#65288;&#35270;&#39057;&#65289;&#20013;&#21333;&#29420;&#35780;&#20272;&#65292;&#24847;&#21619;&#30528;&#22312;&#19979;&#19968;&#20010;&#24207;&#21015;&#20043;&#21069;&#65292;&#27169;&#22411;&#20250;&#34987;&#37325;&#26032;&#32622;&#20026;&#28304;&#27169;&#22411;&#29366;&#24577;&#12290;&#22270;&#20687;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#21040;&#36798;&#65292;&#27599;&#20010;&#24103;&#21040;&#36798;&#26102;&#37117;&#38656;&#35201;&#36827;&#34892;&#39044;&#27979;&#12290;&#27599;&#20010;&#24207;&#21015;&#30001;401&#20010;&#22270;&#20687;&#32452;&#25104;&#65292;&#20174;&#28304;&#39046;&#22495;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#36716;&#21464;&#20026;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#25913;&#21464;&#22825;&#27668;&#25110;&#26102;&#27573;&#65289;&#65292;&#30452;&#21040;&#24207;&#21015;&#30340;&#20013;&#38388;&#37096;&#20998;&#12290;&#22312;&#24207;&#21015;&#30340;&#21518;&#21322;&#37096;&#20998;&#65292;&#39046;&#22495;&#36880;&#28176;&#22238;&#24402;&#21040;&#28304;&#39046;&#22495;&#12290;&#21482;&#26377;SHIFT&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#38598;&#25552;&#20379;&#20102;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the challenge is to develop a test-time adaptation (TTA) method, which could adapt the model to gradually changing domains in video sequences for semantic segmentation task. It is based on a synthetic driving video dataset - SHIFT. The source model is trained on images taken during daytime in clear weather. Domain changes at test-time are mainly caused by varying weather conditions and times of day. The TTA methods are evaluated in each image sequence (video) separately, meaning the model is reset to the source model state before the next sequence. Images come one by one and a prediction has to be made at the arrival of each frame. Each sequence is composed of 401 images and starts with the source domain, then gradually drifts to a different one (changing weather or time of day) until the middle of the sequence. In the second half of the sequence, the domain gradually shifts back to the source one. Ground truth data is available only for the validation split of the SHIFT da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#21463;&#25511;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#21644;&#25991;&#26412;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13526</link><description>&lt;p&gt;
&#21463;&#25511;&#38543;&#26426;&#24615;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Controlled Randomness Improves the Performance of Transformer Models. (arXiv:2310.13526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#21463;&#25511;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#21644;&#25991;&#26412;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#26469;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#30456;&#21453;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#37327;&#24448;&#24448;&#36828;&#36828;&#19981;&#21450;&#19978;&#36848;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#25511;&#38543;&#26426;&#24615;&#65292;&#21363;&#22122;&#22768;&#65292;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#32034;&#30446;&#26631;&#22122;&#22768;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#36825;&#26679;&#30340;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#32852;&#21512;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#20197;&#21450;&#25991;&#26412;&#25688;&#35201;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the pre-training step of natural language models, the main objective is to learn a general representation of the pre-training dataset, usually requiring large amounts of textual data to capture the complexity and diversity of natural language. Contrasting this, in most cases, the size of the data available to solve the specific downstream task is often dwarfed by the aforementioned pre-training dataset, especially in domains where data is scarce. We introduce controlled randomness, i.e. noise, into the training process to improve fine-tuning language models and explore the performance of targeted noise in addition to the parameters of these models. We find that adding such noise can improve the performance in our two downstream tasks of joint named entity recognition and relation extraction and text summarization.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.13524</link><description>&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variational measurement-based quantum computation for generative modeling. (arXiv:2310.13524v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13524
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#37327;&#23376;&#35745;&#31639;&#65288;MBQC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#26412;&#29420;&#29305;&#30340;&#33539;&#20363;&#26469;&#35774;&#35745;&#37327;&#23376;&#31639;&#27861;&#12290;&#22312;MBQC&#20013;&#65292;&#30001;&#20110;&#37327;&#23376;&#27979;&#37327;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#33258;&#28982;&#30340;&#25805;&#20316;&#19981;&#26159;&#30830;&#23450;&#24615;&#21644;&#24186;&#27491;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#27010;&#29575;&#38468;&#24102;&#30340;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;MBQC&#30340;&#20027;&#35201;&#31639;&#27861;&#24212;&#29992;&#26159;&#23436;&#20840;&#25269;&#28040;&#36825;&#31181;&#27010;&#29575;&#24615;&#36136;&#65292;&#20197;&#27169;&#25311;&#34920;&#36798;&#22312;&#30005;&#36335;&#27169;&#22411;&#20013;&#30340;&#24186;&#27491;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35774;&#35745;MBQC&#31639;&#27861;&#30340;&#24605;&#36335;&#65292;&#35813;&#31639;&#27861;&#25509;&#21463;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#24182;&#23558;MBQC&#20013;&#30340;&#38543;&#26426;&#38468;&#24102;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#24615;&#26377;&#30410;&#30340;&#33258;&#28982;&#24212;&#29992;&#65292;&#21363;&#29983;&#25104;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29983;&#25104;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25511;&#21046;&#21442;&#25968;&#30340;&#21464;&#20998;MBQC&#31639;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#35843;&#25972;&#20801;&#35768;&#22312;&#35745;&#31639;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement-based quantum computation (MBQC) offers a fundamentally unique paradigm to design quantum algorithms. Indeed, due to the inherent randomness of quantum measurements, the natural operations in MBQC are not deterministic and unitary, but are rather augmented with probabilistic byproducts. Yet, the main algorithmic use of MBQC so far has been to completely counteract this probabilistic nature in order to simulate unitary computations expressed in the circuit model. In this work, we propose designing MBQC algorithms that embrace this inherent randomness and treat the random byproducts in MBQC as a resource for computation. As a natural application where randomness can be beneficial, we consider generative modeling, a task in machine learning centered around generating complex probability distributions. To address this task, we propose a variational MBQC algorithm equipped with control parameters that allow to directly adjust the degree of randomness to be admitted in the comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26408;&#26448;&#36136;&#37327;&#20998;&#31867;&#20013;&#21516;&#26102;&#35843;&#25972;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21644;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13490</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26408;&#26448;&#36136;&#37327;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Feature Selection and Hyperparameter Fine-tuning in Artificial Neural Networks for Wood Quality Classification. (arXiv:2310.13490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26408;&#26448;&#36136;&#37327;&#20998;&#31867;&#20013;&#21516;&#26102;&#35843;&#25972;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21644;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26408;&#26495;&#36136;&#37327;&#20998;&#31867;&#26159;&#38191;&#26408;&#21378;&#34892;&#19994;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#20013;&#23567;&#20225;&#19994;&#36890;&#24120;&#30001;&#20154;&#24037;&#25805;&#20316;&#21592;&#25191;&#34892;&#27492;&#20219;&#21153;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#30740;&#31350;&#35813;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#26356;&#32463;&#27982;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#23545;&#20174;&#26408;&#26495;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#25935;&#24863;&#65292;&#36825;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24402;&#32435;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#21516;&#26102;&#35843;&#25972;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#30340;&#36229;&#21442;&#25968;&#21644;&#36873;&#25321;&#26356;&#22909;&#25551;&#36848;&#26408;&#26495;&#36136;&#37327;&#30340;&#29305;&#24449;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#20174;&#38191;&#26408;&#21378;&#34892;&#19994;&#33719;&#24471;&#30340;&#22270;&#20687;&#32452;&#25104;&#30340;&#31169;&#20154;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#36827;&#34892;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality classification of wood boards is an essential task in the sawmill industry, which is still usually performed by human operators in small to median companies in developing countries. Machine learning algorithms have been successfully employed to investigate the problem, offering a more affordable alternative compared to other solutions. However, such approaches usually present some drawbacks regarding the proper selection of their hyperparameters. Moreover, the models are susceptible to the features extracted from wood board images, which influence the induction of the model and, consequently, its generalization power. Therefore, in this paper, we investigate the problem of simultaneously tuning the hyperparameters of an artificial neural network (ANN) as well as selecting a subset of characteristics that better describes the wood board quality. Experiments were conducted over a private dataset composed of images obtained from a sawmill industry and described using different fea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20010;&#20307;&#21270;&#30340;&#30315;&#30187;&#32593;&#32476;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#35782;&#21035;&#12289;&#39044;&#27979;&#21644;&#21050;&#28608;&#31070;&#32463;&#25391;&#33633;&#65292;&#20026;&#30315;&#30187;&#27835;&#30103;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#30103;&#27861;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.13480</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35782;&#21035;&#12289;&#39044;&#27979;&#21644;&#21050;&#28608;&#31070;&#32463;&#25391;&#33633;&#65306;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30315;&#30187;&#32593;&#32476;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalized identification, prediction, and stimulation of neural oscillations via data-driven models of epileptic network dynamics. (arXiv:2310.13480v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20010;&#20307;&#21270;&#30340;&#30315;&#30187;&#32593;&#32476;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#35782;&#21035;&#12289;&#39044;&#27979;&#21644;&#21050;&#28608;&#31070;&#32463;&#25391;&#33633;&#65292;&#20026;&#30315;&#30187;&#27835;&#30103;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#30103;&#27861;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25391;&#33633;&#34987;&#35748;&#20026;&#26159;&#22823;&#33041;&#20449;&#24687;&#22788;&#29702;&#21644;&#20256;&#36882;&#30340;&#29305;&#23450;&#26631;&#24535;&#65292;&#20063;&#21453;&#26144;&#20102;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#30340;&#30149;&#29702;&#27963;&#21160;&#65292;&#20026;&#35786;&#26029;&#21644;&#39044;&#27979;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#30315;&#30187;&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20043;&#19968;&#65292;&#20854;&#29305;&#24449;&#26159;&#22823;&#33041;&#25391;&#33633;&#30340;&#24322;&#24120;&#21516;&#27493;&#21644;&#22833;&#21516;&#27493;&#12290;&#32422;&#19977;&#20998;&#20043;&#19968;&#30340;&#30315;&#30187;&#30149;&#20363;&#23545;&#33647;&#29289;&#27835;&#30103;&#26080;&#25928;&#65292;&#22240;&#27492;&#38656;&#35201;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#65292;&#20854;&#20013;&#22823;&#33041;&#21050;&#28608;&#20284;&#20046;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#27835;&#30103;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22823;&#33041;&#21050;&#28608;&#33539;&#24335;&#30340;&#21457;&#23637;&#24448;&#24448;&#22522;&#20110;&#23545;&#22823;&#33041;&#21160;&#24577;&#30340;&#27010;&#25324;&#20551;&#35774;&#65292;&#23613;&#31649;&#24050;&#30693;&#24739;&#32773;&#21644;&#33041;&#29366;&#24577;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20010;&#20307;&#21270;&#30340;&#30315;&#30187;&#32593;&#32476;&#21160;&#21147;&#23398;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#20027;&#35201;&#30340;&#30456;&#24178;&#25391;&#33633;&#21450;&#20854;
&lt;/p&gt;
&lt;p&gt;
Neural oscillations are considered to be brain-specific signatures of information processing and communication in the brain. They also reflect pathological brain activity in neurological disorders, thus offering a basis for diagnoses and forecasting. Epilepsy is one of the most common neurological disorders, characterized by abnormal synchronization and desynchronization of the oscillations in the brain. About one third of epilepsy cases are pharmacoresistant, and as such emphasize the need for novel therapy approaches, where brain stimulation appears to be a promising therapeutic option. The development of brain stimulation paradigms, however, is often based on generalized assumptions about brain dynamics, although it is known that significant differences occur between patients and brain states. We developed a framework to extract individualized predictive models of epileptic network dynamics directly from EEG data. The models are based on the dominant coherent oscillations and their 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;&#33719;&#21462;&#23454;&#20363;&#25513;&#27169;&#12289;&#36873;&#25321;&#27491;&#30830;&#25513;&#27169;&#21644;&#32416;&#27491;&#38169;&#35823;&#25513;&#27169;&#30340;&#19977;&#20010;&#27493;&#39588;&#65292;&#22635;&#34917;&#20102;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.13479</link><description>&lt;p&gt;
&#20998;&#27573;&#12289;&#36873;&#25321;&#12289;&#32416;&#27491;&#65306;&#19968;&#31181;&#24369;&#30417;&#30563;&#25351;&#20195;&#20998;&#21106;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation. (arXiv:2310.13479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;&#33719;&#21462;&#23454;&#20363;&#25513;&#27169;&#12289;&#36873;&#25321;&#27491;&#30830;&#25513;&#27169;&#21644;&#32416;&#27491;&#38169;&#35823;&#25513;&#27169;&#30340;&#19977;&#20010;&#27493;&#39588;&#65292;&#22635;&#34917;&#20102;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#65288;RIS&#65289;&#26159;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#23545;&#35937;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#20027;&#35201;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#25351;&#20195;&#26631;&#27880;&#25513;&#27169;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#19981;&#21450;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;RIS&#20998;&#35299;&#25104;&#19977;&#20010;&#27493;&#39588;&#36827;&#34892;&#22788;&#29702;&#65306;&#33719;&#21462;&#34987;&#25552;&#21450;&#25351;&#20196;&#20013;&#30340;&#23545;&#35937;&#30340;&#23454;&#20363;&#25513;&#27169;&#65288;&#20998;&#27573;&#65289;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26469;&#36873;&#25321;&#32473;&#23450;&#25351;&#20196;&#30340;&#28508;&#22312;&#27491;&#30830;&#25513;&#27169;&#65288;&#36873;&#25321;&#65289;&#65292;&#24182;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#26469;&#20462;&#27491;&#38646;&#26679;&#26412;&#36873;&#25321;&#30340;&#38169;&#35823;&#65288;&#32416;&#27491;&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#20165;&#20351;&#29992;&#21069;&#20004;&#20010;&#27493;&#39588;&#65288;&#38646;&#26679;&#26412;&#20998;&#27573;&#21644;&#36873;&#25321;&#65289;&#27604;&#20854;&#20182;&#38646;&#26679;&#26412;&#22522;&#32447;&#25552;&#39640;&#20102;&#22810;&#36798;19%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Referring Image Segmentation (RIS) - the problem of identifying objects in images through natural language sentences - is a challenging task currently mostly solved through supervised learning. However, while collecting referred annotation masks is a time-consuming process, the few existing weakly-supervised and zero-shot approaches fall significantly short in performance compared to fully-supervised learning ones. To bridge the performance gap without mask annotations, we propose a novel weakly-supervised framework that tackles RIS by decomposing it into three steps: obtaining instance masks for the object mentioned in the referencing instruction (segment), using zero-shot learning to select a potentially correct mask for the given instruction (select), and bootstrapping a model which allows for fixing the mistakes of zero-shot selection (correct). In our experiments, using only the first two steps (zero-shot segment and select) outperforms other zero-shot baselines by as much as 19%,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;$k$-means&#30340;$D^\alpha$&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#23545;&#20110;&#20219;&#20309;$ \alpha \geq 1 $&#37117;&#33021;&#20445;&#35777;&#19968;&#20010;$ O(2^{2\alpha}\cdot \log k) $-&#36817;&#20284;&#35299;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#36824;&#21457;&#29616;&#20351;&#29992;$ \alpha &gt; 2 $&#30340;$ D^\alpha $&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#21487;&#20197;&#24471;&#21040;&#27604;&#26631;&#20934;&#30340;$ k $-means&#30446;&#26631;&#20989;&#25968;&#26356;&#22909;&#30340;&#35299;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#20005;&#26684;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.13474</link><description>&lt;p&gt;
$k$-means&#30340;$D^\alpha$&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of $D^\alpha$ seeding for $k$-means. (arXiv:2310.13474v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;$k$-means&#30340;$D^\alpha$&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#23545;&#20110;&#20219;&#20309;$ \alpha \geq 1 $&#37117;&#33021;&#20445;&#35777;&#19968;&#20010;$ O(2^{2\alpha}\cdot \log k) $-&#36817;&#20284;&#35299;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#36824;&#21457;&#29616;&#20351;&#29992;$ \alpha &gt; 2 $&#30340;$ D^\alpha $&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#21487;&#20197;&#24471;&#21040;&#27604;&#26631;&#20934;&#30340;$ k $-means&#30446;&#26631;&#20989;&#25968;&#26356;&#22909;&#30340;&#35299;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#20005;&#26684;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#26159;Arthur&#21644;Vassilvitskii (2007)&#24320;&#21457;&#30340;&#33879;&#21517;&#30340;$D^\alpha$&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#65288;&#20063;&#34987;&#31216;&#20026;$ k $-means ++&#65292;&#20854;&#20013;$ \alpha = 2 $&#65289;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#23545;&#20110;&#20219;&#20309;$ \alpha \geq 1 $&#37117;&#33021;&#20445;&#35777;&#19968;&#20010;$ O(2^{2\alpha}\cdot \log k) $-&#36817;&#20284;&#35299;&#12290;&#26368;&#36817;&#65292;Balcan&#65292;Dick&#21644;White(2018)&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;$ \alpha &gt; 2 $&#30340;$ D^\alpha $&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#21487;&#20197;&#24471;&#21040;&#27604;&#26631;&#20934;&#30340;$ k $-means&#30446;&#26631;&#20989;&#25968;&#65288;&#21363;$(k,2)$-means&#25439;&#22833;&#20989;&#25968;&#65289;&#26356;&#22909;&#30340;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#29616;&#35937;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35299;&#12290;&#23545;&#20110;&#20219;&#20309;$ \alpha &gt; 2 $&#65292;&#25105;&#20204;&#35777;&#26126;$ D^\alpha $&#31181;&#23376;&#36873;&#25321;&#31639;&#27861;&#21487;&#20197;&#22312;&#26399;&#26395;&#19978;&#20445;&#35777;&#19968;&#20010;&#36817;&#20284;&#31995;&#25968;&#20026;$$ O_\alpha \left((g_\alpha)^{2/\alpha}\cdot \left(\frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}}\right)^{2-4/\alpha}\cdot (\min\{\ell,\log k\})^{2/\alpha}\right)$$&#30340;&#26631;&#20934;$ k $-means&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most popular clustering algorithms is the celebrated $D^\alpha$ seeding algorithm (also know as $k$-means++ when $\alpha=2$) by Arthur and Vassilvitskii (2007), who showed that it guarantees in expectation an $O(2^{2\alpha}\cdot \log k)$-approximate solution to the ($k$,$\alpha$)-means cost (where euclidean distances are raised to the power $\alpha$) for any $\alpha\ge 1$. More recently, Balcan, Dick, and White (2018) observed experimentally that using $D^\alpha$ seeding with $\alpha&gt;2$ can lead to a better solution with respect to the standard $k$-means objective (i.e. the $(k,2)$-means cost).  In this paper, we provide a rigorous understanding of this phenomenon. For any $\alpha&gt;2$, we show that $D^\alpha$ seeding guarantees in expectation an approximation factor of $$ O_\alpha \left((g_\alpha)^{2/\alpha}\cdot \left(\frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}}\right)^{2-4/\alpha}\cdot (\min\{\ell,\log k\})^{2/\alpha}\right)$$ with respect to the standard $k$-means c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13459</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#38750;&#20984;-&#38750;&#20985;&#35757;&#32451;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#32447;&#24615;&#25554;&#20540;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20316;&#20026;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#32447;&#24615;&#25554;&#20540;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#25972;&#33539;&#22260;&#20869;&#30340;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26500;&#36896;&#21487;&#25193;&#23637;&#21040;&#32422;&#26463;&#21644;&#27491;&#21017;&#21270;&#35774;&#32622;&#12290;&#36890;&#36807;&#26367;&#25442;RAPP&#20013;&#30340;&#20869;&#37096;&#20248;&#21270;&#22120;&#65292;&#25105;&#20204;&#37325;&#26032;&#21457;&#29616;&#20102;Lookahead&#31639;&#27861;&#26063;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;Lookahead&#32487;&#25215;&#24615;&#36136;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;Lookahead&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13458</link><description>&lt;p&gt;
&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#30340;&#20219;&#21153;&#28436;&#31034;&#23398;&#20064;&#19982;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Correspondence learning between morphologically different robots through task demonstrations. (arXiv:2310.13458v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#26426;&#22120;&#20154;&#22312;&#20854;&#26426;&#36523;&#12289;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#26041;&#38754;&#26377;&#30528;&#21508;&#31181;&#21508;&#26679;&#30340;&#24046;&#24322;&#12290;&#32771;&#34385;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#65292;&#29420;&#31435;&#22320;&#25945;&#23548;&#27599;&#20010;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#27599;&#20010;&#25216;&#33021;&#26159;&#20302;&#25928;&#19988;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#24863;&#23448;&#36816;&#21160;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#30340;&#25216;&#33021;&#21487;&#20197;&#26356;&#30452;&#25509;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#26426;&#22120;&#20154;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#20851;&#33410;&#25511;&#21046;&#30340;&#22266;&#23450;&#22522;&#24231;&#25805;&#32437;&#26426;&#22120;&#20154;&#21644;&#24046;&#21160;&#39537;&#21160;&#31227;&#21160;&#26426;&#22120;&#20154;&#20043;&#38388;&#23398;&#20064;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#35753;&#20004;&#20010;&#26426;&#22120;&#20154;&#36827;&#34892;&#25191;&#34892;&#30456;&#21516;&#20219;&#21153;&#30340;&#28436;&#31034;&#12290;&#22312;&#23398;&#20064;&#23545;&#24212;&#31574;&#30053;&#30340;&#21516;&#26102;&#24418;&#25104;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#21021;&#22987;&#23398;&#20064;&#38454;&#27573;&#20043;&#21518;&#65292;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#26032;&#20219;&#21153;&#25191;&#34892;&#23601;&#36275;&#20197;&#29983;&#25104;&#19968;&#20010;&#28508;&#21464;&#37327;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24212;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe a large variety of robots in terms of their bodies, sensors, and actuators. Given the commonalities in the skill sets, teaching each skill to each different robot independently is inefficient and not scalable when the large variety in the robotic landscape is considered. If we can learn the correspondences between the sensorimotor spaces of different robots, we can expect a skill that is learned in one robot can be more directly and easily transferred to the other robots. In this paper, we propose a method to learn correspondences between robots that have significant differences in their morphologies: a fixed-based manipulator robot with joint control and a differential drive mobile robot. For this, both robots are first given demonstrations that achieve the same tasks. A common latent representation is formed while learning the corresponding policies. After this initial learning stage, the observation of a new task execution by one robot becomes sufficient to generate a lat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#20998;&#26512;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;QLDS&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24179;&#34913;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26725;&#26753;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#20998;&#31867;&#38169;&#35823;&#30340;&#29702;&#35770;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#28857;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.13434</link><description>&lt;p&gt;
&#38543;&#26426;&#30697;&#38453;&#20998;&#26512;&#22312;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#24179;&#34913;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20043;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption. (arXiv:2310.13434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#20998;&#26512;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;QLDS&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24179;&#34913;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26725;&#26753;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#20998;&#31867;&#38169;&#35823;&#30340;&#29702;&#35770;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#28857;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#24773;&#20917;&#19979;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QLDS&#65292;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#20108;&#27425;&#36793;&#30028;&#26368;&#22823;&#21270;&#26469;&#23454;&#29616;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#35299;&#21644;&#20016;&#23500;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#29305;&#27530;&#24773;&#20917;&#26159;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#35889;&#32858;&#31867;&#65292;&#20197;&#21450;&#19968;&#31867;&#21322;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;QLDS&#22312;&#36825;&#20123;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26725;&#26753;&#12290;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#27491;&#24335;&#25512;&#23548;&#20102;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#35823;&#24046;&#30340;&#29702;&#35770;&#35780;&#20272;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework to analyze semi-supervised classification under the low density separation assumption in a high-dimensional regime. In particular, we introduce QLDS, a linear classification model, where the low density separation assumption is implemented via quadratic margin maximization. The algorithm has an explicit solution with rich theoretical properties, and we show that particular cases of our algorithm are the least-square support vector machine in the supervised case, the spectral clustering in the fully unsupervised regime, and a class of semi-supervised graph-based approaches. As such, QLDS establishes a smooth bridge between these supervised and unsupervised learning methods. Using recent advances in the random matrix theory, we formally derive a theoretical evaluation of the classification error in the asymptotic regime. As an application, we derive a hyperparameter selection policy that finds the best balance between the supervised and the unsupervised
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;Wasserstein&#36317;&#31163;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#26469;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#26399;&#26395;Wasserstein&#36317;&#31163;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21518;&#39564;&#37319;&#26679;&#26041;&#38754;&#30340;&#26377;&#21033;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.13433</link><description>&lt;p&gt;
Y-&#23545;&#35282;&#32447;&#32806;&#21512;: &#29992;&#26465;&#20214;Wasserstein&#36317;&#31163;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances. (arXiv:2310.13433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;Wasserstein&#36317;&#31163;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#26469;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#26399;&#26395;Wasserstein&#36317;&#31163;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21518;&#39564;&#37319;&#26679;&#26041;&#38754;&#30340;&#26377;&#21033;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#65292;&#35768;&#22810;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32852;&#21512;&#20998;&#24067;&#19982;&#20854;&#23398;&#20064;&#21040;&#30340;&#36817;&#20284;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#36924;&#36817;&#21518;&#39564;&#27979;&#24230;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;Kullback Leibler&#25955;&#24230;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#25511;&#21046;&#21518;&#39564;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#23545;&#20110;Wasserstein&#36317;&#31163;&#26469;&#35828;&#21364;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#23558;&#24341;&#20837;&#19968;&#31181;&#24102;&#26377;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;&#20998;&#24067;&#30340;&#26399;&#26395;Wasserstein&#36317;&#31163;&#12290;&#36890;&#36807;&#25512;&#23548;&#20854;&#23545;&#20598;&#24418;&#24335;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#26041;&#24335;&#26469;&#35299;&#37322;&#26465;&#20214;Wasserstein GANs&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20351;&#24471;&#24120;&#35268;Wasserstein&#36317;&#31163;&#21644;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30456;&#31561;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#20351;&#29992;&#26465;&#20214;Wasserstein&#36317;&#31163;&#36827;&#34892;&#35757;&#32451;&#22312;&#21518;&#39564;&#37319;&#26679;&#26041;&#38754;&#20855;&#26377;&#26377;&#21033;&#30340;&#24615;&#36136;&#30340;&#25968;&#20540;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback Leibler divergence, it does not hold true for the Wasserstein distance. We will introduce a conditional Wasserstein distance with a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. By deriving its dual, we find a rigorous way to motivate the loss of conditional Wasserstein GANs. We outline conditions under which the vanilla and the conditional Wasserstein distance coincide. Furthermore, we will show numerical examples where training with the conditional Wasserstein distance yields favorable properties for posterior sampling.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29699;&#24418;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;HRTF&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;HRTF&#25968;&#25454;&#30340;&#29699;&#24418;&#20960;&#20309;&#29305;&#24615;&#21644;&#24038;&#21491;&#22768;&#36947;&#30340;&#28508;&#22312;&#23545;&#31216;&#24615;&#26469;&#32416;&#27491;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.13430</link><description>&lt;p&gt;
&#20351;&#29992;&#29699;&#24418;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;HRTF&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
HRTF Interpolation using a Spherical Neural Process Meta-Learner. (arXiv:2310.13430v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29699;&#24418;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;HRTF&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;HRTF&#25968;&#25454;&#30340;&#29699;&#24418;&#20960;&#20309;&#29305;&#24615;&#21644;&#24038;&#21491;&#22768;&#36947;&#30340;&#28508;&#22312;&#23545;&#31216;&#24615;&#26469;&#32416;&#27491;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#20010;&#24615;&#21270;&#26041;&#27861;&#26469;&#20272;&#35745;&#20027;&#35266;&#22836;&#30456;&#20851;&#20256;&#36882;&#20989;&#25968;&#65288;HRTF&#65289;&#65292;&#20351;&#29992;&#20415;&#25463;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#20363;&#22914;&#20154;&#31867;&#27979;&#37327;&#25110;&#32819;&#24275;&#29031;&#29255;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20351;&#29992;&#19968;&#20123;&#30001;&#22768;&#23398;&#27979;&#37327;&#25110;&#24863;&#30693;&#21453;&#39304;&#33719;&#24471;&#30340;&#20027;&#35266;HRTF&#25968;&#25454;&#28857;&#26679;&#26412;&#26102;&#65292;&#23384;&#22312;&#32416;&#27491;&#20272;&#35745;&#35823;&#24046;&#30340;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;HRTF&#35823;&#24046;&#25554;&#20540;&#30340;&#21367;&#31215;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#22120;&#12290;&#29305;&#21035;&#22320;&#65292;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#29699;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#20197;&#36866;&#24212;HRTF&#25968;&#25454;&#30340;&#29699;&#24418;&#20960;&#20309;&#29305;&#24615;&#12290;&#23427;&#36824;&#21033;&#29992;&#20102;HRTF&#24038;&#21491;&#22768;&#36947;&#22312;&#20013;&#36724;&#32447;&#19978;&#30340;&#28508;&#22312;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#35774;&#32622;&#19979;&#65292;&#32431;&#31929;&#20174;&#26102;&#38388;&#23545;&#40784;&#30340;&#39057;&#35889;&#25554;&#20540;&#35282;&#24230;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#36890;&#29992;&#30340;&#20154;&#32676;&#24179;&#22343;HRTF&#24418;&#25104;&#20102;&#21021;&#22987;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several individualization methods have recently been proposed to estimate a subject's Head-Related Transfer Function (HRTF) using convenient input modalities such as anthropometric measurements or pinnae photographs. There exists a need for adaptively correcting the estimation error committed by such methods using a few data point samples from the subject's HRTF, acquired using acoustic measurements or perceptual feedback. To this end, we introduce a Convolutional Conditional Neural Process meta-learner specialized in HRTF error interpolation. In particular, the model includes a Spherical Convolutional Neural Network component to accommodate the spherical geometry of HRTF data. It also exploits potential symmetries between the HRTF's left and right channels about the median axis. In this work, we evaluate the proposed model's performance purely on time-aligned spectrum interpolation grounds under a simplified setup where a generic population-mean HRTF forms the initial estimates prior 
&lt;/p&gt;</description></item><item><title>FLTracer&#26159;&#31532;&#19968;&#20010;FL&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#22320;&#26816;&#27979;&#21508;&#31181;&#25915;&#20987;&#65292;&#24182;&#36861;&#36394;&#25915;&#20987;&#30340;&#26102;&#38388;&#12289;&#30446;&#26631;&#12289;&#31867;&#22411;&#21644;&#34987;&#27602;&#21270;&#30340;&#20301;&#32622;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;FLTracer&#21033;&#29992;&#26412;&#22320;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.13424</link><description>&lt;p&gt;
FLTracer: &#39640;&#20934;&#30830;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27602;&#21270;&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
FLTracer: Accurate Poisoning Attack Provenance in Federated Learning. (arXiv:2310.13424v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13424
&lt;/p&gt;
&lt;p&gt;
FLTracer&#26159;&#31532;&#19968;&#20010;FL&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#22320;&#26816;&#27979;&#21508;&#31181;&#25915;&#20987;&#65292;&#24182;&#36861;&#36394;&#25915;&#20987;&#30340;&#26102;&#38388;&#12289;&#30446;&#26631;&#12289;&#31867;&#22411;&#21644;&#34987;&#27602;&#21270;&#30340;&#20301;&#32622;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;FLTracer&#21033;&#29992;&#26412;&#22320;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;FL&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#27602;&#21270;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#25110;&#22312;&#20854;&#20013;&#24341;&#20837;&#21518;&#38376;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#20808;&#21069;&#30340;FL&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#21482;&#23545;&#26377;&#38480;&#21644;&#29305;&#23450;&#30340;&#25915;&#20987;&#26377;&#25928;&#12290;&#22823;&#22810;&#25968;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#30340;&#35774;&#32622;&#20013;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLTracer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;FL&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#26816;&#27979;&#21508;&#31181;&#25915;&#20987;&#24182;&#36861;&#36394;&#25915;&#20987;&#30340;&#26102;&#38388;&#12289;&#30446;&#26631;&#12289;&#31867;&#22411;&#21644;&#34987;&#27602;&#21270;&#30340;&#20301;&#32622;&#12290;&#19982;&#29616;&#26377;&#30340;&#20165;&#20381;&#36182;&#20110;&#36328;&#23458;&#25143;&#31471;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#65288;BRLF&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#21306;&#22359;&#38142;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32858;&#21512;&#33410;&#28857;&#12289;&#35889;&#32858;&#31867;&#21644;&#24179;&#22343;&#26799;&#24230;&#35745;&#31639;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13403</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65288;BRFL&#65289;
&lt;/p&gt;
&lt;p&gt;
BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model. (arXiv:2310.13403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13403
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#65288;BRLF&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#21306;&#22359;&#38142;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32858;&#21512;&#33410;&#28857;&#12289;&#35889;&#32858;&#31867;&#21644;&#24179;&#22343;&#26799;&#24230;&#35745;&#31639;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#20998;&#24067;&#24335;&#33410;&#28857;&#20013;&#65292;&#24182;&#20165;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#22240;&#27492;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#25361;&#25112;&#65292;&#21363;&#25308;&#21344;&#24237;&#25915;&#20987;&#38382;&#39064;&#65292;&#24694;&#24847;&#30340;&#26412;&#22320;&#27169;&#22411;&#21487;&#33021;&#22312;&#32858;&#21512;&#36807;&#31243;&#20013;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#65288;BRLF&#65289;&#27169;&#22411;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#21306;&#22359;&#38142;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#36861;&#28335;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#20026;&#26412;&#22320;&#35757;&#32451;&#30340;&#23458;&#25143;&#25552;&#20379;&#28608;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#36873;&#25321;&#32858;&#21512;&#33410;&#28857;&#65292;&#24182;&#20351;&#29992;&#35889;&#32858;&#31867;&#35745;&#31639;&#27599;&#20010;&#32858;&#31867;&#20013;&#30340;&#24179;&#22343;&#26799;&#24230;&#65292;&#24182;&#21033;&#29992;&#32858;&#21512;&#33410;&#28857;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#23545;&#20854;&#20934;&#30830;&#24615;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing importance of machine learning, the privacy and security of training data have become critical. Federated learning, which stores data in distributed nodes and shares only model parameters, has gained significant attention for addressing this concern. However, a challenge arises in federated learning due to the Byzantine Attack Problem, where malicious local models can compromise the global model's performance during aggregation. This article proposes the Blockchain-based Byzantine-Robust Federated Learning (BRLF) model that combines federated learning with blockchain technology. This integration enables traceability of malicious models and provides incentives for locally trained clients. Our approach involves selecting the aggregation node based on Pearson's correlation coefficient, and we perform spectral clustering and calculate the average gradient within each cluster, validating its accuracy using local dataset of the aggregation nodes. Experimental results on p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36873;&#23450;&#30340;&#25674;&#38144;SBI&#25216;&#26415;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#20013;&#28155;&#21152;&#26657;&#20934;&#39033;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#31639;&#27861;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#35745;&#31639;&#27969;&#31243;&#65292;&#23454;&#29616;&#21487;&#38752;&#30340;&#40657;&#30418;&#21518;&#39564;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.13402</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#35206;&#30422;&#27010;&#29575;&#26657;&#20934;&#31070;&#32463;&#20223;&#30495;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability. (arXiv:2310.13402v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36873;&#23450;&#30340;&#25674;&#38144;SBI&#25216;&#26415;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#20013;&#28155;&#21152;&#26657;&#20934;&#39033;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#31639;&#27861;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#35745;&#31639;&#27969;&#31243;&#65292;&#23454;&#29616;&#21487;&#38752;&#30340;&#40657;&#30418;&#21518;&#39564;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#20801;&#35768;&#22312;&#32473;&#23450;&#20808;&#39564;&#20449;&#24687;&#21644;&#35777;&#25454;&#30340;&#20284;&#28982;&#30340;&#27010;&#29575;&#27169;&#22411;&#19979;&#34920;&#36798;&#21518;&#39564;&#20449;&#24565;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#35201;&#26159;&#36890;&#36807;&#27169;&#25311;&#22120;&#38544;&#24335;&#24314;&#31435;&#30340;&#20284;&#28982;&#20989;&#25968;&#38656;&#35201;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#29702;(SBI)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#21487;&#33021;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#21518;&#39564;&#27010;&#29575;(Hermans&#31561;&#65292;2022)&#65292;&#22914;&#26524;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19981;&#20934;&#30830;&#65292;&#23558;&#20987;&#36133;&#25972;&#20010;&#21487;&#20449;&#24615;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#36873;&#23450;&#30340;&#25674;&#38144;SBI&#25216;&#26415;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#20013;&#30452;&#25509;&#21253;&#21547;&#19968;&#20010;&#26657;&#20934;&#39033;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#32463;&#20856;&#26657;&#20934;&#38169;&#35823;&#20844;&#24335;&#30340;&#26494;&#24347;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#21453;&#21521;&#20256;&#25773;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#23616;&#38480;&#20110;&#20219;&#20309;&#29305;&#23450;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#24182;&#24102;&#26469;&#36866;&#24230;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#23427;&#30452;&#25509;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#35745;&#31639;&#27969;&#31243;&#65292;&#23454;&#29616;&#21487;&#38752;&#30340;&#40657;&#30418;&#21518;&#39564;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference allows expressing the uncertainty of posterior belief under a probabilistic model given prior information and the likelihood of the evidence. Predominantly, the likelihood function is only implicitly established by a simulator posing the need for simulation-based inference (SBI). However, the existing algorithms can yield overconfident posteriors (Hermans *et al.*, 2022) defeating the whole purpose of credibility if the uncertainty quantification is inaccurate. We propose to include a calibration term directly into the training objective of the neural model in selected amortized SBI techniques. By introducing a relaxation of the classical formulation of calibration error we enable end-to-end backpropagation. The proposed method is not tied to any particular neural model and brings moderate computational overhead compared to the profits it introduces. It is directly applicable to existing computational pipelines allowing reliable black-box posterior inference. We empi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13397</link><description>&lt;p&gt;
&#31561;&#21464;&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Equivariant Deep Weight Space Alignment. (arXiv:2310.13397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#20351;&#24471;&#31616;&#21333;&#25805;&#20316;&#22914;&#27169;&#22411;&#24179;&#22343;&#21644;&#30456;&#20284;&#24230;&#20272;&#35745;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23545;&#40784;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#21363;&#25214;&#21040;&#23427;&#20204;&#20043;&#38388;&#26368;&#20248;&#25490;&#21015;&#65292;&#26159;&#24517;&#35201;&#30340;&#12290;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26435;&#37325;&#23545;&#40784;&#23545;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20174;&#27169;&#22411;&#21512;&#24182;&#65292;&#36890;&#36807;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#31354;&#38388;&#65292;&#21040;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#26377;&#24847;&#20041;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26435;&#37325;&#23545;&#40784;&#26159;&#19968;&#20010;NP-hard&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#23548;&#33268;&#26041;&#27861;&#32791;&#26102;&#25110;&#32773;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26435;&#37325;&#23545;&#40784;&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#23545;&#31216;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; RL-X &#36825;&#20010;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312; RoboCup &#21644;&#32463;&#20856; DRL &#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#30456;&#27604;&#20110;&#20854;&#20182;&#26694;&#26550;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13396</link><description>&lt;p&gt;
RL-X: &#19968;&#20010;&#29992;&#20110;RoboCup&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65288;&#19981;&#20165;&#38480;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
RL-X: A Deep Reinforcement Learning Library (not only) for RoboCup. (arXiv:2310.13396v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; RL-X &#36825;&#20010;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312; RoboCup &#21644;&#32463;&#20856; DRL &#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#30456;&#27604;&#20110;&#20854;&#20182;&#26694;&#26550;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24211;RL-X&#21450;&#20854;&#22312;RoboCup Soccer Simulation 3D&#32852;&#36187;&#21644;&#32463;&#20856;DRL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#12290;RL-X&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#19988;&#26131;&#20110;&#25193;&#23637;&#30340;&#20195;&#30721;&#24211;&#65292;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#21333;&#30446;&#24405;&#31639;&#27861;&#12290;&#36890;&#36807;&#22522;&#20110;&#24555;&#36895;&#30340;JAX&#23454;&#29616;&#65292;RL-X&#19982;&#30693;&#21517;&#26694;&#26550;&#65288;&#22914;Stable-Baselines3&#65289;&#30456;&#27604;&#65292;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;4.5&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the new Deep Reinforcement Learning (DRL) library RL-X and its application to the RoboCup Soccer Simulation 3D League and classic DRL benchmarks. RL-X provides a flexible and easy-to-extend codebase with self-contained single directory algorithms. Through the fast JAX-based implementations, RL-X can reach up to 4.5x speedups compared to well-known frameworks like Stable-Baselines3.
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#22266;&#23450;&#32622;&#20449;&#24230;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#24179;&#22343;&#20540;&#30340;&#33218;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#20915;&#31574;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#24182;&#24314;&#31435;&#20102;&#20572;&#27490;&#26102;&#38388;&#22686;&#38271;&#29575;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.13393</link><description>&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#20197;&#22266;&#23450;&#32622;&#20449;&#24230;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Optimal Best Arm Identification with Fixed Confidence in Restless Bandits. (arXiv:2310.13393v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13393
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#22266;&#23450;&#32622;&#20449;&#24230;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#24179;&#22343;&#20540;&#30340;&#33218;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#20915;&#31574;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#24182;&#24314;&#31435;&#20102;&#20572;&#27490;&#26102;&#38388;&#22686;&#38271;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25968;&#30446;&#33218;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#20197;&#19981;&#26029;&#21464;&#21270;&#30340;&#24418;&#24335;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#27599;&#20010;&#33218;&#20135;&#29983;&#30340;&#31163;&#25955;&#26102;&#38388;&#25968;&#25454;&#24418;&#25104;&#20102;&#19968;&#20010;&#21462;&#20540;&#22312;&#20849;&#21516;&#12289;&#26377;&#38480;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#21516;&#36136;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#27599;&#20010;&#33218;&#30340;&#29366;&#24577;&#36716;&#31227;&#30001;&#19968;&#20010;&#36981;&#24490;&#21333;&#21442;&#25968;&#25351;&#25968;&#26063;&#30340;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#65288;TPM&#65289;&#25429;&#33719;&#12290;&#27599;&#20010;&#33218;&#30340;TPM&#30340;&#23454;&#20540;&#21442;&#25968;&#26159;&#26410;&#30693;&#30340;&#65292;&#23646;&#20110;&#32473;&#23450;&#31354;&#38388;&#12290;&#32473;&#23450;&#22312;&#33218;&#30340;&#20849;&#21516;&#29366;&#24577;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;f&#65292;&#30446;&#26631;&#26159;&#22312;&#26679;&#26412;&#25968;&#26368;&#23569;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#26368;&#20248;&#33218;&#65292;&#21363;&#22312;&#35813;&#33218;&#30340;&#31283;&#24577;&#20998;&#24067;&#19979;&#35780;&#20272;f&#30340;&#24179;&#22343;&#20540;&#26368;&#22823;&#30340;&#33218;&#65292;&#21516;&#26102;&#28385;&#36275;&#23545;&#20915;&#31574;&#38169;&#35823;&#27010;&#29575;&#65288;&#21363;&#22266;&#23450;&#32622;&#20449;&#24230;&#21306;&#38388;&#65289;&#30340;&#19978;&#30028;&#12290;&#22312;&#28176;&#36827;&#24615;&#30340;&#35823;&#24046;&#27010;&#29575;&#36235;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#31435;&#20102;&#26399;&#26395;&#20572;&#27490;&#26102;&#38388;&#22686;&#38271;&#29575;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#33218;&#35782;&#21035;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study best arm identification in a restless multi-armed bandit setting with finitely many arms. The discrete-time data generated by each arm forms a homogeneous Markov chain taking values in a common, finite state space. The state transitions in each arm are captured by an ergodic transition probability matrix (TPM) that is a member of a single-parameter exponential family of TPMs. The real-valued parameters of the arm TPMs are unknown and belong to a given space. Given a function $f$ defined on the common state space of the arms, the goal is to identify the best arm -- the arm with the largest average value of $f$ evaluated under the arm's stationary distribution -- with the fewest number of samples, subject to an upper bound on the decision's error probability (i.e., the fixed-confidence regime). A lower bound on the growth rate of the expected stopping time is established in the asymptote of a vanishing error probability. Furthermore, a policy for best arm identification is propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13388</link><description>&lt;p&gt;
&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#20013;&#30340;&#38899;&#20048;&#22686;&#24378;&#21644;&#38477;&#22122;
&lt;/p&gt;
&lt;p&gt;
Music Augmentation and Denoising For Peak-Based Audio Fingerprinting. (arXiv:2310.13388v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#26159;&#19968;&#31181;&#20174;&#31616;&#30701;&#24405;&#38899;&#29255;&#27573;&#20013;&#35782;&#21035;&#27468;&#26354;&#30340;&#25104;&#29087;&#35299;&#20915;&#26041;&#26696;&#12290;&#27969;&#34892;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25277;&#21462;&#31232;&#30095;&#34920;&#31034;&#65292;&#36890;&#24120;&#26159;&#39057;&#35889;&#23792;&#20540;&#65292;&#24182;&#19988;&#24050;&#32463;&#35777;&#26126;&#20934;&#30830;&#12289;&#24555;&#36895;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#22823;&#22411;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#38899;&#39057;&#35782;&#21035;&#30340;&#24212;&#29992;&#24448;&#24448;&#21457;&#29983;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21644;&#21457;&#24067;&#19968;&#20010;&#26032;&#30340;&#38899;&#39057;&#22686;&#24378;&#27969;&#27700;&#32447;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27969;&#27700;&#32447;&#20197;&#19968;&#31181;&#36924;&#30495;&#30340;&#26041;&#24335;&#21521;&#38899;&#20048;&#29255;&#27573;&#28155;&#21152;&#22122;&#22768;&#65292;&#36890;&#36807;&#38543;&#26426;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#35889;&#22270;&#20013;&#21435;&#38500;&#22122;&#22768;&#25104;&#20998;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#23792;&#20540;&#30340;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#28155;&#21152;&#25913;&#36827;&#20102;&#24120;&#29992;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#26465;&#20214;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio fingerprinting is a well-established solution for song identification from short recording excerpts. Popular methods rely on the extraction of sparse representations, generally spectral peaks, and have proven to be accurate, fast, and scalable to large collections. However, real-world applications of audio identification often happen in noisy environments, which can cause these systems to fail. In this work, we tackle this problem by introducing and releasing a new audio augmentation pipeline that adds noise to music snippets in a realistic way, by stochastically mimicking real-world scenarios. We then propose and release a deep learning model that removes noisy components from spectrograms in order to improve peak-based fingerprinting systems' accuracy. We show that the addition of our model improves the identification performance of commonly used audio fingerprinting systems, even under noisy conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#19981;&#21516;&#32972;&#26223;&#26465;&#20214;&#19979;&#23545;&#26368;&#36817;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;&#35266;&#23519;&#24615;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#22522;&#20110;score matching&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13387</link><description>&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20551;&#35774;&#36829;&#35268;&#21644;score matching&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assumption violations in causal discovery and the robustness of score matching. (arXiv:2310.13387v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#19981;&#21516;&#32972;&#26223;&#26465;&#20214;&#19979;&#23545;&#26368;&#36817;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;&#35266;&#23519;&#24615;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#22522;&#20110;score matching&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#19988;&#23454;&#39564;&#21463;&#21040;&#36947;&#24503;&#12289;&#36130;&#21153;&#25110;&#26102;&#38388;&#38480;&#21046;&#26102;&#65292;&#20174;&#19994;&#32773;&#20250;&#36716;&#21521;&#35266;&#23519;&#24615;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#26469;&#24674;&#22797;&#22240;&#26524;&#32467;&#26500;&#65292;&#21033;&#29992;&#20854;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#30001;&#20110;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#20551;&#35774;&#65292;&#22240;&#26524;&#21457;&#29616;&#26159;&#19968;&#20010;&#19981;&#36866;&#23450;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#31639;&#27861;&#37117;&#26377;&#20854;&#33258;&#24049;&#30340;&#19968;&#22871;&#36890;&#24120;&#26080;&#27861;&#39564;&#35777;&#30340;&#20551;&#35774;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#24456;&#38590;&#28385;&#36275;&#12290;&#37492;&#20110;&#36825;&#20123;&#32771;&#34385;&#65292;&#26412;&#25991;&#22312;&#19981;&#21516;&#32972;&#26223;&#26465;&#20214;&#19979;&#23545;&#26368;&#36817;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;&#35266;&#23519;&#24615;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20801;&#35768;&#36829;&#21453;&#27599;&#20010;&#36873;&#23450;&#26041;&#27861;&#25152;&#38656;&#30340;&#20851;&#38190;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#65292;&#22522;&#20110;score matching&#30340;&#26041;&#27861;&#22312;&#25512;&#26029;&#30340;&#22270;&#30340;&#35823;&#25253;&#19982;&#28431;&#25253;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#30340;&#29702;&#35770;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
When domain knowledge is limited and experimentation is restricted by ethical, financial, or time constraints, practitioners turn to observational causal discovery methods to recover the causal structure, exploiting the statistical properties of their data. Because causal discovery without further assumptions is an ill-posed problem, each algorithm comes with its own set of usually untestable assumptions, some of which are hard to meet in real datasets. Motivated by these considerations, this paper extensively benchmarks the empirical performance of recent causal discovery methods on observational i.i.d. data generated under different background conditions, allowing for violations of the critical assumptions required by each selected approach. Our experimental findings show that score matching-based methods demonstrate surprising performance in the false positive and false negative rate of the inferred graph in these challenging scenarios, and we provide theoretical insights into their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#25490;&#21517;&#21644;&#19978;&#19979;&#25991;&#25490;&#21517;&#26469;&#22686;&#21152;&#29983;&#25104;&#26356;&#22909;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13385</link><description>&lt;p&gt;
Tuna: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Tuna: Instruction Tuning using Feedback from Large Language Models. (arXiv:2310.13385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#25490;&#21517;&#21644;&#19978;&#19979;&#25991;&#25490;&#21517;&#26469;&#22686;&#21152;&#29983;&#25104;&#26356;&#22909;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Instruct-GPT&#21644;GPT-4&#65289;&#30340;&#30452;&#25509;&#36755;&#20986;&#65292;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#65289;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#20165;&#30475;&#21040;&#27599;&#20010;&#25351;&#20196;&#30340;&#19968;&#20010;&#21709;&#24212;&#65292;&#32570;&#20047;&#21487;&#33021;&#26356;&#22909;&#30340;&#21709;&#24212;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#8220;&#27010;&#29575;&#25490;&#21517;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#25490;&#21517;&#8221;&#26041;&#27861;&#26469;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#21152;&#29983;&#25104;&#26356;&#22909;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#12290;&#27010;&#29575;&#25490;&#21517;&#20351;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#32487;&#25215;&#20102;&#26469;&#33258;&#25945;&#24072;LLM&#30340;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#21709;&#24212;&#30340;&#30456;&#23545;&#25490;&#21517;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23398;&#20064;&#19978;&#19979;&#25991;&#25490;&#21517;&#21487;&#20197;&#35753;&#27169;&#22411;&#20351;&#29992;&#26356;&#24378;&#22823;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#26469;&#36827;&#19968;&#27493;&#20248;&#21270;&#33258;&#24049;&#30340;&#21709;&#24212;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27010;&#29575;&#25490;&#21517;&#21644;&#19978;&#19979;&#25991;&#25490;&#21517;&#25353;&#39034;&#24207;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel \textit{probabilistic ranking} and \textit{contextual ranking} approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30416;&#21270;DNN&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#35753;&#23458;&#25143;&#25511;&#21046;DNN&#36755;&#20986;&#30340;&#35821;&#20041;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19982;&#26631;&#20934;DNN&#20960;&#20046;&#19968;&#33268;&#65292;&#25552;&#21319;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#38544;&#31169;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13384</link><description>&lt;p&gt;
Salted Inference: &#22312;&#31227;&#21160;&#35745;&#31639;&#20013;&#25552;&#21319;&#38544;&#31169;&#24182;&#20445;&#25345;&#20998;&#21106;&#25512;&#29702;&#30340;&#25928;&#29575;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing. (arXiv:2310.13384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30416;&#21270;DNN&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#35753;&#23458;&#25143;&#25511;&#21046;DNN&#36755;&#20986;&#30340;&#35821;&#20041;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19982;&#26631;&#20934;DNN&#20960;&#20046;&#19968;&#33268;&#65292;&#25552;&#21319;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#38544;&#31169;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#25512;&#29702;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21010;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#36793;&#32536;&#37096;&#20998;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#65292;&#21518;&#32493;&#37096;&#20998;&#22312;&#20113;&#31471;&#36816;&#34892;&#12290;&#36825;&#28385;&#36275;&#20102;&#35774;&#22791;&#19978;&#26426;&#22120;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#38656;&#27714;&#65306;&#36755;&#20837;&#38544;&#31169;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#21106;&#25512;&#29702;&#20013;&#65292;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#36755;&#20986;&#38544;&#31169;&#65292;&#22240;&#20026;DNN&#30340;&#36755;&#20986;&#21487;&#35265;&#20110;&#20113;&#31471;&#12290;&#23613;&#31649;&#21152;&#23494;&#35745;&#31639;&#21487;&#20197;&#20445;&#25252;&#36755;&#20986;&#38544;&#31169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#30416;&#21270;DNN&#8221;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35753;&#23458;&#25143;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;DNN&#36755;&#20986;&#30340;&#35821;&#20041;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19982;&#26631;&#20934;DNN&#20960;&#20046;&#19968;&#33268;&#12290;&#22312;&#22270;&#20687;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#30416;&#21270;DNN&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#19982;&#26631;&#20934;DNN&#38750;&#24120;&#25509;&#36817;&#65292;&#23588;&#20854;&#26159;&#24403;&#30416;&#21270;&#23618;&#20301;&#20110;&#36739;&#26089;&#38454;&#27573;&#20197;&#28385;&#36275;&#20998;&#21106;&#25512;&#29702;&#30340;&#35201;&#27714;&#26102;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split inference partitions a deep neural network (DNN) to run the early part at the edge and the later part in the cloud. This meets two key requirements for on-device machine learning: input privacy and compute efficiency. Still, an open question in split inference is output privacy, given that the output of a DNN is visible to the cloud. While encrypted computing can protect output privacy, it mandates extensive computation and communication resources. In this paper, we introduce "Salted DNNs": a novel method that lets clients control the semantic interpretation of DNN output at inference time while maintaining accuracy and efficiency very close to that of a standard DNN. Experimental evaluations conducted on both image and sensor data show that Salted DNNs achieve classification accuracy very close to standard DNNs, particularly when the salted layer is positioned within the early part to meet the requirements of split inference. Our method is general and can be applied to various D
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#31232;&#30095;&#22810;&#36335;&#24452;&#26680;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#23436;&#20840;Cholesky&#20998;&#35299;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#26680;&#24515;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#23545;&#31216;&#29256;&#26412;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13381</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#32858;&#31867;&#38382;&#39064;&#30340;&#21152;&#36895;&#31232;&#30095;&#26680;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Accelerated sparse Kernel Spectral Clustering for large scale data clustering problems. (arXiv:2310.13381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13381
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#31232;&#30095;&#22810;&#36335;&#24452;&#26680;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#23436;&#20840;Cholesky&#20998;&#35299;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#26680;&#24515;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#23545;&#31216;&#29256;&#26412;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#31232;&#30095;&#22810;&#36335;&#24452;&#26680;&#35889;&#32858;&#31867;&#31639;&#27861;&#12290;&#21407;&#22987;&#31639;&#27861;&#26159;&#22312;&#21407;&#22987;-&#23545;&#20598;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;&#19979;&#65292;&#22522;&#20110;&#21152;&#26435;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;(KPCA)&#20174;&#32780;&#24471;&#21040;&#30340;&#12290;&#36890;&#36807;&#19981;&#23436;&#20840;Cholesky&#20998;&#35299;(ICD)&#21644;&#25152;&#35859;&#30340;&#38477;&#32500;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#31232;&#30095;&#24615;&#12290;&#21407;&#22987;&#30340;ICD&#31232;&#30095;KSC&#31639;&#27861;&#22312;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#32858;&#31867;&#38382;&#39064;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#36807;&#39640;&#65292;&#36825;&#23548;&#33268;&#35813;&#31639;&#27861;&#36804;&#20170;&#21482;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#29305;&#24615;&#12290;&#35299;&#20915;&#35745;&#31639;&#22797;&#26434;&#24230;&#26368;&#39640;&#30340;&#26680;&#24515;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#21487;&#26367;&#20195;&#30340;&#23545;&#31216;&#29256;&#26412;&#65292;&#28040;&#38500;&#20102;&#24418;&#25104;&#31232;&#30095;&#20302;&#31209;&#26680;&#30697;&#38453;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An improved version of the sparse multiway kernel spectral clustering (KSC) is presented in this brief. The original algorithm is derived from weighted kernel principal component (KPCA) analysis formulated within the primal-dual least-squares support vector machine (LS-SVM) framework. Sparsity is achieved then by the combination of the incomplete Cholesky decomposition (ICD) based low rank approximation of the kernel matrix with the so called reduced set method. The original ICD based sparse KSC algorithm was reported to be computationally far too demanding, especially when applied on large scale data clustering problems that actually it was designed for, which has prevented to gain more than simply theoretical relevance so far. This is altered by the modifications reported in this brief that drastically improve the computational characteristics. Solving the alternative, symmetrized version of the computationally most demanding core eigenvalue problem eliminates the necessity of formin
&lt;/p&gt;</description></item><item><title>SigFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31614;&#21517;&#21644;Transformer&#30340;&#28145;&#24230;&#23545;&#20914;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#36335;&#24452;&#31614;&#21517;&#30340;&#33021;&#21147;&#25429;&#25417;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24335;&#65292;&#21516;&#26102;&#21033;&#29992;Transformer&#25552;&#20379;&#20248;&#31168;&#30340;&#24207;&#21015;&#27880;&#24847;&#21147;&#65292;&#22312;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SigFormer&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#19981;&#35268;&#21017;&#20215;&#26684;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#20914;SP 500&#25351;&#25968;&#30340;&#30495;&#23454;&#22238;&#27979;&#65292;SigFormer&#23637;&#31034;&#20986;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13369</link><description>&lt;p&gt;
SigFormer: &#22522;&#20110;&#36335;&#24452;&#31614;&#21517;&#21644;Transformer&#30340;&#28145;&#24230;&#23545;&#20914;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SigFormer: Signature Transformers for Deep Hedging. (arXiv:2310.13369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13369
&lt;/p&gt;
&lt;p&gt;
SigFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31614;&#21517;&#21644;Transformer&#30340;&#28145;&#24230;&#23545;&#20914;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#36335;&#24452;&#31614;&#21517;&#30340;&#33021;&#21147;&#25429;&#25417;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24335;&#65292;&#21516;&#26102;&#21033;&#29992;Transformer&#25552;&#20379;&#20248;&#31168;&#30340;&#24207;&#21015;&#27880;&#24847;&#21147;&#65292;&#22312;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SigFormer&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#19981;&#35268;&#21017;&#20215;&#26684;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#20914;SP 500&#25351;&#25968;&#30340;&#30495;&#23454;&#22238;&#27979;&#65292;SigFormer&#23637;&#31034;&#20986;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23545;&#20914;&#26159;&#37327;&#21270;&#37329;&#34701;&#20013;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#27169;&#22411;&#21644;&#25216;&#26415;&#12290;&#34429;&#28982;&#21487;&#20197;&#25552;&#20379;&#20986;&#33394;&#30340;&#23545;&#20914;&#31574;&#30053;&#65292;&#20294;&#27169;&#22411;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SigFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#36335;&#24452;&#31614;&#21517;&#21644;Transformer&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23384;&#22312;&#19981;&#35268;&#21017;&#24615;&#30340;&#24773;&#20917;&#12290;&#36335;&#24452;&#31614;&#21517;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24335;&#65292;&#32780;Transformer&#21017;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#24207;&#21015;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#19981;&#35268;&#21017;&#30340;&#22522;&#30784;&#20215;&#26684;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20914;SP 500&#25351;&#25968;&#30340;&#30495;&#23454;&#22238;&#27979;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hedging is a promising direction in quantitative finance, incorporating models and techniques from deep learning research. While giving excellent hedging strategies, models inherently requires careful treatment in designing architectures for neural networks. To mitigate such difficulties, we introduce SigFormer, a novel deep learning model that combines the power of path signatures and transformers to handle sequential data, particularly in cases with irregularities. Path signatures effectively capture complex data patterns, while transformers provide superior sequential attention. Our proposed model is empirically compared to existing methods on synthetic data, showcasing faster learning and enhanced robustness, especially in the presence of irregular underlying price data. Additionally, we validate our model performance through a real-world backtest on hedging the SP 500 index, demonstrating positive outcomes.
&lt;/p&gt;</description></item><item><title>VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.13367</link><description>&lt;p&gt;
VFedMH: &#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#22810;&#21442;&#19982;&#26041;&#24322;&#26500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13367
&lt;/p&gt;
&lt;p&gt;
VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26679;&#26412;&#23545;&#40784;&#21644;&#29305;&#24449;&#21512;&#24182;&#30340;&#26032;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#22312;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#24433;&#21709;&#20102;&#20248;&#21270;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VFedMH&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26041;&#24322;&#26500;&#27169;&#22411;&#12290;VFedMH&#30340;&#37325;&#28857;&#26159;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#32858;&#21512;&#27599;&#20010;&#21442;&#19982;&#32773;&#30693;&#35782;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#20013;&#38388;&#32467;&#26524;&#12290;&#20027;&#21160;&#26041;&#65292;&#25317;&#26377;&#26679;&#26412;&#30340;&#26631;&#31614;&#21644;&#29305;&#24449;&#65292;&#22312;VFedMH&#20013;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#23884;&#20837;&#20197;&#33719;&#24471;&#20840;&#23616;&#30693;&#35782;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#34987;&#21160;&#26041;&#12290;&#34987;&#21160;&#26041;&#20165;&#25317;&#26377;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#21033;&#29992;&#20840;&#23616;&#23884;&#20837;&#22312;&#20854;&#26412;&#22320;&#24322;&#26500;&#32593;&#32476;&#19978;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#34987;&#21160;&#26041;&#19981;&#25317;&#26377;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22240;&#25968;&#25454;&#29983;&#25104;&#21644;&#25910;&#38598;&#26041;&#24335;&#24341;&#36215;&#30340;&#22240;&#26524;&#20559;&#24046;&#65292;&#25552;&#20379;&#20102;&#20998;&#21035;&#20197;&#27169;&#22411;&#21442;&#25968;&#20026;&#22522;&#30784;&#30340;&#22235;&#31181;&#20559;&#24046;&#28304;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#21487;&#20197;&#20998;&#26512;&#23427;&#20204;&#30340;&#34892;&#20026;&#24182;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#21028;&#26029;&#23427;&#20204;&#30340;&#23384;&#22312;&#21644;&#26368;&#22823;&#21270;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.13364</link><description>&lt;p&gt;
&#25581;&#31034;&#22240;&#26524;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Dissecting Causal Biases. (arXiv:2310.13364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22240;&#25968;&#25454;&#29983;&#25104;&#21644;&#25910;&#38598;&#26041;&#24335;&#24341;&#36215;&#30340;&#22240;&#26524;&#20559;&#24046;&#65292;&#25552;&#20379;&#20102;&#20998;&#21035;&#20197;&#27169;&#22411;&#21442;&#25968;&#20026;&#22522;&#30784;&#30340;&#22235;&#31181;&#20559;&#24046;&#28304;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#21487;&#20197;&#20998;&#26512;&#23427;&#20204;&#30340;&#34892;&#20026;&#24182;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#21028;&#26029;&#23427;&#20204;&#30340;&#23384;&#22312;&#21644;&#26368;&#22823;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#27979;&#37327;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#26159;&#35299;&#20915;&#23376;&#32676;&#20307;&#21644;/&#25110;&#20010;&#20307;&#20043;&#38388;&#20844;&#24179;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#22312;&#27979;&#37327;&#27495;&#35270;&#26102;&#30340;&#20219;&#20309;&#20559;&#24046;&#37117;&#21487;&#33021;&#23548;&#33268;&#23545;&#27495;&#35270;&#30495;&#23454;&#20540;&#30340;&#25918;&#22823;&#25110;&#20302;&#20272;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#31867;&#30001;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21644;/&#25110;&#25910;&#38598;&#26041;&#24335;&#24341;&#36215;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22240;&#26524;&#20559;&#24046;&#65292;&#24182;&#20351;&#29992;&#22240;&#26524;&#24615;&#39046;&#22495;&#30340;&#24037;&#20855;&#26469;&#24418;&#24335;&#21270;&#23450;&#20041;&#21644;&#20998;&#26512;&#36825;&#20123;&#20559;&#24046;&#12290;&#25105;&#20204;&#32771;&#34385;&#22235;&#31181;&#20559;&#24046;&#28304;&#65306;&#28151;&#26434;&#65292;&#36873;&#25321;&#65292;&#27979;&#37327;&#21644;&#20132;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#27599;&#31181;&#20559;&#24046;&#28304;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#27169;&#22411;&#21442;&#25968;&#20026;&#22522;&#30784;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20998;&#26512;&#27599;&#31181;&#20559;&#24046;&#28304;&#30340;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#23427;&#20204;&#19981;&#23384;&#22312;&#20197;&#21450;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#23427;&#20204;&#34987;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#24076;&#26395;&#25552;&#20379;&#30340;&#29305;&#24449;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#30740;&#31350;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately measuring discrimination in machine learning-based automated decision systems is required to address the vital issue of fairness between subpopulations and/or individuals. Any bias in measuring discrimination can lead to either amplification or underestimation of the true value of discrimination. This paper focuses on a class of bias originating in the way training data is generated and/or collected. We call such class causal biases and use tools from the field of causality to formally define and analyze such biases. Four sources of bias are considered, namely, confounding, selection, measurement, and interaction. The main contribution of this paper is to provide, for each source of bias, a closed-form expression in terms of the model parameters. This makes it possible to analyze the behavior of each source of bias, in particular, in which cases they are absent and in which other cases they are maximized. We hope that the provided characterizations help the community better 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.13362</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#27979;&#35797;&#23454;&#29616;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36890;&#29992;&#38169;&#35823;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Towards General Error Diagnosis via Behavioral Testing in Machine Translation. (arXiv:2310.13362v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#27979;&#35797;&#20026;&#35786;&#26029;&#35821;&#35328;&#38169;&#35823;&#21644;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#37325;&#35201;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#23558;&#34892;&#20026;&#27979;&#35797;&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#20154;&#21147;&#26469;&#21046;&#20316;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#22312;&#26032;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#19978;&#30340;&#32763;&#35793;&#36136;&#37327;&#30340;&#21442;&#32771;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#32763;&#35793;&#34892;&#20026;&#27979;&#35797;&#24037;&#20316;&#36890;&#36807;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#26469;&#32469;&#24320;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#38480;&#21046;&#20102;&#23545;&#29305;&#23450;&#31867;&#22411;&#38169;&#35823;&#30340;&#35786;&#26029;&#65292;&#27604;&#22914;&#21333;&#20010;&#25968;&#23383;&#25110;&#36135;&#24065;&#35789;&#30340;&#38169;&#35823;&#32763;&#35793;&#12290;&#20026;&#20102;&#35786;&#26029;&#36890;&#29992;&#38169;&#35823;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#65288;BTPGBT&#65289;&#26694;&#26550;&#26469;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#34892;&#20026;&#27979;&#35797;&#12290;BTPGBT&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#65288;BTPG&#65289;&#26041;&#27861;&#65292;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on var
&lt;/p&gt;</description></item><item><title>DeepFDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#35299;&#20915;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13349</link><description>&lt;p&gt;
DeepFDR&#65306;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13349
&lt;/p&gt;
&lt;p&gt;
DeepFDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#35299;&#20915;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#22312;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20998;&#26512;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#22522;&#20110;&#20307;&#32032;&#30340;&#26816;&#39564;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#27979;&#35797;&#33021;&#21147;&#30340;&#22823;&#24133;&#25439;&#22833;&#12290;&#34429;&#28982;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#31354;&#38388;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#20294;&#26159;&#24403;&#22788;&#29702;&#22797;&#26434;&#30340;&#33041;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#26102;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20248;&#24615;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#32780;&#22270;&#20687;&#20998;&#21106;&#19982;&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepFDR&#30340;&#26032;&#22411;&#31354;&#38388;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26469;&#35299;&#20915;&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#12290;&#21253;&#25324;&#20840;&#38754;&#30340;&#27169;&#25311;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;FDG-PET&#24433;&#20687;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;DeepFDR&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;DeepFDR&#19981;&#20165;&#22312;&#34394;&#35686;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36824;&#26377;&#25928;&#38477;&#20302;&#20102;&#34394;&#20551;&#30340;&#38750;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but als
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22810;&#21305;&#37197;&#21644;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#36127;&#29699;&#38754;&#26494;&#24347;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#36830;&#32493;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#22266;&#23450;&#25972;&#25968;&#21442;&#25968;&#65292;&#19988;&#33021;&#22815;&#30452;&#25509;&#24471;&#21040;&#20108;&#36827;&#21046;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13311</link><description>&lt;p&gt;
&#38024;&#23545;&#26080;&#38480;&#21046;&#22810;&#21305;&#37197;&#21644;&#32858;&#31867;&#38382;&#39064;&#30340;&#38750;&#36127;&#29699;&#38754;&#26494;&#24347;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-Negative Spherical Relaxations for Universe-Free Multi-Matching and Clustering. (arXiv:2310.13311v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13311
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22810;&#21305;&#37197;&#21644;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#36127;&#29699;&#38754;&#26494;&#24347;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#36830;&#32493;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#22266;&#23450;&#25972;&#25968;&#21442;&#25968;&#65292;&#19988;&#33021;&#22815;&#30452;&#25509;&#24471;&#21040;&#20108;&#36827;&#21046;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#36127;&#29699;&#38754;&#26494;&#24347;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#21807;&#19968;&#24615;&#32422;&#26463;&#30340;&#20108;&#36827;&#21046;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#21305;&#37197;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#23558;&#30456;&#24212;&#30340;&#20108;&#36827;&#21046;&#30697;&#38453;&#32422;&#26463;&#26494;&#24347;&#21040;&#65288;&#39640;&#32500;&#65289;&#38750;&#36127;&#29699;&#38754;&#19978;&#12290;&#20026;&#20102;&#20248;&#21270;&#25105;&#20204;&#30340;&#26494;&#24347;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26465;&#20214;&#24130;&#36845;&#20195;&#26041;&#27861;&#26469;&#36845;&#20195;&#25913;&#36827;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#19982;&#23431;&#23449;&#22823;&#23567;&#65288;&#25110;&#31751;&#30340;&#25968;&#37327;&#65289;&#65288;&#38388;&#25509;&#65289;&#30456;&#20851;&#30340;&#36830;&#32493;&#26631;&#37327;&#21442;&#25968;&#19978;&#36827;&#34892;&#25195;&#25551;&#12290;&#19982;&#29616;&#26377;&#30340;&#22312;&#20248;&#21270;&#20043;&#21069;&#38656;&#35201;&#22266;&#23450;&#25972;&#25968;&#23431;&#23449;&#22823;&#23567;&#30340;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#21160;&#35843;&#25972;&#31867;&#20284;&#30340;&#36830;&#32493;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35889;&#22810;&#21305;&#37197;&#21644;&#35889;&#32858;&#31867;&#26377;&#19968;&#23450;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#25105;&#20204;&#30340;&#21046;&#23450;&#26041;&#26696;&#20855;&#26377;&#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21183;&#65292;&#21363;&#25105;&#20204;&#19981;&#20381;&#36182;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#36807;&#31243;&#26469;&#33719;&#24471;&#20108;&#36827;&#21046;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel non-negative spherical relaxation for optimization problems over binary matrices with injectivity constraints, which in particular has applications in multi-matching and clustering. We relax respective binary matrix constraints to the (high-dimensional) non-negative sphere. To optimize our relaxed problem, we use a conditional power iteration method to iteratively improve the objective function, while at same time sweeping over a continuous scalar parameter that is (indirectly) related to the universe size (or number of clusters). Opposed to existing procedures that require to fix the integer universe size before optimization, our method automatically adjusts the analogous continuous parameter. Furthermore, while our approach shares similarities with spectral multi-matching and spectral clustering, our formulation has the strong advantage that we do not rely on additional post-processing procedures to obtain binary results. Our method shows compelling results in vari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#30340;&#23567;&#22411;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#22810;&#20010;&#31572;&#26696;&#24182;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13307</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Test-Time Self-Adaptive Small Language Models for Question Answering. (arXiv:2310.13307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#30340;&#23567;&#22411;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#22810;&#20010;&#31572;&#26696;&#24182;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#32489;&#65292;&#20363;&#22914;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#35760;&#24518;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#24120;&#35782;&#65292;&#20294;&#30001;&#20110;&#33021;&#21147;&#26377;&#38480;&#65292;&#36716;&#31227;&#21644;&#36866;&#24212;&#30693;&#35782;&#21040;&#30446;&#26631;&#20219;&#21153;&#26102;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#23569;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24102;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#24494;&#35843;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#25105;&#20204;&#20063;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#26469;&#36716;&#31227;&#20855;&#26377;&#26377;&#38480;&#30693;&#35782;&#30340;&#26356;&#23567;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#19968;&#20010;&#20540;&#24471;&#25506;&#31350;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#24182;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#30340;&#36739;&#23567;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#38543;&#26426;&#29983;&#25104;&#22810;&#20010;&#31572;&#26696;&#65292;&#28982;&#21518;&#22312;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#26679;&#26412;&#30340;&#21516;&#26102;&#23545;&#23427;&#20204;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#20943;&#36731;&#19981;&#20934;&#30830;&#26631;&#31614;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#22312;&#22522;&#20934;QA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness ac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#22270;&#21644;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#21709;&#24212;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#27809;&#26377;&#29992;&#25143;&#26126;&#30830;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.13297</link><description>&lt;p&gt;
&#35299;&#30721;&#27785;&#40664;&#30340;&#22823;&#22810;&#25968;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#31038;&#20132;&#22270;&#36827;&#34892;&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting. (arXiv:2310.13297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13297
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#22270;&#21644;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#21709;&#24212;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#27809;&#26377;&#29992;&#25143;&#26126;&#30830;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#23186;&#20307;&#30340;&#33258;&#21160;&#21709;&#24212;&#39044;&#27979;&#22312;&#24110;&#21161;&#20869;&#23481;&#29983;&#20135;&#32773;&#26377;&#25928;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#24433;&#21709;&#24182;&#38450;&#27490;&#24847;&#22806;&#30340;&#36127;&#38754;&#32467;&#26524;&#65288;&#22914;&#31038;&#20250;&#20914;&#31361;&#21644;&#36947;&#24503;&#20260;&#23475;&#65289;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#39044;&#27979;&#21709;&#24212;&#65292;&#24517;&#39035;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#20010;&#20307;&#21608;&#22260;&#30340;&#31038;&#20132;&#21160;&#24577;&#21644;&#32972;&#26223;&#20449;&#24687;&#30340;&#25514;&#26045;&#65292;&#23588;&#20854;&#26159;&#22312;&#29992;&#25143;&#26126;&#30830;&#30340;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#28508;&#22312;&#21442;&#19982;&#32773;&#65289;&#12290;&#27491;&#22914;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;97%&#30340;&#25512;&#25991;&#20165;&#30001;&#26368;&#27963;&#36291;&#30340;25%&#29992;&#25143;&#20135;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23545;&#20110;&#22914;&#20309;&#22788;&#29702;&#21644;&#21033;&#29992;&#36825;&#20123;&#37325;&#35201;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#24335;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#26377;&#31038;&#20132;&#32593;&#32476;&#20043;&#19978;&#35825;&#23548;&#20986;&#19968;&#20010;&#20197;&#20449;&#24565;&#20026;&#20013;&#24515;&#30340;&#22270;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;&#25105;&#20204;&#20551;&#35774;&#35825;&#23548;&#20986;&#30340;&#22270;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#35268;&#27169;&#33016;&#37096; X &#23556;&#32447;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#22270;&#20687;-&#26631;&#31614;&#23545;&#20026;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#24182;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#22810;&#20010;&#22270;&#20687;&#21644;&#22810;&#20010;&#37096;&#20998;&#26469;&#35299;&#20915;&#33016;&#37096; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#30340;&#35757;&#32451;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25193;&#20805;&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13292</link><description>&lt;p&gt;
CXR-CLIP&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#33016;&#37096; X &#23556;&#32447;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training. (arXiv:2310.13292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#35268;&#27169;&#33016;&#37096; X &#23556;&#32447;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#22270;&#20687;-&#26631;&#31614;&#23545;&#20026;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#24182;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#22810;&#20010;&#22270;&#20687;&#21644;&#22810;&#20010;&#37096;&#20998;&#26469;&#35299;&#20915;&#33016;&#37096; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#30340;&#35757;&#32451;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25193;&#20805;&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#38598;&#23545;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#22312;&#27809;&#26377;&#26114;&#36149;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#24320;&#21457;&#24378;&#22823;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#22270;&#20687;-&#26631;&#31614;&#23545;&#20026;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#24182;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#22810;&#20010;&#22270;&#20687;&#21644;&#22810;&#20010;&#37096;&#20998;&#26469;&#35299;&#20915;&#33016;&#37096; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#22270;&#25991;&#25968;&#25454;&#32570;&#20047;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20004;&#31181;&#23545;&#27604;&#25439;&#22833;&#65292;&#20998;&#21035;&#31216;&#20026; ICL &#21644; TCL&#65292;&#29992;&#20110;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#21644;&#25253;&#21578;&#30340;&#30740;&#31350;&#23618;&#38754;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#30340;&#35757;&#32451;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25193;&#22823;&#30340;&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#65292;&#32780;&#29306;&#29298;&#20102;&#36739;&#23567;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/kakaobrain/cxr-clip &#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21457;&#29616;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#20445;&#25252;&#25514;&#26045;&#21644;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13291</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65306;&#20197;&#25688;&#35201;&#20219;&#21153;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks. (arXiv:2310.13291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21457;&#29616;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#20445;&#25252;&#25514;&#26045;&#21644;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#35843;&#26597;&#25104;&#21592;&#25512;&#26029;&#65288;MI&#65289;&#25915;&#20987;&#65306;&#36890;&#36807;&#32473;&#20986;&#19968;&#20010;&#26679;&#26412;&#21644;&#23545;&#27169;&#22411;API&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#21487;&#20197;&#30830;&#23450;&#26679;&#26412;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#21644;&#27169;&#22411;&#23545;&#25991;&#26723;&#20462;&#25913;&#30340;&#25269;&#25239;&#21147;&#20316;&#20026;&#28508;&#22312;&#30340;MI&#20449;&#21495;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#29978;&#33267;&#22312;&#27809;&#26377;&#21442;&#32771;&#25688;&#35201;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;&#25688;&#35201;&#27169;&#22411;&#20197;&#38450;&#27490;MI&#25915;&#20987;&#30340;&#20960;&#31181;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model's resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#24403;&#21069;&#29366;&#24577;&#21644;&#20808;&#21069;&#29366;&#24577;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#36882;&#35745;&#31639;&#12290;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#36890;&#24120;&#38656;&#35201;&#21453;&#21521;&#20256;&#36882;&#35745;&#31639;&#30340;&#25968;&#25454;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.13284</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#23616;&#37096;&#35268;&#21017;&#23398;&#20064;&#24490;&#29615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Recurrent Models with Temporally Local Rules. (arXiv:2310.13284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#24403;&#21069;&#29366;&#24577;&#21644;&#20808;&#21069;&#29366;&#24577;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#36882;&#35745;&#31639;&#12290;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#36890;&#24120;&#38656;&#35201;&#21453;&#21521;&#20256;&#36882;&#35745;&#31639;&#30340;&#25968;&#25454;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#25104;&#27169;&#22411;&#25311;&#21512;&#21040;&#39034;&#24207;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#26102;&#38388;&#36827;&#34892;&#20004;&#20010;&#36882;&#24402;&#35745;&#31639;&#65292;&#19968;&#20010;&#27491;&#21521;&#35745;&#31639;&#65292;&#19968;&#20010;&#21453;&#21521;&#35745;&#31639;&#12290;&#21518;&#32773;&#21487;&#20197;&#26159;&#25439;&#22833;&#26799;&#24230;&#30340;&#35745;&#31639;&#65288;&#22914;&#21453;&#21521;&#20256;&#25773;&#65289;&#65292;&#20063;&#21487;&#20197;&#26159;&#25512;&#29702;&#31639;&#27861;&#30340;&#35745;&#31639;&#65288;&#22914;RTS/Kalman&#24179;&#28369;&#22120;&#65289;&#12290;&#29305;&#21035;&#26159;&#21453;&#21521;&#20256;&#36882;&#35745;&#31639;&#22312;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#65288;&#22240;&#20026;&#23427;&#26159;&#20018;&#34892;&#30340;&#65292;&#26080;&#27861;&#21033;&#29992;GPU&#65289;&#65292;&#32780;&#19988;&#24456;&#38590;&#26144;&#23556;&#21040;&#29983;&#29289;&#36807;&#31243;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#27861;&#65307;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#38750;&#24120;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#35201;&#27714;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#24403;&#21069;&#29366;&#24577;&#21644;&#20808;&#21069;&#29366;&#24577;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#36716;&#31227;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#19968;&#21407;&#21017;&#30340;&#19981;&#21516;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#21453;&#21521;&#20256;&#36882;&#35745;&#31639;&#30340;&#25968;&#25454;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fitting generative models to sequential data typically involves two recursive computations through time, one forward and one backward. The latter could be a computation of the loss gradient (as in backpropagation through time), or an inference algorithm (as in the RTS/Kalman smoother). The backward pass in particular is computationally expensive (since it is inherently serial and cannot exploit GPUs), and difficult to map onto biological processes. Work-arounds have been proposed; here we explore a very different one: requiring the generative model to learn the joint distribution over current and previous states, rather than merely the transition probabilities. We show on toy datasets that different architectures employing this principle can learn aspects of the data typically requiring the backward pass.
&lt;/p&gt;</description></item><item><title>FedLoRA&#26159;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#27599;&#20010;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#35757;&#32451;&#20010;&#24615;&#21270;&#19988;&#24322;&#26500;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13283</link><description>&lt;p&gt;
FedLoRA&#65306;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning. (arXiv:2310.13283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13283
&lt;/p&gt;
&lt;p&gt;
FedLoRA&#26159;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#27599;&#20010;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#35757;&#32451;&#20010;&#24615;&#21270;&#19988;&#24322;&#26500;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; Paradig&#65292;&#20854;&#20013;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#35843;&#22810;&#20010;&#21442;&#19982;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#22312;&#20998;&#25955;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#36825;&#31181;&#33539; Paradig &#38480;&#21046;&#20102;&#25152;&#26377;&#23458;&#25143;&#31471;&#24517;&#39035;&#20351;&#29992;&#30456;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#65288;&#21516;&#26500;&#65289;&#12290;&#23454;&#36341;&#20013;&#65292;FL&#32463;&#24120;&#38754;&#20020;&#32479;&#35745;&#24322;&#36136;&#24615;&#12289;&#31995;&#32479;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#31561;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#28608;&#21457;&#20102;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;MHPFL&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#20026;&#27599;&#20010;FL&#23458;&#25143;&#31471;&#35757;&#32451;&#19968;&#20010;&#20010;&#24615;&#21270;&#19988;&#24322;&#26500;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;MHPFL&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#25509;&#21463;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65288;FedLoRA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (a.k.a. FL clients) to train a model collaboratively on decentralized data with privacy protection. This paradigm constrains that all clients have to train models with the same structures (homogeneous). In practice, FL often faces statistical heterogeneity, system heterogeneity and model heterogeneity challenges. These challenging issues inspire the field of Model-Heterogeneous Personalized Federated Learning (MHPFL) which aims to train a personalized and heterogeneous local model for each FL client. Existing MHPFL approaches cannot achieve satisfactory model performance, acceptable computational overhead and efficient communication simultaneously. To bridge this gap, we propose a novel computation- and communication-efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (FedLoRA). It is designed to incorporate a homogeneous 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.13276</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#23454;&#29616;&#40065;&#26834;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#37325;&#22823;&#36827;&#23637;&#20027;&#35201;&#26159;&#36890;&#36807;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#31361;&#30772;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#34920;&#31034;&#24448;&#24448;&#22312;&#26377;&#38480;&#30340;&#20984;&#38181;&#20869;&#32858;&#38598;&#65288;&#20316;&#20026;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65289;&#65292;&#36825;&#30001;&#20110;&#36825;&#20123;&#34920;&#31034;&#30340;&#19981;&#21487;&#20998;&#31163;&#24615;&#32780;&#38459;&#30861;&#20102;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22810;&#20010;&#36328;&#27169;&#24577;&#22522;&#20934;&#21644;&#26041;&#27861;&#32463;&#39564;&#35777;&#23454;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#30340;&#23384;&#22312;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;InvGC&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#22270;&#21367;&#31215;&#21644;&#24179;&#22343;&#27744;&#21270;&#21551;&#21457;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InvGC&#22312;&#25968;&#25454;&#38598;&#20013;&#23450;&#20041;&#22270;&#25299;&#25169;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#21367;&#31215;&#20197;&#19968;&#31181;&#20943;&#27861;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#22320;&#20998;&#31163;&#34920;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;InvGC&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32423;&#22270;&#25299;&#25169;&#65292;Lo
&lt;/p&gt;
&lt;p&gt;
Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called InvGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, InvGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of InvGC, we propose an advanced graph topology, Lo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#36890;&#36807;&#32534;&#30721;&#38382;&#39064;&#34920;&#31034;&#21644;&#37319;&#29992;&#29289;&#29702;&#23398;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.13270</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#29289;&#29702;&#23398;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#27714;&#35299;&#26032;&#32473;&#23450;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Meta-learning of Physics-informed Neural Networks for Efficiently Solving Newly Given PDEs. (arXiv:2310.13270v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#36890;&#36807;&#32534;&#30721;&#38382;&#39064;&#34920;&#31034;&#21644;&#37319;&#29992;&#29289;&#29702;&#23398;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#20803;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#21508;&#31181;PDE&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#30693;&#35782;&#24212;&#29992;&#20110;&#35299;&#20915;&#26032;&#32473;&#23450;&#30340;PDE&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;PDE&#38382;&#39064;&#32534;&#30721;&#20026;&#38382;&#39064;&#34920;&#31034;&#65292;&#20854;&#20013;&#25511;&#21046;&#26041;&#31243;&#36890;&#36807;&#20559;&#23548;&#25968;&#30340;&#22810;&#39033;&#24335;&#20989;&#25968;&#31995;&#25968;&#34920;&#31034;&#65292;&#36793;&#30028;&#26465;&#20214;&#36890;&#36807;&#19968;&#32452;&#28857;&#26465;&#20214;&#23545;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#31034;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#29992;&#20110;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#36807;&#31243;&#39640;&#25928;&#22320;&#39044;&#27979;&#38382;&#39064;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#26368;&#23567;&#21270;&#22522;&#20110;&#8220;&#29289;&#29702;&#23398;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#8221;&#26694;&#26550;&#36866;&#24212;&#20110;PDE&#38382;&#39064;&#26102;&#30340;&#39044;&#26399;&#35823;&#24046;&#65292;&#20511;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#35299;&#20915;&#26041;&#26696;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural network-based meta-learning method to efficiently solve partial differential equation (PDE) problems. The proposed method is designed to meta-learn how to solve a wide variety of PDE problems, and uses the knowledge for solving newly given PDE problems. We encode a PDE problem into a problem representation using neural networks, where governing equations are represented by coefficients of a polynomial function of partial derivatives, and boundary conditions are represented by a set of point-condition pairs. We use the problem representation as an input of a neural network for predicting solutions, which enables us to efficiently predict problem-specific solutions by the forwarding process of the neural network without updating model parameters. To train our model, we minimize the expected error when adapted to a PDE problem based on the physics-informed neural network framework, by which we can evaluate the error even when solutions are unknown. We demonstrate that 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#25490;&#24207;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36827;&#23637;&#21442;&#25968;&#26469;&#26377;&#25928;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13269</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#27169;&#25311;&#36864;&#28779;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank. (arXiv:2310.13269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#25490;&#24207;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36827;&#23637;&#21442;&#25968;&#26469;&#26377;&#25928;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#39046;&#22495;&#12290;&#30001;&#20110;&#29305;&#24449;&#36873;&#25321;&#24050;&#34987;&#21457;&#29616;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#30740;&#31350;&#23558;&#35813;&#36807;&#31243;&#24212;&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#39046;&#22495;&#26159;&#38750;&#24120;&#26377;&#36259;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#31216;&#20026;&#27169;&#25311;&#36864;&#28779;&#30340;&#27969;&#34892;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#27169;&#25311;&#36864;&#28779;&#30340;&#19968;&#33324;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#37051;&#22495;&#36873;&#25321;&#31574;&#30053;&#21644;&#28201;&#24230;&#20919;&#21364;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#31216;&#20026;&#36827;&#23637;&#21442;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20116;&#20010;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#39564;&#35777;&#65292;&#25105;&#20204;&#36824;&#23558;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#19982;&#21478;&#19968;&#31181;&#26377;&#25928;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21363;&#23616;&#37096;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-to-rank is an applied domain of supervised machine learning. As feature selection has been found to be effective for improving the accuracy of learning models in general, it is intriguing to investigate this process for learning-to-rank domain. In this study, we investigate the use of a popular meta-heuristic approach called simulated annealing for this task. Under the general framework of simulated annealing, we explore various neighborhood selection strategies and temperature cooling schemes. We further introduce a new hyper-parameter called the progress parameter that can effectively be used to traverse the search space. Our algorithms are evaluated on five publicly benchmark datasets of learning-to-rank. For a better validation, we also compare the simulated annealing-based feature selection algorithm with another effective meta-heuristic algorithm, namely local beam search. Extensive experimental results shows the efficacy of our proposed models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPM-Solver-v3&#65292;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#30340;&#26032;&#22411;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20248;&#21270;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20197;&#20943;&#23567;ODE&#35299;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#22810;&#27493;&#26041;&#27861;&#21644;&#39044;&#27979;-&#26657;&#27491;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13268</link><description>&lt;p&gt;
DPM-Solver-v3: &#20351;&#29992;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#25913;&#36827;&#30340;&#25193;&#25955;ODE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. (arXiv:2310.13268v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPM-Solver-v3&#65292;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#30340;&#26032;&#22411;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20248;&#21270;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20197;&#20943;&#23567;ODE&#35299;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#22810;&#27493;&#26041;&#27861;&#21644;&#39044;&#27979;-&#26657;&#27491;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21033;&#29992;DPMs&#30340;&#29305;&#23450;ODE&#24418;&#24335;&#30340;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#26469;&#21152;&#36895;&#37319;&#26679;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#39640;&#24230;&#20381;&#36182;&#29305;&#23450;&#21442;&#25968;&#21270;&#65288;&#22914;&#22122;&#22768;/&#25968;&#25454;&#39044;&#27979;&#65289;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#20197;&#33719;&#24471;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#26368;&#20339;&#21442;&#25968;&#21270;&#65292;&#20197;&#26368;&#23567;&#21270;ODE&#35299;&#30340;&#19968;&#38454;&#31163;&#25955;&#21270;&#38169;&#35823;&#12290;&#22522;&#20110;&#36825;&#31181;&#20844;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#39640;&#25928;&#35745;&#31639;&#30340;&#31995;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DPM-Solver-v3&#30340;&#26032;&#24555;&#36895;ODE&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#32463;&#39564;&#27169;&#22411;&#32479;&#35745;&#8221;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#20102;&#22810;&#27493;&#26041;&#27861;&#21644;&#39044;&#27979;-&#26657;&#27491;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25913;&#21892;&#22312;&#23569;&#37327;&#20989;&#25968;&#35780;&#20272;&#65288;NFE&#65289;&#25110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have exhibited excellent performance for high-fidelity image generation while suffering from inefficient sampling. Recent works accelerate the sampling procedure by proposing fast ODE solvers that leverage the specific ODE form of DPMs. However, they highly rely on specific parameterization during inference (such as noise/data prediction), which might not be the optimal choice. In this work, we propose a novel formulation towards the optimal parameterization during sampling that minimizes the first-order discretization error of the ODE solution. Based on such formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs by introducing several coefficients efficiently computed on the pretrained model, which we call \textit{empirical model statistics}. We further incorporate multistep methods and a predictor-corrector framework, and propose some techniques for improving sample quality at small numbers of function evaluations (NFE) or
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#21457;&#29616;&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#36136;&#37327;&#24182;&#25913;&#21892;&#20102;&#36328;&#27169;&#24577;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;&#38899;&#39057;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25928;&#26524;&#36739;&#23567;&#12290;&#21478;&#22806;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25913;&#21892;&#20102;&#25991;&#26412;&#31354;&#38388;&#30340;&#22343;&#21248;&#24615;&#65292;&#20294;&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#23545;&#40784;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.13267</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Language Encoder of Contrastive Cross-modal Models. (arXiv:2310.13267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#21457;&#29616;&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#36136;&#37327;&#24182;&#25913;&#21892;&#20102;&#36328;&#27169;&#24577;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;&#38899;&#39057;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25928;&#26524;&#36739;&#23567;&#12290;&#21478;&#22806;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25913;&#21892;&#20102;&#25991;&#26412;&#31354;&#38388;&#30340;&#22343;&#21248;&#24615;&#65292;&#20294;&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#22914;CLIP&#21644;CLAP&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#21644;&#38899;&#39057;&#35821;&#35328;&#20219;&#21153;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#21644;&#25913;&#36827;&#36824;&#24456;&#26377;&#38480;&#65292;&#32780;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#23558;&#22270;&#20687;/&#38899;&#39057;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32534;&#30721;&#25104;&#21521;&#37327;&#34920;&#31034;&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#30340;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#36136;&#37327;&#21644;&#36328;&#27169;&#24577;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#32534;&#30721;&#22120;&#36136;&#37327;&#65292;&#24182;&#26377;&#21161;&#20110;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25913;&#36827;&#20102;&#35832;&#22914;CyCLIP&#31561;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38899;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#30340;&#25928;&#30410;&#36739;&#23567;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#34920;&#31034;&#31354;&#38388;&#65292;&#20197;&#20102;&#35299;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#30340;&#20248;&#21183;&#65292;&#24182;&#21457;&#29616;&#23427;&#25913;&#21892;&#20102;&#25991;&#26412;&#31354;&#38388;&#30340;&#22343;&#21248;&#24615;&#65292;&#20294;&#20195;&#20215;&#26159;&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive cross-modal models such as CLIP and CLAP aid various vision-language (VL) and audio-language (AL) tasks. However, there has been limited investigation of and improvement in their language encoder, which is the central component of encoding natural language descriptions of image/audio into vector representations. We extensively evaluate how unsupervised and supervised sentence embedding training affect language encoder quality and cross-modal task performance. In VL pretraining, we found that sentence embedding training language encoder quality and aids in cross-modal tasks, improving contrastive VL models such as CyCLIP. In contrast, AL pretraining benefits less from sentence embedding training, which may result from the limited amount of pretraining data. We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity, at the cost of decreased cross-modal alignment.
&lt;/p&gt;</description></item><item><title>DIG-MILP&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20174;&#38750;&#24120;&#26377;&#38480;&#30340;MILP&#25968;&#25454;&#20013;&#25552;&#21462;&#28145;&#23618;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19982;&#30446;&#26631;&#25968;&#25454;&#38750;&#24120;&#25509;&#36817;&#30340;&#26679;&#20363;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;MILP&#30340;&#23545;&#20598;&#24615;&#26469;&#30830;&#20445;&#29983;&#25104;&#31354;&#38388;&#30340;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#20197;&#21450;&#29983;&#25104;&#23454;&#20363;&#30340;&#26377;&#30028;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13261</link><description>&lt;p&gt;
DIG-MILP: &#19968;&#31181;&#20855;&#26377;&#21487;&#34892;&#24615;&#20445;&#35777;&#30340;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#28145;&#24230;&#26679;&#20363;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee. (arXiv:2310.13261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13261
&lt;/p&gt;
&lt;p&gt;
DIG-MILP&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20174;&#38750;&#24120;&#26377;&#38480;&#30340;MILP&#25968;&#25454;&#20013;&#25552;&#21462;&#28145;&#23618;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19982;&#30446;&#26631;&#25968;&#25454;&#38750;&#24120;&#25509;&#36817;&#30340;&#26679;&#20363;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;MILP&#30340;&#23545;&#20598;&#24615;&#26469;&#30830;&#20445;&#29983;&#25104;&#31354;&#38388;&#30340;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#20197;&#21450;&#29983;&#25104;&#23454;&#20363;&#30340;&#26377;&#30028;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#26159;&#35768;&#22810;&#20851;&#38190;&#24037;&#19994;&#24212;&#29992;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;NP&#38590;&#38382;&#39064;&#12290;&#26377;&#25928;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#27714;&#35299;&#22120;&#30340;&#35843;&#20248;&#20197;&#21450;&#29992;&#20110;MILP&#35299;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#37117;&#20381;&#36182;&#20110;&#22823;&#37327;&#12289;&#22810;&#26679;&#21644;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#22270;&#20687;&#21644;&#25991;&#26412;&#39046;&#22495;&#20016;&#23500;&#30340;&#33258;&#28982;&#25968;&#25454;&#30456;&#27604;&#65292;MILP&#30340;&#25968;&#25454;&#32570;&#20047;&#65292;&#20984;&#26174;&#20102;&#21512;&#25104;MILP&#29983;&#25104;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DIG-MILP&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#38750;&#24120;&#26377;&#38480;&#30340;MILP&#25968;&#25454;&#20013;&#25552;&#21462;&#28145;&#23618;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19982;&#30446;&#26631;&#25968;&#25454;&#38750;&#24120;&#25509;&#36817;&#30340;&#26679;&#20363;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;MILP&#30340;&#23545;&#20598;&#24615;&#65292;DIG-MILP&#20445;&#35777;&#20102;&#27491;&#30830;&#21644;&#23436;&#25972;&#30340;&#29983;&#25104;&#31354;&#38388;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#23454;&#20363;&#30340;&#26377;&#30028;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#31361;&#20986;&#20102;DIG-MILP&#29983;&#25104;&#30340;&#26679;&#20363;&#30340;&#26032;&#39062;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed-integer linear programming (MILP) stands as a notable NP-hard problem pivotal to numerous crucial industrial applications. The development of effective algorithms, the tuning of solvers, and the training of machine learning models for MILP resolution all hinge on access to extensive, diverse, and representative data. Yet compared to the abundant naturally occurring data in image and text realms, MILP is markedly data deficient, underscoring the vital role of synthetic MILP generation. We present DIG-MILP, a deep generative framework based on variational auto-encoder (VAE), adept at extracting deep-level structural features from highly limited MILP data and producing instances that closely mirror the target data. Notably, by leveraging the MILP duality, DIG-MILP guarantees a correct and complete generation space as well as ensures the boundedness and feasibility of the generated instances. Our empirical study highlights the novelty and quality of the instances generated by DIG-MIL
&lt;/p&gt;</description></item><item><title>ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.13258</link><description>&lt;p&gt;
ManiCast: &#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13258
&lt;/p&gt;
&lt;p&gt;
ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#23494;&#21512;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20381;&#36182;&#20934;&#30830;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#22823;&#35268;&#27169;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#25805;&#20316;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20851;&#38190;&#36716;&#25240;&#28857;&#22788;&#31215;&#32047;&#20102;&#36739;&#39640;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#19979;&#28216;&#35268;&#21010;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#19982;&#20854;&#39044;&#27979;&#26368;&#26377;&#21487;&#33021;&#30340;&#20154;&#20307;&#36816;&#21160;&#65292;&#20135;&#29983;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#23601;&#36275;&#22815;&#20102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ManiCast&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#35268;&#21010;&#22120;&#20197;&#25191;&#34892;&#21327;&#21516;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#20154;&#31867;&#21644;7&#20010;&#33258;&#30001;&#24230;&#30340;&#26426;&#26800;&#33218;&#22312;&#22914;&#21453;&#24212;&#25605;&#25292;&#12289;&#29289;&#20307;&#20132;&#25509;&#21644;&#21327;&#21516;&#25670;&#26700;&#31561;&#22810;&#20010;&#30495;&#23454;&#20219;&#21153;&#20013;&#30340;&#27969;&#30021;&#12289;&#23454;&#26102;&#20132;&#20114;&#12290;&#25105;&#20204;&#23545;&#36816;&#21160;&#39044;&#27979;&#21644;&#31471;&#21040;&#31471;&#30340;&#39044;&#27979;-&#35268;&#21010;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13253</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#22810;&#26679;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Context-Enhanced Diversified Recommendation. (arXiv:2310.13253v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#19968;&#30452;&#33268;&#21147;&#20110;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36861;&#27714;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#24448;&#24448;&#23548;&#33268;&#20102;&#22810;&#26679;&#24615;&#30340;&#38477;&#20302;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#23545;&#31574;&#24212;&#36816;&#32780;&#29983;&#65292;&#23558;&#22810;&#26679;&#24615;&#19982;&#20934;&#30830;&#24615;&#21516;&#31561;&#30475;&#24453;&#65292;&#24182;&#22312;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#23454;&#36341;&#32773;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#26159;&#36830;&#25509;&#23454;&#20307;&#21644;&#39033;&#30446;&#30340;&#20449;&#24687;&#24211;&#65292;&#36890;&#36807;&#21152;&#20837;&#28145;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#22686;&#21152;&#25512;&#33616;&#22810;&#26679;&#24615;&#30340;&#26377;&#21033;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#65292;&#26377;&#25928;&#22320;&#37327;&#21270;&#20102;&#30693;&#35782;&#22270;&#35889;&#39046;&#22495;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22810;&#26679;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#26469;&#25552;&#39640;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Recommender Systems (RecSys) has been extensively studied to enhance accuracy by leveraging users' historical interactions. Nonetheless, this persistent pursuit of accuracy frequently engenders diminished diversity, culminating in the well-recognized "echo chamber" phenomenon. Diversified RecSys has emerged as a countermeasure, placing diversity on par with accuracy and garnering noteworthy attention from academic circles and industry practitioners. This research explores the realm of diversified RecSys within the intricate context of knowledge graphs (KG). These KGs act as repositories of interconnected information concerning entities and items, offering a propitious avenue to amplify recommendation diversity through the incorporation of insightful contextual information. Our contributions include introducing an innovative metric, Entity Coverage, and Relation Coverage, which effectively quantifies diversity within the KG domain. Additionally, we introduce the Diversified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLEE-GNN&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#30340;&#36793;&#32536;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;FLEE-GNN&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#38887;&#24615;&#30340;&#26222;&#36866;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2310.13248</link><description>&lt;p&gt;
FLEE-GNN&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#30340;&#36793;&#32536;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows. (arXiv:2310.13248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLEE-GNN&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#30340;&#36793;&#32536;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;FLEE-GNN&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#38887;&#24615;&#30340;&#26222;&#36866;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#34913;&#37327;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#30340;&#38887;&#24615;&#26159;&#35299;&#20915;&#26085;&#30410;&#20005;&#37325;&#30340;&#39135;&#21697;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#29699;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#23427;&#20204;&#30340;&#22810;&#32500;&#20132;&#20114;&#21644;&#20915;&#31574;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLEE-GNN&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#36793;&#32536;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#22686;&#24378;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#32593;&#32476;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#20998;&#26512;&#65292;&#23427;&#26159;&#31354;&#38388;&#32593;&#32476;&#30340;&#19968;&#31181;&#31867;&#22411;&#12290;FLEE-GNN&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#22312;&#26222;&#36866;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#38544;&#31169;&#24847;&#35782;&#21644;&#20998;&#25955;&#29305;&#24615;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#36328;&#22320;&#29702;&#21306;&#22495;&#30340;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#38887;&#24615;&#20998;&#26512;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;FLEE-GNN&#30340;&#21019;&#26032;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#23454;&#39564;&#35774;&#35745;&#21644;f
&lt;/p&gt;
&lt;p&gt;
Understanding and measuring the resilience of food supply networks is a global imperative to tackle increasing food insecurity. However, the complexity of these networks, with their multidimensional interactions and decisions, presents significant challenges. This paper proposes FLEE-GNN, a novel Federated Learning System for Edge-Enhanced Graph Neural Network, designed to overcome these challenges and enhance the analysis of geospatial resilience of multicommodity food flow network, which is one type of spatial networks. FLEE-GNN addresses the limitations of current methodologies, such as entropy-based methods, in terms of generalizability, scalability, and data privacy. It combines the robustness and adaptability of graph neural networks with the privacy-conscious and decentralized aspects of federated learning on food supply network resilience analysis across geographical regions. This paper also discusses FLEE-GNN's innovative data generation techniques, experimental designs, and f
&lt;/p&gt;</description></item><item><title>&#36879;&#26126;&#24230;&#38382;&#39064;&#26159;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#25919;&#31574;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#40657;&#30418;&#23376;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#21644;&#38382;&#36131;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21644;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13240</link><description>&lt;p&gt;
&#36879;&#26126;&#24230;&#25361;&#25112;&#19982;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#35780;&#20272; - &#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#38382;&#36131;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability. (arXiv:2310.13240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13240
&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#24230;&#38382;&#39064;&#26159;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#25919;&#31574;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#40657;&#30418;&#23376;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#21644;&#38382;&#36131;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21644;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#24320;&#22987;&#22312;&#23454;&#38469;&#25919;&#31574;&#35780;&#20272;&#20219;&#21153;&#20013;&#20351;&#29992;&#65292;&#28789;&#27963;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#25152;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#40657;&#30418;&#23376;&#65292;&#21363;&#27809;&#26377;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#20272;&#35745;&#12290;&#36825;&#22312;&#25919;&#31574;&#35780;&#20272;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25919;&#24220;&#39046;&#22495;&#65292;&#22240;&#20026;&#24456;&#38590;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#25353;&#29031;&#20844;&#27491;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#22522;&#20110;&#27491;&#30830;&#30340;&#35777;&#25454;&#35299;&#37322;&#65292;&#24182;&#19988;&#36879;&#26126;&#21040;&#36275;&#20197;&#20801;&#35768;&#22312;&#20986;&#29616;&#38382;&#39064;&#26102;&#36827;&#34892;&#38382;&#36131;&#12290;&#28982;&#32780;&#65292;&#22312;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#35752;&#35770;&#36879;&#26126;&#24230;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#36879;&#26126;&#24230;&#38382;&#39064;&#26159;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#20844;&#20849;&#25919;&#31574;&#35780;&#20272;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21644;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal machine learning tools are beginning to see use in real-world policy evaluation tasks to flexibly estimate treatment effects. One issue with these methods is that the machine learning models used are generally black boxes, i.e., there is no globally interpretable way to understand how a model makes estimates. This is a clear problem in policy evaluation applications, particularly in government, because it is difficult to understand whether such models are functioning in ways that are fair, based on the correct interpretation of evidence and transparent enough to allow for accountability if things go wrong. However, there has been little discussion of transparency problems in the causal machine learning literature and how these might be overcome. This paper explores why transparency issues are a problem for causal machine learning in public policy evaluation applications and considers ways these problems might be addressed through explainable AI tools and by simplifying models in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#26041;&#24335;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#39640;&#20102;&#32593;&#32476;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13236</link><description>&lt;p&gt;
&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Training A Semantic Communication System with Federated Learning. (arXiv:2310.13236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#26041;&#24335;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#39640;&#20102;&#32593;&#32476;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#22240;&#20854;&#20943;&#23569;&#25968;&#25454;&#20887;&#20313;&#30340;&#21151;&#33021;&#25104;&#20026;&#19979;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#30340;&#37325;&#35201;&#25903;&#26609;&#12290;&#22823;&#22810;&#25968;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#20551;&#35774;&#26377;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#25968;&#25454;&#20027;&#35201;&#26159;&#30001;&#29992;&#25143;&#29983;&#25104;&#30340;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#25968;&#25454;&#20256;&#36755;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#23545;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#26696;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#20013;&#25506;&#32034;&#35821;&#20041;&#36890;&#20449;&#65292;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#35299;&#20915;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#20256;&#36865;&#30340;&#20449;&#24687;&#37327;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#33410;&#30465;&#22823;&#37327;&#24102;&#23485;&#65292;&#20943;&#23569;&#25972;&#20307;&#32593;&#32476;&#27969;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Semantic communication has emerged as a pillar for the next generation of communication systems due to its capabilities in alleviating data redundancy. Most semantic communication systems are built using advanced deep learning models whose performance heavily depends on data availability. These studies assume that an abundance of training data is available, which is unrealistic. In practice, data is mainly created on the user side. Due to privacy and security concerns, the transmission of data is restricted, which is necessary for conventional centralized training schemes. To address this challenge, we explore semantic communication in federated learning (FL) setting that utilizes user data without leaking privacy. Additionally, we design our system to tackle the communication overhead by reducing the quantity of information delivered in each global round. In this way, we can save significant bandwidth for resource-limited devices and reduce overall network traffic. Finally, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21095;&#26412;&#20013;&#35282;&#33394;&#30340;&#29702;&#35299;&#65292;&#20174;&#35282;&#33394;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#20854;&#20010;&#24615;&#21644;&#36523;&#20221;&#65292;&#24182;&#22312;&#22810;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#19978;&#36890;&#36807;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.13231</link><description>&lt;p&gt;
&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#21095;&#26412;&#30340;&#35282;&#33394;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multi-level Contrastive Learning for Script-based Character Understanding. (arXiv:2310.13231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21095;&#26412;&#20013;&#35282;&#33394;&#30340;&#29702;&#35299;&#65292;&#20174;&#35282;&#33394;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#20854;&#20010;&#24615;&#21644;&#36523;&#20221;&#65292;&#24182;&#22312;&#22810;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#19978;&#36890;&#36807;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21095;&#26412;&#20013;&#30340;&#35282;&#33394;&#29702;&#35299;&#22330;&#26223;&#65292;&#26088;&#22312;&#20174;&#20182;&#20204;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#35282;&#33394;&#30340;&#20010;&#24615;&#21644;&#36523;&#20221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#36825;&#19968;&#22330;&#26223;&#20013;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#25429;&#25417;&#35282;&#33394;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;SpanBERT&#65292;Longformer&#65292;BigBird&#21644;ChatGPT-3.5&#65289;&#36827;&#34892;&#20102;&#19977;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#25361;&#25112;&#21644;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#35282;&#33394;&#29702;&#35299;&#22330;&#26223;&#30340;&#32447;&#32034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#22312;github&#19978;&#24320;&#28304;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#32593;&#22336;&#20026;https://github.com/David-Li0406/Script-based-Character-Understanding&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters' personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters' global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work on github at https://github.com/David-Li0406/Script-based-Character-Understanding.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>ToolChain*&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20915;&#31574;&#21644;&#35268;&#21010;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13227</link><description>&lt;p&gt;
ToolChain*: &#20351;&#29992;A*&#25628;&#32034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search. (arXiv:2310.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13227
&lt;/p&gt;
&lt;p&gt;
ToolChain*&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20915;&#31574;&#21644;&#35268;&#21010;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#20915;&#31574;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#21487;&#20197;&#19982;&#21508;&#31181;&#24037;&#20855;&#65288;&#20363;&#22914;&#21151;&#33021;&#24615;API&#65289;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#29983;&#25104;&#25191;&#34892;&#19968;&#31995;&#21015;API&#20989;&#25968;&#35843;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#35745;&#21010;&#12290;&#20505;&#36873;API&#20989;&#25968;&#35843;&#29992;&#30340;&#22810;&#26679;&#24615;&#26174;&#33879;&#25193;&#23637;&#20102;&#21160;&#20316;&#31354;&#38388;&#65292;&#21152;&#22823;&#20102;&#23545;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#22312;&#24222;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#38754;&#20020;&#21333;&#21521;&#25506;&#32034;&#22256;&#38590;&#65292;&#38519;&#20837;&#23616;&#37096;&#20248;&#21270;&#35299;&#65292;&#35201;&#20040;&#36973;&#21463;&#31351;&#23613;&#22320;&#36941;&#21382;&#25152;&#26377;&#28508;&#22312;&#21160;&#20316;&#25152;&#23548;&#33268;&#30340;&#20302;&#25928;&#23548;&#33322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolChain*&#65292;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#20195;&#29702;&#35268;&#21010;&#31639;&#27861;&#12290;&#23427;&#23558;&#25972;&#20010;&#21160;&#20316;&#31354;&#38388;&#26500;&#24314;&#20026;&#19968;&#20010;&#20915;&#31574;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#35299;&#20915;&#26041;&#26696;&#35745;&#21010;&#20013;&#28041;&#21450;&#30340;&#21487;&#33021;&#30340;API&#20989;&#25968;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A
&lt;/p&gt;</description></item><item><title>&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.13225</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13225
&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#24120;&#35268;&#21069;&#39304;&#23618;&#65288;FFLs&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#21518;&#32773;&#65292;&#20294;&#20855;&#26377;&#26377;&#21033;&#30340;&#35745;&#31639;&#23646;&#24615;&#12290;SNNKs&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;FFL&#20013;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#28857;&#31215;&#20869;&#26680;&#22312;&#26368;&#32456;&#35745;&#31639;&#20013;&#36830;&#25509;&#23427;&#20204;&#12290;&#23427;&#20204;&#20063;&#26356;&#21152;&#34920;&#36798;&#21147;&#24378;&#65292;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#65292;&#36229;&#20986;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#20989;&#25968;&#33539;&#22260;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33719;&#24471;&#39069;&#22806;&#30340;&#21387;&#32553;&#25928;&#30410;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#23548;&#33268;&#23436;&#20840;&#25414;&#32465;&#32593;&#32476;&#65292;&#20854;&#26368;&#20248;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#22343;&#26041;&#35823;&#24046;&#65289;&#30340;&#26174;&#24335;&#20844;&#24335;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26222;&#36941;&#24615;&#26426;&#21046;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universa
&lt;/p&gt;</description></item><item><title>&#31561;&#21464;Transformer&#33021;&#22815;&#35299;&#20915;&#33258;&#23398;&#20064;Monte-Carlo&#20013;&#30340;&#20302;&#25509;&#21463;&#29575;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#23481;&#37327;&#65292;&#22312;&#27169;&#25311;&#29289;&#29702;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13222</link><description>&lt;p&gt;
&#31561;&#21464;Transformer&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Equivariant Transformer is all you need. (arXiv:2310.13222v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13222
&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;Transformer&#33021;&#22815;&#35299;&#20915;&#33258;&#23398;&#20064;Monte-Carlo&#20013;&#30340;&#20302;&#25509;&#21463;&#29575;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#23481;&#37327;&#65292;&#22312;&#27169;&#25311;&#29289;&#29702;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22312;&#35745;&#31639;&#29289;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#21152;&#36895;&#65292;&#29992;&#20110;&#22312;&#26684;&#28857;&#19978;&#27169;&#25311;&#31995;&#32479;&#12290;&#31561;&#21464;&#24615;&#23545;&#20110;&#27169;&#25311;&#29289;&#29702;&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25551;&#36848;&#30340;&#27010;&#29575;&#20998;&#24067;&#24341;&#20837;&#20102;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#38477;&#20302;&#20102;&#25968;&#25454;&#23545;&#31216;&#24615;&#21644;&#29289;&#29702;&#23450;&#24459;&#20559;&#31163;&#30340;&#38169;&#35823;&#22806;&#25512;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#23558;&#23545;&#31216;&#24615;&#24378;&#21152;&#20110;&#27169;&#22411;&#26377;&#26102;&#20250;&#23548;&#33268;&#33258;&#23398;&#20064;Monte-Carlo&#65288;SLMC&#65289;&#20013;&#30340;&#25509;&#21463;&#29575;&#19981;&#20339;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Transformers&#20013;&#20351;&#29992;&#30340;Attention&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#23481;&#37327;&#12290;&#25105;&#20204;&#23558;&#31561;&#21464;&#24615;Attention&#24341;&#20837;&#21040;SLMC&#20013;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#20108;&#32500;&#26684;&#28857;&#33258;&#26059;&#36153;&#31859;&#27169;&#22411;&#19978;&#30340;&#26032;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#20811;&#26381;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#20302;&#25509;&#21463;&#29575;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;Transformers&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20284;&#30340;&#25509;&#21463;&#29575;&#30340;&#26631;&#24230;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning, deep learning, has been accelerating computational physics, which has been used to simulate systems on a lattice. Equivariance is essential to simulate a physical system because it imposes a strong induction bias for the probability distribution described by a machine learning model. This reduces the risk of erroneous extrapolation that deviates from data symmetries and physical laws. However, imposing symmetry on the model sometimes occur a poor acceptance rate in self-learning Monte-Carlo (SLMC). On the other hand, Attention used in Transformers like GPT realizes a large model capacity. We introduce symmetry equivariant attention to SLMC. To evaluate our architecture, we apply it to our proposed new architecture on a spin-fermion model on a two-dimensional lattice. We find that it overcomes poor acceptance rates for linear models and observe the scaling law of the acceptance rate as in the large language models with Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#20026;&#23545;&#27604;&#23398;&#20064;&#27169;&#24335;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#36890;&#36807;&#24314;&#31435;&#26799;&#24230;&#19979;&#38477;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#24212;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#35774;&#35745;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.13220</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#23545;&#27604;&#23398;&#20064;&#27169;&#24335;&#26159;&#31561;&#20215;&#30340;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern. (arXiv:2310.13220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#20026;&#23545;&#27604;&#23398;&#20064;&#27169;&#24335;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#36890;&#36807;&#24314;&#31435;&#26799;&#24230;&#19979;&#38477;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#24212;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#35774;&#35745;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#20960;&#20010;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#20026;&#23545;&#27604;&#23398;&#20064;&#27169;&#24335;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#26680;&#26041;&#27861;&#24314;&#31435;&#26799;&#24230;&#19979;&#38477;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22312;&#19968;&#33324;&#20351;&#29992;&#30340;softmax&#27880;&#24847;&#35774;&#32622;&#19979;&#32780;&#19981;&#26159;&#32447;&#24615;&#27880;&#24847;&#35774;&#32622;&#19979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#23545;&#27604;&#23398;&#20064;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23545;&#24212;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#35752;&#35770;&#20102;&#22312;&#36825;&#31181;&#23545;&#27604;&#23398;&#20064;&#27169;&#24335;&#19979;&#21487;&#33021;&#30340;&#25913;&#36827;&#65292;&#22522;&#20110;&#36825;&#20123;&#25913;&#36827;&#21487;&#20197;&#36827;&#19968;&#27493;&#20462;&#25913;&#33258;&#27880;&#24847;&#23618;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#23545;&#27604;&#23398;&#20064;&#27169;&#24335;&#31561;&#20215;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models based on Transformers have demonstrated amazing in-context learning (ICL) abilities. Given several demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we interpret the inference process of ICL as a gradient descent process in a contrastive learning pattern. Firstly, leveraging kernel methods, we establish the relationship between gradient descent and self-attention mechanism under generally used softmax attention setting instead of linear attention setting. Then, we analyze the corresponding gradient descent process of ICL from the perspective of contrastive learning without negative samples and discuss possible improvements of this contrastive learning pattern, based on which the self-attention layer can be further modified. Finally, we design experiments to support our opinions. To the best of our knowledge, our work is the f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;&#20102;&#27668;&#20505;&#21464;&#21270;&#12289;&#21019;&#26032;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#23545;&#20915;&#31574;&#21644;&#31038;&#20250;&#20272;&#20540;&#20135;&#29983;&#39318;&#35201;&#24433;&#21709;&#65292;&#24182;&#22312;&#25972;&#21512;&#30340;&#27668;&#20505;&#32463;&#27982;&#21019;&#26032;&#26694;&#26550;&#20013;&#25552;&#20986;&#20102;&#23545;&#25237;&#36164;&#35843;&#25972;&#30340;&#37325;&#35201;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2310.13200</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;&#27668;&#20505;&#21464;&#21270;&#12289;&#21019;&#26032;&#21644;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Analysis of Climate Change, Innovation, and Uncertainty. (arXiv:2310.13200v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;&#20102;&#27668;&#20505;&#21464;&#21270;&#12289;&#21019;&#26032;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#23545;&#20915;&#31574;&#21644;&#31038;&#20250;&#20272;&#20540;&#20135;&#29983;&#39318;&#35201;&#24433;&#21709;&#65292;&#24182;&#22312;&#25972;&#21512;&#30340;&#27668;&#20505;&#32463;&#27982;&#21019;&#26032;&#26694;&#26550;&#20013;&#25552;&#20986;&#20102;&#23545;&#25237;&#36164;&#35843;&#25972;&#30340;&#37325;&#35201;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#27668;&#20505;&#32463;&#27982;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#35813;&#26694;&#26550;&#28041;&#21450;&#19977;&#31181;&#31867;&#22411;&#30340;&#36164;&#26412;&#65306;&#22312;&#29983;&#20135;&#36807;&#31243;&#20013;&#20135;&#29983;&#30899;&#25490;&#25918;&#30340;"&#33039;"&#36164;&#26412;&#65292;&#27809;&#26377;&#25490;&#25918;&#20294;&#36215;&#21021;&#19981;&#22914;"&#33039;"&#36164;&#26412;&#25928;&#29575;&#39640;&#30340;"&#28165;&#27905;"&#36164;&#26412;&#65292;&#20197;&#21450;&#38543;&#30528;&#30740;&#21457;&#25237;&#36164;&#22686;&#21152;&#24182;&#23548;&#33268;&#32511;&#33394;&#37096;&#38376;&#29983;&#20135;&#21147;&#25216;&#26415;&#21019;&#26032;&#30340;&#30693;&#35782;&#36164;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#25105;&#20204;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#27169;&#22411;&#26694;&#26550;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#23545;&#26368;&#20248;&#20915;&#31574;&#21644;&#31038;&#20250;&#20272;&#20540;&#22312;&#25105;&#20204;&#30340;&#25972;&#21512;&#27668;&#20505;&#32463;&#27982;&#21019;&#26032;&#26694;&#26550;&#20013;&#30340;&#19968;&#38454;&#24433;&#21709;&#12290;&#32771;&#34385;&#21040;&#27668;&#20505;&#21160;&#21147;&#23398;&#30340;&#30456;&#20114;&#20381;&#36182;&#19981;&#30830;&#23450;&#24615;&#12289;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#32463;&#27982;&#25439;&#23475;&#20197;&#21450;&#32511;&#33394;&#25216;&#26415;&#21464;&#38761;&#30340;&#21040;&#26469;&#65292;&#20250;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#36164;&#26412;&#25237;&#36164;&#36827;&#34892;&#22823;&#24133;&#35843;&#25972;&#65292;&#20197;&#24212;&#23545;&#25216;&#26415;&#21464;&#38761;&#21644;&#27668;&#20505;&#25439;&#23475;&#20005;&#37325;&#31243;&#24230;&#30340;&#25581;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the implications of model uncertainty in a climate-economics framework with three types of capital: "dirty" capital that produces carbon emissions when used for production, "clean" capital that generates no emissions but is initially less productive than dirty capital, and knowledge capital that increases with R\&amp;D investment and leads to technological innovation in green sector productivity. To solve our high-dimensional, non-linear model framework we implement a neural-network-based global solution method. We show there are first-order impacts of model uncertainty on optimal decisions and social valuations in our integrated climate-economic-innovation framework. Accounting for interconnected uncertainty over climate dynamics, economic damages from climate change, and the arrival of a green technological change leads to substantial adjustments to investment in the different capital types in anticipation of technological change and the revelation of climate damage severity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#34920;&#26684;&#25968;&#25454;&#20013;&#21015;&#21517;&#31216;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32553;&#20889;&#21015;&#21517;&#31216;&#23545;&#25968;&#25454;&#25628;&#32034;&#12289;&#35775;&#38382;&#21644;&#29702;&#35299;&#20219;&#21153;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;NameGuess&#26041;&#27861;&#36890;&#36807;&#23545;&#34920;&#26684;&#20869;&#23481;&#21644;&#21015;&#22836;&#21517;&#31216;&#36827;&#34892;&#35843;&#25972;&#65292;&#24471;&#21040;&#20102;&#19982;&#20154;&#31867;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13196</link><description>&lt;p&gt;
NameGuess&#65306;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#21015;&#21517;&#31216;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
NameGuess: Column Name Expansion for Tabular Data. (arXiv:2310.13196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#34920;&#26684;&#25968;&#25454;&#20013;&#21015;&#21517;&#31216;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32553;&#20889;&#21015;&#21517;&#31216;&#23545;&#25968;&#25454;&#25628;&#32034;&#12289;&#35775;&#38382;&#21644;&#29702;&#35299;&#20219;&#21153;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;NameGuess&#26041;&#27861;&#36890;&#36807;&#23545;&#34920;&#26684;&#20869;&#23481;&#21644;&#21015;&#22836;&#21517;&#31216;&#36827;&#34892;&#35843;&#25972;&#65292;&#24471;&#21040;&#20102;&#19982;&#20154;&#31867;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#21253;&#25324;&#25968;&#25454;&#24211;&#34892;&#19994;&#12290;&#22312;&#22788;&#29702;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#26222;&#36941;&#20351;&#29992;&#32553;&#20889;&#30340;&#21015;&#21517;&#31216;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#21508;&#31181;&#25968;&#25454;&#25628;&#32034;&#12289;&#35775;&#38382;&#21644;&#29702;&#35299;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;NameGuess&#65292;&#23558;&#25193;&#23637;&#21015;&#21517;&#31216;&#65288;&#29992;&#20110;&#25968;&#25454;&#24211;&#27169;&#24335;&#65289;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21046;&#20316;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;384K&#20010;&#32553;&#20889;-&#25193;&#23637;&#21015;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#34920;&#26684;&#30340;9.2K&#20010;&#20363;&#23376;&#12290;&#20026;&#20102;&#24212;&#23545;NameGuess&#20013;&#30340;&#22810;&#20041;&#21644;&#27495;&#20041;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#34920;&#26684;&#20869;&#23481;&#21644;&#21015;&#22836;&#21517;&#31216;&#36827;&#34892;&#35843;&#25972;&#26469;&#22686;&#24378;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#65288;&#20855;&#26377;2.7B&#20010;&#21442;&#25968;&#65289;&#65292;&#20854;&#24615;&#33021;&#19982;&#20154;&#31867;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65288;o
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names -- yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13193</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment. (arXiv:2310.13193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20998;&#37197;&#38382;&#39064;&#26159;&#20132;&#36890;&#27969;&#20998;&#26512;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#22478;&#24066;&#20132;&#36890;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#35757;&#32451;&#25439;&#22833;&#21644;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32593;&#32476;&#25299;&#25169;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22797;&#26434;&#20132;&#36890;&#27969;&#20998;&#26512;&#21644;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traffic assignment problem is one of the significant components of traffic flow analysis for which various solution approaches have been proposed. However, deploying these approaches for large-scale networks poses significant challenges. In this paper, we leverage the power of heterogeneous graph neural networks to propose a novel data-driven approach for traffic assignment and traffic flow learning. The proposed model is capable of capturing spatial traffic patterns across different links, yielding highly accurate results. We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy. Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies. This approach offers a promising solution for complex traffic flow analysis and predictio
&lt;/p&gt;</description></item><item><title>CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.13165</link><description>&lt;p&gt;
CycleNet&#65306;&#37325;&#26032;&#24605;&#32771;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#20013;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20197;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13165
&lt;/p&gt;
&lt;p&gt;
CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#30452;&#35266;&#30340;&#19968;&#33268;&#22270;&#20687;&#21040;&#22270;&#20687;&#65288;I2I&#65289;&#32763;&#35793;&#25509;&#21475;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DMs&#36827;&#34892;&#26080;&#37197;&#23545;&#30340;I2I&#32763;&#35793;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Cyclenet&#65292;&#19968;&#31181;&#26032;&#39062;&#20294;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#32435;&#20837;DMs&#20013;&#65292;&#20197;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;Cyclenet&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#26080;&#37197;&#23545;I2I&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;&#38500;&#20102;&#22330;&#26223;&#21644;&#23545;&#35937;&#32423;&#21035;&#30340;&#32763;&#35793;&#65292;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;I2I&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#29289;&#20307;&#30340;&#29289;&#29702;&#29366;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Cyclenet&#22312;&#32763;&#35793;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#25913;&#21464;&#25991;&#26412;&#25551;&#36848;&#26102;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13164</link><description>&lt;p&gt;
&#20960;&#20046;&#31561;&#21464;&#24615;&#36890;&#36807;&#26446;&#20195;&#25968;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#30456;&#23545;&#20110;&#32676;&#20316;&#29992;&#30340;&#31561;&#21464;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#36171;&#20104;&#19968;&#20010;&#26550;&#26500;&#20855;&#20307;&#30340;&#32676;&#31561;&#21464;&#24615;&#23545;&#27169;&#22411;&#25152;&#26399;&#26395;&#30475;&#21040;&#30340;&#25968;&#25454;&#21464;&#25442;&#31867;&#22411;&#26045;&#21152;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#12290;&#20005;&#26684;&#31561;&#21464;&#27169;&#22411;&#24378;&#21046;&#25191;&#34892;&#23545;&#31216;&#24615;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#36825;&#26679;&#30340;&#20005;&#26684;&#31561;&#21464;&#24615;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25110;&#20165;&#32534;&#30721;&#20102;&#36817;&#20284;&#25110;&#37096;&#20998;&#23545;&#31216;&#24615;&#30340;&#28508;&#22312;&#29289;&#29702;&#23450;&#24459;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20005;&#26684;&#31561;&#21464;&#24615;&#30340;&#20808;&#39564;&#23454;&#38469;&#19978;&#21487;&#33021;&#36807;&#20110;&#24378;&#22823;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#21363;&#20960;&#20046;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#29616;&#26377;&#23450;&#20041;&#19981;&#21516;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13161</link><description>&lt;p&gt;
&#19968;&#31181;&#20998;&#24067;&#24335;&#27668;&#35937;&#39044;&#27979;&#26041;&#27861;&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35299;&#20915;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs. (arXiv:2310.13161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#25968;&#25454;&#30340;&#20998;&#31867;&#38656;&#35201;&#23558;&#27668;&#35937;&#29616;&#35937;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#20026;&#20892;&#19994;&#12289;&#33322;&#31354;&#21644;&#28798;&#23475;&#31649;&#29702;&#31561;&#21508;&#20010;&#39046;&#22495;&#25552;&#20379;&#32454;&#33268;&#30340;&#20998;&#26512;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36825;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20998;&#26512;&#22823;&#22411;&#22810;&#32500;&#22825;&#27668;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#21253;&#25324;&#28201;&#24230;&#12289;&#28287;&#24230;&#12289;&#39118;&#36895;&#21644;&#27668;&#21387;&#31561;&#21464;&#37327;&#65292;&#36825;&#20123;&#21464;&#37327;&#23545;&#27668;&#35937;&#26465;&#20214;&#36215;&#30528;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#65292;&#20854;&#20013;&#26576;&#20123;&#22825;&#27668;&#20107;&#20214;&#65288;&#22914;&#39118;&#26292;&#25110;&#26497;&#31471;&#28201;&#24230;&#65289;&#21487;&#33021;&#34987;&#20302;&#20272;&#12290;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38598;&#20013;&#21644;&#32852;&#37030;&#29615;&#22659;&#20013;&#35299;&#20915;&#34920;&#26684;&#29366;&#22825;&#27668;&#25968;&#25454;&#20013;&#19981;&#24179;&#34913;&#31867;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#37319;&#29992;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#25110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of weather data involves categorizing meteorological phenomena into classes, thereby facilitating nuanced analyses and precise predictions for various sectors such as agriculture, aviation, and disaster management. This involves utilizing machine learning models to analyze large, multidimensional weather datasets for patterns and trends. These datasets may include variables such as temperature, humidity, wind speed, and pressure, contributing to meteorological conditions. Furthermore, it's imperative that classification algorithms proficiently navigate challenges such as data imbalances, where certain weather events (e.g., storms or extreme temperatures) might be underrepresented. This empirical study explores data augmentation methods to address imbalanced classes in tabular weather data in centralized and federated settings. Employing data augmentation techniques such as the Synthetic Minority Over-sampling Technique or Generative Adversarial Networks can improve t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24418;&#24335;&#21644;&#21019;&#26032;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;ODE&#23545;&#35270;&#39057;&#21160;&#24577;&#24314;&#27169;&#65292;&#20197;&#21450;&#36830;&#32493;&#26631;&#20934;&#27969;&#30340;&#26465;&#20214;&#21464;&#20307;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.13157</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#29992;&#20110;&#22270;&#20687;&#12289;3D&#21160;&#30011;&#21644;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Modeling for Images, 3D Animations, and Video. (arXiv:2310.13157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24418;&#24335;&#21644;&#21019;&#26032;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;ODE&#23545;&#35270;&#39057;&#21160;&#24577;&#24314;&#27169;&#65292;&#20197;&#21450;&#36830;&#32493;&#26631;&#20934;&#27969;&#30340;&#26465;&#20214;&#21464;&#20307;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#36890;&#36807;&#25506;&#32034;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24418;&#24335;&#21644;&#22270;&#20687;&#12289;3D&#21160;&#30011;&#21644;&#35270;&#39057;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#20379;&#22122;&#22768;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#21487;&#36870;&#21464;&#25442;&#30340;&#26550;&#26500;&#65292;&#24182;&#24212;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#36827;&#34892;&#29983;&#25104;&#20219;&#21153;&#21644;3D&#20869;&#23481;&#25805;&#20316;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37117;&#23558;&#26465;&#20214;&#20449;&#24687;&#32435;&#20837;&#21040;&#22686;&#24378;&#35270;&#35273;&#25968;&#25454;&#21512;&#25104;&#20013;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#31070;&#32463;ODE&#23545;&#35270;&#39057;&#21160;&#24577;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23613;&#31649;&#20165;&#35757;&#32451;&#29992;&#20110;&#37325;&#26500;&#24403;&#21069;&#24103;&#65292;&#20294;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#30340;&#35270;&#39057;&#24103;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#26631;&#20934;&#27969;&#30340;&#26465;&#20214;&#21464;&#20307;&#65292;&#36890;&#36807;&#36739;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#23454;&#29616;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
This dissertation attempts to drive innovation in the field of generative modeling for computer vision, by exploring novel formulations of conditional generative models, and innovative applications in images, 3D animations, and video. Our research focuses on architectures that offer reversible transformations of noise and visual data, and the application of encoder-decoder architectures for generative tasks and 3D content manipulation. In all instances, we incorporate conditional information to enhance the synthesis of visual data, improving the efficiency of the generation process as well as the generated content.  We introduce the use of Neural ODEs to model video dynamics using an encoder-decoder architecture, demonstrating their ability to predict future video frames despite being trained solely to reconstruct current frames. Next, we propose a conditional variant of continuous normalizing flows that enables higher-resolution image generation based on lower-resolution input, achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20020;&#24202;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#27979;&#35797;&#24179;&#21488;CLIFT&#65292;&#20854;&#20013;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36861;&#36394;&#35813;&#26041;&#21521;&#36827;&#23637;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.13146</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#38382;&#31572;&#27169;&#22411;&#20013;&#20998;&#26512;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#30340;CLIFT
&lt;/p&gt;
&lt;p&gt;
CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain. (arXiv:2310.13146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20020;&#24202;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#27979;&#35797;&#24179;&#21488;CLIFT&#65292;&#20854;&#20013;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36861;&#36394;&#35813;&#26041;&#21521;&#36827;&#23637;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#24179;&#21488;CLIFT&#65288;&#20020;&#24202;&#20998;&#24067;&#20559;&#31227;&#65289;&#65292;&#29992;&#20110;&#20020;&#24202;&#39046;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27979;&#35797;&#24179;&#21488;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#19988;&#21487;&#38752;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#22312;&#35813;&#27979;&#35797;&#24179;&#21488;&#19979;&#35780;&#20272;&#20102;&#20960;&#20010;QA&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#12290;&#35813;&#27979;&#35797;&#24179;&#21488;&#20026;&#36861;&#36394;&#35813;&#26041;&#21521;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#23427;&#36824;&#24378;&#35843;&#20102;&#37319;&#29992;&#32771;&#34385;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;&#36890;&#36807;&#28155;&#21152;&#26356;&#22810;&#26679;&#26412;&#21644;&#27169;&#22411;&#32467;&#26524;&#26469;&#25193;&#23637;&#35821;&#26009;&#24211;&#12290;&#23436;&#25972;&#30340;&#35770;&#25991;&#21644;&#26356;&#26032;&#30340;&#22522;&#20934;&#21487;&#20197;&#22312;github.com/openlifescience-ai/clift&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical domain Question-answering task. The testbed includes 7.5k high-quality question answering samples to provide a diverse and reliable benchmark. We performed a comprehensive experimental study and evaluated several QA deep-learning models under the proposed testbed. Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift. Our findings emphasize the need for and the potential for increasing the robustness of clinical domain models under distributional shifts. The testbed offers one way to track progress in that direction. It also highlights the necessity of adopting evaluation metrics that consider robustness to natural distribution shifts. We plan to expand the corpus by adding more samples and model results. The full paper and the updated benchmark are available at github.com/openlifescience-ai/clift
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13139</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#23436;&#20840;&#30001;&#36866;&#24403;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#26469;&#25551;&#36848;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20219;&#20309;&#22312;&#26631;&#35760;&#22270;&#19978;&#35299;&#37322;&#30340;&#20851;&#20110;&#20108;&#20803;&#36923;&#36753;&#29255;&#27573;&#65288;GC2&#65289;&#30340;&#26597;&#35810;&#37117;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#22823;&#23567;&#20165;&#21462;&#20915;&#20110;&#26597;&#35810;&#28145;&#24230;&#30340;GNN&#26469;&#34920;&#31034;&#12290;&#27491;&#22914;[Barcelo&#65286;Al&#12290;&#65292;2020&#65292;Grohe&#65292;2021]&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#36825;&#20010;&#25551;&#36848;&#36866;&#29992;&#20110;&#19968;&#32452;&#28608;&#27963;&#20989;&#25968;&#30340;&#23478;&#26063;&#65292;&#36825;&#34920;&#26126;GNN&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#26469;&#34920;&#36798;&#19981;&#21516;&#30340;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#23618;&#27425;&#32467;&#26500;&#30340;&#23384;&#22312;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#26080;&#27861;&#34920;&#31034;GC2&#26597;&#35810;&#12290;&#36825;&#24847;&#21619;&#30528;&#22810;&#39033;&#24335;&#21644;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;sigmoid&#12289;&#21452;&#26354;&#27491;&#20999;&#31561;&#65289;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20998;&#31163;&#65292;&#24182;&#22238;&#31572;&#20102;[Grohe&#65292;2021]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#38544;&#31169;&#38656;&#27714;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25509;&#36817;&#32447;&#24615;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#33021;&#22815;&#28385;&#36275;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#21035;&#38544;&#31169;&#35201;&#27714;&#65292;&#24182;&#19988;&#33719;&#24471;&#26497;&#23567;&#21270;&#26368;&#20248;&#32467;&#26524;&#12290;&#26368;&#20005;&#26684;&#30340;&#29992;&#25143;&#30340;&#38544;&#31169;&#35201;&#27714;&#20915;&#23450;&#20102;&#25972;&#20307;&#30340;&#35823;&#24046;&#29575;&#65292;&#19988;&#25317;&#26377;&#36739;&#23569;&#20294;&#19981;&#21516;&#38544;&#31169;&#35201;&#27714;&#30340;&#29992;&#25143;&#37117;&#20250;&#33719;&#24471;&#36229;&#36807;&#33258;&#36523;&#38656;&#27714;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#19988;&#20445;&#25252;&#31243;&#24230;&#30456;&#21516;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#38544;&#31169;&#19981;&#25935;&#24863;&#30340;&#29992;&#25143;&#21487;&#20197;&#20813;&#36153;&#33719;&#24471;&#38750;&#24179;&#20961;&#31243;&#24230;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2310.13137</link><description>&lt;p&gt;
&#24322;&#26500;&#38544;&#31169;&#38656;&#27714;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mean Estimation Under Heterogeneous Privacy Demands. (arXiv:2310.13137v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#38544;&#31169;&#38656;&#27714;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25509;&#36817;&#32447;&#24615;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#33021;&#22815;&#28385;&#36275;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#21035;&#38544;&#31169;&#35201;&#27714;&#65292;&#24182;&#19988;&#33719;&#24471;&#26497;&#23567;&#21270;&#26368;&#20248;&#32467;&#26524;&#12290;&#26368;&#20005;&#26684;&#30340;&#29992;&#25143;&#30340;&#38544;&#31169;&#35201;&#27714;&#20915;&#23450;&#20102;&#25972;&#20307;&#30340;&#35823;&#24046;&#29575;&#65292;&#19988;&#25317;&#26377;&#36739;&#23569;&#20294;&#19981;&#21516;&#38544;&#31169;&#35201;&#27714;&#30340;&#29992;&#25143;&#37117;&#20250;&#33719;&#24471;&#36229;&#36807;&#33258;&#36523;&#38656;&#27714;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#19988;&#20445;&#25252;&#31243;&#24230;&#30456;&#21516;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#38544;&#31169;&#19981;&#25935;&#24863;&#30340;&#29992;&#25143;&#21487;&#20197;&#20813;&#36153;&#33719;&#24471;&#38750;&#24179;&#20961;&#31243;&#24230;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#37327;&#21270;&#31639;&#27861;&#36896;&#25104;&#30340;&#38544;&#31169;&#25439;&#22833;&#30340;&#25104;&#29087;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#23545;&#25152;&#26377;&#29992;&#25143;&#37117;&#26045;&#21152;&#32479;&#19968;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#36825;&#19982;&#29616;&#23454;&#24773;&#26223;&#19981;&#19968;&#33268;&#65292;&#22240;&#20026;&#29992;&#25143;&#21487;&#20197;&#20010;&#21035;&#20915;&#23450;&#33258;&#24049;&#30340;&#38544;&#31169;&#20559;&#22909;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22343;&#20540;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#21487;&#20197;&#26045;&#21152;&#33258;&#24049;&#19981;&#21516;&#30340;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#65292;&#19988;&#20855;&#26377;&#25509;&#36817;&#32447;&#24615;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#39281;&#21644;&#29616;&#35937;&#12290;&#21363;&#26368;&#20005;&#26684;&#29992;&#25143;&#30340;&#38544;&#31169;&#35201;&#27714;&#20915;&#23450;&#20102;&#25972;&#20307;&#30340;&#35823;&#24046;&#29575;&#12290;&#22240;&#27492;&#65292;&#25317;&#26377;&#36739;&#23569;&#20294;&#19981;&#21516;&#38544;&#31169;&#35201;&#27714;&#30340;&#29992;&#25143;&#37117;&#20250;&#33719;&#24471;&#36229;&#36807;&#33258;&#36523;&#38656;&#27714;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#19988;&#20445;&#25252;&#31243;&#24230;&#30456;&#21516;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#20123;&#23545;&#38544;&#31169;&#19981;&#25935;&#24863;&#30340;&#29992;&#25143;&#21487;&#20197;&#20813;&#36153;&#33719;&#24471;&#38750;&#24179;&#20961;&#31243;&#24230;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#32780;&#26080;&#38656;&#22312;&#20272;&#35745;&#22120;&#24615;&#33021;&#19978;&#20570;&#20986;&#20219;&#20309;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy (DP) is a well-established framework to quantify privacy loss incurred by any algorithm. Traditional formulations impose a uniform privacy requirement for all users, which is often inconsistent with real-world scenarios in which users dictate their privacy preferences individually. This work considers the problem of mean estimation, where each user can impose their own distinct privacy level. The algorithm we propose is shown to be minimax optimal and has a near-linear run-time. Our results elicit an interesting saturation phenomenon that occurs. Namely, the privacy requirements of the most stringent users dictate the overall error rates. As a consequence, users with less but differing privacy requirements are all given more privacy than they require, in equal amounts. In other words, these privacy-indifferent users are given a nontrivial degree of privacy for free, without any sacrifice in the performance of the estimator.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;AI&#39044;&#27979;&#26448;&#26009;&#24615;&#33021;&#30340;&#20010;&#20307;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13136</link><description>&lt;p&gt;
AI&#39044;&#27979;&#26448;&#26009;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Approaches for Uncertainty Quantification of AI-predicted Material Properties: A Comparison. (arXiv:2310.13136v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;AI&#39044;&#27979;&#26448;&#26009;&#24615;&#33021;&#30340;&#20010;&#20307;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#26448;&#26009;&#24615;&#33021;&#25968;&#25454;&#24211;&#30340;&#24314;&#31435;&#21644;&#24378;&#22823;&#35745;&#31639;&#26426;&#30340;&#21487;&#29992;&#24615;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24314;&#27169;&#25104;&#20026;&#39044;&#27979;&#26448;&#26009;&#24615;&#33021;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#34429;&#28982;ML&#27169;&#22411;&#24120;&#24120;&#25253;&#21578;&#32622;&#20449;&#21306;&#38388;&#65292;&#20294;&#39044;&#27979;&#21306;&#38388;&#65292;&#21363;&#27599;&#20010;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21364;&#19981;&#24120;&#35265;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36825;&#31181;&#20010;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21253;&#25324;&#33021;&#37327;&#23398;&#12289;&#21147;&#23398;&#12289;&#30005;&#23376;&#23398;&#12289;&#20809;&#23398;&#21644;&#20809;&#35889;&#24615;&#36136;&#22312;&#20869;&#30340;&#21313;&#31181;ML&#37327;&#19978;&#65292;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#37327;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#21306;&#38388;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large databases of material properties, together with the availability of powerful computers, has allowed machine learning (ML) modeling to become a widely used tool for predicting material performances. While confidence intervals are commonly reported for such ML models, prediction intervals, i.e., the uncertainty on each prediction, are not as frequently available. Here, we investigate three easy-to-implement approaches to determine such individual uncertainty, comparing them across ten ML quantities spanning energetics, mechanical, electronic, optical, and spectral properties. Specifically, we focused on the Quantile approach, the direct machine learning of the prediction intervals and Ensemble methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13129</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;CO2&#25490;&#25918;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions. (arXiv:2310.13129v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20132;&#36890;&#32593;&#32476;&#38754;&#20020;&#30528;&#27425;&#20248;&#25511;&#21046;&#31574;&#30053;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23545;&#20154;&#31867;&#20581;&#24247;&#12289;&#29615;&#22659;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182; contribute to traffic congestion. &#30001;&#20110;&#20132;&#36890;&#25317;&#22581;&#23548;&#33268;&#30340;&#31354;&#27668;&#27745;&#26579;&#27700;&#24179;&#19978;&#21319;&#21644;&#36890;&#21220;&#26102;&#38388;&#24310;&#38271;&#65292;&#20132;&#21449;&#36335;&#21475;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#25104;&#20026;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#20010;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#65292;&#20294;&#23545;&#20854;&#27604;&#36739;&#24615;&#33021;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#25490;&#25918;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#25991;&#29486;&#23545;&#35813;&#39046;&#22495;&#20851;&#27880;&#19981;&#22815;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EcoLight&#65292;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#26696;&#65292;&#19981;&#20165;&#33021;&#20943;&#23569;CO2&#25490;&#25918;&#65292;&#36824;&#33021;&#22312;&#35832;&#22914;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#39542;&#26102;&#38388;&#12289;CO2&#25490;&#25918;&#12289;&#31561;&#25351;&#26631;&#27604;&#36739;&#20102;&#34920;&#26684;&#24335;Q-Learning&#12289;DQN&#12289;SARSA&#21644;A2C&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19994;&#21153;&#25968;&#25454;&#39044;&#27979;&#23458;&#36816;&#28193;&#36718;&#29123;&#26009;&#28040;&#36153;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20837;&#21464;&#37327;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13123</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19994;&#21153;&#25968;&#25454;&#39044;&#27979;&#23458;&#36816;&#28193;&#36718;&#29123;&#26009;&#28040;&#36153;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study. (arXiv:2310.13123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19994;&#21153;&#25968;&#25454;&#39044;&#27979;&#23458;&#36816;&#28193;&#36718;&#29123;&#26009;&#28040;&#36153;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20837;&#21464;&#37327;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29615;&#20445;&#20132;&#36890;&#30340;&#37325;&#35201;&#24615;&#22686;&#21152;&#65292;&#20026;&#28023;&#36816;&#33337;&#21482;&#25552;&#20379;&#39640;&#25928;&#30340;&#25805;&#20316;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#32771;&#34385;&#21040;&#22825;&#27668;&#26465;&#20214;&#21644;&#20351;&#29992;&#33337;&#21482;&#19994;&#21153;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#29366;&#24577;&#30417;&#25511;&#26041;&#27861;&#38656;&#35201;&#20934;&#30830;&#23436;&#25972;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#33337;&#21482;&#30340;energy efficiency&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#25152;&#26377;&#25805;&#20316;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#23458;&#36816;&#33337;&#21482;&#25910;&#38598;&#30340;&#19994;&#21153;&#25968;&#25454;&#26469;&#39044;&#27979;&#29123;&#26009;&#28040;&#32791;&#30340;&#27169;&#22411;&#12290;&#32479;&#35745;&#21644;&#39046;&#22495;&#30693;&#35782;&#26041;&#27861;&#34987;&#29992;&#26469;&#36873;&#25321;&#27169;&#22411;&#30340;&#36866;&#24403;&#36755;&#20837;&#21464;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#38450;&#27490;&#36807;&#25311;&#21512;&#12289;&#32570;&#22833;&#25968;&#25454;&#21644;&#22810;&#37325;&#20849;&#32447;&#24615;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;&#30740;&#31350;&#20013;&#35843;&#26597;&#30340;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65288;MLR&#65289;&#12289;&#20915;&#31574;&#26641;&#26041;&#27861;&#65288;DT&#65289;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#26368;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#26469;&#33258;&#20110;&#20351;&#29992;ensemble&#26041;&#27861;&#24320;&#21457;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
As the importance of eco-friendly transportation increases, providing an efficient approach for marine vessel operation is essential. Methods for status monitoring with consideration to the weather condition and forecasting with the use of in-service data from ships requires accurate and complete models for predicting the energy efficiency of a ship. The models need to effectively process all the operational data in real-time. This paper presents models that can predict fuel consumption using in-service data collected from a passenger ship. Statistical and domain-knowledge methods were used to select the proper input variables for the models. These methods prevent over-fitting, missing data, and multicollinearity while providing practical applicability. Prediction models that were investigated include multiple linear regression (MLR), decision tree approach (DT), an artificial neural network (ANN), and ensemble methods. The best predictive performance was from a model developed using t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>RSAdapter&#26159;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#32447;&#24615;&#36716;&#25442;&#23618;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13120</link><description>&lt;p&gt;
RSAdapter: &#36866;&#24212;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering. (arXiv:2310.13120v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13120
&lt;/p&gt;
&lt;p&gt;
RSAdapter&#26159;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#32447;&#24615;&#36716;&#25442;&#23618;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#22270;&#20687;&#25551;&#36848;&#12289;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21644;&#22270;&#20687;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36965;&#24863;VQA&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#36164;&#28304;&#23494;&#38598;&#22411;&#25216;&#26415;&#65292;&#22914;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#25110;&#20174;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#25552;&#21462;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#35299;&#30721;&#22120;&#36827;&#34892;&#27169;&#24577;&#34701;&#21512;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24403;&#25968;&#37327;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSAdapter&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;RSAdapter&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#25554;&#20837;&#22312;&#36866;&#37197;&#22120;&#30340;&#27599;&#20010;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#21518;&#30340;&#39069;&#22806;&#32447;&#24615;&#36716;&#25442;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, with the rapid advancement of transformer models, transformer-based multimodal architectures have found wide application in various downstream tasks, including but not limited to Image Captioning, Visual Question Answering (VQA), and Image-Text Generation. However, contemporary approaches to Remote Sensing (RS) VQA often involve resource-intensive techniques, such as full fine-tuning of large models or the extraction of image-text features from pre-trained multimodal models, followed by modality fusion using decoders. These approaches demand significant computational resources and time, and a considerable number of trainable parameters are introduced. To address these challenges, we introduce a novel method known as RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter comprises two key components: the Parallel Adapter and an additional linear transformation layer inserted after each fully connected (FC) layer within the Adapter. This approach not on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TS-NODE&#65292;&#35813;&#26041;&#27861;&#26159;&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#30340;&#39318;&#20010;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;TS-NODE&#36890;&#36807;&#29983;&#25104;&#20266;&#26679;&#26412;&#26469;&#25299;&#23485;&#29366;&#24577;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#24182;&#21033;&#29992;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#31995;&#32479;&#25968;&#25454;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.13110</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#65306;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning of Dynamical Systems with Neural Ordinary Differential Equations: A Teacher-Student Model Approach. (arXiv:2310.13110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TS-NODE&#65292;&#35813;&#26041;&#27861;&#26159;&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#30340;&#39318;&#20010;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;TS-NODE&#36890;&#36807;&#29983;&#25104;&#20266;&#26679;&#26412;&#26469;&#25299;&#23485;&#29366;&#24577;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#24182;&#21033;&#29992;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#31995;&#32479;&#25968;&#25454;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21160;&#21147;&#31995;&#32479;&#23545;&#20110;&#21508;&#31181;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12289;&#26377;&#38480;&#30340;&#35266;&#23519;&#25110;&#32570;&#20047;&#20808;&#39564;&#30693;&#35782;&#65292;&#24314;&#27169;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#24314;&#27169;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20363;&#22914;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(NODE)&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#26377;&#38480;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#27425;&#20248;&#39044;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21322;&#30417;&#30563;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TS-NODE&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;NODE&#36827;&#34892;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#30340;&#39318;&#20010;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;TS-NODE&#21033;&#29992;&#24265;&#20215;&#29983;&#25104;&#30340;&#20266;&#26679;&#26412;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#31995;&#32479;&#25968;&#25454;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling dynamical systems is crucial for a wide range of tasks, but it remains challenging due to complex nonlinear dynamics, limited observations, or lack of prior knowledge. Recently, data-driven approaches such as Neural Ordinary Differential Equations (NODE) have shown promising results by leveraging the expressive power of neural networks to model unknown dynamics. However, these approaches often suffer from limited labeled training data, leading to poor generalization and suboptimal predictions. On the other hand, semi-supervised algorithms can utilize abundant unlabeled data and have demonstrated good performance in classification and regression tasks. We propose TS-NODE, the first semi-supervised approach to modeling dynamical systems with NODE. TS-NODE explores cheaply generated synthetic pseudo rollouts to broaden exploration in the state space and to tackle the challenges brought by lack of ground-truth system data under a teacher-student model. TS-NODE employs an unified o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#23450;&#20041;&#36801;&#31227;&#23398;&#20064;&#32593;&#32476;&#23545;MRI&#22270;&#20687;&#20013;&#30340;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;VGG-19&#26550;&#26500;&#21644;&#39069;&#22806;&#30340;&#38544;&#34255;&#23618;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13108</link><description>&lt;p&gt;
&#29992;&#33258;&#23450;&#20041;&#36801;&#31227;&#23398;&#20064;&#31616;&#21270;MRI&#22270;&#20687;&#20013;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Streamlining Brain Tumor Classification with Custom Transfer Learning in MRI Images. (arXiv:2310.13108v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#23450;&#20041;&#36801;&#31227;&#23398;&#20064;&#32593;&#32476;&#23545;MRI&#22270;&#20687;&#20013;&#30340;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;VGG-19&#26550;&#26500;&#21644;&#39069;&#22806;&#30340;&#38544;&#34255;&#23618;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#30340;&#21457;&#30149;&#29575;&#19981;&#26029;&#22686;&#21152;&#65292;&#20854;&#29305;&#24449;&#26159;&#24322;&#24120;&#32452;&#32455;&#22312;&#22823;&#33041;&#20013;&#30340;&#19981;&#21463;&#25511;&#21046;&#25193;&#25955;&#65292;&#20840;&#29699;&#27599;&#24180;&#20960;&#20046;&#26377;70&#19975;&#26032;&#30149;&#20363;&#34987;&#35786;&#26029;&#20986;&#26469;&#12290;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#24120;&#24120;&#29992;&#20110;&#33041;&#32959;&#30244;&#30340;&#35786;&#26029;&#65292;&#20934;&#30830;&#30340;&#20998;&#31867;&#26159;&#20851;&#38190;&#30340;&#20020;&#24202;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#23450;&#20041;&#36801;&#31227;&#23398;&#20064;&#32593;&#32476;&#23545;MRI&#22270;&#20687;&#20013;&#30340;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#26550;&#26500;&#65292;&#22914;RESNET-50&#65292;ALEXNET&#65292;VGG-16&#21644;VGG-19&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#20197;&#20943;&#23569;&#22797;&#26434;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;VGG-19&#26550;&#26500;&#65292;&#24182;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#38544;&#34255;&#23618;&#65292;&#36825;&#20943;&#23569;&#20102;&#22522;&#30784;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#20294;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumors are increasingly prevalent, characterized by the uncontrolled spread of aberrant tissues in the brain, with almost 700,000 new cases diagnosed globally each year. Magnetic Resonance Imaging (MRI) is commonly used for the diagnosis of brain tumors and accurate classification is a critical clinical procedure. In this study, we propose an efficient solution for classifying brain tumors from MRI images using custom transfer learning networks. While several researchers have employed various pre-trained architectures such as RESNET-50, ALEXNET, VGG-16, and VGG-19, these methods often suffer from high computational complexity. To address this issue, we present a custom and lightweight model using a Convolutional Neural Network-based pre-trained architecture with reduced complexity. Specifically, we employ the VGG-19 architecture with additional hidden layers, which reduces the complexity of the base architecture but improves computational efficiency. The objective is to achieve h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.13103</link><description>&lt;p&gt;
AVTENet: &#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20998;&#20139;&#30340;&#20266;&#36896;&#20869;&#23481;&#26159;&#19968;&#20010;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#65292;&#35201;&#27714;&#21152;&#24378;&#30417;&#31649;&#24182;&#32473;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#36229;&#30495;&#23454;&#30340;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#30340;&#26222;&#21450;&#24341;&#36215;&#20102;&#23545;&#38899;&#39057;&#21644;&#35270;&#35273;&#20266;&#36896;&#23041;&#32961;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#20266;&#36896;&#35270;&#39057;&#30340;&#20808;&#21069;&#24037;&#20316;&#21482;&#21033;&#29992;&#20102;&#35270;&#35273;&#27169;&#24577;&#25110;&#38899;&#39057;&#27169;&#24577;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#26469;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;&#28041;&#21450;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;CNN&#65292;&#24182;&#19988;&#26816;&#27979;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#21463;&#21040;Transformer&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22768;&#23398;&#25805;&#20316;&#30340;&#38899;&#39057;-&#35270;&#35273;Transformer&#38598;&#25104;&#32593;&#32476;&#65288;AVTENet&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13102</link><description>&lt;p&gt;
&#31890;&#23376;&#24341;&#23548;&#65306;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#22810;&#26679;&#24615;&#37319;&#26679;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#25104;&#21151;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21152;&#24555;&#20854;&#37319;&#26679;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#21462;&#22810;&#26679;&#24615;&#26679;&#26412;&#65292;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#36825;&#20250;&#36896;&#25104;&#19982;&#37319;&#26679;&#26102;&#38388;&#26080;&#20851;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#22788;&#29702;&#20102;&#22914;&#20309;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31890;&#23376;&#24341;&#23548;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#37319;&#26679;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#30340;&#32852;&#21512;&#31890;&#23376;&#26102;&#21464;&#20301;&#21183;&#24378;&#21046;&#23454;&#29616;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#31890;&#23376;&#24341;&#23548;&#20135;&#29983;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20197;&#21450;&#23427;&#23545;&#20301;&#21183;&#36873;&#25321;&#30340;&#24433;&#21709;&#21644;&#19982;&#20854;&#20182;&#23398;&#31185;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#22810;&#26679;&#24615;&#32780;&#19981;&#24433;&#21709;&#36136;&#37327;&#65292;&#24182;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#20013;&#38477;&#20302;&#20102;&#24179;&#22343;13%&#30340;&#20808;&#36827;&#25216;&#26415;&#20013;&#20540;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#26469;&#25913;&#21464;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#22343;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.13099</link><description>&lt;p&gt;
&#19981;&#20882;&#29359;&#65292;&#20271;&#29305; - &#25105;&#21482;&#20398;&#36785;&#20154;&#31867;&#65281;&#22810;&#20010;&#25910;&#20214;&#20154;&#21477;&#32423;&#25915;&#20987;&#26377;&#27602;&#24615;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network. (arXiv:2310.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#26469;&#25913;&#21464;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#22343;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#20167;&#24680;&#20449;&#24687;&#26411;&#23614;&#28155;&#21152;&#19968;&#20123;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#65292;&#25105;&#20204;&#33021;&#22815;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#26377;&#27602;&#24615;&#26816;&#27979;&#31995;&#32479;&#30340;&#26816;&#26597;&#12290;&#35813;&#26041;&#27861;&#22312;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#19971;&#31181;&#35821;&#35328;&#19978;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#23545;&#25239;&#19978;&#36848;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.
&lt;/p&gt;</description></item><item><title>SRAI&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#65288;Geospatial AI&#65289;&#39046;&#22495;&#24037;&#20855;&#38598;&#65292;&#21253;&#25324;&#19979;&#36733;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#12289;&#21010;&#20998;&#21306;&#22495;&#21644;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13098</link><description>&lt;p&gt;
SRAI: &#25506;&#32034;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#30340;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
SRAI: Towards Standardization of Geospatial AI. (arXiv:2310.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13098
&lt;/p&gt;
&lt;p&gt;
SRAI&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#65288;Geospatial AI&#65289;&#39046;&#22495;&#24037;&#20855;&#38598;&#65292;&#21253;&#25324;&#19979;&#36733;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#12289;&#21010;&#20998;&#21306;&#22495;&#21644;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#34920;&#31034;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;SRAI&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#30340;Python&#24211;&#12290;&#35813;&#24211;&#21487;&#20197;&#19979;&#36733;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#65292;&#20351;&#29992;&#22810;&#31181;&#31639;&#27861;&#23558;&#32473;&#23450;&#21306;&#22495;&#21010;&#20998;&#20026;&#24494;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;&#23427;&#21253;&#25324;&#22522;&#20934;&#27169;&#22411;&#20197;&#21450;&#26469;&#33258;&#24050;&#21457;&#34920;&#20316;&#21697;&#30340;&#26356;&#22797;&#26434;&#26041;&#27861;&#12290;&#36825;&#20123;&#21151;&#33021;&#20351;&#24471;&#22312;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#23436;&#25972;&#27969;&#27700;&#32447;&#20013;&#20351;&#29992;SRAI&#25104;&#20026;&#21487;&#33021;&#12290;&#35813;&#25552;&#35758;&#30340;&#24211;&#26159;&#26631;&#20934;&#21270;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24037;&#20855;&#38598;&#30340;&#31532;&#19968;&#27493;&#12290;&#23427;&#26159;&#23436;&#20840;&#24320;&#28304;&#30340;&#65292;&#24182;&#22312;Apache 2.0&#35768;&#21487;&#19979;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial Representations for Artificial Intelligence (\textit{srai}) is a Python library for working with geospatial data. The library can download geospatial data, split a given area into micro-regions using multiple algorithms and train an embedding model using various architectures. It includes baseline models as well as more complex methods from published works. Those capabilities make it possible to use \textit{srai} in a complete pipeline for geospatial task solving. The proposed library is the first step to standardize the geospatial AI domain toolset. It is fully open-source and published under Apache 2.0 licence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33136;&#37096;IMU&#35782;&#21035;&#25490;&#29699;&#36339;&#36291;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22810;&#23618;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65288;MS-TCN&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.13097</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25490;&#29699;&#36339;&#36291;&#20998;&#31867;&#30340;&#22810;&#38454;&#27573;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#20351;&#29992;&#33136;&#37096;&#34701;&#21512;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Multi-Stage Temporal Convolutional Network for Volleyball Jumps Classification Using a Waist-Mounted IMU. (arXiv:2310.13097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33136;&#37096;IMU&#35782;&#21035;&#25490;&#29699;&#36339;&#36291;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22810;&#23618;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65288;MS-TCN&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#25490;&#29699;&#36816;&#21160;&#21592;&#22312;&#35757;&#32451;&#25110;&#27604;&#36187;&#20013;&#30340;&#36339;&#36291;&#27425;&#25968;&#23545;&#38450;&#27490;&#20260;&#23475;&#38750;&#24120;&#37325;&#35201;&#65292;&#28982;&#32780;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#22914;&#35270;&#39057;&#20998;&#26512;&#26469;&#36827;&#34892;&#27979;&#37327;&#38656;&#35201;&#24456;&#22823;&#30340;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#36339;&#36291;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#33136;&#37096;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#35782;&#21035;&#25490;&#29699;&#36339;&#36291;&#31867;&#22411;&#30340;&#26080;&#38556;&#30861;&#31995;&#32479;&#12290;&#22810;&#23618;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65288;MS-TCN&#65289;&#34987;&#24212;&#29992;&#20110;&#36880;&#26679;&#26412;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#22312;&#21313;&#21517;&#21644;&#20108;&#21313;&#20845;&#21517;&#25490;&#29699;&#36816;&#21160;&#21592;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#22266;&#23450;&#21327;&#35758;&#30340;&#36339;&#36291;&#21644;&#30528;&#22320;&#20219;&#21153;&#30340;&#23454;&#39564;&#23460;&#20250;&#35805;&#26399;&#38388;&#20197;&#21450;&#22235;&#20010;&#25490;&#29699;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;MS-TCN&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#23454;&#39564;&#23460;&#20250;&#35805;&#26399;&#38388;&#65292;&#22823;&#22810;&#25968;&#36339;&#36291;&#27425;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the number of jumps for volleyball players during training or a match can be crucial to prevent injuries, yet the measurement requires considerable workload and cost using traditional methods such as video analysis. Also, existing methods do not provide accurate differentiation between different types of jumps. In this study, an unobtrusive system with a single inertial measurement unit (IMU) on the waist was proposed to recognize the types of volleyball jumps. A Multi-Layer Temporal Convolutional Network (MS-TCN) was applied for sample-wise classification. The model was evaluated on ten volleyball players and twenty-six volleyball players, during a lab session with a fixed protocol of jumping and landing tasks, and during four volleyball training sessions, respectively. The MS-TCN model achieved better performance than a state-of-the-art deep learning model but with lower computational cost. In the lab sessions, most jump counts showed small differences between the predicte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#19981;&#20381;&#36182;&#20110;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;Transformer&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25968;&#30028;&#38480;&#26469;&#19978;&#30028;Transformer&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#25513;&#30721;&#39044;&#27979;&#25513;&#30721;&#23383;&#31561;&#24120;&#35265;&#30340;Transformer&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#20063;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13088</link><description>&lt;p&gt;
Transformer&#30340;&#22522;&#20110;&#33539;&#25968;&#30340;&#38271;&#24230;&#26080;&#20851;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Sequence Length Independent Norm-Based Generalization Bounds for Transformers. (arXiv:2310.13088v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#19981;&#20381;&#36182;&#20110;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;Transformer&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25968;&#30028;&#38480;&#26469;&#19978;&#30028;Transformer&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#25513;&#30721;&#39044;&#27979;&#25513;&#30721;&#23383;&#31561;&#24120;&#35265;&#30340;Transformer&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#20063;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;Transformer&#26550;&#26500;&#19988;&#19981;&#20381;&#36182;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#38271;&#24230;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#35206;&#30422;&#25968;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#26032;&#39062;&#30340;&#35206;&#30422;&#25968;&#30028;&#38480;&#26469;&#19978;&#30028;Transformer&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#35813;&#20989;&#25968;&#31867;&#20026;&#26377;&#30028;&#32447;&#24615;&#21464;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20010;&#27867;&#21270;&#30028;&#38480;&#36866;&#29992;&#20110;&#24120;&#35265;&#30340;Transformer&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;&#25513;&#30721;&#39044;&#27979;&#25513;&#30721;&#23383;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31232;&#30095;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#30740;&#31350;&#65292;&#20174;&#23454;&#35777;&#35282;&#24230;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides norm-based generalization bounds for the Transformer architecture that do not depend on the input sequence length. We employ a covering number based approach to prove our bounds. We use three novel covering number bounds for the function class of bounded linear transformations to upper bound the Rademacher complexity of the Transformer. Furthermore, we show this generalization bound applies to the common Transformer training technique of masking and then predicting the masked word. We also run a simulated study on a sparse majority data set that empirically validates our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#20013;&#24212;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13085</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21161;&#21147;&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning. (arXiv:2310.13085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#20013;&#24212;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#25110;&#20803;&#23398;&#20064;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#65292;&#30417;&#30563;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#21644;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#30340;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#22686;&#24378;&#26679;&#26412;&#20316;&#20026;&#26597;&#35810;&#38598;&#12290;&#22312;&#20803;&#23398;&#20064;&#30340;&#20869;&#24490;&#29615;&#20013;&#20351;&#29992;&#28201;&#24230;&#32553;&#25918;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#38450;&#27490;&#26080;&#30417;&#30563;&#23398;&#20064;&#36807;&#25311;&#21512;&#12290;&#20174;&#36825;&#19968;&#27493;&#23398;&#21040;&#30340;&#21442;&#25968;&#20197;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#30446;&#26631;&#30417;&#30563;&#20803;&#23398;&#20064;&#65292;&#29992;&#20110;&#21021;&#22987;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#24110;&#21161;&#20219;&#20309;&#20803;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;Omniglot&#21644;mini-Imagenet&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#21644;&#20851;&#31995;&#32593;&#32476;&#65288;RN&#65289;&#26469;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning or meta-learning leverages the data scarcity problem in machine learning. Traditionally, training data requires a multitude of samples and labeling for supervised learning. To address this issue, we propose a one-shot unsupervised meta-learning to learn the latent representation of the training samples. We use augmented samples as the query set during the training phase of the unsupervised meta-learning. A temperature-scaled cross-entropy loss is used in the inner loop of meta-learning to prevent overfitting during unsupervised learning. The learned parameters from this step are applied to the targeted supervised meta-learning in a transfer-learning fashion for initialization and fast adaptation with improved accuracy. The proposed method is model agnostic and can aid any meta-learning model to improve accuracy. We use model agnostic meta-learning (MAML) and relation network (RN) on Omniglot and mini-Imagenet datasets to demonstrate the performance of the proposed met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#20934;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#24341;&#23548;&#31995;&#32479;&#26469;&#25351;&#23548;&#20154;&#31867;&#25945;&#24072;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#28436;&#31034;&#65292;&#20174;&#32780;&#20419;&#36827;&#26426;&#22120;&#20154;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13083</link><description>&lt;p&gt;
&#26085;&#24120;&#29992;&#25143;&#22914;&#20309;&#36890;&#36807;&#28436;&#31034;&#26377;&#25928;&#22320;&#25945;&#25480;&#26426;&#22120;&#20154;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Can Everyday Users Efficiently Teach Robots by Demonstrations?. (arXiv:2310.13083v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13083
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#20934;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#24341;&#23548;&#31995;&#32479;&#26469;&#25351;&#23548;&#20154;&#31867;&#25945;&#24072;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#28436;&#31034;&#65292;&#20174;&#32780;&#20419;&#36827;&#26426;&#22120;&#20154;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#26159;&#19968;&#20010;&#20801;&#35768;&#26222;&#36890;&#29992;&#25143;&#36731;&#26494;&#32534;&#31243;&#26426;&#22120;&#20154;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25928;&#29575;&#20197;&#21450;&#26426;&#22120;&#20154;&#23545;&#20219;&#21153;&#21464;&#21270;&#30340;&#27867;&#21270;&#33021;&#21147;&#21462;&#20915;&#20110;&#25552;&#20379;&#30340;&#28436;&#31034;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25351;&#23548;&#20154;&#31867;&#25945;&#24072;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#28436;&#31034;&#65292;&#20174;&#32780;&#20419;&#36827;&#26426;&#22120;&#20154;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#65292;&#21363;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#29109;&#65292;&#20316;&#20026;&#21521;&#20154;&#31867;&#25945;&#24072;&#24314;&#35758;&#20449;&#24687;&#20016;&#23500;&#30340;&#28436;&#31034;&#31034;&#20363;&#30340;&#20934;&#21017;&#65292;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#25945;&#23398;&#25216;&#33021;&#12290;&#22312;&#19968;&#39033;&#23454;&#39564;&#20013;&#65288;N=24&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#24341;&#23548;&#31995;&#32479;&#26469;&#35757;&#32451;&#26032;&#25163;&#29992;&#25143;&#20174;&#24037;&#20316;&#31354;&#38388;&#20013;&#20855;&#26377;&#26368;&#39640;&#29109;&#30340;&#21306;&#22495;&#20135;&#29983;&#39069;&#22806;&#30340;&#28436;&#31034;&#12290;&#36825;&#20123;&#26032;&#25163;&#29992;&#25143;&#32463;&#36807;&#20960;&#27425;&#35797;&#39564;&#65292;&#25945;&#26426;&#22120;&#20154;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#28436;&#31034;&#26469;&#23436;&#25104;&#19968;&#20010;&#21487;&#27867;&#21270;&#30340;&#20219;&#21153;&#12290;&#38543;&#21518;&#65292;&#22312;&#35757;&#32451;&#21518;&#35780;&#20272;&#29992;&#25143;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration (LfD) is a framework that allows lay users to easily program robots. However, the efficiency of robot learning and the robot's ability to generalize to task variations hinges upon the quality and quantity of the provided demonstrations. Our objective is to guide human teachers to furnish more effective demonstrations, thus facilitating efficient robot learning. To achieve this, we propose to use a measure of uncertainty, namely task-related information entropy, as a criterion for suggesting informative demonstration examples to human teachers to improve their teaching skills. In a conducted experiment (N=24), an augmented reality (AR)-based guidance system was employed to train novice users to produce additional demonstrations from areas with the highest entropy within the workspace. These novice users were trained for a few trials to teach the robot a generalizable task using a limited number of demonstrations. Subsequently, the users' performance after tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#20540;&#36755;&#20837;&#21644;&#36755;&#20986;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#23450;&#37327;&#35745;&#31639;&#22797;&#26434;&#24615;&#26469;&#20272;&#35745;&#28014;&#28857;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#26377;&#21161;&#20110;&#22312;&#20302;&#21151;&#32791;&#31995;&#32479;&#20013;&#20915;&#23450;&#23454;&#26045;&#21738;&#31181;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13075</link><description>&lt;p&gt;
&#20851;&#20110;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Computational Complexities of Complex-valued Neural Networks. (arXiv:2310.13075v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#20540;&#36755;&#20837;&#21644;&#36755;&#20986;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#23450;&#37327;&#35745;&#31639;&#22797;&#26434;&#24615;&#26469;&#20272;&#35745;&#28014;&#28857;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#26377;&#21161;&#20110;&#22312;&#20302;&#21151;&#32791;&#31995;&#32479;&#20013;&#20915;&#23450;&#23454;&#26045;&#21738;&#31181;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;CVNNs&#65289;&#26159;&#29992;&#20110;&#22797;&#22495;&#25968;&#25454;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#30340;&#38750;&#32447;&#24615;&#28388;&#27874;&#22120;&#12290;&#19982;&#23454;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;RVNNs&#65289;&#30456;&#27604;&#65292;CVNNs&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#20540;&#36755;&#20837;&#21644;&#36755;&#20986;&#20449;&#21495;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#22495;&#21442;&#25968;&#21644;&#28608;&#27963;&#20989;&#25968;&#12290;&#38543;&#30528;&#20302;&#21151;&#32791;&#31995;&#32479;&#30340;&#36235;&#21183;&#65292;&#35745;&#31639;&#22797;&#26434;&#24615;&#20998;&#26512;&#24050;&#25104;&#20026;&#34913;&#37327;&#31639;&#27861;&#21151;&#32791;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;CVNNs&#30340;&#23450;&#37327;&#21644;&#28176;&#36827;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#36825;&#23545;&#20110;&#20915;&#23450;&#23454;&#26045;&#21738;&#31181;&#31639;&#27861;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25968;&#23398;&#36816;&#31639;&#20197;&#23454;&#20540;&#20056;&#27861;&#30340;&#25968;&#37327;&#26469;&#25551;&#36848;&#65292;&#22240;&#20026;&#36825;&#20123;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#23450;&#37327;&#35745;&#31639;&#22797;&#26434;&#24615;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#28014;&#28857;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#31181;CVNN&#21487;&#20197;&#22312;&#20302;&#21151;&#32791;&#31995;&#32479;&#20013;&#23454;&#26045;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex-valued neural networks (CVNNs) are nonlinear filters used in the digital signal processing of complex-domain data. Compared with real-valued neural networks~(RVNNs), CVNNs can directly handle complex-valued input and output signals due to their complex domain parameters and activation functions. With the trend toward low-power systems, computational complexity analysis has become essential for measuring an algorithm's power consumption. Therefore, this paper presents both the quantitative and asymptotic computational complexities of CVNNs. This is a crucial tool in deciding which algorithm to implement. The mathematical operations are described in terms of the number of real-valued multiplications, as these are the most demanding operations. To determine which CVNN can be implemented in a low-power system, quantitative computational complexities can be used to accurately estimate the number of floating-point operations. We have also investigated the computational complexities o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36923;&#36753;&#32534;&#31243;&#21644;&#26680;&#32452;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#25214;&#21040;&#30456;&#20284;&#30340;&#26680;&#24182;&#29983;&#25104;&#35268;&#21017;&#38598;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#24213;&#23618;&#30693;&#35782;&#26356;&#21152;&#23481;&#26131;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.13073</link><description>&lt;p&gt;
&#20351;&#29992;&#36923;&#36753;&#32534;&#31243;&#21644;&#26680;&#32452;&#21512;&#26469;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Using Logic Programming and Kernel-Grouping for Improving Interpretability of Convolutional Neural Networks. (arXiv:2310.13073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36923;&#36753;&#32534;&#31243;&#21644;&#26680;&#32452;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#25214;&#21040;&#30456;&#20284;&#30340;&#26680;&#24182;&#29983;&#25104;&#35268;&#21017;&#38598;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#24213;&#23618;&#30693;&#35782;&#26356;&#21152;&#23481;&#26131;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#65292;&#23588;&#20854;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;NeSyFOLD-G&#65292;&#23427;&#20351;&#29992;CNN&#30340;&#26368;&#21518;&#19968;&#23618;&#26680;&#26469;&#29983;&#25104;&#19968;&#20010;&#31526;&#21495;&#35268;&#21017;&#38598;&#65292;&#20197;&#20351;&#20854;&#24213;&#23618;&#30693;&#35782;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;NeSyFOLD-G&#19982;&#20854;&#20182;&#31867;&#20284;&#26694;&#26550;&#30340;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21508;&#20010;&#26680;&#29983;&#25104;&#30340;&#29305;&#24449;&#22270;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#25214;&#21040;CNN&#20013;&#30456;&#20284;&#26680;&#30340;&#32452;&#21512;&#65288;&#26680;&#32452;&#21512;&#65289;&#12290;&#19968;&#26086;&#25214;&#21040;&#20102;&#36825;&#26679;&#30340;&#26680;&#32452;&#65292;&#25105;&#20204;&#23545;CNN&#30340;&#27599;&#20010;&#26680;&#32452;&#30340;&#36755;&#20986;&#36827;&#34892;&#20108;&#20540;&#21270;&#65292;&#24182;&#20351;&#29992;&#23427;&#29983;&#25104;&#19968;&#20010;&#20108;&#20540;&#21270;&#34920;&#20316;&#20026;FOLD-SE-M&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;FOLD-SE-M&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;RBML&#65289;&#31639;&#27861;&#12290;FOLD-SE-M&#28982;&#21518;&#29983;&#25104;&#19968;&#20010;&#21487;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#35268;&#21017;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#32452;&#21512;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#32452;&#21512;&#30456;&#20284;&#26680;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the realm of deep learning, the interpretability of Convolutional Neural Networks (CNNs), particularly in the context of image classification tasks, remains a formidable challenge. To this end we present a neurosymbolic framework, NeSyFOLD-G that generates a symbolic rule-set using the last layer kernels of the CNN to make its underlying knowledge interpretable. What makes NeSyFOLD-G different from other similar frameworks is that we first find groups of similar kernels in the CNN (kernel-grouping) using the cosine-similarity between the feature maps generated by various kernels. Once such kernel groups are found, we binarize each kernel group's output in the CNN and use it to generate a binarization table which serves as input data to FOLD-SE-M which is a Rule Based Machine Learning (RBML) algorithm. FOLD-SE-M then generates a rule-set that can be used to make predictions. We present a novel kernel grouping algorithm and show that grouping similar kernels leads to a significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21517;&#20026;RoboTool&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#28041;&#21450;&#38544;&#21547;&#29289;&#29702;&#32422;&#26463;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#21019;&#36896;&#24615;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#35299;&#26512;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;&#29983;&#25104;&#20840;&#38754;&#30340;&#31574;&#30053;&#12289;&#35745;&#31639;&#25216;&#33021;&#21442;&#25968;&#24182;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoboTool&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.13065</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21019;&#36896;&#24615;&#26426;&#22120;&#20154;&#24037;&#20855;&#20351;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creative Robot Tool Use with Large Language Models. (arXiv:2310.13065v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21517;&#20026;RoboTool&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#28041;&#21450;&#38544;&#21547;&#29289;&#29702;&#32422;&#26463;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#21019;&#36896;&#24615;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#35299;&#26512;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;&#29983;&#25104;&#20840;&#38754;&#30340;&#31574;&#30053;&#12289;&#35745;&#31639;&#25216;&#33021;&#21442;&#25968;&#24182;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoboTool&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20351;&#29992;&#26159;&#39640;&#32423;&#26234;&#33021;&#30340;&#26631;&#24535;&#65292;&#19981;&#20165;&#23384;&#22312;&#20110;&#21160;&#29289;&#34892;&#20026;&#20013;&#65292;&#20063;&#20307;&#29616;&#22312;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#20154;&#36171;&#20104;&#21019;&#36896;&#24615;&#20351;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#28041;&#21450;&#38544;&#21547;&#29289;&#29702;&#32422;&#26463;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RoboTool&#65292;&#19968;&#20010;&#31995;&#32479;&#65292;&#23427;&#25509;&#21463;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#36755;&#20986;&#25511;&#21046;&#26426;&#22120;&#20154;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#12290;RoboTool&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;i&#65289;"&#35299;&#26512;&#22120;"&#65292;&#29992;&#20110;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#20197;&#35782;&#21035;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#65288;ii&#65289;"&#35268;&#21010;&#22120;"&#65292;&#26681;&#25454;&#35821;&#35328;&#36755;&#20837;&#21644;&#20851;&#38190;&#27010;&#24565;&#29983;&#25104;&#20840;&#38754;&#30340;&#31574;&#30053;&#65292;&#65288;iii&#65289;"&#35745;&#31639;&#22120;"&#65292;&#29992;&#20110;&#35745;&#31639;&#27599;&#20010;&#25216;&#33021;&#30340;&#21442;&#25968;&#65292;&#65288;iv&#65289;"&#32534;&#30721;&#22120;"&#65292;&#23558;&#36825;&#20123;&#35745;&#21010;&#36716;&#25442;&#25104;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RoboTool&#19981;&#20165;&#21487;&#20197;&#29702;&#35299;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#22240;&#32032;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an "Analyzer" that interprets natural language to discern key task-related concepts, (ii) a "Planner" that generates comprehensive strategies based on the language input and key concepts, (iii) a "Calculator" that computes parameters for each skill, and (iv) a "Coder" that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend explicit or implicit physical constraints and environmental factors bu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#25968;&#31639;&#26415;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#35760;&#20303;&#25439;&#22351;&#30340;&#26631;&#31614;&#24182;&#23454;&#29616;100%&#30340;&#27867;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#21644;&#21098;&#26525;&#35760;&#24518;&#21270;&#30340;&#31070;&#32463;&#20803;&#26469;&#38477;&#20302;&#23545;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#23545;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13061</link><description>&lt;p&gt;
&#21040;&#24213;&#26159;&#29702;&#35299;&#36824;&#26159;&#35760;&#24518;&#65306;&#35299;&#26512;&#31639;&#27861;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. (arXiv:2310.13061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#25968;&#31639;&#26415;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#35760;&#20303;&#25439;&#22351;&#30340;&#26631;&#31614;&#24182;&#23454;&#29616;100%&#30340;&#27867;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#21644;&#21098;&#26525;&#35760;&#24518;&#21270;&#30340;&#31070;&#32463;&#20803;&#26469;&#38477;&#20302;&#23545;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#23545;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#38750;&#24120;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#30693;&#36947;&#32593;&#32476;&#26159;&#21542;&#24050;&#32463;&#35760;&#20303;&#20102;&#19968;&#32452;&#29305;&#23450;&#30340;&#26679;&#26412;&#65292;&#36824;&#26159;&#29702;&#35299;&#20102;&#20854;&#20013;&#30340;&#22522;&#26412;&#35268;&#24459;&#65288;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#65289;&#12290;&#21463;&#21040;&#36825;&#20010;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#27867;&#21270;&#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#26469;&#29702;&#35299;&#65292;&#24182;&#19988;&#24456;&#23481;&#26131;&#19982;&#35760;&#24518;&#24615;&#34920;&#31034;&#21306;&#20998;&#24320;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#27169;&#25968;&#31639;&#26415;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#65288;&#958;&#183;100%&#65289;&#30340;&#26631;&#31614;&#26159;&#34987;&#25439;&#22351;&#30340;&#65288;&#21363;&#35757;&#32451;&#38598;&#20013;&#30340;&#19968;&#20123;&#27169;&#25968;&#36816;&#31639;&#32467;&#26524;&#26159;&#38169;&#35823;&#30340;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#35760;&#20303;&#25439;&#22351;&#30340;&#26631;&#31614;&#24182;&#23454;&#29616;100%&#30340;&#27867;&#21270;&#65307;&#65288;ii&#65289;&#21487;&#20197;&#35782;&#21035;&#21644;&#21098;&#26525;&#35760;&#24518;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#38477;&#20302;&#23545;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#23545;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20934;&#30830;&#29575;&#65307;&#65288;iii&#65289;&#27491;&#21017;&#21270;&#26041;&#27861;&#22914;w
&lt;/p&gt;
&lt;p&gt;
Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider two-layer neural networks trained on modular arithmetic tasks where ($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \emph{and} achieve $100\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as w
&lt;/p&gt;</description></item><item><title>&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.13040</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20855;&#26377;&#24322;&#24120;&#29305;&#24449;&#24182;&#32534;&#30721;&#26356;&#22810;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Robust multimodal models have outlier features and encode more concepts. (arXiv:2310.13040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13040
&lt;/p&gt;
&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20160;&#20040;&#21306;&#20998;&#20581;&#22766;&#27169;&#22411;&#19982;&#38750;&#20581;&#22766;&#27169;&#22411;&#65311;&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#21464;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20581;&#22766;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#20581;&#22766;&#24615;&#30340;&#24046;&#24322;&#21487;&#20197;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#19981;&#28165;&#26970;&#36825;&#23545;&#20110;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20160;&#20040;&#24847;&#21619;&#30528;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;12&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#24178;&#65288;ResNets&#21644;ViTs&#65289;&#21644;&#39044;&#35757;&#32451;&#38598;&#65288;OpenAI&#65292;LAION-400M&#65292;LAION-2B&#65292;YFCC15M&#65292;CC12M&#21644;DataComp&#65289;&#30340;&#20581;&#22766;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#23384;&#22312;&#20004;&#20010;&#20581;&#22766;&#24615;&#30340;&#29305;&#24449;&#65306;&#65288;1&#65289;&#20581;&#22766;&#27169;&#22411;&#20855;&#26377;&#30001;&#20854;&#28608;&#27963;&#29305;&#24449;&#34920;&#24449;&#30340;&#24322;&#24120;&#29305;&#24449;&#65292;&#20854;&#20013;&#19968;&#20123;&#29305;&#24449;&#20540;&#27604;&#24179;&#22343;&#20540;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#24322;&#24120;&#29305;&#24449;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#29305;&#26435;&#26041;&#21521;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demon
&lt;/p&gt;</description></item><item><title>Agri-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20316;&#29289;&#20043;&#38388;&#30340;&#22797;&#26434;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20248;&#21270;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.13037</link><description>&lt;p&gt;
Agri-GNN:&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24314;&#31435;&#22312;GraphSAGE&#19978;&#65292;&#29992;&#20110;&#20248;&#21270;&#20135;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction. (arXiv:2310.13037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13037
&lt;/p&gt;
&lt;p&gt;
Agri-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20316;&#29289;&#20043;&#38388;&#30340;&#22797;&#26434;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20248;&#21270;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#20316;&#20026;&#20154;&#31867;&#25991;&#26126;&#30340;&#22522;&#30707;&#65292;&#19981;&#26029;&#23547;&#27714;&#25972;&#21512;&#25216;&#26415;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Agri-GNN&#30340;&#26032;&#22411;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#20316;&#29289;&#20043;&#38388;&#22797;&#26434;&#30340;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#20248;&#21270;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#38138;&#24179;&#36947;&#36335;&#12290;Agri-GNN&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;G&#65292;&#23558;&#20892;&#30000;&#22320;&#22359;&#35270;&#20026;&#33410;&#28857;&#65292;&#24182;&#26681;&#25454;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20284;&#24615;&#26041;&#27861;&#22320;&#26500;&#24314;&#33410;&#28857;&#20043;&#38388;&#30340;&#36793;&#65292;&#36890;&#36807;&#22522;&#22240;&#22411;-&#25299;&#25169;&#36807;&#28388;&#22120;&#23545;&#33410;&#28857;&#20449;&#24687;&#36827;&#34892;&#32858;&#21512;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#36807;&#35774;&#35745;&#32771;&#34385;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#20892;&#19994;&#29983;&#24577;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#21033;&#29992;GNN&#30340;&#33021;&#21147;&#65292;Agri-GNN&#21487;&#20197;&#20174;&#26893;&#29289;&#20013;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#32771;&#34385;&#23427;&#20204;&#22522;&#20110;&#31354;&#38388;&#23646;&#24615;&#30340;&#20869;&#22312;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agriculture, as the cornerstone of human civilization, constantly seeks to integrate technology for enhanced productivity and sustainability. This paper introduces $\textit{Agri-GNN}$, a novel Genotypic-Topological Graph Neural Network Framework tailored to capture the intricate spatial and genotypic interactions of crops, paving the way for optimized predictions of harvest yields. $\textit{Agri-GNN}$ constructs a Graph $\mathcal{G}$ that considers farming plots as nodes, and then methodically constructs edges between nodes based on spatial and genotypic similarity, allowing for the aggregation of node information through a genotypic-topological filter. Graph Neural Networks (GNN), by design, consider the relationships between data points, enabling them to efficiently model the interconnected agricultural ecosystem. By harnessing the power of GNNs, $\textit{Agri-GNN}$ encapsulates both local and global information from plants, considering their inherent connections based on spatial pro
&lt;/p&gt;</description></item><item><title>LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13033</link><description>&lt;p&gt;
LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13033
&lt;/p&gt;
&lt;p&gt;
LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24182;&#34892;SGD&#26159;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#20107;&#23454;&#19978;&#30340;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#12290;&#23613;&#31649;&#23427;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#36890;&#20449;&#29942;&#39048;&#26159;&#20854;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#21387;&#32553;&#26041;&#26696;&#35201;&#20040;&#20551;&#35774;&#36890;&#20449;&#38142;&#36335;&#26080;&#22122;&#22768;&#65292;&#35201;&#20040;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#20171;&#32461;&#20102;LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;&#12290;LASER&#21033;&#29992;&#26799;&#24230;&#30340;&#22266;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#12290;&#23613;&#31649;&#20139;&#21463;&#19982;&#32463;&#20856;SGD&#30456;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;LASER&#22312;&#21508;&#31181;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#25345;&#32493;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#22312;&#21518;&#32773;&#20013;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#22122;&#22768;&#36890;&#36947;&#19978;&#30340;&#22522;&#20934;&#27169;&#22411;&#22312;&#22256;&#24785;&#24230;&#19978;&#33719;&#24471;&#20102;50-64%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#30340;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#26597;&#35810;&#20197;&#25913;&#21892;&#25628;&#32034;&#24341;&#25806;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13031</link><description>&lt;p&gt;
&#23558;&#26597;&#35810;&#37325;&#20889;&#37325;&#26032;&#23450;&#20041;&#20026;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#30340;&#29992;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem. (arXiv:2310.13031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#30340;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#26597;&#35810;&#20197;&#25913;&#21892;&#25628;&#32034;&#24341;&#25806;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#30340;&#32593;&#32476;&#20869;&#23481;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#25361;&#25112;&#65292;&#25628;&#32034;&#24341;&#25806;&#26377;&#19968;&#20010;&#27169;&#22359;&#26469;&#37325;&#20889;&#29992;&#25143;&#26597;&#35810;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19968;&#20123;&#32479;&#35745;&#21644;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;NLP&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26597;&#35810;&#37325;&#20889;&#27969;&#31243;&#65292;&#23398;&#20064;&#22914;&#20309;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#25628;&#32034;&#26597;&#35810;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#21019;&#24314;&#29992;&#25143;&#26597;&#35810;&#21644;&#32593;&#39029;&#26631;&#39064;&#20043;&#38388;&#26144;&#23556;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges for modern search engines is to retrieve relevant web content based on user queries. In order to achieve this challenge, search engines have a module to rewrite user queries. That is why modern web search engines utilize some statistical and neural models used in the natural language processing domain. Statistical machine translation is a well-known NLP method among them. The paper proposes a query rewriting pipeline based on a monolingual machine translation model that learns to rewrite Arabic user search queries. This paper also describes preprocessing steps to create a mapping between user queries and web page titles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#30340;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13029</link><description>&lt;p&gt;
&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#30340;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series. (arXiv:2310.13029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#30340;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25551;&#36848;&#19968;&#31181;&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26368;&#36817;&#30340;M5&#31454;&#36187;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#36187;&#36947;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#21253;&#25324;&#65306;a)&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#23545;&#27599;&#22825;&#38144;&#21806;&#39069;&#30340;&#22238;&#24402;&#38382;&#39064;&#65307;b)&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#24037;&#31243;&#65307;c)&#21019;&#24314;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;d)&#31934;&#24515;&#26500;&#24314;&#29992;&#20110;&#27169;&#22411;&#35843;&#20248;&#30340;&#39564;&#35777;&#38598;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#39564;&#35777;&#26679;&#26412;&#30340;&#31934;&#24515;&#36873;&#25321;&#26159;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;&#23613;&#31649;&#39044;&#27979;&#25968;&#25454;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#65288;12&#20010;&#23618;&#32423;&#65289;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#27809;&#26377;&#21033;&#29992;&#35813;&#23618;&#27425;&#32467;&#26500;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#37329;&#29260;&#33539;&#22260;&#20869;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we tackle the problem of point and probabilistic forecasting by describing a blending methodology of machine learning models that belong to gradient boosted trees and neural networks families. These principles were successfully applied in the recent M5 Competition on both Accuracy and Uncertainty tracks. The keypoints of our methodology are: a) transform the task to regression on sales for a single day b) information rich feature engineering c) create a diverse set of state-of-the-art machine learning models and d) carefully construct validation sets for model tuning. We argue that the diversity of the machine learning models along with the careful selection of validation examples, where the most important ingredients for the effectiveness of our approach. Although forecasting data had an inherent hierarchy structure (12 levels), none of our proposed solutions exploited that hierarchical scheme. Using the proposed methodology, our team was ranked within the gold medal ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13027</link><description>&lt;p&gt;
&#36890;&#36807;&#38468;&#21152;&#20214;&#21464;&#24471;&#36125;&#21494;&#26031;&#65292;&#25429;&#25417;&#26356;&#22810;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Be Bayesian by Attachments to Catch More Uncertainty. (arXiv:2310.13027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNNs)&#24050;&#25104;&#20026;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;BNNs&#30340;&#24615;&#33021;&#21463;&#21040;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#38468;&#21152;&#32467;&#26500;&#20174;&#36275;&#22815;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;(OOD)&#20013;&#25429;&#25417;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#20808;&#39564;&#20998;&#24067;&#20026;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#23398;&#25551;&#36848;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#38468;&#21152;&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23558;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#25972;&#21512;&#21040;&#20027;&#24178;&#32593;&#32476;&#20013;&#12290;ABNN&#30001;&#26399;&#26395;&#27169;&#22359;&#21644;&#33509;&#24178;&#20998;&#24067;&#27169;&#22359;&#32452;&#25104;&#12290;&#26399;&#26395;&#27169;&#22359;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#21407;&#22987;&#20219;&#21153;&#30340;&#20027;&#24178;&#28145;&#24230;&#32593;&#32476;&#65292;&#32780;&#20998;&#24067;&#27169;&#22359;&#21017;&#26159;&#20316;&#20026;&#20027;&#24178;&#30340;&#38468;&#21152;&#32467;&#26500;&#30340;&#23567;&#36125;&#21494;&#26031;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#20998;&#24067;&#27169;&#22359;&#30340;&#30446;&#30340;&#26159;&#26816;&#27979;&#21644;&#20256;&#25773;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BNNs) have become one of the promising approaches for uncertainty estimation due to the solid theorical foundations. However, the performance of BNNs is affected by the ability of catching uncertainty. Instead of only seeking the distribution of neural network weights by in-distribution (ID) data, in this paper, we propose a new Bayesian Neural Network with an Attached structure (ABNN) to catch more uncertainty from out-of-distribution (OOD) data. We first construct a mathematical description for the uncertainty of OOD data according to the prior distribution, and then develop an attached Bayesian structure to integrate the uncertainty of OOD data into the backbone network. ABNN is composed of an expectation module and several distribution modules. The expectation module is a backbone deep network which focuses on the original task, and the distribution modules are mini Bayesian structures which serve as attachments of the backbone. In particular, the distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2310.13024</link><description>&lt;p&gt;
&#26397;&#21521;&#38543;&#26102;&#24494;&#35843;&#65306;&#20351;&#29992;&#36229;&#32593;&#32476;&#25552;&#31034;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt. (arXiv:2310.13024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#19990;&#30028;&#20013;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#21464;&#24471;&#36843;&#20999;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#26399;&#26395;&#19981;&#20165;&#22312;&#39044;&#35757;&#32451;&#39046;&#22495;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#20855;&#26377;&#26356;&#22823;&#30340;&#23481;&#37327;&#65292;&#32780;&#19988;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#19981;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#29616;&#26377;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#38543;&#26102;&#24494;&#35843;&#25928;&#26524;&#65292;&#24471;&#20986;&#20102;&#23545;&#26410;&#30693;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#33021;&#19979;&#38477;&#30340;&#32467;&#35770;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#35757;&#32451;&#36229;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#12290;&#19968;&#33268;&#24615;&#25439;&#22833;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#21017;&#20445;&#25252;&#20102;&#20026;&#27599;&#20010;&#39046;&#22495;&#29983;&#25104;&#30340;&#38544;&#34255;&#29366;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#36229;&#32593;&#32476;&#29983;&#25104;&#30340;&#25552;&#31034;&#22312;&#24494;&#35843;&#26102;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance on unseen domains. To this end, we propose a prompt-guided continual pre-training method, where we train a hypernetwork to generate domain-specific prompts by both agreement and disagreement losses. The agreement loss maximally preserves the generalization of a pre-trained model to new domains, and the disagreement one guards the exclusiveness of the generated hidden states for each domain. Remarkably, prompts by the hypernetwork alleviate the domain identity when fine-tuning and promote knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#65288;UPET&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;Monte Carlo dropout&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13022</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding. (arXiv:2310.13022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#65288;UPET&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;Monte Carlo dropout&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#35757;&#32451;&#20316;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#20013;&#30340;&#19968;&#31181;&#20027;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22826;&#22810;&#30340;&#22122;&#22768;&#26631;&#31614;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19988;&#33258;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#22914;&#26524;&#26356;&#26032;PLM&#30340;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#20250;&#26356;&#21152;&#26114;&#36149;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UPET&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33945;&#29305;&#21345;&#32599;&#65288;MC&#65289;dropout&#24341;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#20197;&#36827;&#34892;&#25945;&#24072;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#28982;&#21518;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#26469;&#31934;&#30830;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the stude
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.13018</link><description>&lt;p&gt;
&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#36798;&#25104;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#26500;&#24314;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#20998;&#31867;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#12289;&#23548;&#33322;&#21644;&#20915;&#31574;&#30340;&#19990;&#30028;&#34920;&#31034;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#31995;&#32479;&#25152;&#26500;&#24314;&#30340;&#34920;&#31034;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65311;&#21363;&#20351;&#34920;&#31034;&#19981;&#21516;&#65292;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#23548;&#33268;&#30456;&#21516;&#30340;&#34892;&#20026;&#65311;&#31995;&#32479;&#22914;&#20309;&#20462;&#25913;&#23427;&#20204;&#30340;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#21478;&#19968;&#20010;&#31995;&#32479;&#30340;&#34920;&#31034;&#65311;&#36825;&#20123;&#20851;&#20110;&#34920;&#31034;&#19968;&#33268;&#24615;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#24403;&#20195;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#31038;&#21306;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#26377;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22312;&#19968;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#26368;&#32456;&#20250;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#29420;&#31435;&#22320;&#37325;&#26032;&#21457;&#29616;&#65292;&#32780;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#38388;&#20132;&#27969;&#23558;&#26159;&#26377;&#21033;&#30340;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20849;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25913;&#36827;ALiBi&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.13017</link><description>&lt;p&gt;
&#20301;&#32622;&#25554;&#20540;&#25913;&#36827;&#20102;ALiBi&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Position Interpolation Improves ALiBi Extrapolation. (arXiv:2310.13017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25913;&#36827;ALiBi&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26377;&#21161;&#20110;&#20351;&#29992;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#26356;&#38271;&#30340;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#22806;&#25512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25193;&#23637;&#20351;&#29992;&#24102;&#26377;&#32447;&#24615;&#20559;&#24046;&#30340;&#27880;&#24847;&#21147;&#65288;ALiBi&#65289;&#30340;&#27169;&#22411;&#30340;&#22806;&#25512;&#33539;&#22260;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20301;&#32622;&#25554;&#20540;&#26174;&#33879;&#25913;&#21892;&#20102;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21442;&#19982;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#23454;&#35777;&#27979;&#35797;&#20102;OpenAI&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#39044;&#27979;&#30456;&#27604;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.13014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65306;&#26469;&#33258;&#29616;&#23454;&#39044;&#27979;&#31454;&#36187;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. (arXiv:2310.13014v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21442;&#19982;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#23454;&#35777;&#27979;&#35797;&#20102;OpenAI&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#39044;&#27979;&#30456;&#27604;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#23558;&#26159;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20851;&#20110;&#26410;&#26469;&#20107;&#20214;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#32463;&#39564;&#24615;&#22320;&#27979;&#35797;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;OpenAI&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#32435;&#20837;&#20102;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#39044;&#27979;&#31454;&#36187;&#12290;&#36825;&#22330;&#20174;2023&#24180;7&#26376;&#21040;10&#26376;&#36827;&#34892;&#30340;&#31454;&#36187;&#21560;&#24341;&#20102;843&#21517;&#21442;&#19982;&#32773;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#12289;&#32654;&#22269;&#25919;&#27835;&#12289;&#30149;&#27602;&#29190;&#21457;&#21644;&#20044;&#20811;&#20848;&#20914;&#31361;&#22312;&#20869;&#30340;&#21508;&#31181;&#20027;&#39064;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#20108;&#36827;&#21046;&#39044;&#27979;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20154;&#32676;&#20013;&#20301;&#25968;&#39044;&#27979;&#30456;&#27604;&#65292;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#30340;&#39044;&#27979;&#19982;&#23558;&#27599;&#20010;&#38382;&#39064;&#30340;&#27010;&#29575;&#20998;&#37197;&#20026;50%&#30340;&#26080;&#20449;&#24687;&#39044;&#27979;&#31574;&#30053;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#37322;&#65292;&#21363;GPT-4&#21487;&#33021;&#26377;&#20542;&#21521;&#24615;&#22320;&#39044;&#27979;&#27010;&#29575;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities clo
&lt;/p&gt;</description></item><item><title>&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13011</link><description>&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13011
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#21450;&#23545;&#20559;&#22909;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#20998;&#35299;&#20026;&#22810;&#20010;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20174;&#19968;&#20010;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#36825;&#20123;&#29305;&#24449;&#30340;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#32858;&#21512;&#36825;&#20123;&#35780;&#20998;&#12290;CPMs&#20801;&#35768;&#25511;&#21046;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#20351;&#29992;&#21738;&#20123;&#23646;&#24615;&#26469;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#22522;&#30784;&#30340;&#29305;&#24449;&#26500;&#24314;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPMs&#19981;&#20165;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#27604;&#26631;&#20934;&#20559;&#22909;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;CPMs&#33719;&#24471;&#30340;&#26368;&#20339;n&#20010;&#26679;&#26412;&#27604;&#20351;&#29992;&#26631;&#20934;PMs&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13008</link><description>&lt;p&gt;
LoBaSS&#65306;&#22312;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#20013;&#27979;&#37327;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoBaSS: Gauging Learnability in Supervised Fine-tuning Data. (arXiv:2310.13008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#20915;&#26465;&#20214;&#23545;&#40784;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#24494;&#35843;&#25968;&#25454;&#30340;&#36873;&#25321;&#28145;&#21051;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20256;&#32479;&#19978;&#20197;&#25968;&#25454;&#36136;&#37327;&#21644;&#20998;&#24067;&#20026;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SFT&#25968;&#25454;&#36873;&#25321;&#30340;&#19968;&#20010;&#26032;&#32500;&#24230;&#65306;&#21487;&#23398;&#20064;&#24615;&#12290;&#36825;&#20010;&#26032;&#32500;&#24230;&#30340;&#21160;&#26426;&#26159;&#30001;LLM&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#36866;&#21512;&#19968;&#20010;&#27169;&#22411;&#30340;SFT&#25968;&#25454;&#21487;&#33021;&#19981;&#36866;&#21512;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#33021;&#21147;&#36825;&#20010;&#26415;&#35821;&#26469;&#23450;&#20041;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#30340;&#36866;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25439;&#22833;&#30340;SFT&#25968;&#25454;&#36873;&#25321;&#65288;LoBaSS&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;SFT&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#22266;&#26377;&#30340;&#27169;&#22411;&#33021;&#21147;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#25552;&#21319;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#12290;</title><link>http://arxiv.org/abs/2310.13006</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#25968;&#25454;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Software Metadata Classification based on Generative Artificial Intelligence. (arXiv:2310.13006v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#25552;&#21319;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(AI)&#26469;&#25552;&#39640;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;OpenAI API&#65292;&#23558;&#20174;&#21508;&#31181;GitHub&#20179;&#24211;&#21644;&#24320;&#28304;&#39033;&#30446;&#20013;&#25552;&#21462;&#30340;1239&#20010;&#26032;&#29983;&#25104;&#30340;&#20195;&#30721;-&#35780;&#35770;&#23545;&#25968;&#25454;&#38598;&#26631;&#35760;&#20026;&#8220;&#26377;&#29992;&#8221;&#25110;&#8220;&#26080;&#29992;&#8221;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;9048&#20010;C&#32534;&#31243;&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#38598;&#25104;&#12290;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#38598;&#25104;&#21040;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#27169;&#22411;&#20013;&#26102;&#65292;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20174;0.79&#22686;&#21152;&#21040;0.85&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#65292;&#20174;0.731&#22686;&#21152;&#21040;0.746&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22686;&#24378;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to enhance the performance of binary code comment quality classification models through the application of Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a dataset comprising 1239 newly generated code-comment pairs, extracted from various GitHub repositories and open-source projects, has been labelled as "Useful" or "Not Useful", and integrated into the existing corpus of 9048 pairs in the C programming language. Employing a cutting-edge Large Language Model Architecture, the generated dataset demonstrates notable improvements in model accuracy. Specifically, when incorporated into the Support Vector Machine (SVM) model, a 6% increase in precision is observed, rising from 0.79 to 0.85. Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5% increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the potential of Generative AI in augmenting code comment quality classification models. The res
&lt;/p&gt;</description></item><item><title>CEIL&#26159;&#19968;&#31181;&#28176;&#36827;&#39640;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32473;&#23398;&#20064;&#20195;&#29702;&#25552;&#20379;&#19968;&#20010;&#25277;&#35937;&#12289;&#21160;&#24577;&#30340;&#35821;&#35328;&#21644;&#19968;&#31181;&#20869;&#22312;&#30340;&#21160;&#26426;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27807;&#36890;&#20195;&#20215;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#36880;&#27493;&#39640;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#12290;&#22312;2D MineCraft&#39046;&#22495;&#19978;&#65292;CEIL&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13004</link><description>&lt;p&gt;
&#28176;&#36827;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Progressively Efficient Learning. (arXiv:2310.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13004
&lt;/p&gt;
&lt;p&gt;
CEIL&#26159;&#19968;&#31181;&#28176;&#36827;&#39640;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32473;&#23398;&#20064;&#20195;&#29702;&#25552;&#20379;&#19968;&#20010;&#25277;&#35937;&#12289;&#21160;&#24577;&#30340;&#35821;&#35328;&#21644;&#19968;&#31181;&#20869;&#22312;&#30340;&#21160;&#26426;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27807;&#36890;&#20195;&#20215;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#36880;&#27493;&#39640;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#12290;&#22312;2D MineCraft&#39046;&#22495;&#19978;&#65292;CEIL&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21161;&#29702; AI &#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36805;&#36895;&#33719;&#21462;&#26032;&#25216;&#33021;&#24182;&#36866;&#24212;&#29992;&#25143;&#30340;&#26032;&#20559;&#22909;&#12290;&#20256;&#32479;&#30340;&#26694;&#26550;&#22914;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#19981;&#33021;&#25903;&#25345;&#36825;&#31181;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#25903;&#25345;&#20302;&#32423;&#12289;&#20302;&#25928;&#30340;&#27807;&#36890;&#24418;&#24335;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#36890;&#36807;&#23450;&#20041;&#21644;&#20998;&#20139;&#25277;&#35937;&#24847;&#22270;&#26469;&#23454;&#29616;&#36880;&#27493;&#39640;&#25928;&#30340;&#27807;&#36890;&#12290;&#20026;&#20102;&#22312; AI &#20195;&#29702;&#20013;&#20877;&#29616;&#31867;&#20284;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#27807;&#36890;&#39640;&#25928;&#20132;&#20114;&#23398;&#20064;&#65288;CEIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20026;&#23398;&#20064;&#20195;&#29702;&#25552;&#20379;&#19968;&#20010;&#25277;&#35937;&#12289;&#21160;&#24577;&#30340;&#35821;&#35328;&#21644;&#19968;&#31181;&#20869;&#22312;&#30340;&#21160;&#26426;&#65292;&#21363;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27807;&#36890;&#20195;&#20215;&#23398;&#20064;&#65292;CEIL &#21487;&#20197;&#23548;&#33268;&#23398;&#20064;&#32773;&#21644;&#25945;&#24072;&#36880;&#28176;&#39640;&#25928;&#22320;&#20132;&#27969;&#65292;&#36890;&#36807;&#20132;&#25442;&#36234;&#26469;&#36234;&#25277;&#35937;&#30340;&#24847;&#22270;&#12290;CEIL &#22312;&#19968;&#20010;&#21253;&#21547;&#38271;&#26399;&#20915;&#31574;&#20219;&#21153;&#30340; 2D MineCraft &#39046;&#22495;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assistant AI agents should be capable of rapidly acquiring novel skills and adapting to new user preferences. Traditional frameworks like imitation learning and reinforcement learning do not facilitate this capability because they support only low-level, inefficient forms of communication. In contrast, humans communicate with progressive efficiency by defining and sharing abstract intentions. Reproducing similar capability in AI agents, we develop a novel learning framework named Communication-Efficient Interactive Learning (CEIL). By equipping a learning agent with an abstract, dynamic language and an intrinsic motivation to learn with minimal communication effort, CEIL leads to emergence of a human-like pattern where the learner and the teacher communicate progressively efficiently by exchanging increasingly more abstract intentions. CEIL demonstrates impressive performance and communication efficiency on a 2D MineCraft domain featuring long-horizon decision-making tasks. Agents trai
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#30456;&#20284;&#33647;&#29289;&#30340;&#20808;&#21069;&#21453;&#24212;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#23545;&#26410;&#26631;&#35760;&#21270;&#21512;&#29289;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12996</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening. (arXiv:2310.12996v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#30456;&#20284;&#33647;&#29289;&#30340;&#20808;&#21069;&#21453;&#24212;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#23545;&#26410;&#26631;&#35760;&#21270;&#21512;&#29289;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26377;&#30417;&#30563;&#23398;&#20064;&#26469;&#36827;&#34892;&#33647;&#29289;&#21453;&#24212;&#30340;&#39044;&#27979;&#65292;&#36825;&#38656;&#35201;&#20381;&#36182;&#20110;&#24050;&#26631;&#35760;&#30340;&#33647;&#29289;&#21453;&#24212;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#38454;&#27573;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#33647;&#29289;&#21453;&#24212;&#27169;&#22411;&#26469;&#39044;&#27979;&#26032;&#21270;&#21512;&#29289;&#30340;&#21453;&#24212;&#65292;&#36825;&#20123;&#21270;&#21512;&#29289;&#30340;&#33647;&#29289;&#21453;&#24212;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#36825;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#29992;&#20110;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#27979;&#35797;&#22686;&#24378;&#25554;&#20214;(MSDA)&#65292;MSDA&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#65292;&#20174;&#30456;&#20284;&#33647;&#29289;&#30340;&#20808;&#21069;&#21453;&#24212;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#26631;&#35760;&#21270;&#21512;&#29289;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;GDSCv2&#21644;CellMiner&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MSDA&#33021;&#22815;&#39640;&#25928;&#22320;&#39044;&#27979;&#33647;&#29289;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional deep learning methods typically employ supervised learning for drug response prediction (DRP). This entails dependence on labeled response data from drugs for model training. However, practical applications in the preclinical drug screening phase demand that DRP models predict responses for novel compounds, often with unknown drug responses. This presents a challenge, rendering supervised deep learning methods unsuitable for such scenarios. In this paper, we propose a zero-shot learning solution for the DRP task in preclinical drug screening. Specifically, we propose a Multi-branch Multi-Source Domain Adaptation Test Enhancement Plug-in, called MSDA. MSDA can be seamlessly integrated with conventional DRP methods, learning invariant features from the prior response data of similar drugs to enhance real-time predictions of unlabeled compounds. We conducted experiments using the GDSCv2 and CellMiner datasets. The results demonstrate that MSDA efficiently predicts drug respon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#20171;&#36136;&#20013;&#21033;&#29992;&#22823;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#21644;&#22810;&#32500;&#23610;&#24230;&#32553;&#25918;&#25216;&#26415;&#23454;&#29616;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12990</link><description>&lt;p&gt;
&#22522;&#20110;&#27874;&#24418;&#20449;&#24687;&#30340;&#22797;&#26434;&#20171;&#36136;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#30340;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Wave-informed dictionary learning for high-resolution imaging in complex media. (arXiv:2310.12990v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#20171;&#36136;&#20013;&#21033;&#29992;&#22823;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#21644;&#22810;&#32500;&#23610;&#24230;&#32553;&#25918;&#25216;&#26415;&#23454;&#29616;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25955;&#23556;&#20171;&#36136;&#20013;&#21033;&#29992;&#22823;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25104;&#20687;&#30340;&#26041;&#27861;&#12290;&#23427;&#26377;&#20004;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;&#30495;&#23454;&#30340;&#26684;&#26519;&#20989;&#25968;&#21521;&#37327;&#20316;&#20026;&#26080;&#24207;&#24863;&#30693;&#30697;&#38453;&#20013;&#30340;&#21015;&#12290;&#38453;&#21015;&#25968;&#25454;&#26469;&#33258;&#35768;&#22810;&#31232;&#30095;&#30340;&#28304;&#38598;&#65292;&#20854;&#20301;&#32622;&#21644;&#24378;&#24230;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#20351;&#29992;&#22810;&#32500;&#23610;&#24230;&#32553;&#25918;&#24182;&#21033;&#29992;&#24863;&#30693;&#30697;&#38453;&#21015;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;&#23548;&#20986;&#30340;&#36830;&#25509;&#20449;&#24687;&#23545;&#24863;&#30693;&#30697;&#38453;&#30340;&#21015;&#36827;&#34892;&#25490;&#24207;&#65292;&#23601;&#20687;&#26102;&#38388;&#21453;&#28436;&#19968;&#26679;&#12290;&#20026;&#20102;&#20351;&#36825;&#20004;&#20010;&#27493;&#39588;&#33021;&#22815;&#30456;&#20114;&#37197;&#21512;&#24037;&#20316;&#65292;&#25105;&#20204;&#38656;&#35201;&#26469;&#33258;&#22823;&#22411;&#25509;&#25910;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#65292;&#20197;&#20351;&#24863;&#30693;&#30697;&#38453;&#30340;&#21015;&#22312;&#31532;&#19968;&#27493;&#20013;&#19981;&#30456;&#20851;&#65292;&#20197;&#21450;&#26469;&#33258;&#23376;&#38453;&#21015;&#30340;&#25968;&#25454;&#65292;&#20197;&#20351;&#23427;&#20204;&#22312;&#31532;&#20108;&#27493;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#30456;&#20851;&#24615;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#22797;&#26434;&#20171;&#36136;&#20013;&#25552;&#20379;&#20855;&#26377;&#22343;&#21248;  &#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for imaging in scattering media when large and diverse data sets are available. It has two steps. Using a dictionary learning algorithm the first step estimates the true Green's function vectors as columns in an unordered sensing matrix. The array data comes from many sparse sets of sources whose location and strength are not known to us. In the second step, the columns of the estimated sensing matrix are ordered for imaging using Multi-Dimensional Scaling with connectivity information derived from cross-correlations of its columns, as in time reversal. For these two steps to work together we need data from large arrays of receivers so the columns of the sensing matrix are incoherent for the first step, as well as from sub-arrays so that they are coherent enough to obtain the connectivity needed in the second step. Through simulation experiments, we show that the proposed approach is able to provide images in complex media whose resolution is that of a homogeneou
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#21306;&#22495;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#28857;&#21608;&#22260;&#32447;&#24615;&#21306;&#22495;&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#35757;&#32451;&#36807;&#31243;&#20013;&#36825;&#31181;&#22797;&#26434;&#24615;&#32463;&#21382;&#20102;&#20960;&#20010;&#38454;&#27573;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#35266;&#23519;&#21040;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#26377;&#19968;&#20010;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12977</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32447;&#24615;&#21306;&#22495;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Deep Network Linear Regions. (arXiv:2310.12977v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#21306;&#22495;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#28857;&#21608;&#22260;&#32447;&#24615;&#21306;&#22495;&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#35757;&#32451;&#36807;&#31243;&#20013;&#36825;&#31181;&#22797;&#26434;&#24615;&#32463;&#21382;&#20102;&#20960;&#20010;&#38454;&#27573;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#35266;&#23519;&#21040;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#26377;&#19968;&#20010;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DN&#65289;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#28436;&#21464;&#19978;&#65292;&#36825;&#20123;&#28436;&#21464;&#26159;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#28857;&#38468;&#36817;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;DN&#29616;&#35937;&#26368;&#21021;&#26159;&#22312;&#25991;&#29486;&#20013;&#20197;&#27492;&#20026;&#22522;&#30784;&#24341;&#20837;&#30340;&#65292;&#20363;&#22914;&#65292;&#21452;&#37325;&#19979;&#38477;&#12289;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;DN&#65288;&#20363;&#22914;&#20855;&#26377;&#65288;&#27844;&#28431;&#30340;&#65289;ReLU&#38750;&#32447;&#24615;&#30340;&#32593;&#32476;&#65289;&#24418;&#25104;&#30340;&#36755;&#20837;&#31354;&#38388;&#21010;&#20998;&#25110;&#32447;&#24615;&#21306;&#22495;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#26681;&#25454;&#25968;&#25454;&#28857;&#21608;&#22260;&#20219;&#24847;&#32500;&#24230;&#37051;&#22495;&#20013;&#30340;&#32447;&#24615;&#21306;&#22495;&#30340;&#38598;&#20013;&#31243;&#24230;&#26469;&#22218;&#25324;DN&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#65288;LC&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#28857;&#21608;&#22260;&#30340;LC&#32463;&#21382;&#20102;&#35768;&#22810;&#38454;&#27573;&#65292;&#20174;&#21021;&#22987;&#21270;&#21518;&#30340;&#19979;&#38477;&#36235;&#21183;&#24320;&#22987;&#65292;&#28982;&#21518;&#19978;&#21319;&#65292;&#24182;&#26368;&#32456;&#20197;&#19979;&#38477;&#36235;&#21183;&#32467;&#26463;&#12290;&#36890;&#36807;&#20351;&#29992;&#31934;&#30830;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;LC&#19979;&#38477;&#38454;&#27573;&#65292;li
&lt;/p&gt;
&lt;p&gt;
The study of Deep Network (DN) training dynamics has largely focused on the evolution of the loss function, evaluated on or around train and test set data points. In fact, many DN phenomenon were first introduced in literature with that respect, e.g., double descent, grokking. In this study, we look at the training dynamics of the input space partition or linear regions formed by continuous piecewise affine DNs, e.g., networks with (leaky)ReLU nonlinearities. First, we present a novel statistic that encompasses the local complexity (LC) of the DN based on the concentration of linear regions inside arbitrary dimensional neighborhoods around data points. We observe that during training, the LC around data points undergoes a number of phases, starting with a decreasing trend after initialization, followed by an ascent and ending with a final descending trend. Using exact visualization methods, we come across the perplexing observation that during the final LC descent phase of training, li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12781</link><description>&lt;p&gt;
&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#22312;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#20445;&#35777;&#65292;&#21363;&#20010;&#20307;&#29992;&#25143;&#20449;&#24687;&#19981;&#20250;&#27844;&#38706;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21521;&#20445;&#23494;&#25968;&#25454;&#27880;&#20837;&#26657;&#20934;&#30340;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#25110;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#20250;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#65292;&#38590;&#20197;&#23545;&#22522;&#30784;&#26426;&#23494;&#25968;&#25454;&#30340;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#20316;&#20026;&#19968;&#32452;&#28789;&#27963;&#30340;&#20998;&#24067;&#26469;&#36817;&#20284;&#32473;&#23450;&#35266;&#27979;&#21040;&#30340;&#31169;&#26377;&#26597;&#35810;&#32467;&#26524;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20256;&#26579;&#30149;&#27169;&#22411;&#19979;&#30340;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20197;&#21450;&#26222;&#36890;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CARP&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#38450;&#27490;&#20102;&#23849;&#28291;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#65292;&#35777;&#26126;CARP&#30340;&#34920;&#31034;&#36866;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19982;11&#31181;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.12692</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Consistent Assignment of Views over Random Partitions. (arXiv:2310.12692v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CARP&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#38450;&#27490;&#20102;&#23849;&#28291;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#65292;&#35777;&#26126;CARP&#30340;&#34920;&#31034;&#36866;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19982;11&#31181;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#23545;&#38543;&#26426;&#20998;&#21306;&#30340;&#35270;&#22270;&#36827;&#34892;&#19968;&#33268;&#20998;&#37197;&#65288;CARP&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;CARP&#20197;&#31471;&#21040;&#31471;&#22312;&#32447;&#30340;&#26041;&#24335;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26469;&#23398;&#20064;&#21407;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#38750;&#21487;&#24494;&#27169;&#22359;&#26469;&#35299;&#20915;&#32858;&#31867;&#20998;&#37197;&#38382;&#39064;&#12290;CARP&#36890;&#36807;&#22522;&#20110;&#21407;&#22411;&#30340;&#38543;&#26426;&#20998;&#21306;&#20248;&#21270;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#24182;&#24378;&#21046;&#35270;&#22270;&#20043;&#38388;&#30340;&#20998;&#37197;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#38450;&#27490;&#20102;&#32852;&#21512;&#23884;&#20837;&#35757;&#32451;&#20013;&#30340;&#23849;&#28291;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;CARP&#30340;&#34920;&#31034;&#36866;&#29992;&#20110;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;17&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;CARP&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21253;&#25324;&#32447;&#24615;&#35780;&#20272;&#12289;&#23569;&#26679;&#26412;&#20998;&#31867;&#12289;k-NN&#12289;k-means&#12289;&#22270;&#20687;&#26816;&#32034;&#21644;&#22797;&#21046;&#26816;&#27979;&#31561;&#35768;&#22810;&#26631;&#20934;&#21327;&#35758;&#12290;&#25105;&#20204;&#23558;CARP&#30340;&#24615;&#33021;&#19982;11&#31181;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively abla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#20849;&#27169;&#22411;&#19978;&#35780;&#20272;&#25915;&#20987;&#21644;&#38450;&#24481;&#26469;&#20840;&#38754;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#20197;&#35299;&#20915;&#23545;&#39640;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#36739;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12665</link><description>&lt;p&gt;
SecurityNet&#65306;&#35780;&#20272;&#20844;&#20849;&#27169;&#22411;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models. (arXiv:2310.12665v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#20849;&#27169;&#22411;&#19978;&#35780;&#20272;&#25915;&#20987;&#21644;&#38450;&#24481;&#26469;&#20840;&#38754;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#20197;&#35299;&#20915;&#23545;&#39640;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#36739;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#34987;&#37096;&#32626;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#28431;&#27934;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#26377;&#21508;&#31181;&#32463;&#39564;&#24615;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23454;&#39564;&#26159;&#22312;&#30001;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#33258;&#24049;&#35757;&#32451;&#30340;&#30446;&#26631;ML&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#12290;&#30001;&#20110;&#20351;&#29992;&#22797;&#26434;&#26550;&#26500;&#35757;&#32451;&#39640;&#32423;&#27169;&#22411;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#36873;&#25321;&#20351;&#29992;&#30456;&#23545;&#31616;&#21333;&#30340;&#26550;&#26500;&#22312;&#20856;&#22411;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23569;&#25968;&#30446;&#26631;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;ML&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#24212;&#35813;&#22312;&#35757;&#32451;&#26377;&#19981;&#21516;&#30446;&#30340;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#35780;&#20272;ML&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#30446;&#30340;&#65289;&#30340;&#22823;&#37327;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#20844;&#24320;&#27169;&#22411;&#26435;&#37325;&#65288;&#20844;&#20849;&#27169;&#22411;&#65289;&#26469;&#35780;&#20272;&#23545;ML&#27169;&#22411;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#24211;&#65292;&#21517;&#20026;SecurityNet
&lt;/p&gt;
&lt;p&gt;
While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12632</link><description>&lt;p&gt;
&#38754;&#21521;&#28938;&#25509;&#36807;&#31243;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#36136;&#37327;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#36807;&#31243;&#30340;&#25968;&#23383;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#36136;&#37327;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#30340;&#21046;&#36896;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#26159;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#65288;GMAW&#65289;&#12290;&#28938;&#25509;&#36807;&#31243;&#20197;&#26448;&#26009;&#24615;&#36136;&#12289;&#24037;&#33402;&#26465;&#20214;&#21644;&#28938;&#25509;&#36136;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#20026;&#29305;&#24449;&#12290;&#22312;&#39057;&#32321;&#26356;&#25913;&#24037;&#33402;&#21442;&#25968;&#30340;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#30772;&#22351;&#24615;&#27979;&#35797;&#20934;&#30830;&#30830;&#23450;&#28938;&#32541;&#36136;&#37327;&#26159;&#32463;&#27982;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#20174;&#24037;&#33402;&#35266;&#23519;&#20013;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#12290;&#26680;&#24515;&#27010;&#24565;&#21253;&#25324;&#30001;&#22235;&#20010;&#20027;&#35201;&#38454;&#27573;&#32452;&#25104;&#30340;&#31649;&#32447;&#65306;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#22914;&#30005;&#27969;&#21644;&#30005;&#21387;&#65289;&#30340;&#25910;&#38598;&#21644;&#31649;&#29702;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series 
&lt;/p&gt;</description></item><item><title>DA-TransUNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#22359;&#12290;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12570</link><description>&lt;p&gt;
DA-TransUNet: &#23558;Spatial&#21644;Channel Dual Attention&#19982;Transformer U-Net&#38598;&#25104;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12570
&lt;/p&gt;
&lt;p&gt;
DA-TransUNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#22359;&#12290;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24378;&#22823;&#30340;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#65292;&#33258;&#21160;&#21270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;Transformer&#30340;&#24433;&#21709;&#23548;&#33268;&#20102;&#23545;&#20854;&#21464;&#20307;&#30340;&#30740;&#31350;&#65292;&#24182;&#22823;&#35268;&#27169;&#26367;&#20195;&#20256;&#32479;&#30340;CNN&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#32463;&#24120;&#24573;&#35270;&#20102;Transformer&#30340;&#22266;&#26377;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#24494;&#23567;&#35843;&#25972;&#23545;&#27169;&#22411;&#21644;Transformer&#27169;&#22359;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#31216;&#20026;DA-TransUNet&#65292;&#26088;&#22312;&#23558;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#22359;&#24341;&#20837;&#20256;&#32479;U&#24418;&#26550;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;Transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;DA-TransUNet&#21033;&#29992;&#20102;Transformer&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;DA-Block&#30340;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20043;&#21069;&#30340;Transformer U-Net&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#21452;&#37325;&#27880;&#24847;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Great progress has been made in automatic medical image segmentation due to powerful deep representation learning. The influence of transformer has led to research into its variants, and large-scale replacement of traditional CNN modules. However, such trend often overlooks the intrinsic feature extraction capabilities of the transformer and potential refinements to both the model and the transformer module through minor adjustments. This study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to introduce the Transformer and dual attention block into the encoder and decoder of the traditional U-shaped architecture. Unlike prior transformer-based solutions, our DA-TransUNet utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block, which can efficiently combine global, local, and multi-scale features to enhance medical image segmentation. Meanwhile, experimental results show that a dual attention block is added bef
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;(ID-ExpO)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#21306;&#20998;&#30340;&#39044;&#27979;&#22120;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#24182;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ID-ExpO&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.12553</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#21306;&#20998;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#27491;&#21017;&#21270;&#36827;&#34892;&#35299;&#37322;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;(ID-ExpO)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#21306;&#20998;&#30340;&#39044;&#27979;&#22120;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#24182;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ID-ExpO&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#35299;&#37322;&#36136;&#37327;&#36890;&#24120;&#20351;&#29992;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#36827;&#34892;&#34913;&#37327;&#65292;&#36825;&#20123;&#25351;&#26631;&#35780;&#20272;&#35299;&#37322;&#30340;&#24544;&#23454;&#24230;&#65292;&#21363;&#35299;&#37322;&#27491;&#30830;&#22320;&#21453;&#26144;&#20102;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#31243;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;&#65288;ID-ExpO&#65289;&#65292;&#35813;&#20248;&#21270;&#33021;&#22815;&#25913;&#21892;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#30001;&#20110;&#21407;&#22987;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#23545;&#20110;&#35299;&#37322;&#26469;&#35828;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20123;&#25351;&#26631;&#20197;&#20351;&#20854;&#21487;&#21306;&#20998;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24418;&#24335;&#21270;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#30340;&#27491;&#21017;&#21270;&#12290;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ID-ExpO&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#22120;&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#24341;&#23548;&#31639;&#27861;&#30340;&#31561;&#21464;&#24418;&#24335;&#65292;&#21487;&#20197;&#22312;&#25104;&#20687;&#21453;&#38382;&#39064;&#20013;&#37327;&#21270;&#37325;&#26500;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.11838</link><description>&lt;p&gt;
&#31561;&#21464;&#24341;&#23548;&#27861;&#22312;&#25104;&#20687;&#21453;&#38382;&#39064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems. (arXiv:2310.11838v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#24341;&#23548;&#31639;&#27861;&#30340;&#31561;&#21464;&#24418;&#24335;&#65292;&#21487;&#20197;&#22312;&#25104;&#20687;&#21453;&#38382;&#39064;&#20013;&#37327;&#21270;&#37325;&#26500;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25104;&#20687;&#38382;&#39064;&#36890;&#24120;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36866;&#23450;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#37325;&#35201;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20934;&#30830;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#20005;&#26684;&#35299;&#37322;&#23454;&#39564;&#32467;&#26524;&#20197;&#21450;&#21487;&#38752;&#22320;&#20351;&#29992;&#37325;&#26500;&#22270;&#20687;&#20316;&#20026;&#31185;&#23398;&#35777;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25104;&#20687;&#26041;&#27861;&#26080;&#27861;&#20197;&#23545;&#23454;&#39564;&#37325;&#22797;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26041;&#24335;&#37327;&#21270;&#37325;&#26500;&#22270;&#20687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;&#21442;&#25968;&#24341;&#23548;&#31639;&#27861;&#30340;&#31561;&#21464;&#24418;&#24335;&#65292;&#21033;&#29992;&#22312;&#25104;&#20687;&#38382;&#39064;&#20013;&#24120;&#35265;&#30340;&#23545;&#31216;&#24615;&#21644;&#19981;&#21464;&#24615;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#20219;&#20309;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#21253;&#25324;&#21482;&#33021;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hamming&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20108;&#36827;&#21046;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25366;&#25496;&#21306;&#20998;&#24615;k-mer&#38598;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.10321</link><description>&lt;p&gt;
Hamming&#32534;&#30721;&#22120;&#65306;&#20026;&#31163;&#25955;&#24207;&#21015;&#20998;&#31867;&#25366;&#25496;&#21306;&#20998;&#24615;k-mers
&lt;/p&gt;
&lt;p&gt;
Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence Classification. (arXiv:2310.10321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hamming&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20108;&#36827;&#21046;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25366;&#25496;&#21306;&#20998;&#24615;k-mer&#38598;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#20998;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#23613;&#31649;&#36807;&#21435;&#20960;&#21313;&#24180;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#20013;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#22312;&#25366;&#25496;&#36807;&#31243;&#20013;&#21333;&#29420;&#34913;&#37327;&#27599;&#20010;&#29305;&#24449;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#23548;&#33268;&#36951;&#28431;&#20102;&#19968;&#20123;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#30340;&#29305;&#24449;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#22312;&#23558;&#24207;&#21015;&#36716;&#25442;&#20026;&#29305;&#24449;&#21521;&#37327;&#21518;&#65292;&#24456;&#38590;&#30830;&#20445;&#25972;&#20307;&#30340;&#21306;&#20998;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Hamming&#32534;&#30721;&#22120;&#65292;&#23427;&#21033;&#29992;&#20108;&#36827;&#21046;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1DCNN&#65289;&#32467;&#26500;&#26469;&#25366;&#25496;&#21306;&#20998;&#24615;k-mer&#38598;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;Hamming&#36317;&#31163;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#30830;&#20445;&#29305;&#24449;&#25366;&#25496;&#21644;&#20998;&#31867;&#36807;&#31243;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;CNN&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#25628;&#32034;&#26469;&#23547;&#25214;&#21306;&#20998;&#24615;k-mer&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence classification has numerous applications in various fields. Despite extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. Existing pattern-based methods measure the discriminative power of each feature individually during the mining process, leading to the result of missing some combinations of features with discriminative power. Furthermore, it is difficult to ensure the overall discriminative performance after converting sequences into feature vectors. To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#20204;&#19981;&#20165;&#24102;&#26469;&#20102;&#22686;&#24378;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.10196</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#65306;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. (arXiv:2310.10196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#20204;&#19981;&#20165;&#24102;&#26469;&#20102;&#22686;&#24378;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#23427;&#20204;&#25429;&#25417;&#21160;&#24577;&#31995;&#32479;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#30001;&#29289;&#29702;&#21644;&#34394;&#25311;&#20256;&#24863;&#22120;&#22823;&#37327;&#20135;&#29983;&#12290;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#23545;&#20110;&#21033;&#29992;&#23427;&#20204;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#20197;&#21450;&#21463;&#30410;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#21644;&#20854;&#20182;&#22522;&#30784;&#27169;&#22411;&#30340;&#31361;&#30772;&#25512;&#21160;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#22686;&#24378;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#65292;&#36824;&#20026;&#33021;&#22815;&#29702;&#35299;&#21644;&#22788;&#29702;&#24120;&#35265;&#26102;&#38388;&#25968;&#25454;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#22312;&#27492;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#23450;&#21046;&#30340;&#22823;&#22411;&#27169;&#22411;&#30340;&#20840;&#38754;&#21644;&#26368;&#26032;&#32508;&#36848;&#65292;&#21253;&#25324;&#25968;&#25454;&#31867;&#22411;&#12289;&#27169;&#22411;&#31867;&#21035;&#12289;&#27169;&#22411;&#33539;&#22260;&#21644;&#24212;&#29992;&#39046;&#22495;/&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to 
&lt;/p&gt;</description></item><item><title>&#20197;&#21069;&#30740;&#31350;&#34920;&#26126;&#26420;&#32032;&#30340;LBA&#21644;LLP&#19981;&#33021;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;&#20294;&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;&#20855;&#26377;&#38543;&#26426;&#25277;&#26679;&#30340;&#21152;&#26435;LBA&#21487;&#20197;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.10092</link><description>&lt;p&gt;
&#36890;&#36807;&#32858;&#21512;&#23454;&#29616;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Label Differential Privacy via Aggregation. (arXiv:2310.10092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10092
&lt;/p&gt;
&lt;p&gt;
&#20197;&#21069;&#30740;&#31350;&#34920;&#26126;&#26420;&#32032;&#30340;LBA&#21644;LLP&#19981;&#33021;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;&#20294;&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;&#20855;&#26377;&#38543;&#26426;&#25277;&#26679;&#30340;&#21152;&#26435;LBA&#21487;&#20197;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#38544;&#31169;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#20445;&#25252;&#25935;&#24863;&#35757;&#32451;&#26631;&#31614;&#30340;&#38544;&#31169;&#12290;&#22312;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;(LLP)&#26694;&#26550;&#20013;&#65292;&#25968;&#25454;&#38598;&#34987;&#21010;&#20998;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#21253;&#65292;&#21482;&#33021;&#33719;&#24471;&#27599;&#20010;&#21253;&#20013;&#26631;&#31614;&#30340;&#24635;&#21644;&#12290;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#38480;&#21046;&#23398;&#20064;(LBA)&#26159;&#21482;&#33021;&#33719;&#24471;&#21253;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#24635;&#21644;&#65288;&#21487;&#33021;&#26159;&#21152;&#26435;&#30340;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#31181;&#32858;&#21512;&#25216;&#26415;&#26159;&#21542;&#33021;&#22815;&#22312;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;(label-DP)&#30340;&#27010;&#24565;&#19979;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#35813;&#27010;&#24565;&#20043;&#21069;&#22312;[Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22]&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#24456;&#23481;&#26131;&#30475;&#20986;&#65292;&#26420;&#32032;&#30340;LBA&#21644;LLP&#19981;&#33021;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20855;&#26377;$m$&#20010;&#38543;&#26426;&#25277;&#26679;&#30340;&#19981;&#30456;&#20132;$k$-&#22823;&#23567;&#21253;&#30340;&#21152;&#26435;LBA&#23454;&#38469;&#19978;&#26159;$(\varepsilon,
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, in particular due to recent developments in the privacy landscape, training data may be aggregated to preserve the privacy of sensitive training labels. In the learning from label proportions (LLP) framework, the dataset is partitioned into bags of feature-vectors which are available only with the sum of the labels per bag. A further restriction, which we call learning from bag aggregates (LBA) is where instead of individual feature-vectors, only the (possibly weighted) sum of the feature-vectors per bag is available. We study whether such aggregation techniques can provide privacy guarantees under the notion of label differential privacy (label-DP) previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22].  It is easily seen that naive LBA and LLP do not provide label-DP. Our main result however, shows that weighted LBA using iid Gaussian weights with $m$ randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#32771;&#26041;&#27861;&#65292;&#20197;&#34913;&#37327;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#21442;&#32771;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.08394</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#25351;&#20196;&#36981;&#24490;&#35780;&#20272;&#65306;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization. (arXiv:2310.08394v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#32771;&#26041;&#27861;&#65292;&#20197;&#34913;&#37327;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#21442;&#32771;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;&#20309;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#19978;&#26377;&#25152;&#22686;&#38271;&#65292;&#20294;&#26159;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27491;&#30830;&#24615;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20803;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#34913;&#37327;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#22312;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#20010;&#26032;&#30340;&#30701;&#25991;&#24418;&#24335;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#20854;&#20013;&#21253;&#21547;300&#20010;&#25991;&#26723;&#25351;&#20196;&#23545;&#65292;&#27599;&#20010;&#23545;&#24212;3&#20010;&#31572;&#26696;&#12290;&#25152;&#26377;900&#20010;&#31572;&#26696;&#30001;3&#21517;&#20154;&#31867;&#26631;&#27880;&#21592;&#36827;&#34892;&#35780;&#20998;&#12290;&#21033;&#29992;riSum&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35780;&#20272;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#26032;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#26114;&#36149;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.06923</link><description>&lt;p&gt;
PICProp&#65306;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#65292;&#26631;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#23384;&#22312;&#30528;&#25345;&#20037;&#30340;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20808;&#39564;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#21518;&#39564;&#21482;&#33021;&#20197;&#36817;&#20284;&#30340;&#26041;&#24335;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#30001;&#20110;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#36817;&#20284;&#31934;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#23545;&#30830;&#23450;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#12290;&#21363;&#65292;&#22312;&#25972;&#20010;&#21306;&#22495;&#20013;&#20197;&#27010;&#29575;&#25285;&#20445;&#30340;&#24418;&#24335;&#20256;&#25773;&#32622;&#20449;&#24230;&#65292;&#20197;&#36798;&#21040;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;&#65288;PICProp&#65289;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26469;&#35745;&#31639;&#19968;&#20010;&#26377;&#25928;&#30340;CI&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#23450;&#29702;&#20197;&#21450;&#38024;&#23545;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#33021;&#22815;&#39564;&#35777;&#22797;&#26434;&#30340;&#20027;&#24352;&#24182;&#29983;&#25104;&#35299;&#37322;&#65292;&#23545;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. (arXiv:2310.05253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#33021;&#22815;&#39564;&#35777;&#22797;&#26434;&#30340;&#20027;&#24352;&#24182;&#29983;&#25104;&#35299;&#37322;&#65292;&#23545;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#24352;&#39564;&#35777;&#22312;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20027;&#24352;&#39564;&#35777;&#24037;&#20316;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#37027;&#23601;&#26159;&#22914;&#20309;&#22312;&#19981;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20027;&#24352;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#25552;&#20379;&#20840;&#38754;&#35299;&#37322;&#20197;&#35777;&#26126;&#20854;&#20915;&#31574;&#24182;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#20027;&#24352;&#39564;&#35777;&#21644;&#29983;&#25104;&#35299;&#37322;&#30340;&#19968;&#38454;&#36923;&#36753;&#25351;&#23548;&#30340;&#30693;&#35782;&#22522;&#30784;&#30340;&#25512;&#29702;&#26041;&#27861;&#65288;FOLK&#65289;&#12290;FOLK&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23558;&#20027;&#24352;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#23376;&#21477;&#65292;&#24182;&#29983;&#25104;&#19968;&#32452;&#35859;&#35789;&#65292;&#27599;&#20010;&#35859;&#35789;&#23545;&#24212;&#19968;&#20010;&#38656;&#35201;&#39564;&#35777;&#30340;&#23376;&#20027;&#24352;&#12290;&#28982;&#21518;&#65292;FOLK&#36890;&#36807;&#19968;&#32452;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#23545;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#25351;&#23548;&#30340;&#25512;&#29702;&#65292;&#20174;&#32780;&#36827;&#34892;&#30495;&#23454;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity pr
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2309.15302</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#26080;&#32422;&#26463;&#26426;&#22120;&#20154;&#32463;&#39564;&#20013;&#30340;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience. (arXiv:2309.15302v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#24418;&#35748;&#30693;&#65292;&#21363;&#36776;&#21035;&#21644;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22320;&#24418;&#65292;&#26159;&#26426;&#22120;&#20154;&#22312;&#33258;&#20027;&#36234;&#37326;&#23548;&#33322;&#20013;&#24517;&#39035;&#20855;&#22791;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#30446;&#21069;&#25552;&#20379;&#26426;&#22120;&#20154;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#25910;&#38598;&#65292;&#35201;&#20040;&#20381;&#36182;&#26080;&#27861;&#27867;&#21270;&#30340;&#24037;&#31243;&#29305;&#24449;&#21644;&#25104;&#26412;&#20989;&#25968;&#65292;&#25110;&#32773;&#20381;&#36182;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#30340;&#19987;&#23478;&#20154;&#31867;&#31034;&#33539;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#19981;&#21463;&#36825;&#20123;&#38480;&#21046;&#22320;&#20855;&#22791;&#22320;&#24418;&#35748;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#65288;STERLING&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#22320;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#26131;&#20110;&#25910;&#38598;&#30340;&#12289;&#26080;&#32422;&#26463;&#65288;&#20363;&#22914;&#65292;&#38750;&#19987;&#23478;&#30340;&#65289;&#21644;&#26410;&#26631;&#35760;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#65292;&#23545;&#25968;&#25454;&#37319;&#38598;&#27809;&#26377;&#39069;&#22806;&#30340;&#38480;&#21046;&#12290;STERLING&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65292;&#36890;&#36807;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#22320;&#24418;&#30456;&#20851;&#30340;&#34920;&#31034;&#20197;&#29992;&#20110;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;&#36890;&#36807;&#23454;&#29289;&#26426;&#22120;&#20154;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.12273</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#22522;&#20110;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#25913;&#36827;VTE&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12273
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#38745;&#33033;&#34880;&#26643;&#26643;&#22622;&#65288;VTE&#65289;&#65292;&#21253;&#25324;&#28145;&#38745;&#33033;&#34880;&#26643;&#65288;DVT&#65289;&#21644;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#65292;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#22312;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;VTE&#20107;&#20214;&#25110;&#24110;&#21161;&#20020;&#24202;&#19987;&#23478;&#35782;&#21035;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;VTE&#20107;&#20214;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#26377;&#38480;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#26377;&#25928;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;NLP&#27169;&#22411;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DL&#26041;&#27861;&#30340;&#26032;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#12289;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;NLP&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#39640;&#38750;&#32467;&#26500;&#21270;&#65288;&#33258;&#30001;&#25991;&#26412;&#65289;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;VTE&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#24314;&#31435;&#20102;&#20379;&#38656;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25581;&#31034;&#20102;&#36816;&#33829;&#26381;&#21153;&#20013;&#30340;&#31354;&#32570;&#12290;</title><link>http://arxiv.org/abs/2309.06299</link><description>&lt;p&gt;
&#22312;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#24314;&#27169;&#20379;&#38656;
&lt;/p&gt;
&lt;p&gt;
Modeling Supply and Demand in Public Transportation Systems. (arXiv:2309.06299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#24314;&#31435;&#20102;&#20379;&#38656;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25581;&#31034;&#20102;&#36816;&#33829;&#26381;&#21153;&#20013;&#30340;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21704;&#37324;&#26862;&#22561;&#20844;&#20849;&#20132;&#36890;&#37096;&#38376;&#26088;&#22312;&#21033;&#29992;&#20854;&#25968;&#25454;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#20379;&#38656;&#27169;&#22411;&#65292;&#24110;&#21161;&#37096;&#38376;&#35782;&#21035;&#26381;&#21153;&#20013;&#30340;&#31354;&#32570;&#12290;&#27169;&#22411;&#32771;&#34385;&#20102;&#35768;&#22810;&#21464;&#37327;&#65292;&#21253;&#25324;&#21704;&#37324;&#26862;&#22561;&#24066;&#21521;&#32852;&#37030;&#25919;&#24220;&#25253;&#21578;&#30340;&#26041;&#24335;&#20197;&#21450;&#26368;&#33030;&#24369;&#20154;&#21475;&#32858;&#38598;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Harrisonburg Department of Public Transportation (HDPT) aims to leverage their data to improve the efficiency and effectiveness of their operations. We construct two supply and demand models that help the department identify gaps in their service. The models take many variables into account, including the way that the HDPT reports to the federal government and the areas with the most vulnerable populations in Harrisonburg City. We employ data analysis and machine learning techniques to make our predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#36136;&#37327;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#32500;&#24230;&#36739;&#39640;&#19988;&#25968;&#25454;&#20174;&#23545;&#25968;&#20985;&#38598;&#30340;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#36817;&#20284;&#26368;&#22823;&#28145;&#24230;&#21644;&#25509;&#36817;&#38646;&#30340;&#28145;&#24230;&#65292;&#32780;&#23545;&#20110;&#20013;&#38388;&#28145;&#24230;&#30340;&#28857;&#65292;&#20219;&#20309;&#22909;&#30340;&#36817;&#20284;&#37117;&#38656;&#35201;&#25351;&#25968;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05657</link><description>&lt;p&gt;
&#20851;&#20110;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the quality of randomized approximations of Tukey's depth. (arXiv:2309.05657v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#36136;&#37327;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#32500;&#24230;&#36739;&#39640;&#19988;&#25968;&#25454;&#20174;&#23545;&#25968;&#20985;&#38598;&#30340;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#36817;&#20284;&#26368;&#22823;&#28145;&#24230;&#21644;&#25509;&#36817;&#38646;&#30340;&#28145;&#24230;&#65292;&#32780;&#23545;&#20110;&#20013;&#38388;&#28145;&#24230;&#30340;&#28857;&#65292;&#20219;&#20309;&#22909;&#30340;&#36817;&#20284;&#37117;&#38656;&#35201;&#25351;&#25968;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tukey&#28145;&#24230;&#65288;&#25110;&#21322;&#31354;&#38388;&#28145;&#24230;&#65289;&#26159;&#29992;&#20110;&#22810;&#20803;&#25968;&#25454;&#20013;&#24515;&#24230;&#37327;&#30340;&#24191;&#27867;&#24212;&#29992;&#30340;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#24230;&#19979;&#65292;Tukey&#28145;&#24230;&#30340;&#31934;&#30830;&#35745;&#31639;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#26679;&#30340;&#38543;&#26426;&#31639;&#27861;&#20309;&#26102;&#33021;&#22815;&#36820;&#22238;&#19968;&#20010;&#33391;&#22909;&#30340;Tukey&#28145;&#24230;&#36817;&#20284;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#20174;&#23545;&#25968;&#20985;&#38519;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#35201;&#27714;&#31639;&#27861;&#22312;&#32500;&#24230;&#19978;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#36816;&#34892;&#65292;&#38543;&#26426;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#22320;&#36817;&#20284;&#26368;&#22823;&#28145;&#24230;1/2&#21644;&#25509;&#36817;&#38646;&#30340;&#28145;&#24230;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#20110;&#20219;&#20309;&#20013;&#38388;&#28145;&#24230;&#30340;&#28857;&#65292;&#20219;&#20309;&#22909;&#30340;&#36817;&#20284;&#37117;&#38656;&#35201;&#25351;&#25968;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tukey's depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey's depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey's depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey's depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that, if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth $1/2$ and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COVERT&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#21487;&#36870;&#25200;&#21160;&#24674;&#22797;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#28860;&#21487;&#38752;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#20445;&#35777;&#23545;&#27604;&#23398;&#20064;&#20013;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08963</link><description>&lt;p&gt;
CONVERT: &#21487;&#38752;&#22686;&#24378;&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
CONVERT:Contrastive Graph Clustering with Reliable Augmentation. (arXiv:2308.08963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COVERT&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#21487;&#36870;&#25200;&#21160;&#24674;&#22797;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#28860;&#21487;&#38752;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#20445;&#35777;&#23545;&#27604;&#23398;&#20064;&#20013;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#23545;&#27604;&#22270;&#33410;&#28857;&#32858;&#31867;&#26159;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#39044;&#23450;&#20041;&#22686;&#24378;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#39537;&#21160;&#22686;&#24378;&#12290;&#34429;&#28982;&#24050;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#26041;&#27861;&#20173;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#22686;&#24378;&#65292;&#22686;&#24378;&#21518;&#30340;&#22270;&#30340;&#35821;&#20041;&#23481;&#26131;&#28418;&#31227;&#65292;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22686;&#24378;&#35270;&#22270;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#26080;&#27861;&#20445;&#35777;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#38752;&#22686;&#24378;&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#32593;&#32476;&#65288;COVERT&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#22686;&#24378;&#25968;&#25454;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#36870;&#25200;&#21160;&#24674;&#22797;&#32593;&#32476;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#36890;&#36807;&#24674;&#22797;&#25200;&#21160;&#21518;&#30340;&#28508;&#22312;&#23884;&#20837;&#26469;&#25552;&#28860;&#21487;&#38752;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#35777;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;Loss&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive graph node clustering via learnable data augmentation is a hot research spot in the field of unsupervised graph learning. The existing methods learn the sampling distribution of a pre-defined augmentation to generate data-driven augmentations automatically. Although promising clustering performance has been achieved, we observe that these strategies still rely on pre-defined augmentations, the semantics of the augmented graph can easily drift. The reliability of the augmented view semantics for contrastive learning can not be guaranteed, thus limiting the model performance. To address these problems, we propose a novel CONtrastiVe Graph ClustEring network with Reliable AugmenTation (COVERT). Specifically, in our method, the data augmentations are processed by the proposed reversible perturb-recover network. It distills reliable semantic information by recovering the perturbed latent embeddings. Moreover, to further guarantee the reliability of semantics, a novel semantic lo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06838</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#36827;&#34892;&#19968;&#33324;&#21270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476; (arXiv:2308.06838v2 [cs.LG] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Generalizing Topological Graph Neural Networks with Paths. (arXiv:2308.06838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#23613;&#31649;&#39640;&#38454;GNNs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#20197;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#22270;&#32452;&#20214;&#65292;&#22914;&#22242;&#25110;&#29615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36208;&#20102;&#19981;&#21516;&#30340;&#36335;&#32447;&#12290;&#25105;&#20204;&#24378;&#35843;&#36335;&#24452;&#65292;&#36825;&#26159;&#27599;&#20010;&#22270;&#20013;&#22266;&#26377;&#30340;&#12290;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#24182;&#19982;&#20854;&#20182;&#25299;&#25169;&#39046;&#22495;&#30340;&#19968;&#20123;&#25104;&#29087;&#29702;&#35770;&#24418;&#25104;&#26725;&#26753;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#27809;&#26377;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#35813;&#39046;&#22495;&#26089;&#26399;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24212;&#29992;&#39532;&#27663;&#36317;&#31163;&#21518;&#22788;&#29702;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#23545;&#36234;&#30028;&#22270;&#20687;&#30340;&#39640;&#25928;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.03723</link><description>&lt;p&gt;
&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#36234;&#30028;&#26816;&#27979;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation. (arXiv:2308.03723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24212;&#29992;&#39532;&#27663;&#36317;&#31163;&#21518;&#22788;&#29702;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#23545;&#36234;&#30028;&#22270;&#20687;&#30340;&#39640;&#25928;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#20020;&#24202;&#24212;&#29992;&#30340;&#20998;&#21106;&#27169;&#22411;&#22312;&#20854;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#27492;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26816;&#27979;&#36234;&#30028;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#33258;&#21160;&#21270;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#23558;&#39532;&#27663;&#36317;&#31163;&#21518;&#22788;&#29702;&#24212;&#29992;&#20110;Swin UNETR&#27169;&#22411;&#30340;&#29942;&#39048;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#23545;T1&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#19978;&#30340;&#32925;&#33039;&#36827;&#34892;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#38477;&#20302;&#29942;&#39048;&#29305;&#24449;&#30340;&#32500;&#25968;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#21040;&#36234;&#30028;&#22270;&#20687;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#20351;&#29992;&#19982;&#22024;&#26434;&#25968;&#25454;&#35821;&#35328;&#23436;&#20840;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#19982;&#20256;&#32479;&#30340;&#39057;&#35889;&#22270;&#25110;&#26102;&#38388;&#22495;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14502</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25439;&#22833;&#20989;&#25968;&#30740;&#31350;&#21475;&#35821;&#23545;&#35821;&#38899;&#22686;&#24378;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#20351;&#29992;&#19982;&#22024;&#26434;&#25968;&#25454;&#35821;&#35328;&#23436;&#20840;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#19982;&#20256;&#32479;&#30340;&#39057;&#35889;&#22270;&#25110;&#26102;&#38388;&#22495;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#65288;SSSRs&#65289;&#20316;&#20026;&#29305;&#24449;&#36716;&#25442;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#24456;&#23569;&#27880;&#24847;&#21040;&#29992;&#20110;&#35757;&#32451;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#38899;&#39057;&#35821;&#35328;&#19982;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20351;&#29992;&#23558;&#33258;&#30417;&#30563;&#34920;&#31034;&#19982;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#22024;&#26434;&#25968;&#25454;&#30340;&#35821;&#35328;&#23436;&#20840;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#22686;&#24378;&#27169;&#22411;&#34920;&#29616;&#27604;&#19981;&#23436;&#20840;&#21305;&#37197;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22686;&#24378;&#31995;&#32479;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#22909;&#65292;&#32780;&#20351;&#29992;&#20256;&#32479;&#30340;&#39057;&#35889;&#22270;&#25110;&#26102;&#38388;&#22495;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#27169;&#22411;&#21017;&#19981;&#20250;&#20986;&#29616;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#22312;&#22810;&#31181;&#19981;&#21516;&#35821;&#35328;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#32463;&#36807;&#19981;&#21516;&#35821;&#35328;&#32452;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinati
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#21462;&#26089;&#26399;&#23551;&#21629;&#20013;&#30340;&#23481;&#37327;-&#30005;&#21387;&#25968;&#25454;&#30340;&#26032;&#29305;&#24449;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#22312;&#19981;&#21516;&#20351;&#29992;&#26465;&#20214;&#19979;&#30340;&#30005;&#27744;&#23551;&#21629;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26089;&#26399;&#29305;&#24449;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#21644;&#32769;&#21270;&#27169;&#24335;&#30340;&#21464;&#21270;&#36895;&#29575;&#65292;&#20026;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2307.08382</link><description>&lt;p&gt;
&#20174;&#26089;&#26399;&#32769;&#21270;&#25968;&#25454;&#20013;&#39044;&#27979;&#19981;&#21516;&#20351;&#29992;&#26465;&#20214;&#19979;&#30340;&#30005;&#27744;&#23551;&#21629;
&lt;/p&gt;
&lt;p&gt;
Predicting Battery Lifetime Under Varying Usage Conditions from Early Aging Data. (arXiv:2307.08382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#21462;&#26089;&#26399;&#23551;&#21629;&#20013;&#30340;&#23481;&#37327;-&#30005;&#21387;&#25968;&#25454;&#30340;&#26032;&#29305;&#24449;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#22312;&#19981;&#21516;&#20351;&#29992;&#26465;&#20214;&#19979;&#30340;&#30005;&#27744;&#23551;&#21629;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26089;&#26399;&#29305;&#24449;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#21644;&#32769;&#21270;&#27169;&#24335;&#30340;&#21464;&#21270;&#36895;&#29575;&#65292;&#20026;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#23545;&#20110;&#39044;&#38450;&#24615;&#32500;&#25252;&#12289;&#20445;&#20462;&#21644;&#25913;&#36827;&#30340;&#30005;&#27744;&#35774;&#35745;&#21644;&#21046;&#36896;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21046;&#36896;&#21487;&#21464;&#24615;&#21644;&#20351;&#29992;&#20381;&#36182;&#24615;&#38477;&#35299;&#20351;&#24471;&#23551;&#21629;&#39044;&#27979;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20174;&#26089;&#26399;&#23551;&#21629;&#20013;&#25552;&#21462;&#30340;&#23481;&#37327;-&#30005;&#21387;&#25968;&#25454;&#30340;&#26032;&#29305;&#24449;&#26469;&#39044;&#27979;&#22312;&#20805;&#30005;&#36895;&#29575;&#12289;&#25918;&#30005;&#36895;&#29575;&#21644;&#25918;&#30005;&#28145;&#24230;&#24046;&#24322;&#36739;&#22823;&#30340;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#20174;&#23450;&#26399;&#23433;&#25490;&#30340;&#21442;&#32771;&#24615;&#33021;&#27979;&#35797;&#20013;&#25552;&#21462;&#30340;&#65288;&#21363;&#20302;&#36895;&#23436;&#20840;&#24490;&#29615;&#65289;&#12290;&#26089;&#26399;&#29983;&#21629;&#21608;&#26399;&#29305;&#24449;&#25429;&#25417;&#21040;&#20102;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#21644;&#21508;&#32452;&#20214;&#32423;&#21035;&#32769;&#21270;&#27169;&#24335;&#30340;&#21464;&#21270;&#36895;&#29575;&#65292;&#20854;&#20013;&#19968;&#20123;&#19982;&#30005;&#27744;&#23551;&#21629;&#21576;&#24378;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23545;&#30001;225&#20010;&#38221;-&#38192;-&#38068;/&#30707;&#22696;&#38146;&#31163;&#23376;&#30005;&#27744;&#22312;&#24191;&#27867;&#26465;&#20214;&#19979;&#32769;&#21270;&#20135;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20998;&#24067;&#20869;&#30340;&#30005;&#27744;&#30340;&#23551;&#21629;&#39044;&#27979;&#33021;&#22815;&#36798;&#21040;15.1%&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#20102;&#21069;15%&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate battery lifetime prediction is important for preventative maintenance, warranties, and improved cell design and manufacturing. However, manufacturing variability and usage-dependent degradation make life prediction challenging. Here, we investigate new features derived from capacity-voltage data in early life to predict the lifetime of cells cycled under widely varying charge rates, discharge rates, and depths of discharge. Features were extracted from regularly scheduled reference performance tests (i.e., low rate full cycles) during cycling. The early-life features capture a cell's state of health and the rate of change of component-level degradation modes, some of which correlate strongly with cell lifetime. Using a newly generated dataset from 225 nickel-manganese-cobalt/graphite Li-ion cells aged under a wide range of conditions, we demonstrate a lifetime prediction of in-distribution cells with 15.1% mean absolute percentage error using no more than the first 15% of data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#38544;&#24335;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#27604;DEQ&#21464;&#20307;&#26356;&#26377;&#25928;&#12289;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#31232;&#30095;&#25513;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08013</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38544;&#24335;&#27169;&#22411;&#65306;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#30340;&#31232;&#30095;&#24230;&#26435;&#34913;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#38544;&#24335;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#27604;DEQ&#21464;&#20307;&#26356;&#26377;&#25928;&#12289;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#31232;&#30095;&#25513;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#27169;&#22411;&#22914;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;Deep Equilibrium Models, DEQs&#65289;&#22240;&#20854;&#33021;&#22815;&#29992;&#20248;&#38597;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#24658;&#23450;&#30340;&#20869;&#23384;&#21344;&#29992;&#35757;&#32451;&#26080;&#38480;&#23618;&#27169;&#22411;&#32780;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#20960;&#27425;&#23581;&#35797;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#27169;&#22411;&#20302;&#25928;&#21644;&#20248;&#21270;&#19981;&#31283;&#23450;&#24615;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#35270;&#35273;&#20219;&#21153;&#65292;&#32570;&#20047;&#30456;&#20851;&#26041;&#27861;&#30340;&#20844;&#24179;&#22522;&#20934;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#38544;&#24335;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#23558;&#20854;&#36861;&#28335;&#21040;&#21407;&#22987;&#30340;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;DEQ&#21464;&#20307;&#30456;&#27604;&#65292;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#19978;&#26356;&#21152;&#26377;&#25928;&#12289;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290;&#36890;&#36807;&#36825;&#20123;&#31616;&#21333;&#32780;&#28165;&#26224;&#30340;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#22411;&#23481;&#37327;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516;&#31232;&#30095;&#25513;&#27169;&#26469;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#20851;&#20110;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#31232;&#30095;&#24230;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and spars
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07847</link><description>&lt;p&gt;
&#20113;&#28216;&#25103;&#20013;&#30340;&#31070;&#32463;&#35270;&#39057;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#28216;&#25103;&#26159;&#19968;&#20010;&#20215;&#20540;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#12290;&#22312;&#20113;&#28216;&#25103;&#20013;&#65292;&#23458;&#25143;&#31471;&#23558;&#33258;&#24049;&#30340;&#31227;&#21160;&#21457;&#36865;&#21040;&#20114;&#32852;&#32593;&#19978;&#30340;&#28216;&#25103;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#28210;&#26579;&#24182;&#20256;&#36755;&#32467;&#26524;&#35270;&#39057;&#22238;&#26469;&#12290;&#20026;&#20102;&#25552;&#20379;&#33391;&#22909;&#30340;&#28216;&#25103;&#20307;&#39564;&#65292;&#38656;&#35201;&#20302;&#20110;80&#27627;&#31186;&#30340;&#24310;&#36831;&#12290;&#36825;&#24847;&#21619;&#30528;&#35270;&#39057;&#30340;&#28210;&#26579;&#12289;&#32534;&#30721;&#12289;&#20256;&#36755;&#12289;&#35299;&#30721;&#21644;&#26174;&#31034;&#24517;&#39035;&#22312;&#36825;&#20010;&#26102;&#38388;&#33539;&#22260;&#20869;&#23436;&#25104;&#65292;&#30001;&#20110;&#26381;&#21153;&#22120;&#36807;&#36733;&#12289;&#32593;&#32476;&#25317;&#22622;&#21644;&#20002;&#21253;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#28857;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#35270;&#39057;&#24103;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#35270;&#39057;&#24103;&#24674;&#22797;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#26174;&#33879;&#25552;&#21319;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#37096;&#20998;&#35299;&#30721;&#30340;&#24103;&#26469;&#24674;&#22797;&#20002;&#22833;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;(i)&#39640;&#25928;&#25552;&#21462;&#28216;&#25103;&#29366;&#24577;&#65292;(ii)&#20462;&#25913; H.264 &#35270;&#39057;&#35299;&#30721;&#22120;&#29983;&#25104;&#19968;&#20010;&#25351;&#31034;&#38656;&#35201;&#24674;&#22797;&#35270;&#39057;&#24103;&#21738;&#20123;&#37096;&#20998;&#30340;&#25513;&#30721;&#65292;&#21644; (iii)&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35270;&#39057;&#24103;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural networ
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.17817</link><description>&lt;p&gt;
Act3D&#65306;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26816;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation. (arXiv:2306.17817v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17817
&lt;/p&gt;
&lt;p&gt;
Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#34920;&#24449;&#38750;&#24120;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#32534;&#30721;&#36974;&#25377;&#24773;&#20917;&#24182;&#31616;&#21270;&#31354;&#38388;&#25512;&#29702;&#12290;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#38656;&#35201;&#23545;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#36827;&#34892;&#39640;&#31354;&#38388;&#31934;&#24230;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;3D&#24863;&#30693;&#32593;&#26684;&#36827;&#34892;&#35745;&#31639;&#65292;&#36825;&#22312;&#22788;&#29702;&#19978;&#38750;&#24120;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25805;&#20316;&#31574;&#30053;&#30452;&#25509;&#22312;2D&#20013;&#36816;&#20316;&#65292;&#25918;&#24323;&#20102;3D&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Act3D&#65292;&#19968;&#31181;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#35270;&#20026;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#25805;&#20316;&#31574;&#30053;Transformer&#12290;&#23427;&#20197;&#19968;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#30340;&#26410;&#25237;&#24433;3D&#29305;&#24449;&#20113;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#31895;-&#31934;&#26041;&#24335;&#22312;&#33258;&#30001;&#31354;&#38388;&#20013;&#36845;&#20195;&#37319;&#26679;3D&#28857;&#32593;&#26684;&#65292;&#20351;&#29992;&#30456;&#23545;&#31354;&#38388;&#27880;&#24847;&#21147;&#23558;&#20854;&#29305;&#24449;&#21270;&#20026;&#29289;&#29702;&#29305;&#24449;&#20113;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#28857;&#36827;&#34892;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#12290;Act3D&#22312;&#24050;&#24314;&#31435;&#30340;&#25805;&#32437;&#22522;&#20934;RLbench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#25104;&#32489;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#20013;&#23454;&#29616;&#20102;10%&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute impr
&lt;/p&gt;</description></item><item><title>CoarsenConf&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#31561;&#21464;&#31895;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#27880;&#24847;&#26426;&#21046;&#24674;&#22797;&#20174;&#31895;&#31890;&#24230;&#34920;&#31034;&#20013;&#24471;&#21040;&#30340;&#32454;&#31890;&#24230;&#21407;&#23376;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#26500;&#35937;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.14852</link><description>&lt;p&gt;
CoarsenConf: &#22522;&#20110;&#32858;&#21512;&#27880;&#24847;&#21147;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#31561;&#21464;&#31895;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation. (arXiv:2306.14852v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14852
&lt;/p&gt;
&lt;p&gt;
CoarsenConf&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#31561;&#21464;&#31895;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#27880;&#24847;&#26426;&#21046;&#24674;&#22797;&#20174;&#31895;&#31890;&#24230;&#34920;&#31034;&#20013;&#24471;&#21040;&#30340;&#32454;&#31890;&#24230;&#21407;&#23376;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#26500;&#35937;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26159;&#21270;&#23398;&#20449;&#24687;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#20302;&#33021;&#37327;&#30340;&#19977;&#32500;&#32467;&#26500;&#21487;&#20197;&#36991;&#20813;&#26114;&#36149;&#30340;&#37327;&#23376;&#21147;&#23398;&#27169;&#25311;&#65292;&#20174;&#32780;&#21152;&#36895;&#34394;&#25311;&#31579;&#36873;&#21644;&#22686;&#24378;&#32467;&#26500;&#25506;&#32034;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#29992;&#20110;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#24456;&#22810;&#27169;&#22411;&#38590;&#20197;&#25345;&#32493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#26500;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoarsenConf&#65292;&#23427;&#22522;&#20110;&#25197;&#36716;&#35282;&#24230;&#36827;&#34892;&#20998;&#23376;&#22270;&#30340;&#31895;&#21270;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;SE(3)-&#31561;&#21464;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#12290;&#36890;&#36807;&#31561;&#21464;&#30340;&#31895;&#21270;&#65292;&#25105;&#20204;&#32858;&#21512;&#20102;&#36890;&#36807;&#21487;&#36716;&#21160;&#38190;&#36830;&#25509;&#30340;&#23376;&#22270;&#30340;&#32454;&#31890;&#24230;&#21407;&#23376;&#22352;&#26631;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#21464;&#38271;&#24230;&#30340;&#31895;&#31890;&#24230;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#21512;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#31895;&#31890;&#24230;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#24674;&#22797;&#32454;&#31890;&#24230;&#30340;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#26500;&#35937;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular conformer generation (MCG) is an important task in cheminformatics and drug discovery. The ability to efficiently generate low-energy 3D structures can avoid expensive quantum mechanical simulations, leading to accelerated virtual screenings and enhanced structural exploration. Several generative models have been developed for MCG, but many struggle to consistently produce high-quality conformers. To address these issues, we introduce CoarsenConf, which coarse-grains molecular graphs based on torsional angles and integrates them into an SE(3)-equivariant hierarchical variational autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained atomic coordinates of subgraphs connected via rotatable bonds, creating a variable-length coarse-grained latent representation. Our model uses a novel aggregated attention mechanism to restore fine-grained coordinates from the coarse-grained latent representation, enabling efficient generation of accurate conformers. Furth
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.12873</link><description>&lt;p&gt;
FuXi: &#19968;&#20010;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12873
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;0.25&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;10&#22825;&#22825;&#27668;&#39044;&#25253;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#27604;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;(ECMWF)&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;(HRES)&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#19982;ECMWF&#38598;&#21512;&#24179;&#22343;(EM)&#30456;&#24403;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32531;&#35299;&#39044;&#25253;&#35823;&#24046;&#30340;&#31215;&#32047;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#39044;&#25253;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20943;&#23569;&#31215;&#32047;&#35823;&#24046;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#33258;&#22238;&#24402;&#22810;&#26102;&#38388;&#27493;&#38271;&#25439;&#22833;&#65292;&#20294;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21457;&#29616;&#26080;&#27861;&#22312;&#30701;&#21644;&#38271;&#23548;&#20986;&#26102;&#38388;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuXi&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20998;&#36776;&#29575;&#20026;0.25&#24230;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;6&#23567;&#26102;&#30340;15&#22825;&#20840;&#29699;&#39044;&#27979;&#12290;FuXi&#22522;&#20110;&#32423;&#32852;&#38598;&#21512;&#27169;&#22411;&#24320;&#21457;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#20943;&#23569;&#20102;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#12290;&#20351;&#29992;&#31354;&#27668;&#28201;&#24230;&#65292;&#27604;&#28287;&#24230;&#21644;&#20301;&#21183;&#39640;&#24230;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#21644;&#24322;&#24120;&#30456;&#20851;&#31995;&#25968;(ACC)&#35780;&#20272;&#20102;FuXi&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;ECMWF HRES&#30456;&#27604;&#65292;FuXi&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#31215;&#32047;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12802</link><description>&lt;p&gt;
Otter-Knowledge&#65306;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#21033;&#29992;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#25110;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#33647;&#29289;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#22914;&#39044;&#27979;&#33647;&#29289;&#21644;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#27169;&#24577;&#30340;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#24207;&#21015;&#25110;SMILES&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;7&#20010;&#20844;&#20849;&#26469;&#28304;&#30340;&#39044;&#22788;&#29702;&#21644;&#25972;&#21512;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;30M&#20010;&#19977;&#20803;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;Therapeutic Data Commons (TDC)&#22522;&#20934;&#27979;&#35797;&#20013;&#24615;&#33021;&#25253;&#21578;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>PDCA&#26159;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;Lagrangian&#20989;&#25968;&#19978;&#36816;&#34892;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26469;&#25214;&#21040;&#36817;&#20284;&#38797;&#28857;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#24615;&#21644;&#24378;Bellman&#23436;&#22791;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2306.07818</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning. (arXiv:2306.07818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07818
&lt;/p&gt;
&lt;p&gt;
PDCA&#26159;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;Lagrangian&#20989;&#25968;&#19978;&#36816;&#34892;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26469;&#25214;&#21040;&#36817;&#20284;&#38797;&#28857;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#24615;&#21644;&#24378;Bellman&#23436;&#22791;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#65292;&#20197;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#28385;&#36275;&#23545;&#25104;&#26412;&#20989;&#25968;&#26399;&#26395;&#20540;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#22987;-&#23545;&#20598;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PDCA&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#12290;PDCA&#22312;&#35780;&#35770;&#23478;&#20272;&#35745;&#30340;Lagrangian&#20989;&#25968;&#19978;&#36816;&#34892;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#21407;&#22987;&#29609;&#23478;&#37319;&#29992;&#26080;&#24724;&#31574;&#30053;&#20248;&#21270;&#31070;&#35861;&#65292;&#22312;&#32473;&#23450;&#20219;&#20309;&#35780;&#35770;&#23478;&#21644;&#23545;&#20598;&#29609;&#23478;&#30340;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;&#23545;&#20598;&#29609;&#23478;&#36890;&#36807;&#37319;&#29992;&#26080;&#24724;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#31070;&#35861;&#65292;&#22312;&#32473;&#23450;&#35780;&#35770;&#23478;&#21644;&#21407;&#22987;&#29609;&#23478;&#30340;&#20219;&#20309;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PDCA&#21487;&#20197;&#25104;&#21151;&#22320;&#25214;&#21040;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#36817;&#20284;&#38797;&#28857;&#65292;&#36825;&#23545;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#19982;&#20197;&#21069;&#38656;&#35201;&#38598;&#20013;&#24615;&#21644;&#24378;Bellman&#23436;&#22791;&#24615;&#20551;&#35774;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;PDCA&#21482;&#38656;&#35201;&#19968;&#33268;&#24615;&#21644;&#33258;&#38381;&#24615;&#36825;&#20004;&#20010;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected value of cost functions using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player. We show that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and strong Bellman completeness assumptions, PDCA only requires co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#12289;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#36890;&#36807;&#32771;&#34385;&#24178;&#20928;&#22270;&#20687;&#21644;&#21463;&#25200;&#21160;&#22270;&#20687;&#30340;&#29305;&#24449;&#12289;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#25628;&#32034;&#40065;&#26834;NAS&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.05031</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#21487;&#25512;&#24191;&#36731;&#37327;&#32423;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations. (arXiv:2306.05031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#12289;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#36890;&#36807;&#32771;&#34385;&#24178;&#20928;&#22270;&#20687;&#21644;&#21463;&#25200;&#21160;&#22270;&#20687;&#30340;&#29305;&#24449;&#12289;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#25628;&#32034;&#40065;&#26834;NAS&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#24050;&#25104;&#21151;&#22312;&#32473;&#23450;&#26465;&#20214;&#19979;&#65288;&#20363;&#22914;&#24615;&#33021;&#25110;&#24310;&#36831;&#65289;&#25214;&#21040;&#26368;&#20339;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21482;&#38024;&#23545;&#24178;&#20928;&#22270;&#20687;&#30340;&#24615;&#33021;&#23547;&#25214;&#26368;&#20339;&#26550;&#26500;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25200;&#21160;&#25110;&#25439;&#22351;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#20010;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35757;&#32451;&#38598;&#25104;&#21040;&#21333;&#27425;NAS&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;NAS&#26694;&#26550;&#65292;&#20294;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#21482;&#32771;&#34385;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#20026;&#21333;&#20010;&#20219;&#21153;&#21457;&#29616;&#26368;&#20339;&#26550;&#26500;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#40065;&#26834;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#22312;&#21021;&#22987;&#21270;&#29366;&#24577;&#19979;&#32771;&#34385;&#24178;&#20928;&#22270;&#20687;&#21644;&#21463;&#25200;&#21160;&#22270;&#20687;&#30340;&#29305;&#24449;&#12289;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#24555;&#36895;&#12289;&#39640;&#25928;&#22320;&#25628;&#32034;&#40065;&#26834;NAS&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural architecture search (NAS) frameworks have been successful in finding optimal architectures for given conditions (e.g., performance or latency). However, they search for optimal architectures in terms of their performance on clean images only, while robustness against various types of perturbations or corruptions is crucial in practice. Although there exist several robust NAS frameworks that tackle this issue by integrating adversarial training into one-shot NAS, however, they are limited in that they only consider robustness against adversarial attacks and require significant computational resources to discover optimal architectures for a single task, which makes them impractical in real-world scenarios. To address these challenges, we propose a novel lightweight robust zero-cost proxy that considers the consistency across features, parameters, and gradients of both clean and perturbed images at the initialization state. Our approach facilitates an efficient and rapid sea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#20449;&#21387;&#32553;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21387;&#32553;&#38477;&#20302;&#24635;&#36890;&#20449;&#25104;&#26412;&#30340;&#26465;&#20214;&#21644;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16297</link><description>&lt;p&gt;
&#26080;&#20559;&#21387;&#32553;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#33410;&#30465;&#36890;&#20449;&#65306;&#20309;&#26102;&#20197;&#21450;&#22810;&#23569;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?. (arXiv:2305.16297v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#20449;&#21387;&#32553;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21387;&#32553;&#38477;&#20302;&#24635;&#36890;&#20449;&#25104;&#26412;&#30340;&#26465;&#20214;&#21644;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#26159;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#20256;&#36755;&#21387;&#32553;&#26799;&#24230;&#21644;&#27169;&#22411;&#21442;&#25968;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#21387;&#32553;&#20250;&#24341;&#20837;&#20449;&#24687;&#22833;&#30495;&#65292;&#23548;&#33268;&#25910;&#25947;&#20943;&#24930;&#24182;&#22686;&#21152;&#36890;&#20449;&#36718;&#27425;&#20197;&#36798;&#21040;&#26399;&#26395;&#35299;&#12290;&#37492;&#20110;&#27599;&#36718;&#36890;&#20449;&#25104;&#26412;&#30340;&#38477;&#20302;&#21644;&#39069;&#22806;&#30340;&#36890;&#20449;&#36718;&#27425;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23578;&#19981;&#28165;&#26970;&#36890;&#20449;&#21387;&#32553;&#26159;&#21542;&#38477;&#20302;&#20102;&#24635;&#36890;&#20449;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#20559;&#21387;&#32553;&#65292;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21387;&#32553;&#24418;&#24335;&#65292;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#21487;&#20197;&#38477;&#20302;&#24635;&#36890;&#20449;&#25104;&#26412;&#65292;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#38477;&#20302;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#34920;&#24449;&#20855;&#26377;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#24635;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20165;&#20165;&#20351;&#29992;&#26080;&#20559;&#21387;&#32553;&#24182;&#19981;&#33021;&#19968;&#23450;&#33410;&#30465;&#24635;&#36890;&#20449;&#25104;&#26412;&#65292;&#20294;&#36825;&#31181;&#32467;&#26524;&#26377;
&lt;/p&gt;
&lt;p&gt;
Communication compression is a common technique in distributed optimization that can alleviate communication overhead by transmitting compressed gradients and model parameters. However, compression can introduce information distortion, which slows down convergence and incurs more communication rounds to achieve desired solutions. Given the trade-off between lower per-round communication costs and additional rounds of communication, it is unclear whether communication compression reduces the total communication cost.  This paper explores the conditions under which unbiased compression, a widely used form of compression, can reduce the total communication cost, as well as the extent to which it can do so. To this end, we present the first theoretical formulation for characterizing the total communication cost in distributed optimization with communication compression. We demonstrate that unbiased compression alone does not necessarily save the total communication cost, but this outcome c
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.15425</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15425
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#20026;&#27492;&#36827;&#34892;&#36807;&#35757;&#32451;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#36755;&#20986;&#36136;&#37327;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20998;&#35789;&#38454;&#27573;&#20986;&#29616;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#22788;&#29702;&#24046;&#24322;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#34987;&#35843;&#29992;&#20043;&#21069;&#23601;&#24050;&#32463;&#20986;&#29616;&#20102;&#12290;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#20197;&#26377;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#24046;&#24322;&#21487;&#39640;&#36798;15&#20493;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;17&#31181;&#20998;&#35789;&#22120;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#20351;&#23427;&#20204;&#26159;&#26377;&#24847;&#20026;&#22810;&#35821;&#35328;&#25903;&#25345;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26576;&#20123;&#35821;&#35328;&#23545;&#30340;&#23383;&#31526;&#32423;&#21644;&#23383;&#33410;&#32423;&#27169;&#22411;&#20063;&#26174;&#31034;&#20986;4&#20493;&#20197;&#19978;&#30340;&#32534;&#30721;&#38271;&#24230;&#24046;&#24322;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have shown impressive multilingual performance, even when not explicitly trained for it. Despite this, concerns have been raised about the quality of their outputs across different languages. In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked. The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases. These disparities persist across the 17 tokenizers we evaluate, even if they are intentionally trained for multilingual support. Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs. This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the m
&lt;/p&gt;</description></item><item><title>SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15255</link><description>&lt;p&gt;
&#24102;&#26377;&#35821;&#38899;&#30340;LM&#65306;&#36229;&#36234;&#35821;&#38899;&#20196;&#29260;&#30340;&#21475;&#35821;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15255
&lt;/p&gt;
&lt;p&gt;
SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SPECTRON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#25191;&#34892;&#35821;&#38899;&#24310;&#32493;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#25972;&#20010;&#31995;&#32479;&#37117;&#22312;&#39057;&#35889;&#22270;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#39057;&#35889;&#22270;&#39046;&#22495;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20351;&#29992;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#30340;&#29616;&#26377;&#32423;&#32852;&#26041;&#27861;&#31616;&#21270;&#20102;&#25105;&#20204;&#30340;&#35821;&#38899;&#24310;&#32493;&#31995;&#32479;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32593;&#31449;https://michelleramanovich.github.io/spectron/spectron&#19978;&#21487;&#20197;&#25214;&#21040;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26426;&#21046;&#35299;&#37322;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#20256;&#36755;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452;MLP&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.15054</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis. (arXiv:2305.15054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26426;&#21046;&#35299;&#37322;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#20256;&#36755;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452;MLP&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#21644;&#23384;&#20648;&#19982;&#31639;&#26415;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#29702;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#21152;&#28145;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#36825;&#19968;&#26041;&#38754;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#38382;&#39064;&#19978;&#36827;&#34892;&#26426;&#21046;&#35299;&#37322;&#12290;&#36890;&#36807;&#23545;&#29305;&#23450;&#27169;&#22411;&#32452;&#20214;&#30340;&#28608;&#27963;&#36827;&#34892;&#24178;&#39044;&#24182;&#27979;&#37327;&#39044;&#27979;&#27010;&#29575;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#29305;&#23450;&#39044;&#27979;&#25152;&#36127;&#36131;&#30340;&#21442;&#25968;&#23376;&#38598;&#12290;&#36825;&#20026;&#25105;&#20204;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#19982;&#31639;&#26415;&#30456;&#20851;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#23558;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#20174;&#20013;&#38388;&#23618;&#20256;&#36755;&#21040;&#26368;&#32456;&#30340;&#20196;&#29260;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#32452;MLP&#27169;&#22359;&#22788;&#29702;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Newton-Cotes&#20844;&#24335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#36739;&#65292; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14642</link><description>&lt;p&gt;
Newton-Cotes&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#35770;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems. (arXiv:2305.14642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Newton-Cotes&#20844;&#24335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#36739;&#65292; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31995;&#32479;&#21160;&#24577;&#24615;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#20013;&#26368;&#37325;&#35201;&#30340;&#20998;&#26512;&#26041;&#27861;&#20043;&#19968;&#12290;&#20351;&#29992;&#31995;&#32479;&#30340;&#21021;&#22987;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#65292;&#26368;&#36817;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#36828;&#31163;&#21021;&#22987;&#29366;&#24577;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#24314;&#27169;&#31995;&#32479;&#30340;&#22352;&#26631;&#21644;&#30456;&#20114;&#20316;&#29992;&#21147;&#26041;&#38754;&#26377;&#19981;&#21516;&#30340;&#35774;&#35745;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23454;&#38469;&#19978;&#20849;&#20139;&#19968;&#31181;&#24120;&#25968;&#30340;&#31215;&#20998;&#23398;&#20064;&#33539;&#20363;&#65292; &#33021;&#22815;&#22312;&#21021;&#22987;&#21644;&#32456;&#31471;&#22352;&#26631;&#20043;&#38388;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#39044;&#27979;&#36895;&#24230;&#30340;&#31215;&#20998;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;Newton-Cotes&#20844;&#24335;&#20272;&#31639;&#30340;&#33509;&#24178;&#36895;&#24230;&#20272;&#35745;&#26469;&#39044;&#27979;&#31215;&#20998;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290; &#22312;&#20960;&#20010;&#22522;&#20934;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#25345;&#32493;&#19988;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton-Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14600</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#30456;&#20132;&#26631;&#31614;&#38598;&#20043;&#38388;&#30340;&#20860;&#23481;&#32467;&#26500;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#20855;&#20307;&#22320;&#65292;&#26631;&#35760;&#20855;&#26377;&#20004;&#20010;&#35282;&#33394;&#24207;&#21015;&#30340;&#21477;&#23376;&#65306;VerbNet&#21442;&#25968;&#21644;PropBank&#21442;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#36328;&#20219;&#21153;&#20132;&#20114;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#20173;&#28982;&#26159;&#20998;&#21035;&#35299;&#30721;&#30340;&#65292;&#23384;&#22312;&#29983;&#25104;&#32467;&#26500;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#24207;&#21015; (&#22312;&#20687;SEMLINK&#30340;&#35789;&#20856;&#20013;)&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#32622;&#65292;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#12290;&#36890;&#36807;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;SEMLINK&#32422;&#26463;&#19981;&#26029;&#25552;&#39640;&#24635;F1&#20540;&#12290;&#36890;&#36807;&#29305;&#27530;&#30340;&#36755;&#20837;&#26500;&#36896;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24615;&#20174;PropBank&#21442;&#25968;&#20013;&#25512;&#26029;&#20986;VerbNet&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11707</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#20250;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#20219;&#20309;&#36755;&#20837;&#65292;&#23384;&#22312;&#22810;&#20010;&#21487;&#34892;&#30340;&#20132;&#38469;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#22810;&#31181;&#26041;&#24335;&#23558;&#20219;&#20309;&#30446;&#26631;&#29992;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#25110;&#36827;&#34892;&#29983;&#20135;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#20154;&#31867;&#29983;&#20135;&#22312;&#22235;&#20010;NLG&#20219;&#21153;&#20013;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21464;&#24322;&#31243;&#24230;&#65292;&#24182;&#23558;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#31995;&#32479;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;&#35299;&#30721;&#31639;&#27861;&#25152;&#24418;&#25104;&#30340;&#36755;&#20986;&#23383;&#31526;&#20018;&#31354;&#38388;&#65292;&#20197;&#25506;&#31350;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#29983;&#25104;&#22120;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;NLG&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#25506;&#27979;&#65292;&#25552;&#20379;&#20102;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#25152;&#24517;&#38656;&#30340;&#35814;&#32454;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
&lt;/p&gt;</description></item><item><title>Tinto&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#25968;&#23383;&#38706;&#22836;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#39564;&#35777;&#22320;&#36136;&#21046;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#30340;3D&#25968;&#25454;&#22914;&#28857;&#20113;&#12290;</title><link>http://arxiv.org/abs/2305.09928</link><description>&lt;p&gt;
Tinto&#65306;&#22320;&#29699;&#31185;&#23398;&#20013;3D&#39640;&#20809;&#35889;&#28857;&#20113;&#20998;&#21106;&#30340;&#22810;&#20256;&#24863;&#22120;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences. (arXiv:2305.09928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09928
&lt;/p&gt;
&lt;p&gt;
Tinto&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#25968;&#23383;&#38706;&#22836;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#39564;&#35777;&#22320;&#36136;&#21046;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#30340;3D&#25968;&#25454;&#22914;&#28857;&#20113;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20351;&#29992;&#20943;&#23569;&#20102;&#22320;&#36136;&#22270;&#30340;&#35299;&#37322;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#20174;&#25968;&#23383;&#38706;&#22836;&#27169;&#22411;&#33258;&#21160;&#27966;&#29983;&#22320;&#36136;&#22270;&#26469;&#29702;&#24819;&#22320;&#20943;&#23569;&#20102;&#35299;&#37322;&#32773;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#36136;&#22270;&#35299;&#37322;&#30340;&#20027;&#35266;&#24615;&#20197;&#21450;&#25910;&#38598;&#23450;&#37327;&#39564;&#35777;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#23545;&#36825;&#20123;&#33258;&#21160;&#21270;&#21046;&#22270;&#26041;&#27861;&#30340;&#20934;&#30830;&#39564;&#35777;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20165;&#38480;&#20110;2D&#22270;&#20687;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#35832;&#22914;&#36229;&#32423;&#20113;&#36825;&#26679;&#30340;3D&#25968;&#23383;&#38706;&#22836;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tinto&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#25968;&#23383;&#38706;&#22836;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#39564;&#35777;&#22320;&#36136;&#21046;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#30340;3D&#25968;&#25454;&#22914;&#28857;&#20113;&#12290;Tinto&#21253;&#25324;&#20004;&#20010;&#20114;&#34917;&#30340;&#25968;&#25454;&#38598;&#65306;1&#65289;&#26469;&#33258;Corta Atalaya&#65288;&#35199;&#29677;&#29273;&#65289;&#30340;&#30495;&#23454;&#25968;&#23383;&#38706;&#22836;&#27169;&#22411;&#65292;&#20855;&#26377;&#20809;&#35889;&#23646;&#24615;&#21644;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;2&#65289;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#27169;&#25311;&#30495;&#23454;&#25968;&#25454;&#30340;&#21508;&#31181;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of deep learning techniques has reduced interpretation time and, ideally, reduced interpreter bias by automatically deriving geological maps from digital outcrop models. However, accurate validation of these automated mapping approaches is a significant challenge due to the subjective nature of geological mapping and the difficulty in collecting quantitative validation data. Additionally, many state-of-the-art deep learning methods are limited to 2D image data, which is insufficient for 3D digital outcrops, such as hyperclouds. To address these challenges, we present Tinto, a multi-sensor benchmark digital outcrop dataset designed to facilitate the development and validation of deep learning approaches for geological mapping, especially for non-structured 3D data like point clouds. Tinto comprises two complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain), with spectral attributes and ground-truth data, and 2) a synthetic twin that uses latent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06677</link><description>&lt;p&gt;
INGENIOUS&#65306;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#30528;&#29305;&#28857;&#26159;&#22312;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#26032;&#33021;&#21147;&#26041;&#38754;&#38543;&#30528;&#27169;&#22411;&#23481;&#37327;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780; achieved. &#28982;&#32780;&#65292;&#24517;&#39035;&#35748;&#35782;&#21040;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12289;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#23475;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20165;&#20351;&#29992;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451; PTLM&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#20854;&#19979;&#28216;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20083;&#33146;&#31579;&#26597; X &#20809;&#29255;&#24322;&#24120;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#19982;&#20154;&#21475;&#23398;&#21644;&#25104;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26088;&#22312;&#24320;&#21457;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.04422</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#31579;&#26597;&#20083;&#33146; X &#20809;&#29255;&#30340;&#24615;&#33021;&#24046;&#36317; -- &#36808;&#21521;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20083;&#33146;&#31579;&#26597; X &#20809;&#29255;&#24322;&#24120;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#19982;&#20154;&#21475;&#23398;&#21644;&#25104;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26088;&#22312;&#24320;&#21457;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#31579;&#26597;&#20083;&#33146; X &#20809;&#29255;&#30340;&#24322;&#24120;&#20998;&#31867;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19982;&#22686;&#21152;&#24322;&#24120;&#20998;&#31867;&#22833;&#36133;&#39118;&#38505;&#30456;&#20851;&#30340;&#20154;&#21475;&#23398;&#21644;&#25104;&#20687;&#29305;&#24449;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#20351;&#29992; Emory BrEast Imaging Dataset&#65288;EMBED&#65289;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;2013&#24180;&#33267;2020&#24180;&#38388; Emory University Healthcare &#30340; 115,931 &#21517;&#24739;&#32773;&#30340;&#20083;&#33146; X &#20809;&#29255;&#12290;&#20020;&#24202;&#21644;&#25104;&#20687;&#25968;&#25454;&#21253;&#25324;&#20083;&#33146;&#25104;&#20687;&#25253;&#21578;&#21644;&#25968;&#25454;&#31995;&#32479;&#65288;BI-RADS&#65289;&#35780;&#20272;&#65292;&#24322;&#24120;&#21306;&#22495;&#30340;&#20852;&#36259;&#28857;&#22352;&#26631;&#65292;&#25104;&#20687;&#29305;&#24449;&#65292;&#30149;&#29702;&#32467;&#26524;&#21644;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#12290;&#24320;&#21457;&#20102; InceptionV3&#12289;VGG16&#12289;ResNet50V2 &#21644; ResNet152V2 &#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#31579;&#26597;&#20083;&#33146; X &#20809;&#29255;&#20013;&#24322;&#24120;&#32452;&#32455;&#21306;&#22495;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#27491;&#24120;&#32452;&#32455;&#21306;&#22495;&#12290;&#35757;&#32451;&#38598;&#12289;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#20026; 29,144&#65288;55.6%&#65289;&#20010;&#24322;&#24120;&#32452;&#32455;&#21306;&#22495;&#21644;&#26469;&#33258; 10,678&#65288;54.2%&#65289;&#20301;&#24739;&#32773;&#30340;&#33192;&#32960;&#32452;&#32455;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though deep learning models for abnormality classification can perform well in screening mammography, the demographic and imaging characteristics associated with increased risk of failure for abnormality classification in screening mammograms remain unclear. This retrospective study used data from the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931 patients imaged at Emory University Healthcare between 2013 to 2020. Clinical and imaging data includes Breast Imaging Reporting and Data System (BI-RADS) assessment, region of interest coordinates for abnormalities, imaging features, pathologic outcomes, and patient demographics. Deep learning models including InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish between patches of abnormal tissue and randomly selected patches of normal tissue from the screening mammograms. The distributions of the training, validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients, 9,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03686</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#36817;&#20284;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20027;&#35201;&#20851;&#27880;&#23616;&#37096;&#40065;&#26834;&#24615;&#65292;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#30693;&#36947;&#32473;&#23450;&#23646;&#24615;&#26159;&#21542;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#20869;&#20840;&#23616;&#25104;&#31435;&#65292;&#22914;&#26524;&#19981;&#25104;&#31435;&#65292;&#21017;&#38656;&#35201;&#30693;&#36947;&#23646;&#24615;&#25104;&#31435;&#30340;&#36755;&#20837;&#27604;&#20363;&#26159;&#22810;&#23569;&#12290;&#23613;&#31649;&#31934;&#30830;&#30340;&#21407;&#20687;&#29983;&#25104;&#21487;&#20197;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#20215;&#34920;&#31034;&#65292;&#20294;&#22312;&#35268;&#27169;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21306;&#22495;&#21010;&#20998;&#20026;&#23376;&#21306;&#22495;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#26494;&#24347;&#36793;&#30028;&#21464;&#24471;&#26356;&#32039;&#65292;&#36845;&#20195;&#22320;&#26368;&#23567;&#21270;&#20307;&#31215;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#37319;&#26679;&#21644;&#21487;&#24494;&#20307;&#31215;&#36924;&#36817;&#26469;&#20248;&#20808;&#21010;&#20998;&#21306;&#22495;&#65292;&#24182;&#20248;&#21270;&#26494;&#24347;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03626</link><description>&lt;p&gt;
&#40065;&#26834;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340;&#21487;&#39564;&#35777;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#65292;&#23545;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159; NP-hard &#65292;&#22240;&#27492;&#23545;&#20110;&#29305;&#23450;&#30340;&#36755;&#20837;&#26469;&#35828;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#21463;&#38480;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#31216;&#20026; large-spread &#38598;&#25104;&#65292;&#20854;&#20801;&#35768;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#23433;&#20840;&#39564;&#35777;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39564;&#35777;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20513;&#23548;&#35757;&#32451;&#36825;&#31181;&#26131;&#20110;&#39564;&#35777;&#30340;&#21463;&#38480;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064; large-spread &#20915;&#31574;&#26641;&#38598;&#25104;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#30410;&#22788;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#12290;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#35757;&#32451;&#30340; large-spread &#38598;&#25104;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20351;&#29992;&#26631;&#20934;&#21322;&#23450;&#32534;&#31243;&#27714;&#35299;&#22120;&#36827;&#34892;&#39564;&#35777;&#65292;&#21516;&#26102;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03215</link><description>&lt;p&gt;
&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching. (arXiv:2304.03215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#21578;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#20247;&#22810;&#39046;&#22495;&#65292;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#23427;&#28041;&#21450;&#20351;&#29992;&#24207;&#21015;&#26085;&#24535;&#26469;&#35782;&#21035;&#21644;&#38142;&#25509;&#23646;&#20110;&#21516;&#19968;&#20154;&#30340;&#19981;&#21516;&#35774;&#22791;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#38590;&#20197;&#35299;&#20915;&#26085;&#24535;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#21644;&#39640;&#38454;&#36830;&#25509;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#22270;&#19978;&#19979;&#25991;&#23884;&#20837;(TGCE)&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;HGNN&#65289;&#65292;&#23427;&#20855;&#26377;&#27604;TGCE&#26356;&#20026;&#35745;&#31639;&#25928;&#29575;&#30340;&#20108;&#32423;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;Cross-Att&#65289;&#26426;&#21046;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21453;&#26790;&#24187;&#26426;&#30340;&#38450;&#24481;&#31995;&#32479;&#65292;&#36890;&#36807;&#21521;&#29992;&#25143;&#22270;&#20687;&#28155;&#21152;&#24494;&#23567;&#30340;&#22122;&#22768;&#25200;&#21160;&#26469;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#24694;&#24847;&#20351;&#29992;&#65292;&#20174;&#32780;&#36991;&#20813;&#20135;&#29983;&#34394;&#20551;&#26032;&#38395;&#25110;&#38024;&#23545;&#20010;&#20154;&#21463;&#23475;&#32773;&#30340;&#20196;&#20154;&#19981;&#23433;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2303.15433</link><description>&lt;p&gt;
&#25269;&#25239;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#21453;&#26790;&#24187;&#26426;&#65306;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#20266;&#36896;&#22270;&#20687;&#30340;&#20405;&#23475;
&lt;/p&gt;
&lt;p&gt;
Anti-DreamBooth: Protecting users from personalized text-to-image synthesis. (arXiv:2303.15433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21453;&#26790;&#24187;&#26426;&#30340;&#38450;&#24481;&#31995;&#32479;&#65292;&#36890;&#36807;&#21521;&#29992;&#25143;&#22270;&#20687;&#28155;&#21152;&#24494;&#23567;&#30340;&#22122;&#22768;&#25200;&#21160;&#26469;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#24694;&#24847;&#20351;&#29992;&#65292;&#20174;&#32780;&#36991;&#20813;&#20135;&#29983;&#34394;&#20551;&#26032;&#38395;&#25110;&#38024;&#23545;&#20010;&#20154;&#21463;&#23475;&#32773;&#30340;&#20196;&#20154;&#19981;&#23433;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#22330;&#38761;&#21629;&#65292;&#23427;&#20351;&#24471;&#20219;&#20309;&#20154;&#65292;&#29978;&#33267;&#27809;&#26377;&#35774;&#35745;&#25216;&#24039;&#30340;&#20154;&#65292;&#37117;&#33021;&#22815;&#20174;&#31616;&#21333;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#20687;&#26790;&#24187;&#26426;&#36825;&#26679;&#30340;&#24378;&#22823;&#20010;&#24615;&#21270;&#24037;&#20855;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#20154;&#29289;&#30340;&#20960;&#24352;&#21442;&#32771;&#22270;&#29255;&#29983;&#25104;&#35813;&#20154;&#29289;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#24403;&#34987;&#28389;&#29992;&#26102;&#65292;&#36825;&#31181;&#24378;&#22823;&#32780;&#26041;&#20415;&#30340;&#24037;&#20855;&#21487;&#33021;&#20250;&#20135;&#29983;&#34394;&#20551;&#26032;&#38395;&#25110;&#38024;&#23545;&#20219;&#20309;&#20010;&#20154;&#21463;&#23475;&#32773;&#30340;&#20196;&#20154;&#19981;&#23433;&#30340;&#20869;&#23481;&#65292;&#32473;&#31038;&#20250;&#24102;&#26469;&#20005;&#37325;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#21453;&#26790;&#24187;&#26426;&#30340;&#38450;&#24481;&#31995;&#32479;&#65292;&#20197;&#23545;&#25239;&#23545;&#26790;&#24187;&#26426;&#30340;&#24694;&#24847;&#20351;&#29992;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#22312;&#21457;&#24067;&#29992;&#25143;&#22270;&#20687;&#20043;&#21069;&#21521;&#27599;&#20010;&#29992;&#25143;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#23567;&#30340;&#22122;&#22768;&#25200;&#21160;&#65292;&#20197;&#24178;&#25200;&#23545;&#36825;&#20123;&#25200;&#21160;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#20219;&#20309;&#26790;&#24187;&#26426;&#27169;&#22411;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#23545;&#25200;&#21160;&#20248;&#21270;&#30340;&#22810;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#20004;&#20010;&#38754;&#37096;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#29256;&#26412;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;&#23613;&#31649;&#26790;&#24187;&#26426;&#30340;&#20844;&#24335;&#22797;&#26434;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models are nothing but a revolution, allowing anyone, even without design skills, to create realistic images from simple text inputs. With powerful personalization tools like DreamBooth, they can generate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual victim, posing a severe negative social impact. In this paper, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user's image before publishing in order to disrupt the generation quality of any DreamBooth model trained on these perturbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of DreamBooth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CHiLL&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29992;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12343</link><description>&lt;p&gt;
CHiLL: &#38646;-shot&#23450;&#21046;&#21270;&#12289;&#21487;&#35299;&#37322;&#30340;&#21307;&#30103;&#35760;&#24405;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CHiLL&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29992;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHiLL (Crafting High-Level Latents)&#65292;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21270;&#32447;&#24615;&#27169;&#22411;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;CHiLL&#36890;&#36807;&#19987;&#23478;&#21046;&#20316;&#30340;&#26597;&#35810;&#25351;&#23548;LLMs&#29983;&#25104;&#35299;&#37322;&#24615;&#29305;&#24449;&#65292;&#29992;&#20110;&#20174;&#20581;&#24247;&#35760;&#24405;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#21487;&#20197;&#20351;&#21307;&#29983;&#21033;&#29992;&#20182;&#20204;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#20174;&#21407;&#22987;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21463;&#21040;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;MIMIC-III&#21644;MIMIC-CXR&#25968;&#25454;&#21644;&#26631;&#20934;&#30340;&#39044;&#27979;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;30&#22825;&#20877;&#20837;&#38498;&#65289;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#33258;&#21160;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;&#21442;&#32771;&#29305;&#24449;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#27604;&#20351;&#29992;&#8220;&#35789;&#34955;&#8221;&#29305;&#24449;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26435;&#37325;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using "Bag-of-Words" features. We verify that learned feature weig
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#24102;&#26377;&#19987;&#23478;&#24314;&#35758;&#30340;&#22312;&#32447;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36873;&#25321;&#24615;&#37319;&#26679;&#26041;&#26696;&#30340;&#26631;&#31614;&#39640;&#25928;&#39044;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#27604;&#26631;&#20934;&#31243;&#24207;&#23569;&#24471;&#22810;&#30340;&#26631;&#31614;&#65292;&#24182;&#20445;&#25345;&#26368;&#20248;&#26368;&#22351;&#24773;&#20917;&#21518;&#24724;&#20445;&#35777;&#12290;&#23545;&#20110;&#22312;&#26399;&#26395;&#19978;&#26126;&#26174;&#26356;&#22909;&#30340;&#19987;&#23478;&#65292;&#31639;&#27861;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#19982;&#22238;&#21512;&#25968;&#30340;&#24179;&#26041;&#26681;&#25104;&#27604;&#20363;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19982;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#36895;&#29575;&#30456;&#21305;&#37197;&#30340;&#24402;&#19968;&#21270;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2302.08397</link><description>&lt;p&gt;
&#23545;&#20110;&#24102;&#26377;&#19987;&#23478;&#24314;&#35758;&#30340;&#22312;&#32447;&#39044;&#27979;&#65292;&#33258;&#36866;&#24212;&#36873;&#25321;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Selective Sampling for Online Prediction with Experts. (arXiv:2302.08397v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08397
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#19987;&#23478;&#24314;&#35758;&#30340;&#22312;&#32447;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36873;&#25321;&#24615;&#37319;&#26679;&#26041;&#26696;&#30340;&#26631;&#31614;&#39640;&#25928;&#39044;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#27604;&#26631;&#20934;&#31243;&#24207;&#23569;&#24471;&#22810;&#30340;&#26631;&#31614;&#65292;&#24182;&#20445;&#25345;&#26368;&#20248;&#26368;&#22351;&#24773;&#20917;&#21518;&#24724;&#20445;&#35777;&#12290;&#23545;&#20110;&#22312;&#26399;&#26395;&#19978;&#26126;&#26174;&#26356;&#22909;&#30340;&#19987;&#23478;&#65292;&#31639;&#27861;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#19982;&#22238;&#21512;&#25968;&#30340;&#24179;&#26041;&#26681;&#25104;&#27604;&#20363;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19982;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#36895;&#29575;&#30456;&#21305;&#37197;&#30340;&#24402;&#19968;&#21270;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23545;&#20110;&#20108;&#36827;&#21046;&#24207;&#21015;&#30340;&#22312;&#32447;&#39044;&#27979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26631;&#31614;&#39640;&#25928;&#39044;&#27979;&#31639;&#27861;&#20351;&#29992;&#20102;&#36873;&#25321;&#24615;&#37319;&#26679;&#26041;&#26696;&#65292;&#22312;&#20445;&#25345;&#26368;&#20248;&#26368;&#22351;&#24773;&#20917;&#21518;&#24724;&#20445;&#35777;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#20351;&#29992;&#27604;&#26631;&#20934;&#31243;&#24207;&#23569;&#24471;&#22810;&#30340;&#26631;&#31614;&#12290;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#25351;&#25968;&#21152;&#26435;&#39044;&#27979;&#22120;&#65292;&#36866;&#29992;&#20110;&#26377;&#25110;&#26080;&#23436;&#32654;&#19987;&#23478;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#19968;&#20010;&#22312;&#26399;&#26395;&#19978;&#26126;&#26174;&#26356;&#22909;&#30340;&#19987;&#23478;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#31614;&#39640;&#25928;&#39044;&#27979;&#22120;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#22823;&#33268;&#19982;&#22238;&#21512;&#25968;&#30340;&#24179;&#26041;&#26681;&#25104;&#27604;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#26631;&#31614;&#39640;&#25928;&#39044;&#27979;&#22120;&#30340;&#24402;&#19968;&#21270;&#21518;&#24724;&#21487;&#20197;&#28176;&#36817;&#21305;&#37197;&#24050;&#30693;&#30340;&#22522;&#20110;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#36895;&#29575;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#22312;&#33391;&#24615;&#29615;&#22659;&#20013;&#36827;&#34892;&#26368;&#20248;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online prediction of a binary sequence with expert advice. For this setting, we devise label-efficient forecasting algorithms, which use a selective sampling scheme that enables collecting much fewer labels than standard procedures, while still retaining optimal worst-case regret guarantees. These algorithms are based on exponentially weighted forecasters, suitable for settings with and without a perfect expert. For a scenario where one expert is strictly better than the others in expectation, we show that the label complexity of the label-efficient forecaster scales roughly as the square root of the number of rounds. Finally, we present numerical experiments empirically showing that the normalized regret of the label-efficient forecaster can asymptotically match known minimax rates for pool-based active learning, suggesting it can optimally adapt to benign settings.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06578</link><description>&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Kernel Ridge Regression Inference. (arXiv:2302.06578v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;(KRR)&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#25490;&#21517;&#12289;&#22270;&#20687;&#21644;&#22270;&#34920;&#22312;&#20869;&#30340;&#19968;&#33324;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#25968;&#25454;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#22914;&#23398;&#26657;&#20998;&#37197;&#20013;&#30340;&#25490;&#24207;&#20248;&#20808;&#32423;&#21015;&#34920;&#65292;&#20294;KRR&#30340;&#25512;&#26029;&#29702;&#35770;&#23578;&#26410;&#23436;&#20840;&#30693;&#24713;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#32463;&#27982;&#23398;&#21644;&#20854;&#20182;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#38024;&#23545;&#19968;&#33324;&#22238;&#24402;&#22120;&#30340;&#23574;&#38160;&#12289;&#19968;&#33268;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#20026;&#20102;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#20030;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#31216;&#21270;&#26469;&#28040;&#38500;&#20559;&#24046;&#24182;&#38480;&#21046;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#31243;&#24207;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#37096;&#20998;&#21644;&#30340;&#26377;&#38480;&#26679;&#26412;&#12289;&#22343;&#21248;&#39640;&#26031;&#21644;&#33258;&#20030;&#32806;&#21512;&#12290;&#36825;&#20123;&#25512;&#23548;&#26263;&#31034;&#20102;&#22522;&#20110;RKHS&#21333;&#20301;&#29699;&#30340;&#32463;&#39564;&#36807;&#31243;&#30340;&#24378;&#36924;&#36817;&#65292;&#23545;&#35206;&#30422;&#25968;&#20855;&#26377;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide uniform inference and confidence bands for kernel ridge regression (KRR), a widely-used non-parametric regression estimator for general data types including rankings, images, and graphs. Despite the prevalence of these data -e.g., ranked preference lists in school assignment -- the inferential theory of KRR is not fully known, limiting its role in economics and other scientific domains. We construct sharp, uniform confidence sets for KRR, which shrink at nearly the minimax rate, for general regressors. To conduct inference, we develop an efficient bootstrap procedure that uses symmetrization to cancel bias and limit computational overhead. To justify the procedure, we derive finite-sample, uniform Gaussian and bootstrap couplings for partial sums in a reproducing kernel Hilbert space (RKHS). These imply strong approximation for empirical processes indexed by the RKHS unit ball with logarithmic dependence on the covering number. Simulations verify coverage. We use our proce
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;LSTM&#21644;CFNN&#65289;&#19982;&#26631;&#20934;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#65288;PEM&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11604</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#25209;&#21028;&#24615;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
A critical look at deep neural network for dynamic system modeling. (arXiv:2301.11604v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11604
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;LSTM&#21644;CFNN&#65289;&#19982;&#26631;&#20934;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#65288;PEM&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#39046;&#22495;&#20013;&#30340;&#21160;&#24577;&#24314;&#27169;&#24037;&#20855;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23427;&#20204;&#20855;&#26377;&#38750;&#32447;&#24615;&#32467;&#26500;&#65292;&#33021;&#22815;&#36924;&#36817;&#20219;&#20309;&#20989;&#25968;&#31561;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#21069;&#26223;&#65292;&#20294;&#26412;&#25991;&#23545;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#36827;&#34892;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#30340;&#33021;&#21147;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#23545;&#20110;&#32447;&#24615;&#26102;&#19981;&#21464;&#65288;LTI&#65289;&#21160;&#24577;&#31995;&#32479;&#30340;&#35782;&#21035;&#65292;&#23558;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#32423;&#32852;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;CFNN&#65289;&#65292;&#19982;&#31995;&#32479;&#36776;&#35782;&#30340;&#26631;&#20934;&#39044;&#27979;&#35823;&#24046;&#27861;&#65288;PEM&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#27604;&#36739;&#20013;&#65292;&#32771;&#34385;&#20102;&#31995;&#32479;&#36776;&#35782;&#30340;&#22235;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#28982;&#21518;&#25351;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24314;&#27169;&#21487;&#33021;&#23384;&#22312;&#30340;&#19968;&#20123;&#32570;&#38519;&#21644;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20223;&#30495;&#30740;&#31350;&#26469;&#39564;&#35777;&#36825;&#20123;&#32570;&#38519;&#65306;&#23545;&#20110;LTI&#31995;&#32479;&#65292;&#26080;&#35770;&#26159;LSTM&#36824;&#26159;CFNN&#37117;&#26080;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models become increasingly popular as dynamic modeling tools in the control community. They have many appealing features including nonlinear structures, being able to approximate any functions. While most researchers hold optimistic attitudes towards such models, this paper questions the capability of (deep) neural networks for the modeling of dynamic systems using input-output data. For the identification of linear time-invariant (LTI) dynamic systems, two representative neural network models, Long Short-Term Memory (LSTM) and Cascade Foward Neural Network (CFNN) are compared to the standard Prediction Error Method (PEM) of system identification. In the comparison, four essential aspects of system identification are considered, then several possible defects and neglected issues of neural network based modeling are pointed out. Detailed simulation studies are performed to verify these defects: for the LTI system, both LSTM and CFNN fail to deliver consistent models even 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.05294</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#22797;&#26434;&#21644;&#26080;&#20449;&#21495;&#30340;&#20132;&#21449;&#21475;&#20013;&#23398;&#20064;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;
&lt;/p&gt;
&lt;p&gt;
Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections. (arXiv:2301.05294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#21475;&#26159;&#29616;&#20195;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#20107;&#25925;&#25110;&#32570;&#20047;&#20132;&#36890;&#21327;&#35843;&#26426;&#21046;&#65288;&#22914;&#20132;&#36890;&#20449;&#21495;&#28783;&#65289;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#25104;&#20026;&#20132;&#36890;&#27969;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#36229;&#36234;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#20132;&#21449;&#21475;&#20132;&#36890;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#25511;&#21046;&#21487;&#39044;&#35265;&#30340;&#21253;&#21547;&#20154;&#39550;&#39542;&#36710;&#36742;&#65288;HVs&#65289;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#65288;RVs&#65289;&#30340;&#28151;&#21512;&#20132;&#36890;&#24050;&#32463;&#20986;&#29616;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#28151;&#21512;&#20132;&#36890;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#36807;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#20132;&#36890;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;5%&#30340;RVs&#65292;&#25105;&#20204;&#21487;&#20197;&#38450;&#27490;&#22797;&#26434;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersections are essential road infrastructures for traffic in modern metropolises. However, they can also be the bottleneck of traffic flows as a result of traffic incidents or the absence of traffic coordination mechanisms such as traffic lights. Recently, various control and coordination mechanisms that are beyond traditional control methods have been proposed to improve the efficiency of intersection traffic. Amongst these methods, the control of foreseeable mixed traffic that consists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged. In this project, we propose a decentralized multi-agent reinforcement learning approach for the control and coordination of mixed traffic at real-world, complex intersections--a topic that has not been previously explored. Comprehensive experiments are conducted to show the effectiveness of our approach. In particular, we show that using 5% RVs, we can prevent congestion formation inside a complex intersection under the actual traf
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#35268;&#27169;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.05253</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#32422;&#26463;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#21487;&#25193;&#23637;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Scalable Technique for Weak-Supervised Learning with Domain Constraints. (arXiv:2301.05253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05253
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#35268;&#27169;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#27969;&#27700;&#32447;&#25216;&#26415;&#65292;&#21033;&#29992;&#31526;&#21495;&#39046;&#22495;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#30340;&#32452;&#65288;&#31867;&#65289;&#65292;&#36866;&#23452;&#20110;&#32858;&#31867;&#21451;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#35757;&#32451;&#31034;&#20363;&#19968;&#27425;&#24615;&#20351;&#29992;&#39640;&#25928;&#30340;&#25968;&#23398;&#20248;&#21270;&#25216;&#26415;&#26469;&#37325;&#26032;&#21046;&#23450;&#39046;&#22495;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel scalable end-to-end pipeline that uses symbolic domain knowledge as constraints for learning a neural network for classifying unlabeled data in a weak-supervised manner. Our approach is particularly well-suited for settings where the data consists of distinct groups (classes) that lends itself to clustering-friendly representation learning and the domain constraints can be reformulated for use of efficient mathematical optimization techniques by considering multiple training examples at once. We evaluate our approach on a variant of the MNIST image classification problem where a training example consists of image sequences and the sum of the numbers represented by the sequences, and show that our approach scales significantly better than previous approaches that rely on computing all constraint satisfying combinations for each training example.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#36830;&#32493;&#30340;&#36827;&#23637;&#27979;&#37327;&#65292;&#20197;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013; emergent behavior &#30340;&#20135;&#29983;&#21407;&#22240;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#20197;&#8220;grokking&#8221;&#29616;&#35937;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#24335;&#23436;&#20840;&#29702;&#35299;&#20102;&#23567;&#22411;transformer&#32593;&#32476;&#22312;&#27169;&#22359;&#21270;&#21152;&#27861;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#36827;&#23637;&#27979;&#37327;&#26469;&#30740;&#31350;&#36825;&#20010;&#29616;&#35937;&#30340;&#36827;&#23637;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2301.05217</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#30340;&#36827;&#23637;&#27979;&#37327;&#26469;&#29702;&#35299;"grokking"
&lt;/p&gt;
&lt;p&gt;
Progress measures for grokking via mechanistic interpretability. (arXiv:2301.05217v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05217
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#36830;&#32493;&#30340;&#36827;&#23637;&#27979;&#37327;&#65292;&#20197;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013; emergent behavior &#30340;&#20135;&#29983;&#21407;&#22240;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#20197;&#8220;grokking&#8221;&#29616;&#35937;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#24335;&#23436;&#20840;&#29702;&#35299;&#20102;&#23567;&#22411;transformer&#32593;&#32476;&#22312;&#27169;&#22359;&#21270;&#21152;&#27861;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#36827;&#23637;&#27979;&#37327;&#26469;&#30740;&#31350;&#36825;&#20010;&#29616;&#35937;&#30340;&#36827;&#23637;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#34920;&#29616;&#20986;&#26032;&#20852;&#34892;&#20026;&#65292;&#20063;&#23601;&#26159;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35757;&#32451;&#27493;&#39588;&#26469;&#20135;&#29983;&#36136;&#30340;&#21464;&#21270;&#12290;&#29702;&#35299;&#26032;&#20852;&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#25214;&#21040;&#36830;&#32493;&#30340;&#8220;&#36827;&#23637;&#27979;&#37327;&#8221;&#65292;&#36825;&#20123;&#27979;&#37327;&#26159;&#30475;&#20284;&#19981;&#36830;&#32493;&#30340;&#36136;&#30340;&#21464;&#21270;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#26469;&#25214;&#21040;&#36827;&#23637;&#27979;&#37327;&#65306;&#23558;&#23398;&#21040;&#30340;&#34892;&#20026;&#36870;&#21521;&#24037;&#31243;&#25104;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23567;&#22411;transformer&#35757;&#32451;&#27169;&#22359;&#21270;&#21152;&#27861;&#20219;&#21153;&#26102;&#20986;&#29616;&#30340;&#8220;grokking&#8221;&#29616;&#35937;&#12290;&#25105;&#20204;&#23436;&#20840;&#36870;&#21521;&#24037;&#31243;&#20102;&#36825;&#20123;&#32593;&#32476;&#23398;&#21040;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#19977;&#35282;&#24658;&#31561;&#24335;&#23558;&#21152;&#27861;&#36716;&#25442;&#20026;&#22278;&#21608;&#26059;&#36716;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#21644;&#26435;&#37325;&#65292;&#24182;&#22312;&#20613;&#37324;&#21494;&#31354;&#38388;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20010;&#31639;&#27861;&#12290;&#22522;&#20110;&#36825;&#31181;&#29702;&#35299;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36827;&#23637;&#27979;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#36825;&#20010;&#29616;&#35937;&#30340;&#36827;&#23637;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SHARE&#65292;&#19968;&#31181;&#32771;&#34385;&#20849;&#20139;&#26631;&#31614;&#32467;&#26500;&#30340;HAR&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#20849;&#21516;&#30340;&#32467;&#26500;&#20174;&#32780;&#25581;&#31034;&#19981;&#21516;&#27963;&#21160;&#20043;&#38388;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2301.03462</link><description>&lt;p&gt;
&#37322;&#25918;&#20849;&#20139;&#26631;&#31614;&#32467;&#26500;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Shared Label Structures for Human Activity Recognition. (arXiv:2301.03462v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SHARE&#65292;&#19968;&#31181;&#32771;&#34385;&#20849;&#20139;&#26631;&#31614;&#32467;&#26500;&#30340;HAR&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#20849;&#21516;&#30340;&#32467;&#26500;&#20174;&#32780;&#25581;&#31034;&#19981;&#21516;&#27963;&#21160;&#20043;&#38388;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#25216;&#26415;&#23558;&#27963;&#21160;&#26631;&#31614;&#35270;&#20026;&#25972;&#25968;&#31867;&#21035;ID&#65292;&#27809;&#26377;&#26126;&#30830;&#22320;&#23545;&#31867;&#21035;&#26631;&#31614;&#30340;&#35821;&#20041;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#27963;&#21160;&#21517;&#31216;&#36890;&#24120;&#20855;&#26377;&#20849;&#20139;&#30340;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#8220;&#25171;&#24320;&#38376;&#8221;&#21644;&#8220;&#25171;&#24320;&#20912;&#31665;&#8221;&#37117;&#26377;&#8220;&#25171;&#24320;&#8221;&#20316;&#20026;&#21160;&#20316;&#65307;&#8220;&#36386;&#36275;&#29699;&#8221;&#21644;&#8220;&#25171;&#32593;&#29699;&#8221;&#37117;&#26377;&#8220;&#29699;&#8221;&#20316;&#20026;&#29289;&#20307;&#12290;&#26631;&#31614;&#21517;&#31216;&#20013;&#30340;&#36825;&#31181;&#20849;&#20139;&#32467;&#26500;&#21487;&#20197;&#36716;&#21270;&#20026;&#24863;&#30693;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#24314;&#27169;&#20849;&#21516;&#30340;&#32467;&#26500;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;&#19981;&#21516;&#27963;&#21160;&#20043;&#38388;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26679;&#26412;&#26377;&#38480;&#30340;&#27963;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SHARE&#65292;&#19968;&#31181;&#32771;&#34385;&#19981;&#21516;&#27963;&#21160;&#30340;&#26631;&#31614;&#21517;&#31216;&#20849;&#20139;&#32467;&#26500;&#30340;HAR&#26694;&#26550;&#12290;&#20026;&#20102;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#65292;SHARE&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#20174;&#36755;&#20837;&#30340;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#26631;&#31614;&#21517;&#31216;&#20316;&#20026;&#20196;&#29260;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#26631;&#31614;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#21152;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current human activity recognition (HAR) techniques regard activity labels as integer class IDs without explicitly modeling the semantics of class labels. We observe that different activity names often have shared structures. For example, "open door" and "open fridge" both have "open" as the action; "kicking soccer ball" and "playing tennis ball" both have "ball" as the object. Such shared structures in label names can be translated to the similarity in sensory data and modeling common structures would help uncover knowledge across different activities, especially for activities with limited samples. In this paper, we propose SHARE, a HAR framework that takes into account shared structures of label names for different activities. To exploit the shared structures, SHARE comprises an encoder for extracting features from input sensory time series and a decoder for generating label names as a token sequence. We also propose three label augmentation techniques to help the model more effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#30340;&#32467;&#26500;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#21457;&#29616;&#36880;&#32500;&#26799;&#24230;&#36890;&#24120;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#65292;&#32780;&#36880;&#27425;&#36845;&#20195;&#30340;&#26799;&#24230;&#21644;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#36890;&#24120;&#19981;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#12290;</title><link>http://arxiv.org/abs/2212.02083</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#34987;&#24573;&#35270;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
On the Overlooked Structure of Stochastic Gradients. (arXiv:2212.02083v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#30340;&#32467;&#26500;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#21457;&#29616;&#36880;&#32500;&#26799;&#24230;&#36890;&#24120;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#65292;&#32780;&#36880;&#27425;&#36845;&#20195;&#30340;&#26799;&#24230;&#21644;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#36890;&#24120;&#19981;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#23494;&#20999;&#30456;&#20851;&#12290;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#26799;&#24230;&#22122;&#22768;&#30340;&#37325;&#23614;&#24615;&#36136;&#26469;&#35299;&#37322;&#38543;&#26426;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#21017;&#25552;&#20986;&#20102;&#23545;&#26799;&#24230;&#22122;&#22768;&#30340;&#37325;&#23614;&#20551;&#35774;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#38543;&#26426;&#26799;&#24230;&#32467;&#26500;&#21644;&#37325;&#23614;&#30340;&#27491;&#24335;&#32479;&#35745;&#26816;&#39564;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20570;&#20986;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#38543;&#26426;&#26799;&#24230;&#21644;&#26799;&#24230;&#22122;&#22768;&#22312;&#21442;&#25968;&#21644;&#36845;&#20195;&#20013;&#30340;&#20998;&#24067;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#35745;&#26816;&#39564;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#26816;&#39564;&#21457;&#29616;&#65292;&#36880;&#32500;&#26799;&#24230;&#36890;&#24120;&#34920;&#29616;&#20986;&#24130;&#24459;&#37325;&#23614;&#65292;&#32780;&#36880;&#27425;&#36845;&#20195;&#30340;&#26799;&#24230;&#21644;&#30001;&#23567;&#25209;&#37327;&#35757;&#32451;&#24341;&#36215;&#30340;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#36890;&#24120;&#19981;&#34920;&#29616;&#20986;&#24130;&#24459;&#37325;&#23614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#21327;&#26041;&#24046;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradients closely relate to both optimization and generalization of deep neural networks (DNNs). Some works attempted to explain the success of stochastic optimization for deep learning by the arguably heavy-tail properties of gradient noise, while other works presented theoretical and empirical evidence against the heavy-tail hypothesis on gradient noise. Unfortunately, formal statistical tests for analyzing the structure and heavy tails of stochastic gradients in deep learning are still under-explored. In this paper, we mainly make two contributions. First, we conduct formal statistical tests on the distribution of stochastic gradients and gradient noise across both parameters and iterations. Our statistical tests reveal that dimension-wise gradients usually exhibit power-law heavy tails, while iteration-wise gradients and stochastic gradient noise caused by minibatch training usually do not exhibit power-law heavy tails. Second, we further discover that the covariance spe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#30340;&#26579;&#33394;&#20307;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.14312</link><description>&lt;p&gt;
&#31934;&#20934;&#32959;&#30244;&#23398;&#30340;&#26579;&#33394;&#20307;AI
&lt;/p&gt;
&lt;p&gt;
Karyotype AI for Precision Oncology. (arXiv:2211.14312v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#30340;&#26579;&#33394;&#20307;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#20307;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#36951;&#20256;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#34880;&#28082;&#31995;&#32479;&#24694;&#24615;&#32959;&#30244;&#65292;&#36890;&#36807;&#26579;&#33394;&#20307;&#32452;&#22411;&#20998;&#26512;&#26469;&#21457;&#29616;&#20307;&#32454;&#32990;&#31361;&#21464;&#26159;&#26631;&#20934;&#30340;&#25252;&#29702;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26579;&#33394;&#20307;&#32452;&#22411;&#20998;&#26512;&#22240;&#20026;&#22823;&#37096;&#20998;&#26159;&#25163;&#21160;&#25805;&#20316;&#65292;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#31361;&#21464;&#65292;&#25152;&#20197;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#20197;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#36807;&#21435;&#20116;&#24180;&#30340;&#32422;10,000&#20010;&#24739;&#32773;&#26631;&#26412;&#21644;&#32422;50,000&#20010;&#26579;&#33394;&#20307;&#32452;&#22411;&#22270;&#29255;&#20316;&#20026;&#35757;&#32451;&#38598;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#20195;&#34920;&#21333;&#20010;&#26579;&#33394;&#20307;&#30340;&#26631;&#35760;&#22270;&#29255;&#12290;&#36825;&#20123;&#21333;&#20010;&#26579;&#33394;&#20307;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20998;&#31867;&#20154;&#31867;&#30340;24&#26465;&#26579;&#33394;&#20307;&#21644;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#12290;&#20855;&#26377;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#21644;&#20108;&#32423;&#22359;-&#25176;&#26222;&#21033;&#33576;&#33945;&#29256;&#65292;&#20197;&#34701;&#20837;&#32467;&#26500;&#24615;&#24402;&#32435;&#20559;&#32622;&#12290;TopViT&#30340;&#24615;&#33021;&#20248;&#20110;CNN(Inc)
&lt;/p&gt;
&lt;p&gt;
Chromosome analysis is essential for diagnosing genetic disorders. For hematologic malignancies, identification of somatic clonal aberrations by karyotype analysis remains the standard of care. However, karyotyping is costly and time-consuming because of the largely manual process and the expertise required in identifying and annotating aberrations. Efforts to automate karyotype analysis to date fell short in aberration detection. Using a training set of ~10k patient specimens and ~50k karyograms from over 5 years from the Fred Hutchinson Cancer Center, we created a labeled set of images representing individual chromosomes. These individual chromosomes were used to train and assess deep learning models for classifying the 24 human chromosomes and identifying chromosomal aberrations. The top-accuracy models utilized the recently introduced Topological Vision Transformers (TopViTs) with 2-level-block-Toeplitz masking, to incorporate structural inductive bias. TopViT outperformed CNN (Inc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Attention-FFN Adapter&#65292;&#29992;&#20110;&#26500;&#24314;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30456;&#23545;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#24179;&#22343;&#25552;&#39640;&#20102;0.6%&#33267;2%&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#26377;&#25928;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11363</link><description>&lt;p&gt;
AF Adapter: &#36830;&#32493;&#39044;&#35757;&#32451;&#29992;&#20110;&#26500;&#24314;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model. (arXiv:2211.11363v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11363
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Attention-FFN Adapter&#65292;&#29992;&#20110;&#26500;&#24314;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30456;&#23545;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#24179;&#22343;&#25552;&#39640;&#20102;0.6%&#33267;2%&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#26377;&#25928;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#39044;&#35757;&#32451;&#26159;&#20174;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#29305;&#23450;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23613;&#31649;&#39640;&#25928;&#65292;&#20294;&#36830;&#32493;&#39044;&#35757;&#32451;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BERT-based&#27169;&#22411;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Attention-FFN Adapter&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#27599;&#20010;&#33258;&#27880;&#24847;&#23618;&#21644;&#21069;&#39304;&#32593;&#32476;&#20013;&#24341;&#20837;&#23569;&#37327;&#30340;&#27880;&#24847;&#22836;&#21644;&#38544;&#34255;&#21333;&#20803;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;RoBERTa&#30340;&#38024;&#23545;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#65292;&#21517;&#20026;AF Adapter&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#32422;17%&#65292;AF Adapter&#30456;&#23545;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#20102;0.6%&#33267;2%&#12290;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual pretraining is a popular way of building a domain-specific pretrained language model from a general-domain language model. In spite of its high efficiency, continual pretraining suffers from catastrophic forgetting, which may harm the model's performance in downstream tasks. To alleviate the issue, in this paper, we propose a continual pretraining method for the BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a small number of attention heads and hidden units inside each self-attention layer and feed-forward network. Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain. In experiments, models are applied to downstream tasks for evaluation. The results demonstrate that with only about 17% of model parameters trained, AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong baselines. Further experimental results show that our method alleviates the catastrophic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.17426</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#20998;&#26512;&#23454;&#29616;&#19968;&#33268;&#19988;&#30495;&#23454;&#30340;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#38656;&#35201;&#19982;&#24403;&#21069;&#26696;&#20363;&#30456;&#20851;&#30340;&#20551;&#35774;&#24773;&#26223;&#19968;&#33268;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#22240;&#32032;&#25913;&#21464;&#65292;&#27169;&#22411;&#20250;&#22914;&#20309;&#21453;&#24212;&#65311;&#23613;&#31649;&#24402;&#22240;&#26041;&#27861;&#30001;&#20248;&#38597;&#30340;&#20844;&#29702;&#31995;&#32479;&#25903;&#25345;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20837;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#19968;&#33268;&#12290;&#20026;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#24067;&#23572;&#20989;&#25968;&#30340;&#20613;&#31435;&#21494;&#20998;&#26512;&#26469;&#33719;&#24471;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#21508;&#31181;&#21322;&#24452;&#30340;&#37051;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23454;&#29616;2&#20493;&#33267;50&#20493;&#26356;&#20302;&#30340;&#35299;&#37322;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#33258;&#36866;&#24212;&#36882;&#24402;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;ARMCMC&#65289;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#27169;&#22411;&#35782;&#21035;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#22312;&#32447;&#25216;&#26415;&#30340;&#32570;&#28857;&#65292;&#24182;&#35745;&#31639;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2210.12595</link><description>&lt;p&gt;
&#22312;&#32447;&#33258;&#36866;&#24212;&#36882;&#24402;MCMC&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#27169;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Online Probabilistic Model Identification using Adaptive Recursive MCMC. (arXiv:2210.12595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#33258;&#36866;&#24212;&#36882;&#24402;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;ARMCMC&#65289;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#27169;&#22411;&#35782;&#21035;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#22312;&#32447;&#25216;&#26415;&#30340;&#32570;&#28857;&#65292;&#24182;&#35745;&#31639;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36125;&#21494;&#26031;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#20272;&#35745;&#19981;&#30830;&#23450;&#21442;&#25968;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#20294;&#20854;&#22312;&#32447;&#23454;&#29616;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#36882;&#24402;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;ARMCMC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#20256;&#32479;&#22312;&#32447;&#25216;&#26415;&#30340;&#32570;&#28857;&#65292;&#21516;&#26102;&#35745;&#31639;&#27169;&#22411;&#21442;&#25968;&#30340;&#25972;&#20010;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#39640;&#26031;&#22122;&#22768;&#30340;&#38480;&#21046;&#12289;&#20165;&#36866;&#29992;&#20110;&#32447;&#24615;&#21442;&#25968;&#65288;LIP&#65289;&#31995;&#32479;&#20197;&#21450;&#25345;&#32493;&#28608;&#21169;&#65288;PE&#65289;&#38656;&#27714;&#26159;&#20854;&#20013;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#22312;ARMCMC&#20013;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#38388;&#36951;&#24536;&#22240;&#23376;&#65288;TFF&#65289;&#30340;&#21464;&#37327;&#36339;&#36291;&#20998;&#24067;&#12290;&#36951;&#24536;&#22240;&#23376;&#21487;&#20197;&#22312;&#35768;&#22810;&#21160;&#24577;&#31995;&#32479;&#20013;&#29992;TFF&#33258;&#36866;&#24212;&#22320;&#34920;&#31034;&#65292;&#20316;&#20026;&#24120;&#25968;&#36229;&#21442;&#25968;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#24320;&#21457;&#19982;&#25506;&#32034;&#20043;&#38388;&#25552;&#20379;&#26435;&#34913;&#65292;&#20855;&#20307;&#30340;&#36339;&#36291;&#20998;&#24067;&#24050;&#34987;&#20248;&#21270;&#65292;&#20197;&#20415;&#36866;&#24212;&#20801;&#35768;&#25512;&#26029;&#30340;&#28151;&#21512;/&#22810;&#27169;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the Bayesian paradigm offers a formal framework for estimating the entire probability distribution over uncertain parameters, its online implementation can be challenging due to high computational costs. We suggest the Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) method, which eliminates the shortcomings of conventional online techniques while computing the entire probability density function of model parameters. The limitations to Gaussian noise, the application to only linear in the parameters (LIP) systems, and the persistent excitation (PE) needs are some of these drawbacks. In ARMCMC, a temporal forgetting factor (TFF)-based variable jump distribution is proposed. The forgetting factor can be presented adaptively using the TFF in many dynamical systems as an alternative to a constant hyperparameter. By offering a trade-off between exploitation and exploration, the specific jump distribution has been optimised towards hybrid/multi-modal systems that permit inferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#26410;&#30693;&#21464;&#21270;&#36895;&#29575;&#30340;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ET-GP-UCB&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.10790</link><description>&lt;p&gt;
&#20107;&#20214;&#35302;&#21457;&#30340;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Event-Triggered Time-Varying Bayesian Optimization. (arXiv:2208.10790v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#26410;&#30693;&#21464;&#21270;&#36895;&#29575;&#30340;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ET-GP-UCB&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;TVBO&#65289;&#39034;&#24207;&#20248;&#21270;&#26102;&#21464;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#26102;&#38388;&#21464;&#21270;&#19979;&#30340;&#21208;&#25506;&#19982;&#24320;&#21457;&#30340;&#26435;&#34913;&#12290;&#24403;&#21069;&#30340;TVBO&#26041;&#27861;&#38656;&#35201;&#23545;&#21464;&#21270;&#36895;&#29575;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21464;&#21270;&#36895;&#29575;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#23427;&#23558;&#20248;&#21270;&#38382;&#39064;&#35270;&#20026;&#38745;&#24577;&#38382;&#39064;&#65292;&#30452;&#21040;&#22312;&#32447;&#26816;&#27979;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#21464;&#21270;&#24182;&#37325;&#32622;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#20107;&#20214;&#35302;&#21457;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#20351;&#29992;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;ET-GP-UCB&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;ET-GP-UCB&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Here, the key challenge is the exploration-exploitation trade-off under time variations. Current approaches to TVBO require prior knowledge of a constant rate of change. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function online and then resets the dataset. This allows the algorithm to adapt to realized temporal changes without the need for prior knowledge. The event-trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We provide regret bounds for ET-GP-UCB and show in numerical experiments that it outperforms state-of-the-art algorithms on synthetic and real-world data. Furthermore, these results demonstrate that ET-GP-UCB is readily applicable to various set
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.09920</link><description>&lt;p&gt;
DESCN: &#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#29992;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#31934;&#20934;&#21307;&#23398;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;(ITE)&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20998;&#21035;&#22312;&#21508;&#33258;&#30340;&#26679;&#26412;&#31354;&#38388;&#20013;&#24314;&#27169;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#30340;&#21709;&#24212;&#20989;&#25968;&#26469;&#39044;&#27979;ITE&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#36935;&#21040;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#27835;&#30103;&#20559;&#24046;&#32780;&#23548;&#33268;&#30340;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31163;&#65292;&#20197;&#21450;&#20854;&#20154;&#21475;&#35268;&#27169;&#20043;&#38388;&#30340;&#26174;&#33879;&#26679;&#26412;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;(DESCN)&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#24314;&#27169;&#27835;&#30103;&#25928;&#26524;&#12290;DESCN&#36890;&#36807;&#19968;&#20010;&#36328;&#32593;&#32476;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#25429;&#25417;&#27835;&#30103;&#20542;&#21521;&#12289;&#21709;&#24212;&#21644;&#38544;&#34255;&#27835;&#30103;&#25928;&#26524;&#30340;&#32508;&#21512;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#20013;&#32852;&#21512;&#23398;&#20064;&#27835;&#30103;&#21644;&#21709;&#24212;&#20989;&#25968;&#65292;&#20197;&#36991;&#20813;&#27835;&#30103;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#20013;&#38388;&#20266;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#22330;&#20043;&#38388;&#26144;&#23556;&#30340;&#31163;&#25955;&#19981;&#21464;&#32593;&#32476;&#65288;DI-Nets&#65289;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24471;&#20986;&#27169;&#22411;&#36755;&#20986;&#22312;&#19981;&#21516;&#31163;&#25955;&#21270;&#19979;&#30340;&#19978;&#30028;&#65292;&#24182;&#24378;&#35843;&#20102;&#28857;&#38598;&#24046;&#24322;&#22312;&#30028;&#38480;&#34920;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#25968;&#20540;&#31215;&#20998;&#36827;&#34892;&#20302;&#24046;&#24322;&#24230;&#31163;&#25955;&#21270;&#30340;&#32593;&#32476;&#35774;&#35745;&#65292;&#35777;&#26126;DI-Nets&#21487;&#20197;&#26222;&#36941;&#36924;&#36817;&#19968;&#31867;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2206.01178</link><description>&lt;p&gt;
&#31163;&#25955;&#19981;&#21464;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Discretization Invariant Networks for Learning Maps between Neural Fields. (arXiv:2206.01178v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#22330;&#20043;&#38388;&#26144;&#23556;&#30340;&#31163;&#25955;&#19981;&#21464;&#32593;&#32476;&#65288;DI-Nets&#65289;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24471;&#20986;&#27169;&#22411;&#36755;&#20986;&#22312;&#19981;&#21516;&#31163;&#25955;&#21270;&#19979;&#30340;&#19978;&#30028;&#65292;&#24182;&#24378;&#35843;&#20102;&#28857;&#38598;&#24046;&#24322;&#22312;&#30028;&#38480;&#34920;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#25968;&#20540;&#31215;&#20998;&#36827;&#34892;&#20302;&#24046;&#24322;&#24230;&#31163;&#25955;&#21270;&#30340;&#32593;&#32476;&#35774;&#35745;&#65292;&#35777;&#26126;DI-Nets&#21487;&#20197;&#26222;&#36941;&#36924;&#36817;&#19968;&#31867;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#22330;&#24418;&#24335;&#30340;&#36830;&#32493;&#25968;&#25454;&#24378;&#22823;&#34920;&#31034;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#19968;&#31181;&#31163;&#25955;&#19981;&#21464;&#30340;&#23398;&#20064;&#26041;&#27861;&#65306;&#19968;&#31181;&#22312;&#36830;&#32493;&#22495;&#19978;&#23398;&#20064;&#20989;&#25968;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#19981;&#21463;&#20989;&#25968;&#37319;&#26679;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#29702;&#35299;&#21644;&#35774;&#35745;&#31163;&#25955;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;DI-Nets&#65289;&#65292;&#23427;&#24191;&#27867;&#25512;&#24191;&#20102;&#35768;&#22810;&#31163;&#25955;&#32593;&#32476;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#21644;&#36830;&#32493;&#32593;&#32476;&#65288;&#22914;&#31070;&#32463;&#31639;&#23376;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30830;&#23450;&#20102;&#22312;&#19981;&#21516;&#26377;&#38480;&#31163;&#25955;&#21270;&#19979;&#27169;&#22411;&#36755;&#20986;&#30340;&#19978;&#30028;&#65292;&#24182;&#24378;&#35843;&#20102;&#28857;&#38598;&#24046;&#24322;&#22312;&#34920;&#24449;&#36825;&#31181;&#30028;&#38480;&#26041;&#38754;&#30340;&#26680;&#24515;&#20316;&#29992;&#12290;&#36825;&#19968;&#27934;&#23519;&#21147;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31867;&#30001;&#25968;&#20540;&#31215;&#20998;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20302;&#24046;&#24322;&#24230;&#30340;&#31163;&#25955;&#21270;&#36827;&#34892;&#20934;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;DI-Nets&#21487;&#20197;&#26222;&#36941;&#36924;&#36817;&#19968;&#31867;&#20171;&#20110;&#21487;&#31215;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of powerful representations of continuous data in the form of neural fields, there is a need for discretization invariant learning: an approach for learning maps between functions on continuous domains without being sensitive to how the function is sampled. We present a new framework for understanding and designing discretization invariant neural networks (DI-Nets), which generalizes many discrete networks such as convolutional neural networks as well as continuous networks such as neural operators. Our analysis establishes upper bounds on the deviation in model outputs under different finite discretizations, and highlights the central role of point set discrepancy in characterizing such bounds. This insight leads to the design of a family of neural networks driven by numerical integration via quasi-Monte Carlo sampling with discretizations of low discrepancy. We prove by construction that DI-Nets universally approximate a large class of maps between integrable funct
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31216;&#20026;&#32479;&#19968;&#22797;&#26434;&#24230;&#25991;&#26412;&#29983;&#25104;&#65288;UCTG&#65289;&#65292;&#35201;&#27714;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#24773;&#20917;&#19979;&#36981;&#24490;&#32479;&#19968;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#35813;&#27979;&#35797;&#20351;&#29992;&#20102;150&#22810;&#20010;&#19982;&#20154;&#31867;&#21644;&#29983;&#25104;&#25991;&#26412;&#22797;&#26434;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2204.05185</link><description>&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Uniform Complexity for Text Generation. (arXiv:2204.05185v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05185
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31216;&#20026;&#32479;&#19968;&#22797;&#26434;&#24230;&#25991;&#26412;&#29983;&#25104;&#65288;UCTG&#65289;&#65292;&#35201;&#27714;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#24773;&#20917;&#19979;&#36981;&#24490;&#32479;&#19968;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#35813;&#27979;&#35797;&#20351;&#29992;&#20102;150&#22810;&#20010;&#19982;&#20154;&#31867;&#21644;&#29983;&#25104;&#25991;&#26412;&#22797;&#26434;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#29983;&#25104;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#27604;&#22914;&#24635;&#32467;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#22312;&#21465;&#36848;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#27169;&#22411;&#20173;&#26410;&#25429;&#25417;&#21040;&#20135;&#29983;&#19968;&#33268;&#25991;&#26412;&#25152;&#28041;&#21450;&#30340;&#22240;&#32032;&#12290;&#20363;&#22914;&#65292;&#36923;&#36753;&#19978;&#35762;&#65292;&#19968;&#27573;&#25991;&#26412;&#25110;&#19968;&#20010;&#25925;&#20107;&#24212;&#35813;&#22312;&#22987;&#32456;&#21487;&#35835;&#65292;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#21487;&#25511;&#21046;&#30340;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#36755;&#20837;&#25991;&#26412;&#25552;&#31034;&#22312;&#24343;&#21015;&#35768;&#35835;&#26131;&#24230;&#27979;&#35797;&#20013;&#35780;&#20026;&#19968;&#24180;&#32423;&#38405;&#35835;&#27700;&#24179;&#65292;&#37027;&#20040;&#24310;&#32493;&#24773;&#33410;&#30340;&#29983;&#25104;&#25991;&#26412;&#20063;&#24212;&#22312;&#27492;&#22797;&#26434;&#24230;&#33539;&#22260;&#20869;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#22797;&#26434;&#24230;&#65288;UCTG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20986;&#20102;&#20351;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#31034;&#26041;&#38754;&#36981;&#24490;&#32479;&#19968;&#35821;&#35328;&#23646;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;150&#22810;&#20010;&#19982;&#35821;&#35328;&#21644;&#35748;&#30693;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#35780;&#20272;&#20154;&#31867;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#22797;&#26434;&#24230;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance, it is logical that a piece of text or a story should be uniformly readable throughout and that this form of complexity should be controllable. As such, if the complexity of an input text prompt is rated first-grade reading level in the Flesch Reading Ease test, then the generated text continuing the plot should also be within this range of complexity. With this in mind, we introduce Uniform Complexity for Text Generation (UCTG), a new benchmark test which raises the challenge of making generative models observe uniform linguistic properties with respect to prompts. We experiment with over 150+ linguistically and cognitively motivated features for evaluating text complexity in humans and gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CARL&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#36890;&#29992;&#21407;&#22411;&#26469;&#23454;&#29616;&#19968;&#33268;&#20998;&#37197;&#35270;&#22270;&#21040;&#32858;&#31867;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;CARL&#22312;&#22810;&#20010;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.15421</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#20998;&#37197;&#35270;&#22270;&#21040;&#32858;&#31867;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Consistent Assignment of Views to Clusters. (arXiv:2112.15421v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.15421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CARL&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#36890;&#29992;&#21407;&#22411;&#26469;&#23454;&#29616;&#19968;&#33268;&#20998;&#37197;&#35270;&#22270;&#21040;&#32858;&#31867;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;CARL&#22312;&#22810;&#20010;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CARL&#30340;&#19968;&#33268;&#20998;&#37197;&#34920;&#31034;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#30475;&#24453;&#23545;&#27604;&#23398;&#20064;&#65292;CARL&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#36890;&#29992;&#21407;&#22411;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#34920;&#31034;&#65292;&#36825;&#20123;&#21407;&#22411;&#20316;&#20026;&#33021;&#37327;&#38170;&#28857;&#65292;&#24378;&#21046;&#23558;&#32473;&#23450;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#22270;&#20998;&#37197;&#32473;&#30456;&#21516;&#30340;&#21407;&#22411;&#12290;&#19982;&#24403;&#20195;&#30340;&#28145;&#24230;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;&#30456;&#27604;&#65292;CARL&#25552;&#20986;&#20197;&#22312;&#32447;&#26041;&#24335;&#23398;&#20064;&#19968;&#32452;&#36890;&#29992;&#21407;&#22411;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#38750;&#21487;&#24494;&#20998;&#31639;&#27861;&#25110;K-Means&#26469;&#35299;&#20915;&#32858;&#31867;&#20998;&#37197;&#38382;&#39064;&#12290;CARL&#22312;&#35768;&#22810;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#65292;&#21253;&#25324;&#32447;&#24615;&#35780;&#20272;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Consistent Assignment for Representation Learning (CARL), an unsupervised learning method to learn visual representations by combining ideas from self-supervised contrastive learning and deep clustering. By viewing contrastive learning from a clustering perspective, CARL learns unsupervised representations by learning a set of general prototypes that serve as energy anchors to enforce different views of a given image to be assigned to the same prototype. Unlike contemporary work on contrastive learning with deep clustering, CARL proposes to learn the set of general prototypes in an online fashion, using gradient descent without the necessity of using non-differentiable algorithms or K-Means to solve the cluster assignment problem. CARL surpasses its competitors in many representations learning benchmarks, including linear evaluation, semi-supervised learning, and transfer learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2106.12915</link><description>&lt;p&gt;
ReLU'(0)&#23545;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#19978;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013;ReLU'(0)&#22312;[0, 1]&#33539;&#22260;&#20869;&#30340;&#36873;&#25321;&#23545;&#21453;&#21521;&#20256;&#25773;&#21644;&#35757;&#32451;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;32&#20301;&#40664;&#35748;&#31934;&#24230;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#20351;&#20854;&#25104;&#20026;&#35757;&#32451;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#20960;&#31181;&#31934;&#24230;&#27700;&#24179;&#65288;16&#20301;&#65292;32&#20301;&#65292;64&#20301;&#65289;&#12289;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;VGG&#12289;ResNet&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR10&#12289;SVHN&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;32&#20301;&#31934;&#24230;&#19979;&#65292;&#21453;&#21521;&#20256;&#25773;&#36755;&#20986;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#36825;&#31181;&#24773;&#20917;&#22823;&#32422;&#20986;&#29616;&#20102;&#19968;&#21322;&#30340;&#26102;&#38388;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#21452;&#31934;&#24230;&#19979;&#28040;&#22833;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#23545;&#20110;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#32780;&#35328;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25209;&#24402;&#19968;&#21270;&#25110;ADAM&#31561;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24819;&#35201;&#20256;&#36798;&#30340;&#20449;&#24687;&#26159;&#65292;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#31639;&#27861;&#24494;&#20998;&#21487;&#33021;&#38544;&#34255;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#31639;&#27861;&#21644;&#20219;&#21153;&#23481;&#37327;&#25351;&#26631;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#32452;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#21644;&#31574;&#30053;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#22312;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#36991;&#20813;&#29983;&#25104;&#22823;&#37327;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#21644;&#20219;&#21153;&#23481;&#37327;&#39044;&#36873;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.01741</link><description>&lt;p&gt;
&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#19982;&#20219;&#21153;&#23481;&#37327;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#31639;&#27861;&#21644;&#20219;&#21153;&#23481;&#37327;&#25351;&#26631;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#32452;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#21644;&#31574;&#30053;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#22312;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#36991;&#20813;&#29983;&#25104;&#22823;&#37327;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#21644;&#20219;&#21153;&#23481;&#37327;&#39044;&#36873;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19968;&#30452;&#23384;&#22312;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#25353;&#39034;&#24207;&#25509;&#25910;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#31574;&#30053;&#37325;&#29992;&#21644;&#20854;&#20182;&#22810;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#20294;&#21487;&#33021;&#20250;&#29983;&#25104;&#22823;&#37327;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#36129;&#29486;&#65292;&#21363;1) &#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#31574;&#30053;&#37325;&#29992;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#20248;&#21270;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#36873;&#25321;&#30340;&#32452;&#21512;&#26469;&#20248;&#21270;&#19968;&#32452;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#36991;&#20813;&#29983;&#25104;&#36807;&#22810;&#31574;&#30053;&#65307;2) &#20219;&#21153;&#23481;&#37327;&#65292;&#19968;&#31181;&#34913;&#37327;&#31574;&#30053;&#33021;&#20934;&#30830;&#35299;&#20915;&#30340;&#26368;&#22823;&#20219;&#21153;&#25968;&#37327;&#30340;&#25351;&#26631;&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#31181;&#20808;&#36827;&#30340;&#22522;&#30784;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;18&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;Pacman&#20219;&#21153;&#21644;&#19968;&#20010;&#26368;&#22810;&#21253;&#21547;125&#20010;&#20219;&#21153;&#30340;Cartpole&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#21644;&#22522;&#20110;&#20219;&#21153;&#23481;&#37327;&#30340;&#39044;&#36873;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long-standing challenge in artificial intelligence is lifelong reinforcement learning, where learners are given many tasks in sequence and must transfer knowledge between tasks while avoiding catastrophic forgetting. Policy reuse and other multi-policy reinforcement learning techniques can learn multiple tasks but may generate many policies. This paper presents two novel contributions, namely 1) Lifetime Policy Reuse, a model-agnostic policy reuse algorithm that avoids generating many policies by optimising a fixed number of near-optimal policies through a combination of policy optimisation and adaptive policy selection; and 2) the task capacity, a measure for the maximal number of tasks that a policy can accurately solve. Comparing two state-of-the-art base-learners, the results demonstrate the importance of Lifetime Policy Reuse and task capacity based pre-selection on an 18-task partially observable Pacman domain and a Cartpole domain of up to 125 tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21457;&#29616;&#20102;&#26435;&#37325;&#34928;&#20943;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20250;&#23548;&#33268;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#38519;&#38449;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Scheduled Weight Decay&#65288;SWD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#24182;&#24809;&#32602;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.11152</link><description>&lt;p&gt;
&#20851;&#20110;&#26435;&#37325;&#34928;&#20943;&#30340;&#24120;&#34987;&#24573;&#35270;&#30340;&#38519;&#38449;&#21450;&#20854;&#22914;&#20309;&#32531;&#35299;&#65306;&#26799;&#24230;&#33539;&#25968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective. (arXiv:2011.11152v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.11152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21457;&#29616;&#20102;&#26435;&#37325;&#34928;&#20943;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20250;&#23548;&#33268;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#38519;&#38449;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Scheduled Weight Decay&#65288;SWD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#24182;&#24809;&#32602;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#34928;&#20943;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#26435;&#37325;&#34928;&#20943;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#21457;&#29616;&#26435;&#37325;&#34928;&#20943;&#23548;&#33268;&#30340;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#38519;&#38449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#34928;&#20943;&#19981;&#24184;&#22320;&#20250;&#23548;&#33268;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#65288;&#25110;&#32456;&#27490;&#30340;&#35299;&#65289;&#20986;&#29616;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#36825;&#36890;&#24120;&#34920;&#26126;&#20102;&#31967;&#31957;&#30340;&#25910;&#25947;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#26799;&#24230;&#33539;&#25968;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26435;&#37325;&#34928;&#20943;&#35843;&#24230;&#22120;&#65292;&#31216;&#20026;Scheduled Weight Decay&#65288;SWD&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#26799;&#24230;&#33539;&#25968;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#33879;&#24809;&#32602;&#22823;&#26799;&#24230;&#33539;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20063;&#35777;&#26126;&#65292;SWD&#30830;&#23454;&#32531;&#35299;&#20102;&#22823;&#26799;&#24230;&#33539;&#25968;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#26126;&#26174;&#20248;&#20110;&#24120;&#35268;&#30340;&#24658;&#23450;&#26435;&#37325;&#34928;&#20943;&#31574;&#30053;&#65288;AMEN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight decay is a simple yet powerful regularization technique that has been very widely used in training of deep neural networks (DNNs). While weight decay has attracted much attention, previous studies fail to discover some overlooked pitfalls on large gradient norms resulted by weight decay. In this paper, we discover that, weight decay can unfortunately lead to large gradient norms at the final phase (or the terminated solution) of training, which often indicates bad convergence and poor generalization. To mitigate the gradient-norm-centered pitfalls, we present the first practical scheduler for weight decay, called the Scheduled Weight Decay (SWD) method that can dynamically adjust the weight decay strength according to the gradient norm and significantly penalize large gradient norms during training. Our experiments also support that SWD indeed mitigates large gradient norms and often significantly outperforms the conventional constant weight decay strategy for Adaptive Moment Es
&lt;/p&gt;</description></item><item><title>ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2007.01777</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#20998;&#31867;&#36890;&#36807;&#21407;&#22411;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.01777
&lt;/p&gt;
&lt;p&gt;
ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;ProtoryNet&#65292;&#23427;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#26032;&#27010;&#24565;&#12290;&#21463;&#29616;&#20195;&#35821;&#35328;&#23398;&#20013;&#30340;&#21407;&#22411;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;ProtoryNet&#36890;&#36807;&#20026;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#25214;&#21040;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#65292;&#24182;&#23558;&#27599;&#20010;&#21477;&#23376;&#19982;&#30456;&#24212;&#30340;&#27963;&#21160;&#21407;&#22411;&#30340;&#25509;&#36817;&#31243;&#24230;&#36755;&#20837;&#21040;RNN&#20027;&#24178;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;RNN&#20027;&#24178;&#25429;&#25417;&#21040;&#21407;&#22411;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#36712;&#36857;&#12290;&#21407;&#22411;&#36712;&#36857;&#33021;&#22815;&#30452;&#35266;&#32780;&#32454;&#33268;&#22320;&#35299;&#37322;RNN&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#20998;&#26512;&#25991;&#26412;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21407;&#22411;&#20462;&#21098;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#20351;&#29992;&#30340;&#21407;&#22411;&#24635;&#25968;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ProtoryNet&#27604;&#22522;&#32447;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
&lt;/p&gt;</description></item></channel></rss>