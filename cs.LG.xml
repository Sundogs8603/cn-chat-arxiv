<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07950</link><description>&lt;p&gt;
&#36328;&#22359;&#37327;&#21270;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBQ: Cross-Block Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07950
&lt;/p&gt;
&lt;p&gt;
CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#22312;&#20197;&#26497;&#20302;&#25104;&#26412;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#21482;&#20851;&#27880;&#22788;&#29702;&#21333;&#20010;&#23618;&#25110;&#21333;&#20010;&#22359;&#20869;&#30340;&#24322;&#24120;&#20540;&#65292;&#24573;&#30053;&#20102;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#20302;&#20301;&#35774;&#32622;&#20013;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#38388;&#37325;&#26500;&#30340;&#36328;&#22359;PTQ&#26041;&#27861;CBQ&#12290;CBQ&#37319;&#29992;&#20102;&#19968;&#31181;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#23454;&#29616;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;&#27492;&#22806;&#65292;CBQ&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65288;CFP&#65289;&#26469;&#25233;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#37197;&#21512;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;LoRA&#21462;&#25972;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#26435;&#37325;&#37327;&#21270;&#12290;&#36825;&#20123;&#21019;&#26032;&#20351;CBQ&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#65292;&#36824;&#33021;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CBQ&#22312;&#20302;&#20301;&#37327;&#21270;&#65288;W4A4&#65292;W4A8&#31561;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2404.02484</link><description>&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
New methods for drug synergy prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02484
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#23567;&#22411;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20381;&#36182;&#20110;&#39640;&#36890;&#37327;&#32452;&#21512;&#31579;&#36873;&#30340;&#33647;&#29289;&#32452;&#21512;&#21327;&#21516;&#20316;&#29992;&#30340;&#26032;&#39044;&#27979;&#26041;&#27861;&#12290;&#33258;2021&#24180;&#20197;&#26469;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#36895;&#36827;&#23637;&#65292;&#24050;&#21457;&#34920;&#20102;&#36229;&#36807;30&#31181;&#21407;&#21019;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31361;&#26174;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#21644;&#21327;&#21516;&#24471;&#20998;&#65292;&#20197;&#21450;&#35770;&#25991;&#25152;&#28041;&#21450;&#30340;&#39044;&#27979;&#24773;&#26223;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#23558;&#36825;&#20123;&#35770;&#25991;&#25918;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#19979;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65292;&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#22320;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#32780;&#28041;&#21450;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#24773;&#26223;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02484v1 Announce Type: cross  Abstract: In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens. The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques. We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with. Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15706</link><description>&lt;p&gt;
G-ACIL&#65306;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15706
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#22312;&#39034;&#24207;&#20219;&#21153;&#19978;&#35757;&#32451;&#32593;&#32476;&#65292;&#27599;&#20010;&#20219;&#21153;&#26377;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#24191;&#20041;CIL(GCIL)&#26088;&#22312;&#35299;&#20915;&#26356;&#25509;&#36817;&#29616;&#23454;&#24773;&#26223;&#19979;&#30340;CIL&#38382;&#39064;&#65292;&#21363;&#26032;&#25968;&#25454;&#20855;&#26377;&#28151;&#21512;&#25968;&#25454;&#31867;&#21035;&#21644;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#22823;&#23567;&#65292;&#23548;&#33268;&#36951;&#24536;&#21152;&#21095;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;GCIL&#30340;&#23581;&#35797;&#35201;&#20040;&#24615;&#33021;&#19981;&#20339;&#65292;&#35201;&#20040;&#36890;&#36807;&#20445;&#23384;&#21382;&#21490;&#33539;&#20363;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;(G-ACIL)&#12290;G-ACIL&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;(&#19968;&#31181;&#26080;&#26799;&#24230;&#35757;&#32451;&#25216;&#26415;)&#65292;&#24182;&#20026;GCIL&#24773;&#26223;&#25552;&#20379;&#20998;&#26512;&#35299;(&#21363;&#38381;&#21512;&#24418;&#24335;)&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20256;&#20837;&#25968;&#25454;&#20998;&#35299;&#20026;&#26292;&#38706;&#31867;&#21644;&#26410;&#26292;&#38706;&#31867;&#65292;&#23454;&#29616;&#20102;&#22686;&#38271;&#31867;&#20043;&#38388;&#30340;&#31561;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15706v1 Announce Type: new  Abstract: Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12338</link><description>&lt;p&gt;
&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic Halpern iteration in normed spaces and applications to reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;Halpern&#36845;&#20195;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#26088;&#22312;&#36817;&#20284;&#26377;&#30028;&#21644;&#25910;&#32553;&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#22312;&#19968;&#20010;&#26377;&#38480;&#32500;&#36171;&#33539;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#24213;&#23618;&#30340;&#38543;&#26426;Oracle&#20855;&#26377;&#19968;&#33268;&#26377;&#30028;&#30340;&#26041;&#24046;&#65292;&#21017;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#24635;&#30340;Oracle&#22797;&#26434;&#24230;&#20026;$ \tilde{O} (\varepsilon^{-5})$&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#20026;&#38543;&#26426;Krasnoselskii-Mann&#36845;&#20195;&#24314;&#31435;&#30340;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; $\Omega (\varepsilon^{-3})$&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#33539;&#22260;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#25152;&#26377;&#24102;&#26377;&#23567;&#25209;&#22788;&#29702;&#30340;&#24179;&#22343;&#36845;&#20195;&#12290;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#31639;&#23376;&#20026; $\gamma$-&#25910;&#32553;&#30340;&#24773;&#20917;&#19979;&#19968;&#20010; $O(\varepsilon^{-2}(1-\gamma)^{-3})$&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29992;&#20110;&#24179;&#22343;&#22870;&#21169;&#21644;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21516;&#27493;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12338v1 Announce Type: cross  Abstract: We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in which the operator is a $\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, f
&lt;/p&gt;</description></item><item><title>&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12075</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#22810;&#26679;&#21270;&#21361;&#23475;&#30340;&#24320;&#25918;&#24335;&#32418;&#38431;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12075
&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#19981;&#26126;&#26174;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20197;&#20943;&#23569;&#29983;&#25104;&#20882;&#29359;&#24615;&#22270;&#20687;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;"&#38544;&#24615;&#23545;&#25239;"&#25552;&#31034;&#65288;&#35302;&#21457;T2I&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#38750;&#26126;&#26174;&#21407;&#22240;&#65289;&#65292;&#25105;&#20204;&#29420;&#31435;&#36776;&#21035;&#20986;&#19968;&#32452;&#38590;&#20197;&#21457;&#29616;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20154;&#31867;&#21019;&#36896;&#21147;&#24456;&#36866;&#21512;&#25581;&#31034;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Adversarial Nibbler Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#32418;&#38431;&#26041;&#27861;&#65292;&#29992;&#20110;&#20247;&#21253;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#38544;&#24615;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#24050;&#27719;&#24635;&#19968;&#22871;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#65292;&#37319;&#29992;&#31616;&#21333;&#29992;&#25143;&#30028;&#38754;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#21361;&#23475;&#65292;&#24182;&#21560;&#24341;&#24191;&#27867;&#20154;&#32676;&#26469;&#25429;&#25417;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#38271;&#23614;&#23433;&#20840;&#38382;&#39064;&#12290;&#25361;&#25112;&#22312;&#36830;&#32493;&#22238;&#21512;&#20013;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#23545;T2I&#27169;&#22411;&#20013;&#23433;&#20840;&#38544;&#24739;&#30340;&#25345;&#32493;&#21457;&#29616;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20294;&#20854;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#21069;&#21521;&#27491;&#21521;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11004</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Forward Learning of Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11004
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20294;&#20854;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#21069;&#21521;&#27491;&#21521;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#38382;&#31572;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;GNNs&#30340;&#25104;&#21151;&#32972;&#21518;&#65292;&#26159;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;BP&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#36824;&#26159;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#19981;&#20165;&#22312;&#29983;&#29289;&#19978;&#19981;&#21512;&#29702;&#65292;&#32780;&#19988;&#38480;&#21046;&#20102;&#23398;&#20064;NNs&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#24182;&#34892;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26368;&#36817;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#25552;&#20986;&#20102;&#21069;&#21521;&#27491;&#21521;&#65288;FF&#65289;&#31639;&#27861;&#20316;&#20026;BP&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27491;&#36127;&#25968;&#25454;&#19978;&#25191;&#34892;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#35757;&#32451;NNs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11004v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we 
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#28304;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05949</link><description>&lt;p&gt;
&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65306;&#29992;&#20110;&#36890;&#29992;&#22806;&#31185;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General surgery vision transformer: A video pre-trained foundation model for general surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#28304;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#24320;&#25918;&#33719;&#21462;&#30340;&#25968;&#25454;&#21644;&#19987;&#38376;&#30340;&#22522;&#30784;&#27169;&#22411;&#26159;&#22806;&#31185;&#35745;&#31639;&#30740;&#31350;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#28304;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;28&#31181;&#25163;&#26415;&#25216;&#26415;&#30340;680&#23567;&#26102;&#25163;&#26415;&#35270;&#39057;&#25968;&#25454;&#65307;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;&#35270;&#39057;&#39044;&#27979;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#35270;&#39057;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#23454;&#26102;&#36816;&#34892;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;GSViT&#30340;&#20195;&#30721;&#21644;&#26435;&#37325;&#65307;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#38024;&#23545;10&#31181;&#25163;&#26415;&#31243;&#24207;&#30340;&#29305;&#23450;&#31243;&#24207;&#24494;&#35843;&#29256;&#26412;&#30340;GSViT&#30340;&#20195;&#30721;&#21644;&#26435;&#37325;&#65307;&#25105;&#20204;&#23637;&#31034;&#20102;GSViT&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21333;&#24103;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05949v1 Announce Type: cross  Abstract: The absence of openly accessible data and specialized foundation models is a major barrier for computational research in surgery. Toward this, (i) we open-source the largest dataset of general surgery videos to-date, consisting of 680 hours of surgical videos, including data from robotic and laparoscopic techniques across 28 procedures; (ii) we propose a technique for video pre-training a general surgery vision transformer (GSViT) on surgical videos based on forward video prediction that can run in real-time for surgical applications, toward which we open-source the code and weights of GSViT; (iii) we also release code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the Cholec80 phase annotation task, displaying improved performance over state-of-the-art single frame predictors.
&lt;/p&gt;</description></item><item><title>AceMap&#26159;&#19968;&#20010;&#38754;&#21521;&#30693;&#35782;&#21457;&#29616;&#30340;&#23398;&#26415;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#24211;&#21644;&#36816;&#29992;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#12289;&#37327;&#21270;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31185;&#23398;&#25991;&#29486;&#31649;&#29702;&#19982;&#20215;&#20540;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02576</link><description>&lt;p&gt;
AceMap&#65306;&#36890;&#36807;&#23398;&#26415;&#22270;&#35889;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
AceMap: Knowledge Discovery through Academic Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02576
&lt;/p&gt;
&lt;p&gt;
AceMap&#26159;&#19968;&#20010;&#38754;&#21521;&#30693;&#35782;&#21457;&#29616;&#30340;&#23398;&#26415;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#24211;&#21644;&#36816;&#29992;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#12289;&#37327;&#21270;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31185;&#23398;&#25991;&#29486;&#31649;&#29702;&#19982;&#20215;&#20540;&#25552;&#21462;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#29486;&#30340;&#25351;&#25968;&#22686;&#38271;&#38656;&#35201;&#26377;&#25928;&#31649;&#29702;&#21644;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#31185;&#23398;&#25628;&#32034;&#24341;&#25806;&#22312;&#22522;&#20110;&#20851;&#31995;&#25968;&#25454;&#24211;&#25552;&#20379;&#25628;&#32034;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#20102;&#31185;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#21450;&#24605;&#24819;&#28436;&#21270;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#31185;&#23398;&#20986;&#29256;&#29289;&#20869;&#23481;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;&#24322;&#36136;&#22270;&#30340;&#34920;&#31034;&#20197;&#21450;&#36825;&#31181;&#22270;&#30340;&#26377;&#25928;&#27979;&#37327;&#12289;&#20998;&#26512;&#21644;&#25366;&#25496;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AceMap&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#23398;&#26415;&#22270;&#35889;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#30340;&#23398;&#26415;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#36827;&#30340;&#25968;&#25454;&#24211;&#26500;&#24314;&#25216;&#26415;&#65292;&#20197;&#26500;&#24314;&#21253;&#21547;&#20016;&#23500;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#20840;&#38754;AceMap&#25968;&#25454;&#24211;&#12290;AceMap&#36824;&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#12289;&#37327;&#21270;&#21644;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02576v1 Announce Type: cross  Abstract: The exponential growth of scientific literature requires effective management and extraction of valuable insights. While existing scientific search engines excel at delivering search results based on relational databases, they often neglect the analysis of collaborations between scientific entities and the evolution of ideas, as well as the in-depth analysis of content within scientific publications. The representation of heterogeneous graphs and the effective measurement, analysis, and mining of such graphs pose significant challenges. To address these challenges, we present AceMap, an academic system designed for knowledge discovery through academic graph. We present advanced database construction techniques to build the comprehensive AceMap database with large-scale academic publications that contain rich visual, textual, and numerical information. AceMap also employs innovative visualization, quantification, and analysis methods to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#27169;&#22411;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#31574;&#30053;&#12289;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.02469</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#27169;&#22411;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#31574;&#30053;&#12289;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#29992;&#20110;&#20998;&#26512;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#30340;VLMs&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#35774;&#35745;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#32972;&#26223;&#65292;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25216;&#26415;&#32467;&#21512;&#21040;VLMs&#20013;&#65292;&#20197;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#35752;&#35770;&#30340;&#20851;&#38190;&#39046;&#22495;&#21253;&#25324;&#23545;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#65292;&#23545;&#26368;&#36817;&#20540;&#24471;&#20851;&#27880;&#30340;&#21307;&#23398;VLMs&#20013;&#37319;&#29992;&#30340;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#35780;&#20272;VLMs&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#31572;&#20013;&#34920;&#29616;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#35752;&#35770;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#25552;&#39640;&#20020;&#24202;&#26377;&#25928;&#24615;&#21644;&#35299;&#20915;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02469v1 Announce Type: cross  Abstract: Medical vision-language models (VLMs) combine computer vision and natural language processing to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on models designed for medical report generation and visual question answering. We provide background on natural language processing and computer vision, explaining how techniques from both fields are integrated into VLMs to enable learning from multimodal data. Key areas we address include the exploration of medical vision-language datasets, in-depth analyses of architectures and pre-training strategies employed in recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs' performance in medical report generation and visual question answering. We also highlight current challenges and propose future directions, including enhancing clinical validity and addressing p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#32852;&#21512;&#20272;&#35745;&#21644;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#22312;&#32447;&#35757;&#32451;&#21644;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.02215</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#23454;&#29616;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32852;&#21512;&#21442;&#25968;&#21644;&#21442;&#25968;&#21270;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02215
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#32534;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#32852;&#21512;&#20272;&#35745;&#21644;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#22312;&#32447;&#35757;&#32451;&#21644;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#34920;&#31034;&#25968;&#20540;&#27169;&#25311;&#20013;&#26410;&#30693;&#21644;&#20122;&#32593;&#26684;&#29289;&#29702;&#36807;&#31243;&#30340;&#21442;&#25968;&#21270;(&#25110;&#38381;&#21512;)&#24182;&#23545;&#20854;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#23545;&#20110;&#35299;&#26512;&#35768;&#22810;&#38382;&#39064;&#30340;&#31895;&#31890;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#20197;&#21450;&#28237;&#27969;&#27169;&#25311;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#30475;&#21040;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23545;&#36825;&#20123;&#20122;&#32593;&#26684;&#36807;&#31243;&#24314;&#27169;&#65292;&#23548;&#33268;&#20102;&#36890;&#36807;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#38598;&#25104;&#24320;&#21457;&#28151;&#21512;&#29289;&#29702;-ML&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#29289;&#29702;&#21442;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#12290;&#36890;&#36807;&#22312;&#32447;&#35757;&#32451;&#21644;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#30340;&#26377;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#20511;&#21161;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#33021;&#21147;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02215v1 Announce Type: new  Abstract: Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical simulations with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence simulations. Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers. In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming. Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming. This proof 
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#34394;&#25311;&#26631;&#27880;&#22120;&#65292;&#30452;&#25509;&#20351;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#65292;&#21487;&#33021;&#35299;&#20915;&#20256;&#32479;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01133</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#29289;&#29702;&#24863;&#24212;&#25968;&#25454;&#30340;&#34394;&#25311;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01133
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#34394;&#25311;&#26631;&#27880;&#22120;&#65292;&#30452;&#25509;&#20351;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#65292;&#21487;&#33021;&#35299;&#20915;&#20256;&#32479;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65288;&#22914;&#24815;&#24615;&#25968;&#25454;&#65289;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#26469;&#33258;&#29615;&#22659;&#30340;&#35270;&#39057;&#25110;&#38899;&#39057;&#31561;&#20854;&#20182;&#27169;&#24577;&#12290;&#36825;&#20123;&#22791;&#29992;&#26469;&#28304;&#20026;&#20154;&#31867;&#26631;&#27880;&#32773;&#25552;&#20379;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#22240;&#20026;&#21407;&#22987;&#25968;&#23383;&#25968;&#25454;&#23545;&#20110;&#19987;&#23478;&#26469;&#35828;&#36890;&#24120;&#36807;&#20110;&#38590;&#20197;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20256;&#32479;&#26041;&#27861;&#23384;&#22312;&#35768;&#22810;&#20851;&#20110;&#24635;&#20307;&#25104;&#26412;&#12289;&#25928;&#29575;&#12289;&#39069;&#22806;&#27169;&#24577;&#30340;&#23384;&#20648;&#12289;&#26102;&#38388;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#30340;&#38382;&#39064;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#26159;&#36890;&#36807;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#23383;&#27597;&#25968;&#23383;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20351;&#23427;&#20204;&#33021;&#22815;&#29702;&#35299;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20197;&#22806;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#33258;&#28982;&#22320;&#65292;&#36825;&#20026;&#25506;&#32034;LLMs&#20316;&#20026;&#34394;&#25311;&#26631;&#27880;&#22120;&#24320;&#36767;&#20102;&#28508;&#22312;&#36884;&#24452;&#65292;&#20854;&#20013;LLMs&#23558;&#30452;&#25509;&#25552;&#20379;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#22791;&#29992;&#27169;&#24577;&#12290;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#20915;&#25104;&#26412;&#12289;&#25928;&#29575;&#12289;&#23384;&#20648;&#12289;&#26102;&#38388;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01133v1 Announce Type: new  Abstract: Traditional human-in-the-loop-based annotation for time-series data like inertial data often requires access to alternate modalities like video or audio from the environment. These alternate sources provide the necessary information to the human annotator, as the raw numeric data is often too obfuscated even for an expert. However, this traditional approach has many concerns surrounding overall cost, efficiency, storage of additional modalities, time, scalability, and privacy. Interestingly, recent large language models (LLMs) are also trained with vast amounts of publicly available alphanumeric data, which allows them to comprehend and perform well on tasks beyond natural language processing. Naturally, this opens up a potential avenue to explore LLMs as virtual annotators where the LLMs will be directly provided the raw sensor data for annotation instead of relying on any alternate modality. Naturally, this could mitigate the problems 
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#22522;&#20110;&#36866;&#24212;&#37319;&#26679;&#30340;&#21518;&#39564;GP&#30340;&#26368;&#39640;&#20540;&#20013;&#20540;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#36793;&#30028;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20272;&#35745;&#39640;&#23433;&#20840;&#27010;&#29575;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#21152;&#24555;&#20102;&#35780;&#20272;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#21644;&#25506;&#32034;&#36895;&#24230;</title><link>https://arxiv.org/abs/2402.18260</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#39640;&#25928;&#35745;&#31639;&#23433;&#20840;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#22522;&#20110;&#36866;&#24212;&#37319;&#26679;&#30340;&#21518;&#39564;GP&#30340;&#26368;&#39640;&#20540;&#20013;&#20540;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#36793;&#30028;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20272;&#35745;&#39640;&#23433;&#20840;&#27010;&#29575;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#21152;&#24555;&#20102;&#35780;&#20272;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#21644;&#25506;&#32034;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#24517;&#39035;&#26222;&#36941;&#36981;&#23432;&#23454;&#38469;&#23433;&#20840;&#32422;&#26463;&#65292;&#36825;&#38480;&#21046;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#12290;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#21450;&#20854;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#34987;&#24191;&#27867;&#29992;&#20110;&#27492;&#30446;&#30340;&#12290;&#22312;&#35768;&#22810;&#25216;&#26415;&#24212;&#29992;&#20013;&#65292;&#35774;&#35745;&#31354;&#38388;&#36890;&#36807;&#36830;&#32493;&#36712;&#36857;&#36827;&#34892;&#25506;&#32034;&#65292;&#27839;&#30528;&#36712;&#36857;&#38656;&#35201;&#35780;&#20272;&#23433;&#20840;&#24615;&#12290;&#36825;&#23545;GP&#26041;&#27861;&#20013;&#20005;&#26684;&#30340;&#23433;&#20840;&#35201;&#27714;&#26469;&#35828;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#30340;&#39640;&#20998;&#20301;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36866;&#24212;&#37319;&#26679;&#30340;&#21518;&#39564;GP&#30340;&#26368;&#39640;&#20540;&#20013;&#20540;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#23433;&#20840;&#36793;&#30028;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#20272;&#35745;&#39640;&#23433;&#20840;&#27010;&#29575;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#21644;&#25506;&#32034;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#21152;&#24555;&#20102;&#35780;&#20272;&#36895;&#24230;&#12290;&#25105;&#20204;&#23433;&#20840;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18260v1 Announce Type: new  Abstract: Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space. Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose. In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed. This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles. We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP. Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed. The effectiveness of our safe active learning approach is demonstrated through exten
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#33719;&#24471;&#20102;&#31616;&#21333;&#30340;&#35299;&#26512;&#36924;&#36817;&#65292;&#37325;&#26032;&#20248;&#21270;&#20102;halofit&#30340;&#31995;&#25968;&#20197;&#25311;&#21512;&#21508;&#31181;&#23431;&#23449;&#23398;&#21644;&#32418;&#31227;&#33539;&#22260;&#65292;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#25506;&#32034;&#20102;&#29992;&#20110;&#25311;&#21512;&#27531;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#31354;&#38388;&#65292;&#25152;&#26377;&#26041;&#27861;&#22343;&#32463;&#36807;$N$&#20307;&#27169;&#25311;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.17492</link><description>&lt;p&gt;
syren-halofit: &#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#35299;&#37322;&#12289;&#39640;&#31934;&#24230;&#30340;$\Lambda$CDM&#38750;&#32447;&#24615;&#29289;&#36136;&#21151;&#29575;&#35889;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
syren-halofit: A fast, interpretable, high-precision formula for the $\Lambda$CDM nonlinear matter power spectrum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17492
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#33719;&#24471;&#20102;&#31616;&#21333;&#30340;&#35299;&#26512;&#36924;&#36817;&#65292;&#37325;&#26032;&#20248;&#21270;&#20102;halofit&#30340;&#31995;&#25968;&#20197;&#25311;&#21512;&#21508;&#31181;&#23431;&#23449;&#23398;&#21644;&#32418;&#31227;&#33539;&#22260;&#65292;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#25506;&#32034;&#20102;&#29992;&#20110;&#25311;&#21512;&#27531;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#31354;&#38388;&#65292;&#25152;&#26377;&#26041;&#27861;&#22343;&#32463;&#36807;$N$&#20307;&#27169;&#25311;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23431;&#23449;&#23398;&#20013;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#35780;&#20272;&#38750;&#32447;&#24615;&#29289;&#36136;&#21151;&#29575;&#35889;$P(k)$&#20851;&#20110;&#23431;&#23449;&#23398;&#21442;&#25968;&#21644;&#32418;&#31227;&#30340;&#20989;&#25968;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#33719;&#24471;&#20102;&#20851;&#20110;halofit&#27169;&#22411;&#25152;&#38656;&#30340;&#38750;&#32447;&#24615;&#23610;&#24230;$k_\sigma$&#12289;&#26377;&#25928;&#35889;&#25351;&#25968;$n_{\rm eff}$&#21644;&#26354;&#29575;$C$&#30340;&#31616;&#21333;&#35299;&#26512;&#36924;&#36817;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#26032;&#20248;&#21270;halofit&#30340;&#31995;&#25968;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#23431;&#23449;&#23398;&#21644;&#32418;&#31227;&#33539;&#22260;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20877;&#27425;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#26469;&#25506;&#32034;&#29992;&#20110;&#25311;&#21512;$P(k)$&#19982;halofit&#20248;&#21270;&#39044;&#27979;&#20043;&#38388;&#27531;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#31354;&#38388;&#12290;&#25152;&#26377;&#26041;&#27861;&#37117;&#32463;&#36807;&#19982;$N$&#20307;&#27169;&#25311;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17492v1 Announce Type: cross  Abstract: Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$, as a function of cosmological parameters and redshift is of fundamental importance in cosmology. Analytic approximations provide an interpretable solution, yet current approximations are neither fast nor accurate relative to black-box numerical emulators. We use symbolic regression to obtain simple analytic approximations to the nonlinear scale, $k_\sigma$, the effective spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for the halofit model. We then re-optimise the coefficients of halofit to fit a wide range of cosmologies and redshifts. We then again exploit symbolic regression to explore the space of analytic expressions to fit the residuals between $P(k)$ and the optimised predictions of halofit. All methods are validated against $N$-body simulations. Our symbolic expressions for $k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15106</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Distributed Training with Message Passing Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#32780;&#25193;&#23637;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;Nystrom-&#36817;&#20284;&#37319;&#26679;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DS-MPNN&#65288;&#20854;&#20013;D&#21644;S&#20998;&#21035;&#20195;&#34920;&#20998;&#24067;&#24335;&#21644;&#37319;&#26679;&#65289;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;$O(10^5)$&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26696;&#20363;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65306;&#65288;a&#65289;Darcy&#27969;&#25968;&#25454;&#38598;&#21644;&#65288;b&#65289;2-D&#26426;&#32764;&#30340;&#31283;&#24577;RANS&#27169;&#25311;&#65292;&#25552;&#20379;&#20102;&#19982;&#21333;GPU&#23454;&#29616;&#21644;&#22522;&#20110;&#33410;&#28857;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#27604;&#36739;&#12290;DS-MPNN&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#21333;GPU&#23454;&#29616;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#23481;&#32435;&#27604;&#21333;&#20010;GPU&#23454;&#29616;&#26356;&#22810;&#25968;&#37327;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15106v1 Announce Type: new  Abstract: In this study, we introduce a domain-decomposition-based distributed training and inference approach for message-passing neural networks (MPNN). Our objective is to address the challenge of scaling edge-based graph neural networks as the number of nodes increases. Through our distributed training approach, coupled with Nystr\"om-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based graph convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#30340;&#20840;&#23616;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#26469;&#20943;&#23569;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2402.14402</link><description>&lt;p&gt;
&#20840;&#23616;&#23433;&#20840;&#39034;&#24207;&#23398;&#20064;&#36890;&#36807;&#39640;&#25928;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Global Safe Sequential Learning via Efficient Knowledge Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#30340;&#20840;&#23616;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#26469;&#20943;&#23569;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14402v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#20363;&#22914;&#20027;&#21160;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35768;&#22810;&#21307;&#23398;&#25110;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#36873;&#25321;&#21463;&#20808;&#39564;&#26410;&#30693;&#30340;&#23433;&#20840;&#26465;&#20214;&#38480;&#21046;&#12290;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#23433;&#20840;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#24314;&#27169;&#23433;&#20840;&#27010;&#29575;&#65292;&#24182;&#22312;&#20855;&#26377;&#36739;&#39640;&#23433;&#20840;&#32622;&#20449;&#24230;&#30340;&#21306;&#22495;&#20013;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#23433;&#20840;&#24314;&#27169;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#28040;&#32791;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23433;&#20840;&#32622;&#20449;&#24230;&#38598;&#20013;&#22312;&#32473;&#23450;&#30340;&#35266;&#27979;&#20540;&#21608;&#22260;&#65292;&#23548;&#33268;&#23616;&#37096;&#25506;&#32034;&#12290;&#30001;&#20110;&#22312;&#23433;&#20840;&#20851;&#38190;&#23454;&#39564;&#20013;&#36890;&#24120;&#23384;&#22312;&#21487;&#36716;&#31227;&#30340;&#28304;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#39034;&#24207;&#23398;&#20064;&#26469;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#65292;&#20197;&#20943;&#23569;&#24341;&#20837;&#28304;&#25968;&#25454;&#24102;&#26469;&#30340;&#39069;&#22806;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14402v1 Announce Type: new  Abstract: Sequential learning methods such as active learning and Bayesian optimization select the most informative data to learn about a task. In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence. However, accurate safety modeling requires prior knowledge or consumes data. In addition, the safety confidence centers around the given observations which leads to local exploration. As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. In this pap
&lt;/p&gt;</description></item><item><title>OpenTab &#26159;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#25512;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#34920;&#26684;&#26816;&#32034;&#22120;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;SQL&#31243;&#24207;&#21644;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#23454;&#29616;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14361</link><description>&lt;p&gt;
OpenTab&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#36827;&#20026;&#24320;&#25918;&#39046;&#22495;&#30340;&#34920;&#26684;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
OpenTab: Advancing Large Language Models as Open-domain Table Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14361
&lt;/p&gt;
&lt;p&gt;
OpenTab &#26159;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#25512;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#34920;&#26684;&#26816;&#32034;&#22120;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;SQL&#31243;&#24207;&#21644;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#23454;&#29616;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#38656;&#35201;&#26410;&#32463;&#35757;&#32451;&#30340;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290; &#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#19968;&#20010;&#26816;&#32034;&#22120;&#26469;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#25193;&#23637;LLM&#30340;&#30693;&#35782;&#33539;&#22260;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#27169;&#24577;&#21644;&#22823;&#34920;&#26684;&#23610;&#23544;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#25991;&#26412;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;LLMs&#22312;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#19978;&#24182;&#19981;&#29702;&#24819;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenTab&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#25512;&#29702;&#26694;&#26550;&#12290; &#24635;&#20307;&#32780;&#35328;&#65292;OpenTab&#21033;&#29992;&#34920;&#26684;&#26816;&#32034;&#22120;&#26469;&#33719;&#21462;&#30456;&#20851;&#34920;&#26684;&#65292;&#28982;&#21518;&#29983;&#25104;SQL&#31243;&#24207;&#20197;&#39640;&#25928;&#22320;&#35299;&#26512;&#26816;&#32034;&#21040;&#30340;&#34920;&#26684;&#12290; &#21033;&#29992;&#20174;SQL&#25191;&#34892;&#20013;&#23548;&#20986;&#30340;&#20013;&#38388;&#25968;&#25454;&#65292;&#23427;&#36827;&#34892;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290; &#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;OpenTab&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14361v1 Announce Type: new  Abstract: Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDEA&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#21464;&#36801;&#65292;&#24182;&#36827;&#19968;&#27493;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.12767</link><description>&lt;p&gt;
&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#65306;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#36827;&#34892;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDEA&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#21464;&#36801;&#65292;&#24182;&#36827;&#19968;&#27493;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20998;&#24067;&#30340;&#36716;&#31227;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20551;&#23450;&#26102;&#38388;&#20998;&#24067;&#30340;&#36716;&#31227;&#26159;&#22343;&#21248;&#21457;&#29983;&#30340;&#65292;&#20197;&#21306;&#20998;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#24456;&#38590;&#28385;&#36275;&#65292;&#22240;&#20026;&#25105;&#20204;&#19981;&#30693;&#36947;&#20998;&#24067;&#20309;&#26102;&#21457;&#29983;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#65288;IDEA&#65289;&#26469;&#26816;&#27979;&#20998;&#24067;&#20309;&#26102;&#21457;&#29983;&#36716;&#31227;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20805;&#20998;&#35266;&#23519;&#20551;&#35774;&#26469;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#21270;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19982;&#29615;&#22659;&#19981;&#30456;&#20851;&#30340;&#31283;&#23450;&#21464;&#37327;&#21644;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#38750;&#24179;&#31283;&#21464;&#37327;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28508;&#22312;&#29615;&#22659;&#21644;&#31283;&#23450;/&#38750;&#31283;&#23450;&#21464;&#37327;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;IDEA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12767v1 Announce Type: new  Abstract: Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated station- ary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12038</link><description>&lt;p&gt;
Self-AMPLIFY&#65306;&#36890;&#36807;&#33258;&#25105;&#20107;&#21518;&#35299;&#37322;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;Self-AMPLIFY&#26159;&#19968;&#20010;3&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26679;&#26412;&#12289;&#29983;&#25104;&#29702;&#30001;&#21644;&#26500;&#24314;&#26368;&#32456;&#25552;&#31034;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;SLMs&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Self-AMPLIFY&#30340;&#24615;&#33021;&#65306;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;Self-AMPLIFY&#22312;&#19982;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;Self-AMPLIFY&#26159;&#31532;&#19968;&#20010;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;SLMs&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#35299;&#37322;&#24182;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12038v1 Announce Type: new  Abstract: Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a full
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11858</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#38543;&#26426;Hessian&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Hessian Fitting on Lie Group
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;Hessian&#25311;&#21512;&#20934;&#21017;&#65292;&#21487;&#29992;&#20110;&#25512;&#23548;&#22823;&#37096;&#20998;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;BFGS&#12289;&#39640;&#26031;&#29275;&#39039;&#12289;AdaGrad&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#29575;&#65292;&#20363;&#22914;&#65292;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#21644;&#23545;&#31216;&#27491;&#23450;&#65288;SPL&#65289;&#30697;&#38453;&#21644;&#26576;&#20123;&#26446;&#32676;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#32447;&#24615;&#36895;&#29575;&#12290;&#22312;&#29305;&#23450;&#19988;&#36275;&#22815;&#19968;&#33324;&#30340;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#26159;&#24378;&#20984;&#30340;&#12290;&#20026;&#20102;&#30830;&#35748;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#26377;&#22122;&#22768;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#12289;&#26102;&#21464;&#30340;Hessians&#21644;&#20302;&#31934;&#24230;&#31639;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20381;&#36182;&#20110;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23558;&#20854;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.11652</link><description>&lt;p&gt;
&#22240;&#26524;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#31283;&#20581;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Inference in Causal Latent Factor Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11652
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23558;&#20854;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#29615;&#22659;&#20855;&#26377;&#22823;&#37327;&#21333;&#20301;&#21644;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#37327;&#26159;&#21452;&#37325;&#31283;&#20581;&#30340;&#65292;&#32467;&#21512;&#20102;&#32467;&#26524;&#22635;&#34917;&#12289;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#26032;&#22411;&#20132;&#21449;&#37197;&#23545;&#31243;&#24207;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#25910;&#25947;&#21040;&#21442;&#25968;&#36895;&#29575;&#19979;&#30340;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;&#27169;&#25311;&#32467;&#26524;&#23637;&#31034;&#20102;&#26412;&#25991;&#20998;&#26512;&#30340;&#20272;&#35745;&#37327;&#30340;&#24418;&#24335;&#29305;&#24615;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11652v1 Announce Type: cross  Abstract: This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;</title><link>https://arxiv.org/abs/2402.06806</link><description>&lt;p&gt;
&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#30340;&#21407;&#21017;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Assessment of Tabular Data Synthesis Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21512;&#25104;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#21512;&#25104;&#22120;&#65289;&#12290;&#19968;&#20123;&#21512;&#25104;&#22120;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#26088;&#22312;&#20197;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;&#30001;&#20110;&#32570;&#20047;&#21407;&#21017;&#24615;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#23545;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#26368;&#26032;&#30340;&#22522;&#20110;&#36793;&#38469;&#30340;&#21512;&#25104;&#22120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38754;&#23545;&#38754;&#27604;&#36739;&#30340;&#26032;&#24320;&#21457;&#30340;&#21512;&#25104;&#22120;&#30340;&#29702;&#35299;&#23578;&#19981;&#20840;&#38754;&#65292;&#23545;&#36825;&#20123;&#21512;&#25104;&#22120;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#30340;&#20840;&#38754;&#20102;&#35299;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26816;&#26597;&#21644;&#25209;&#35780;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#20854;&#38480;&#21046;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#32452;&#32455;&#26694;&#26550;&#65292;&#20197;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#24182;&#36827;&#34892;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. A large number of tabular data synthesis algorithms (which we call synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to lacking principled evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art marginal-based synthesizers.   In this paper, we present a principled and systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. Based on the proposed metrics, we also devise a un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#20197;&#30495;&#23454;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#20026;&#22522;&#30784;&#65292;&#25171;&#30772;&#20102;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#32473;&#20986;&#20102;&#26080;&#32500;&#24230;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.05576</link><description>&lt;p&gt;
&#25968;&#23383;&#35745;&#31639;&#26426;&#25171;&#30772;&#32500;&#24230;&#35781;&#21650;&#65306;&#36890;&#36807;&#26377;&#38480;&#20960;&#20309;&#30340;&#33258;&#36866;&#24212;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#20197;&#30495;&#23454;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#20026;&#22522;&#30784;&#65292;&#25171;&#30772;&#20102;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#32473;&#20986;&#20102;&#26080;&#32500;&#24230;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#26159;&#24314;&#31435;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#30340;&#21069;&#25552;&#19979;&#65292;&#21363;&#25152;&#26377;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#37117;&#26159;&#26080;&#31351;&#30340;&#65292;&#20363;&#22914;$\mathbb{R}^d$&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#26426;&#22120;&#31934;&#24230;&#12289;&#33293;&#20837;&#21644;&#26377;&#38480;&#30340;&#23384;&#20648;&#31354;&#38388;&#31561;&#25968;&#23383;&#35745;&#31639;&#26426;&#30340;&#38480;&#21046;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#36825;&#20010;&#26680;&#24515;&#20551;&#35774;&#24448;&#24448;&#34987;&#36829;&#32972;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25968;&#23383;&#35745;&#31639;&#26426;&#22312;$\mathbb{R}^d$&#19978;&#25805;&#20316;&#30340;&#26159;&#26377;&#38480;&#30340;&#32593;&#26684;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#31163;&#25955;&#32467;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35745;&#31639;&#26426;&#19978;&#23454;&#29616;&#27169;&#22411;&#26102;&#65292;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#34987;&#31995;&#32479;&#22320;&#25171;&#30772;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#19978;&#23454;&#29616;&#30340;&#26680;&#20989;&#25968;&#21644;&#28145;&#24230;ReLU MLP&#22238;&#24402;&#22120;&#33719;&#24471;&#20102;&#26032;&#30340;&#26080;&#32500;&#24230;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#28176;&#36827;&#27979;&#24230;&#38598;&#20013;&#24615;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#32473;&#20986;&#20102;&#27010;&#29575;&#27979;&#24230;&#21644;&#20854;&#22312;$N$&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#19978;&#30340;&#32463;&#39564;&#29256;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#20026;$1$-Wasserstein&#36317;&#31163;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the foundations of machine learning rely on the idealized premise that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core assumption is systematically violated in practice due to digital computing limitations from finite machine precision, rounding, and limited RAM. In short, digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting these discrete structures, we show the curse of dimensionality in statistical learning is systematically broken when models are implemented on real computers. Consequentially, we obtain new generalization bounds with dimension-free rates for kernel and deep ReLU MLP regressors, which are implemented on real-world machines.   Our results are derived using a new non-asymptotic concentration of measure result between a probability measure over any finite metric space and its empirical version associated with $N$ i.i.d. samples when measured in the $1$-Wasserstein distance. Unlike standard concentration of measure 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04051</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ainsworth&#31561;&#20154;&#23637;&#31034;&#20102;&#20351;&#29992;&#26435;&#37325;&#21305;&#37197;&#65288;WM&#65289;&#26469;&#26368;&#23567;&#21270;&#25490;&#21015;&#25628;&#32034;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;$L_2$&#36317;&#31163;&#26377;&#25928;&#22320;&#35782;&#21035;&#28385;&#36275;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65288;LMC&#65289;&#30340;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#22312;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#31181;&#23376;&#30340;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#32447;&#24615;&#36335;&#24452;&#19978;&#30340;&#25439;&#22833;&#20445;&#25345;&#20960;&#20046;&#24658;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;WM&#25552;&#20379;&#20102;LMC&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#24182;&#19981;&#26174;&#30528;&#20943;&#23569;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;$L_2$&#36317;&#31163;&#65292;&#32780;LMC&#30340;&#20986;&#29616;&#24182;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;WM&#26412;&#36523;&#30340;&#36317;&#31163;&#20943;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#65292;&#34920;&#26126;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#27599;&#23618;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#20027;&#35201;&#25913;&#21464;&#20102;&#26435;&#37325;&#30697;&#38453;&#30340;&#26041;&#21521;&#65292;&#32780;&#19981;&#26159;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;</title><link>https://arxiv.org/abs/2402.03201</link><description>&lt;p&gt;
&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#36827;&#34892;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guidance with Spherical Gaussian Constraint for Conditional Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#21487;&#24494;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#25351;&#23548;&#26469;&#22788;&#29702;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#26679;&#26412;&#36136;&#37327;&#19978;&#20570;&#20986;&#22949;&#21327;&#65292;&#24182;&#38656;&#35201;&#36739;&#23567;&#30340;&#24341;&#23548;&#27493;&#38271;&#65292;&#23548;&#33268;&#37319;&#26679;&#36807;&#31243;&#21464;&#38271;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22312;&#24341;&#23548;&#25439;&#22833;&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#27969;&#24418;&#20559;&#31163;&#30340;&#26681;&#26412;&#38382;&#39064;&#25152;&#22312;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#25439;&#22833;&#24341;&#23548;&#30340;&#20272;&#35745;&#35823;&#24046;&#30340;&#29305;&#23450;&#19979;&#30028;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27969;&#24418;&#20559;&#31163;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#65288;DSG&#65289;&#30340;&#25193;&#25955;&#65292;&#20174;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#38598;&#20013;&#29616;&#35937;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;DSG&#36890;&#36807;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#24341;&#23548;&#27493;&#39588;&#32422;&#26463;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20013;&#28040;&#38500;&#30828;&#26631;&#31614;&#32422;&#26463;&#65292;&#32771;&#34385;&#21040;&#26631;&#31614;&#24179;&#28369;&#21644;mixup&#25216;&#26415;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#24674;&#22797;&#22686;&#24378;&#26631;&#31614;&#21644;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#20026;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#25552;&#20379;&#20102;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.03124</link><description>&lt;p&gt;
&#22312;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20013;&#28040;&#38500;&#30828;&#26631;&#31614;&#32422;&#26463;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20013;&#28040;&#38500;&#30828;&#26631;&#31614;&#32422;&#26463;&#65292;&#32771;&#34385;&#21040;&#26631;&#31614;&#24179;&#28369;&#21644;mixup&#25216;&#26415;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#24674;&#22797;&#22686;&#24378;&#26631;&#31614;&#21644;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#20026;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#25552;&#20379;&#20102;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26088;&#22312;&#20174;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#26292;&#38706;&#30340;&#20013;&#38388;&#26799;&#24230;&#20013;&#37325;&#26500;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#25915;&#20987;&#25104;&#21151;&#65292;&#20294;&#20197;&#24448;&#30340;&#25152;&#26377;&#26041;&#27861;&#65292;&#20174;&#37325;&#26500;&#21333;&#20010;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#25918;&#23485;&#21040;&#25209;&#22788;&#29702;&#32423;&#21035;&#30340;&#21333;&#22270;&#20687;&#38480;&#21046;&#65292;&#37117;&#21482;&#22312;&#30828;&#26631;&#31614;&#32422;&#26463;&#19979;&#36827;&#34892;&#27979;&#35797;&#12290;&#21363;&#20351;&#23545;&#20110;&#21333;&#22270;&#20687;&#37325;&#24314;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#30340;&#31639;&#27861;&#26469;&#24674;&#22797;&#22686;&#24378;&#30340;&#36719;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20174;&#25193;&#22823;&#25209;&#37327;&#22823;&#23567;&#36716;&#21521;&#30740;&#31350;&#30828;&#26631;&#31614;&#32422;&#26463;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26631;&#31614;&#24179;&#28369;&#21644;mixup&#25216;&#26415;&#30340;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21516;&#26102;&#20174;&#21333;&#36755;&#20837;&#26799;&#24230;&#20013;&#24674;&#22797;&#30495;&#23454;&#30340;&#22686;&#24378;&#26631;&#31614;&#21644;&#26368;&#21518;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#20026;&#20219;&#20309;&#22522;&#20110;&#20998;&#26512;&#30340;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#25552;&#20379;&#20102;&#24517;&#35201;&#26465;&#20214;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to 
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.01643</link><description>&lt;p&gt;
L-TUNING&#65306;&#29992;&#20110;LLMs&#20013;&#30340;&#25552;&#31034;&#21644;&#21069;&#32512;&#30340;&#21516;&#27493;&#26631;&#31614;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20219;&#24847;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#24182;&#19988;&#36890;&#29992;&#26631;&#35760;&#22312;&#21508;&#31181;&#31867;&#21035;&#26631;&#31614;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;L-Tuning&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#35774;&#35745;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;L-Tuning&#19987;&#27880;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;LLM&#22788;&#29702;&#30340;&#26631;&#31614;&#26631;&#35760;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#21033;&#29992;&#20854;&#39044;&#20808;&#23384;&#22312;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36824;&#20419;&#36827;&#20102;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#19981;&#21516;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;L-Tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#23398;&#20064;&#30340;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#23383;&#20856;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#23618;&#26512;-&#26368;&#23567;&#20108;&#20056;&#27861;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.00310</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#26631;&#31614;&#23398;&#20064;&#30340;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Seismic Traveltime Tomography with Label-free Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#23398;&#20064;&#30340;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#23383;&#20856;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#23618;&#26512;-&#26368;&#23567;&#20108;&#20056;&#27861;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#22320;&#38663;&#36208;&#26102;&#23618;&#26512;&#25104;&#20687;&#20013;&#34987;&#24212;&#29992;&#20110;&#26500;&#24314;&#36895;&#24230;&#27169;&#22411;&#65288;VMs&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#65288;&#21363;&#36755;&#20837;&#21644;&#26631;&#31614;&#30340;&#23545;&#24212;&#65289;&#65292;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#35757;&#32451;&#65292;&#32780;&#29616;&#23454;&#25968;&#25454;&#21453;&#28436;&#30340;&#30495;&#23454;&#26631;&#31614;&#36890;&#24120;&#32570;&#22833;&#25110;&#38750;&#24120;&#26114;&#36149;&#12290;&#19968;&#20123;&#20256;&#32479;&#23618;&#26512;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#23454;&#26045;&#65292;&#20294;&#20854;&#25928;&#26524;&#36890;&#24120;&#21463;&#21040;&#20808;&#39564;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#36991;&#20813;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#23383;&#20856;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#23618;&#26512;-&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;LSQR&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;VMs&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#27973;&#23618;&#31616;&#21333;&#30340;NN&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#31574;&#30053;&#26469;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#30340;VMs&#65306;&#65288;1&#65289;&#39044;&#28909;&#38454;&#27573;&#12290;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#65292;&#20174;LSQR&#20272;&#35745;&#20013;&#35757;&#32451;&#20986;&#21021;&#22987;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques have been used to build velocity models (VMs) for seismic traveltime tomography and have shown encouraging performance in recent years. However, they need to generate labeled samples (i.e., pairs of input and label) to train the deep neural network (NN) with end-to-end learning, and the real labels for field data inversion are usually missing or very expensive. Some traditional tomographic methods can be implemented quickly, but their effectiveness is often limited by prior assumptions. To avoid generating labeled samples, we propose a novel method by integrating deep learning and dictionary learning to enhance the VMs with low resolution by using the traditional tomography-least square method (LSQR). We first design a type of shallow and simple NN to reduce computational cost followed by proposing a two-step strategy to enhance the VMs with low resolution: (1) Warming up. An initial dictionary is trained from the estimation by LSQR through dictionary learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17542</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#21512;&#21307;&#23398;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Effective Learning: A Comprehensive Medical Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26088;&#22312;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#20854;&#28041;&#21450;&#20851;&#27880;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;&#30340;&#31574;&#30053;&#65292;&#30830;&#20445;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#20449;&#24687;&#20215;&#20540;&#12290;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#22312;&#21152;&#24555;AI&#35757;&#32451;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33410;&#30465;&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#22312;&#36817;&#24180;&#26469;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#36229;&#20986;&#20102;&#35768;&#22810;&#20154;&#30340;&#39044;&#26399;&#26102;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21644;&#32508;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;31&#20010;&#21307;&#30103;&#20013;&#24515;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;(DataDEL)&#65292;&#29992;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#26041;&#27861;(MedDEL)&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#23458;&#35266;&#34913;&#37327;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#24615;&#33021;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;(NormDEL)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#22312;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2401.17541</link><description>&lt;p&gt;
&#36879;&#36807;&#26657;&#20934;&#30340;&#35270;&#35282;&#29702;&#35299;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#27979;&#35797;&#20998;&#24067;&#24448;&#24448;&#19982;&#35757;&#32451;&#19981;&#21516;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#36234;&#22495;&#27867;&#21270;&#65292;&#22312;&#24120;&#35268;&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#12290;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#20316;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#26088;&#22312;&#35782;&#21035;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#36234;&#22495;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;IRM&#30340;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#26159;&#20854;&#21452;&#23618;&#20248;&#21270;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#20123;&#36817;&#20284;IRM&#25216;&#26415;&#65292;&#20351;&#29992;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#12290;ECE&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#23427;&#26159;&#34913;&#37327;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#25429;&#25417;&#21040;&#29615;&#22659;&#19981;&#21464;&#29305;&#24449;&#30340;&#25351;&#26631;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20102;...&#65288;&#25509;&#19979;&#37096;&#20998;&#25688;&#35201;&#36229;&#36807;200&#23383;&#65292;&#25552;&#21462;&#21069;200&#23383;&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses
&lt;/p&gt;</description></item><item><title>SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.18376</link><description>&lt;p&gt;
SQLformer&#65306;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18376
&lt;/p&gt;
&lt;p&gt;
SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#25552;&#21462;&#27665;&#20027;&#21270;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#20027;&#35201;&#38556;&#30861;&#21253;&#25324;&#39046;&#22495;&#27867;&#21270;&#65292;&#21363;&#36866;&#24212;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25968;&#25454;&#24211;&#65292;&#24182;&#19988;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#30456;&#24212;&#30340;SQL&#26597;&#35810;&#23545;&#40784;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SQLformer&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25191;&#34892;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#39044;&#27979;SQL&#26597;&#35810;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23618;&#20013;&#32467;&#21512;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#24211;&#34920;&#21644;&#21015;&#36873;&#25321;&#24341;&#23548;&#30340;&#65292;&#26377;&#21161;&#20110;&#35299;&#30721;&#22120;&#20197;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#35268;&#33539;&#39034;&#24207;&#29983;&#25104;SQL&#26597;&#35810;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#29616;&#38454;&#27573;&#30340;&#25216;&#26415;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;</title><link>https://arxiv.org/abs/2303.03751</link><description>&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#36935;&#21040;&#20154;&#24037;&#21453;&#39304;&#65306;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#23454;&#29616;&#21487;&#35777;&#26126;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.03751
&lt;/p&gt;
&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#30340;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;-&#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#65292;&#29305;&#21035;&#26159;&#24403;&#20989;&#25968;&#30001;&#20154;&#31867;&#35780;&#21028;&#21592;&#35780;&#20272;&#26102;&#12290;&#36825;&#31181;&#25361;&#25112;&#21463;&#21040;&#20102;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#24037;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#29992;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;ZO-RankSGD&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.03751v2 Announce Type: replace-cross  Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;</title><link>https://arxiv.org/abs/2110.08902</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction Based Experience Replay for Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08902
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26102;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#26159;&#24456;&#26377;&#24517;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#32463;&#39564;&#22238;&#25918;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#26159;&#23558;&#25152;&#26377;&#35266;&#27979;&#37117;&#35270;&#20026;&#30456;&#21516;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24046;&#20943;&#23569;&#32463;&#39564;&#22238;&#25918;&#65288;VRER&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#26679;&#26412;&#30340;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;VRER&#20316;&#20026;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20013;&#65292;&#26500;&#24314;&#20102;&#25105;&#20204;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;Policy Optimization with VRER (PG-VRER)&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#23545;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#32771;&#34385;&#26679;&#26412;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc
&lt;/p&gt;</description></item><item><title>H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.16818</link><description>&lt;p&gt;
H2O-Danube-1.8B &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B Technical Report. (arXiv:2401.16818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16818
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; H2O-Danube-1.8B&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#36981;&#24490; LLama 2 &#21644; Mistral &#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#21644;&#25913;&#36827;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#24635;&#26631;&#35760;&#25968;&#37327;&#26126;&#26174;&#23569;&#20110;&#30456;&#20284;&#35268;&#27169;&#30340;&#21442;&#32771;&#27169;&#22411;&#65292;&#20294;&#23427;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;&#25105;&#20204;&#20197; Apache 2.0 &#35768;&#21487;&#35777;&#23558; H2O-Danube-1.8B &#24320;&#25918;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160; LLMs &#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#65292;&#35753;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#25919;&#31574;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65288;FedSARSA&#65289;&#65292;&#21033;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26469;&#35299;&#20915;&#39532;&#23572;&#21487;&#22827;&#21462;&#26679;&#12289;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.15273</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#30340;&#25919;&#31574;&#24322;&#26500;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning. (arXiv:2401.15273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#25919;&#31574;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65288;FedSARSA&#65289;&#65292;&#21033;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26469;&#35299;&#20915;&#39532;&#23572;&#21487;&#22827;&#21462;&#26679;&#12289;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#19981;&#21516;&#20195;&#29702;&#30340;&#20449;&#24687;&#26469;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#21069;&#26223;&#20809;&#26126;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#27599;&#20010;&#20195;&#29702;&#19982;&#19968;&#20010;&#21487;&#33021;&#19981;&#21516;&#30340;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#20851;&#20110;FRL&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#29702;&#35770;&#19978;&#30340;&#20102;&#35299;&#12290;&#36825;&#31181;&#32467;&#26524;&#30340;&#32570;&#20047;&#21487;&#20197;&#24402;&#22240;&#20110;&#21508;&#31181;&#25216;&#26415;&#25361;&#25112;&#21450;&#20854;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#39532;&#23572;&#21487;&#22827;&#21462;&#26679;&#12289;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#33410;&#30465;&#36890;&#20449;&#12289;&#20195;&#29702;&#30340;MDP&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#26680;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#36830;&#32493;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#25919;&#31574;&#19978;&#30340;&#35774;&#32622;&#20013;&#65292;&#34892;&#20026;&#25919;&#31574;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#36827;&#19968;&#27493;&#20351;&#20998;&#26512;&#22797;&#26434;&#21270;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedSARSA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#32852;&#37030;&#25919;&#31574;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-ti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31163;&#31574;&#30053;&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20445;&#23432;&#31574;&#30053;&#20248;&#21270;&#21644;&#23616;&#37096;&#31574;&#30053;&#20984;&#21270;&#26469;&#35299;&#20915;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#35823;&#24046;&#23548;&#33268;&#30340;&#23433;&#20840;&#32422;&#26463;&#19981;&#28385;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14758</link><description>&lt;p&gt;
Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG]) &#35770;&#25991;&#30340;&#39064;&#30446;&#26159;&#65306;&#31163;&#31574;&#30053;&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31163;&#31574;&#30053;&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20445;&#23432;&#31574;&#30053;&#20248;&#21270;&#21644;&#23616;&#37096;&#31574;&#30053;&#20984;&#21270;&#26469;&#35299;&#20915;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#35823;&#24046;&#23548;&#33268;&#30340;&#23433;&#20840;&#32422;&#26463;&#19981;&#28385;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#21452;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#31574;&#30053;&#30340;&#21407;&#22987;&#26356;&#26032;&#21644;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#30340;&#23545;&#20598;&#26356;&#26032;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#12290;&#30001;&#20110;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#20316;&#20026;&#36830;&#25509;&#21407;&#22987;&#21644;&#23545;&#20598;&#26356;&#26032;&#36807;&#31243;&#30340;&#20851;&#38190;&#32852;&#31995;&#65292;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#26497;&#26131;&#21463;&#21040;&#32047;&#31215;&#25104;&#26412;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#31163;&#31574;&#30053;&#26041;&#27861;&#20351;&#29992;&#26102;&#25104;&#26412;&#34987;&#20005;&#37325;&#20302;&#20272;&#65292;&#26080;&#27861;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20445;&#23432;&#31574;&#30053;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#32422;&#26463;&#28385;&#36275;&#30340;&#21306;&#22495;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#25552;&#39640;&#20102;&#32422;&#26463;&#30340;&#28385;&#36275;&#24615;&#65292;&#20294;&#20063;&#21487;&#33021;&#38459;&#30861;&#20102;&#22870;&#21169;&#26368;&#22823;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23616;&#37096;&#31574;&#30053;&#20984;&#21270;&#8221;&#26469;&#21161;&#20110;&#28040;&#38500;&#36825;&#31181;&#27425;&#20248;&#24615;&#65292;&#36880;&#28176;&#20943;&#23567;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#25104;&#20998;&#30340;&#32852;&#21512;&#20316;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose \textit{conservative policy optimization}, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce \textit{local policy convexification} to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25581;&#31034;&#25209;&#27425;&#24402;&#19968;&#21270;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;TEMA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#35757;&#32451;&#21644;&#27979;&#35797;&#25209;&#27425;&#20043;&#38388;&#30340;&#31867;&#21035;&#22810;&#26679;&#24615;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2312.09486</link><description>&lt;p&gt;
&#25581;&#31034;&#29992;&#20110;&#30495;&#23454;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#25209;&#27425;&#24402;&#19968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unraveling Batch Normalization for Realistic Test-Time Adaptation. (arXiv:2312.09486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25581;&#31034;&#25209;&#27425;&#24402;&#19968;&#21270;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;TEMA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#35757;&#32451;&#21644;&#27979;&#35797;&#25209;&#27425;&#20043;&#38388;&#30340;&#31867;&#21035;&#22810;&#26679;&#24615;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#25209;&#27425;&#24402;&#19968;&#21270;&#26469;&#20943;&#23567;&#39046;&#22495;&#24046;&#24322;&#65292;&#20294;&#26159;&#24403;&#20351;&#29992;&#30495;&#23454;&#30340;&#23567;&#25209;&#37327;&#26102;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20250;&#38477;&#20302;&#65292;&#22240;&#20026;&#30446;&#26631;&#20272;&#35745;&#19981;&#20934;&#30830;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#23581;&#35797;&#20165;&#20165;&#26159;&#24341;&#20837;&#28304;&#32479;&#35745;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#27492;&#30446;&#26631;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#20351;&#24471;&#27979;&#35797;&#26102;&#39046;&#22495;&#21464;&#21270;&#38382;&#39064;&#26410;&#35299;&#20915;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#38477;&#32423;&#38382;&#39064;&#12290;&#36890;&#36807;&#25581;&#31034;&#25209;&#27425;&#24402;&#19968;&#21270;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#20934;&#30830;&#30340;&#30446;&#26631;&#32479;&#35745;&#20027;&#35201;&#26469;&#33258;&#20110;&#25209;&#27425;&#20013;&#31867;&#21035;&#22810;&#26679;&#24615;&#30340;&#22823;&#24133;&#20943;&#23569;&#12290;&#26681;&#25454;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30452;&#25509;&#30340;&#24037;&#20855;&#8212;&#8212;&#27979;&#35797;&#26102;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;TEMA&#65289;&#65292;&#26469;&#24357;&#34917;&#35757;&#32451;&#21644;&#27979;&#35797;&#25209;&#27425;&#20043;&#38388;&#31867;&#21035;&#22810;&#26679;&#24615;&#30340;&#24046;&#36317;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;TEMA&#21487;&#36866;&#24212;&#22320;&#25193;&#23637;&#20102;&#20856;&#22411;&#26041;&#27861;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#25209;&#27425;&#30340;&#33539;&#22260;&#65292;&#20197;&#21253;&#21547;&#19968;&#20010;&#22810;&#26679;&#30340;&#31867;&#21035;&#20449;&#24687;&#38598;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent test-time adaptations exhibit efficacy by adjusting batch normalization to narrow domain disparities, their effectiveness diminishes with realistic mini-batches due to inaccurate target estimation. As previous attempts merely introduce source statistics to mitigate this issue, the fundamental problem of inaccurate target estimation still persists, leaving the intrinsic test-time domain shifts unresolved. This paper delves into the problem of mini-batch degradation. By unraveling batch normalization, we discover that the inexact target statistics largely stem from the substantially reduced class diversity in batch. Drawing upon this insight, we introduce a straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge the class diversity gap between training and testing batches. Importantly, our TEMA adaptively extends the scope of typical methods beyond the current batch to incorporate a diverse set of class information, which in turn boosts an accurate targe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01588</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#32422;&#26463;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#65288;&#22914;&#21151;&#29575;&#35889;&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#22797;&#26434;&#23431;&#23449;&#23398;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#25311;&#22871;&#20214;&#20013;&#30340;&#23376;&#32593;&#26684;&#29289;&#29702;&#23454;&#29616;&#21644;&#25968;&#20540;&#36924;&#36817;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;&#22312;&#21478;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20250;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#24212;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#22871;&#20214;&#30340;CAMELS&#27700;&#21160;&#21147;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DA-GNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25429;&#25417;&#26469;&#33258;&#26143;&#31995;&#20998;&#24067;&#30340;&#32467;&#26500;&#26080;&#26631;&#24230;&#23431;&#23449;&#23398;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21253;&#25324;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#37197;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09270</link><description>&lt;p&gt;
Retro-fallback: &#38754;&#21521;&#19981;&#30830;&#23450;&#19990;&#30028;&#30340;&#36870;&#21512;&#25104;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#26159;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#21270;&#23398;&#21453;&#24212;&#20174;&#26356;&#31616;&#21333;&#12289;&#21487;&#36141;&#20080;&#30340;&#20998;&#23376;&#21019;&#24314;&#25152;&#38656;&#20998;&#23376;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#31639;&#27861;&#26469;&#23547;&#25214;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#65288;&#20363;&#22914;&#26368;&#30701;&#36335;&#24452;&#12289;&#26368;&#20302;&#25104;&#26412;&#65289;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#25105;&#20204;&#23545;&#21487;&#33021;&#21453;&#24212;&#31354;&#38388;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#65292;&#36825;&#24847;&#21619;&#30528;&#31639;&#27861;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#26080;&#27861;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#36870;&#21512;&#25104;&#26032;&#39062;&#34920;&#36848;&#65292;&#20197;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#31639;&#27861;&#31216;&#20026; Retro-fallback&#65292;&#26368;&#22823;&#21270;&#33267;&#23569;&#26377;&#19968;&#31181;&#21512;&#25104;&#35745;&#21010;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#25191;&#34892;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126; Retro-fallback &#36890;&#24120;&#29983;&#25104;&#27604;&#27969;&#34892;&#30340; MCTS &#21644; retro* &#31639;&#27861;&#26356;&#22909;&#30340;&#19968;&#32452;&#21512;&#25104;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#25903;&#25345;&#20998;&#24067;&#24335;&#20869;&#23384;&#27169;&#22411;&#19979;&#30340;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#30340;&#25299;&#25169;&#31639;&#27861;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#24615;&#33021;&#20998;&#26512;&#21644;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.08339</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#24067;&#24335;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#30340;&#36890;&#29992;&#36719;&#20214;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generic Software Framework for Distributed Topological Analysis Pipelines. (arXiv:2310.08339v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#25903;&#25345;&#20998;&#24067;&#24335;&#20869;&#23384;&#27169;&#22411;&#19979;&#30340;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#30340;&#25299;&#25169;&#31639;&#27861;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#24615;&#33021;&#20998;&#26512;&#21644;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#20998;&#24067;&#24335;&#20869;&#23384;&#27169;&#22411;&#19979;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#30340;&#36890;&#29992;&#36719;&#20214;&#26694;&#26550;&#12290;&#19982;&#26368;&#36817;&#30340;&#19968;&#20123;&#35770;&#25991;&#38024;&#23545;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#24341;&#20837;&#20102;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#35770;&#25991;&#25253;&#21578;&#30340;&#23454;&#39564;&#32467;&#26524;&#26159;&#36890;&#36807;&#23450;&#21046;&#30340;&#21333;&#19968;&#31639;&#27861;&#23454;&#29616;&#24471;&#21040;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25551;&#36848;&#20102;&#19968;&#20010;&#36890;&#29992;&#12289;&#27867;&#21270;&#30340;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#26694;&#26550;&#65292;&#21363;&#19968;&#31995;&#21015;&#30456;&#20114;&#20316;&#29992;&#30340;&#25299;&#25169;&#31639;&#27861;&#65292;&#21487;&#33021;&#22312;&#19981;&#21516;&#30340;&#36827;&#31243;&#25968;&#37327;&#19978;&#36816;&#34892;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;Topology ToolKit (TTK)&#20013;&#20351;&#29992;MPI&#27169;&#22411;&#26469;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#22312;&#24320;&#21457;&#36825;&#20010;&#26694;&#26550;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#19968;&#20123;&#31639;&#27861;&#21644;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#27492;&#36827;&#34892;&#20102;&#35760;&#24405;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;TTK&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#20869;&#23384;&#25299;&#25169;&#31639;&#27861;&#20998;&#31867;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;MPI+&#32447;&#31243;&#24182;&#34892;&#21270;&#30340;&#31034;&#20363;&#12290;&#35814;&#32454;&#30340;&#24615;&#33021;&#20998;&#26512;&#26174;&#31034;&#65292;p
&lt;/p&gt;
&lt;p&gt;
This system paper presents a software framework for the support of topological analysis pipelines in a distributed-memory model. While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a general-purpose, generic framework for topological analysis pipelines, i.e. a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. Specifically, we instantiated our framework with the MPI model, within the Topology ToolKit (TTK). While developing this framework, we faced several algorithmic and software engineering challenges, which we document in this paper. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.07852</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#39640;&#32500;&#31169;&#26377;&#27169;&#22411;&#36873;&#25321;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24046;&#20998;&#38544;&#31169;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25928;&#29992;&#20445;&#35777;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20026;&#20154;&#30693;&#30340;&#25351;&#25968;&#26426;&#21046;&#26469;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20854;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#25351;&#25968;&#26426;&#21046;&#30340;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#36827;&#34892;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#22312;&#38382;&#39064;&#21442;&#25968;$n$&#12289;$p$&#21644;$s$&#20013;&#24314;&#31435;&#20102;&#20854;&#21040;&#31283;&#24577;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#20854;&#28151;&#21512;&#24615;&#36136;&#24314;&#31435;&#20102;Metropolis-Hastings&#38543;&#26426;&#34892;&#36208;&#30340;&#26368;&#32456;&#20272;&#35745;&#30340;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#35828;&#26126;&#24615;&#27169;&#25311;&#65292;&#21360;&#35777;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26041;&#27861;&#65292;&#21487;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#28436;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;ODE&#26469;&#25429;&#25417;&#32593;&#32476;&#21270;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#28789;&#27963;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#35774;&#35745;&#30340;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21382;&#21490;&#21644;&#20854;&#20182;&#29615;&#22659;&#20013;&#38598;&#25104;&#26368;&#20339;&#31574;&#30053;&#65292;&#23454;&#29616;&#30693;&#35782;&#30340;&#39640;&#25928;&#36716;&#31227;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2310.04159</link><description>&lt;p&gt;
&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Amortized Network Intervention to Steer the Excitatory Point Processes. (arXiv:2310.04159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26041;&#27861;&#65292;&#21487;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#28436;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;ODE&#26469;&#25429;&#25417;&#32593;&#32476;&#21270;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#28789;&#27963;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#35774;&#35745;&#30340;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21382;&#21490;&#21644;&#20854;&#20182;&#29615;&#22659;&#20013;&#38598;&#25104;&#26368;&#20339;&#31574;&#30053;&#65292;&#23454;&#29616;&#30693;&#35782;&#30340;&#39640;&#25928;&#36716;&#31227;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#32593;&#32476;&#24178;&#39044;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#65288;&#22914;&#20256;&#26579;&#30149;&#20256;&#25773;&#25110;&#20132;&#36890;&#25317;&#22581;&#25511;&#21046;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;ODE&#26469;&#25429;&#25417;&#32593;&#32476;&#21270;&#30340;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#22312;&#32593;&#32476;&#25299;&#25169;&#30340;&#26102;&#21464;&#21464;&#21270;&#19979;&#23558;&#22914;&#20309;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;GD-MPC&#65289;&#65292;&#25552;&#20379;&#31574;&#30053;&#28789;&#27963;&#24615;&#20197;&#36866;&#24212;&#20808;&#21069;&#30693;&#35782;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#21010;&#30340;&#22797;&#26434;&#24615;&#24182;&#20811;&#26381;&#27492;&#31867;&#20915;&#31574;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#39640;&#32500;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#65288;ANI&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#20174;&#21382;&#21490;&#21644;&#20854;&#20182;&#29615;&#22659;&#20013;&#27719;&#32858;&#26368;&#20339;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#25490;&#21015;&#31561;&#25928;&#24615;&#12290;&#36825;&#31181;&#24615;&#36136;&#23454;&#29616;&#20102;&#30693;&#35782;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#36716;&#31227;&#21644;&#20849;&#20139;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20174;&#25511;&#21046;&#20256;&#26579;&#30149;&#20256;&#25773;&#21040;&#20943;&#23569;&#30899;&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the challenge of large-scale network intervention for guiding excitatory point processes, such as infectious disease spread or traffic congestion control. Our model-based reinforcement learning utilizes neural ODEs to capture how the networked excitatory point processes will evolve subject to the time-varying changes in network topology. Our approach incorporates Gradient-Descent based Model Predictive Control (GD-MPC), offering policy flexibility to accommodate prior knowledge and constraints. To address the intricacies of planning and overcome the high dimensionality inherent to such decision-making problems, we design an Amortize Network Interventions (ANI) framework, allowing for the pooling of optimal policies from history and other contexts, while ensuring a permutation equivalent property. This property enables efficient knowledge transfer and sharing across diverse contexts. Our approach has broad applications, from curbing infectious disease spread to reducing carbon
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#20026;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;FMs&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#39640;&#21487;&#33021;&#32473;FL-enabled&#30340;&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.04003</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Federated Learning in a Wireless World with Foundation Models. (arXiv:2310.04003v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04003
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#20026;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;FMs&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#39640;&#21487;&#33021;&#32473;FL-enabled&#30340;&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26159;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#26368;&#36817;&#20026;&#22810;&#20010;&#20840;&#26032;&#30340;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;FMs&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#30340;&#24895;&#26223;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32972;&#26223;&#65292;&#20854;&#20013;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#20998;&#24067;&#24335;&#32593;&#32476;&#26234;&#33021;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#30446;&#21069;&#65292;FMs&#21644;FL&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;FMs&#21487;&#20197;&#25552;&#39640;FL&#30340;&#24615;&#33021;&#65292;&#32780;FL&#20063;&#21487;&#20197;&#21033;&#29992;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#36741;&#21161;&#35757;&#32451;FMs&#12290;&#28982;&#32780;&#65292;FMs&#23545;&#35745;&#31639;&#36164;&#28304;&#12289;&#23384;&#20648;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#35201;&#27714;&#24322;&#24120;&#39640;&#65292;&#36825;&#32473;FL-enabled&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;FMs&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#26159;&#21542;&#36866;&#29992;&#20110;FL&#65292;&#21253;&#25324;&#23545;&#30740;&#31350;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#20221;FL&#21644;FL&#36164;&#28304;&#30340;&#38656;&#27714;&#30340;&#32852;&#21512;&#35757;&#32451;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) are general-purpose artificial intelligence (AI) models that have recently enabled multiple brand-new generative AI applications. The rapid advances in FMs serve as an important contextual backdrop for the vision of next-generation wireless networks, where federated learning (FL) is a key enabler of distributed network intelligence. Currently, the exploration of the interplay between FMs and FL is still in its nascent stage. Naturally, FMs are capable of boosting the performance of FL, and FL could also leverage decentralized data and computing resources to assist in the training of FMs. However, the exceptionally high requirements that FMs have for computing resources, storage, and communication overhead would pose critical challenges to FL-enabled wireless networks. In this article, we explore the extent to which FMs are suitable for FL over wireless networks, including a broad overview of research challenges and opportunities. In particular, we discuss multip
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.03302</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23454;&#39564;&#28041;&#21450;&#21019;&#24314;&#20551;&#35774;&#12289;&#35774;&#35745;&#23454;&#39564;&#12289;&#36816;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#32467;&#26524;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;AI&#30740;&#31350;&#20195;&#29702;&#26469;&#25191;&#34892;&#36825;&#20123;&#38271;&#26399;&#30446;&#26631;&#30340;&#20219;&#21153;&#21602;&#65311;&#20026;&#20102;&#26397;&#30528;&#22312;&#27492;&#31867;&#24320;&#25918;&#24615;&#20915;&#31574;&#20219;&#21153;&#19978;&#26500;&#24314;&#21644;&#35780;&#20272;&#30740;&#31350;&#20195;&#29702;&#30340;&#30446;&#26631;&#36808;&#20986;&#19968;&#27493;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#26426;&#22120;&#23398;&#20064;&#24037;&#31243;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#19968;&#20010;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#12290;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#35835;&#20889;&#25991;&#20214;&#12289;&#25191;&#34892;&#20195;&#30721;&#21644;&#26816;&#26597;&#36755;&#20986;&#31561;&#21160;&#20316;&#12290;&#36890;&#36807;&#36825;&#20123;&#21160;&#20316;&#65292;&#20195;&#29702;&#21487;&#20197;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#12289;&#26550;&#26500;&#12289;&#35757;&#32451;&#36807;&#31243;&#31561;&#12290;&#28982;&#21518;&#65292;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#29702;&#22312;&#19982;&#24615;&#33021;&#21644;&#25928;&#29575;&#30456;&#20851;&#30340;&#21508;&#31181;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;LLM-
&lt;/p&gt;
&lt;p&gt;
Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02579</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#36798;&#20301;&#32622;&#32534;&#30721;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#22270;&#20301;&#32622;&#32534;&#30721;&#23545;&#26500;&#24314;&#24378;&#22823;&#30340;&#22270;&#36716;&#25442;&#22120;&#21644;&#22686;&#24378;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#20851;&#38190;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#65292;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#65288;1&#65289;\emph{&#38750;&#21807;&#19968;&#24615;}&#65306;&#21516;&#19968;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#35299;&#65292;&#20197;&#21450;&#65288;2&#65289;\emph{&#19981;&#31283;&#23450;&#24615;}&#65306;&#23545;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23548;&#33268;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21464;&#21270;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#23581;&#35797;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;"&#30828;&#20998;&#21106;"&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#65288;SPE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#29305;&#24449;&#21521;&#37327;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#24449;&#20540;&#23558;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#12290;SPE&#26159;&#39318;&#20010;&#65288;1&#65289;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26222;&#36866;&#22320;&#25552;&#21319;&#22270;&#32467;&#26500;&#27867;&#21270;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.15395</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;CMDPs&#20013;&#65292;&#26080;&#27169;&#22411;&#12289;&#36951;&#25022;&#26368;&#20248;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65288;BPI&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#20302;&#36951;&#25022;&#24182;&#19988;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#26368;&#20248;&#31574;&#30053;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;CMDPs&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#22312;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#36829;&#32422;&#26102;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#21482;&#22312;&#20174;&#20197;&#21069;&#20351;&#29992;&#30340;&#31574;&#30053;&#20013;&#38543;&#26426;&#22343;&#21248;&#25277;&#26679;&#26102;&#25552;&#20379;&#24179;&#22343;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRUNING-REFINEMENT-IDENTIFICATION&#65288;PRI&#65289;&#65292;&#22522;&#20110;&#25105;&#20204;&#21457;&#29616;&#30340;CMDPs&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26500;&#24615;&#36136;&#65292;&#31216;&#20026;&#26377;&#38480;&#38543;&#26426;&#24615;&#12290;&#35813;&#23646;&#24615;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;N&#32422;&#26463;&#30340;CMDP&#65292;&#23384;&#22312;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#33267;&#22810;&#26377;N&#20010;&#38543;&#26426;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#35782;&#21035;&#20986;&#22312;&#21738;&#20010;&#27493;&#39588;&#21644;&#21738;&#20010;&#29366;&#24577;&#38656;&#35201;&#36827;&#34892;&#38543;&#26426;&#20915;&#31574;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#20915;&#31574;&#30340;&#20998;&#24067;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10105</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#25512;&#29702;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Understanding Catastrophic Forgetting in Language Models via Implicit Inference. (arXiv:2309.10105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#65288;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25110;&#20174;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#65289;&#26159;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#40065;&#26834;&#22320;&#25191;&#34892;&#25152;&#38656;&#20219;&#21153;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#30340;&#31995;&#32479;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#29421;&#31364;&#30340;&#24494;&#35843;&#20998;&#24067;&#20043;&#22806;&#30340;&#20219;&#21153;&#19978;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20869;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#30340;&#21516;&#26102;&#65292;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36864;&#21270;&#22312;&#19982;&#24494;&#35843;&#20998;&#24067;&#8220;&#26368;&#25509;&#36817;&#8221;&#30340;&#20219;&#21153;&#20013;&#23588;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20986;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20027;&#35201;&#20559;&#21521;&#20110;&#24494;&#35843;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#65292;&#20197;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#20197;&#26597;&#30475;&#26159;&#21542;&#21487;&#20197;&#24674;&#22797;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#20849;&#36717;&#25552;&#31034;&#20250;&#20154;&#20026;&#22320;&#20351;&#20219;&#21153;&#30475;&#36215;&#26469;&#19982;&#24494;&#35843;&#20998;&#24067;&#36739;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. This degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities. Conjugate prompting artificially makes the task look farther from the fine-tun
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06089</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#33539;&#24335;&#20013;&#27979;&#37327;&#28798;&#38590;&#24615;&#36951;&#24536;&#65306;&#25506;&#32034;&#35843;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#26159;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19982;&#38646;&#23556;&#21644;&#20840;&#23556;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20316;&#20026;&#24494;&#35843;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21442;&#25968;&#25928;&#29575;&#36866;&#37197;&#22120;&#26041;&#27861;&#19982;&#25152;&#26377;&#21442;&#25968;&#24494;&#35843;&#12290;&#20316;&#20026;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#27599;&#20010;&#35821;&#35328;&#20381;&#27425;&#30340;&#20013;&#38388;&#35757;&#32451;&#65288;IT&#65289;&#21644;&#22312;&#24494;&#35843;&#30340;&#39564;&#35777;&#38454;&#27573;&#24050;&#32463;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#39564;&#35777;&#65288;CLV&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#20197;&#21450;&#28304;&#35821;&#35328;&#20013;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#32780;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#65292;&#21363;&#22312;&#23398;&#20064;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#26032;&#20449;&#24687;&#26102;&#20043;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#25439;&#22833;&#20102;&#22810;&#23569;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#20135;&#21697;&#35780;&#35770;&#65292;&#20998;&#21035;&#21253;&#21547;&#20102;&#22810;&#20010;&#35821;&#31181;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several lang
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21387;&#32553;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20869;&#35757;&#32451;&#20840;&#31209;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#20445;&#35777;&#20102;&#22312;&#19981;&#20381;&#36182;&#29615;&#22659;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#21487;&#20197;&#34987;&#25511;&#21046;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.05751</link><description>&lt;p&gt;
&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Intrinsic Dimension on Metric Learning under Compression. (arXiv:2309.05751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21387;&#32553;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20869;&#35757;&#32451;&#20840;&#31209;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#20445;&#35777;&#20102;&#22312;&#19981;&#20381;&#36182;&#29615;&#22659;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#21487;&#20197;&#34987;&#25511;&#21046;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#26088;&#22312;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#36866;&#24403;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#36317;&#31163;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#24230;&#37327;&#23398;&#20064;&#36824;&#21487;&#20197;&#20316;&#20026;&#38477;&#32500;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#23545;&#23398;&#20064;&#30340;&#24230;&#37327;&#26045;&#21152;&#19968;&#20010;&#20302;&#31209;&#32422;&#26463;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#30340;&#26159;&#23545;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#21387;&#32553;&#29256;&#26412;&#65292;&#28982;&#21518;&#22312;&#20854;&#20013;&#35757;&#32451;&#19968;&#20010;&#20840;&#31209;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20851;&#20110;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#19981;&#20381;&#36182;&#20110;&#29615;&#22659;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#36793;&#30028;&#38500;&#20102;&#23545;&#26469;&#33258;&#26377;&#30028;&#25903;&#25345;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#27809;&#26377;&#26174;&#24335;&#30340;&#20551;&#35774;&#20043;&#22806;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#33258;&#21160;&#25910;&#25947;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;Bandit&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#38544;&#31169;&#39044;&#31639;&#19979;&#30340;&#38590;&#24230;&#21306;&#22495;&#65292;&#24182;&#21457;&#29616;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#27604;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#26356;&#26377;&#25928;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00557</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#21644;&#38598;&#20013;&#24335;&#24046;&#20998;&#38544;&#31169;&#22312;Bandit&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interactive and Concentrated Differential Privacy for Bandits. (arXiv:2309.00557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;Bandit&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#38544;&#31169;&#39044;&#31639;&#19979;&#30340;&#38590;&#24230;&#21306;&#22495;&#65292;&#24182;&#21457;&#29616;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#27604;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#26356;&#26377;&#25928;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bandit&#38382;&#39064;&#22312;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#26696;&#21644;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#25935;&#24863;&#30340;&#29992;&#25143;&#25968;&#25454;&#65292;&#22240;&#27492;&#38544;&#31169;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20132;&#20114;&#24335;&#24046;&#20998;&#38544;&#31169;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22522;&#20110;&#21487;&#20449;&#38598;&#20013;&#24335;&#20915;&#31574;&#32773;&#30340;Bandit&#38382;&#39064;&#30340;&#38544;&#31169;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#32431;&#949;-&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#30340;Bandit&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#22312;&#29702;&#35299;&#38646;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;(zCDP)&#30340;Bandit&#38382;&#39064;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#38024;&#23545;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36951;&#25022;&#30340;&#26368;&#23567;&#26368;&#22823;&#21644;&#38382;&#39064;&#30456;&#20851;&#19979;&#30028;&#65292;&#20174;&#32780;&#37327;&#21270;&#20102;&#36825;&#20123;&#24773;&#20917;&#19979;&#961;-&#20840;&#23616;zCDP&#30340;&#20195;&#20215;&#12290;&#36825;&#20123;&#19979;&#30028;&#25581;&#31034;&#20102;&#22522;&#20110;&#38544;&#31169;&#39044;&#31639;&#961;&#30340;&#20004;&#20010;&#22256;&#38590;&#21306;&#22495;&#65292;&#24182;&#34920;&#26126;&#961;-&#20840;&#23616;zCDP&#27604;&#32431;&#949;-&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#20135;&#29983;&#30340;&#36951;&#25022;&#26356;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#30340;&#961;-&#20840;&#23616;zCDP&#31639;&#27861;&#65292;&#21363;AdaC-UCB&#21644;AdaC-GOPE&#12290;&#36825;&#20004;&#20010;&#31639;&#27861;&#37117;&#20351;&#29992;&#20102;&#39640;&#26031;&#26426;&#21046;&#30340;&#20849;&#21516;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDP incurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11654</link><description>&lt;p&gt;
&#22823;&#22411;&#21464;&#21387;&#22120;&#26159;&#26356;&#22909;&#30340;&#33041;&#30005;&#22270;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Transformers are Better EEG Learners. (arXiv:2308.11654v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#21487;&#29992;&#30340;&#26631;&#35760;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#30340;&#35268;&#27169;&#36828;&#36828;&#20302;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#22240;&#27492;&#24456;&#38590;&#23558;&#20174;EEG&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#24320;&#21457;&#21040;&#20687;GPT-4 100T&#36825;&#26679;&#30340;&#35268;&#27169;&#65292;&#20174;&#32780;&#23436;&#20840;&#21457;&#25381;&#35813;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;AdaCE&#65292;&#21363;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#24418;&#24335;&#30340;&#25554;&#25300;&#24335;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21464;&#21387;&#22120;&#12290;&#25552;&#20986;&#30340;AdaCE&#27169;&#22359;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#21516;&#26102;&#22312;&#22810;&#31181;&#22522;&#20110;EEG&#30340;&#39044;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;&#30340;Swin-Transformer&#19978;&#30340;AdaCE&#36798;&#21040;&#20102;99.6&#65285;&#30340;&#31934;&#24230;&#65292;&#32477;&#23545;&#25913;&#21892;&#20102;9.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-deco
&lt;/p&gt;</description></item><item><title>LadleNet&#26159;&#19968;&#31181;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;Handle&#27169;&#22359;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;Bowl&#27169;&#22359;&#35299;&#30721;&#35813;&#31354;&#38388;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.06603</link><description>&lt;p&gt;
LadleNet: &#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net. (arXiv:2308.06603v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06603
&lt;/p&gt;
&lt;p&gt;
LadleNet&#26159;&#19968;&#31181;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;Handle&#27169;&#22359;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;Bowl&#27169;&#22359;&#35299;&#30721;&#35813;&#31354;&#38388;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28909;&#32418;&#22806;&#65288;TIR&#65289;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#65288;VI&#65289;&#22270;&#20687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;TIR-VI&#22270;&#20687;&#37197;&#20934;&#21644;&#34701;&#21512;&#12290;&#21033;&#29992;&#20174;TIR&#22270;&#20687;&#36716;&#25442;&#20013;&#24471;&#21040;&#30340;&#34917;&#20805;&#20449;&#24687;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#31243;&#24207;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#23384;&#22312;&#30340;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324;&#22270;&#20687;&#20445;&#30495;&#24230;&#19981;&#39640;&#21644;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#26550;&#26500;&#30340;&#31639;&#27861;LadleNet&#12290; LadleNet&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;U-Net&#20018;&#32852;&#32467;&#26500;&#65292;&#22686;&#21152;&#20102;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;LadleNet&#30001;&#8220;Handle&#8221;&#21644;&#8220;Bowl&#8221;&#27169;&#22359;&#32452;&#25104;&#65292;Handle&#27169;&#22359;&#29992;&#20110;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;&#32780;Bowl&#27169;&#22359;&#21017;&#35299;&#30721;&#36825;&#20010;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibilit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#20851;&#38190;&#23618;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#23618;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#19979;&#23454;&#29616;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.04466</link><description>&lt;p&gt;
&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#65306;&#36890;&#36807;&#27745;&#26579;&#21518;&#38376;&#20851;&#38190;&#23618;
&lt;/p&gt;
&lt;p&gt;
Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. (arXiv:2308.04466v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#20851;&#38190;&#23618;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#23618;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#19979;&#23454;&#29616;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#25935;&#24863;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20998;&#25955;&#24335;&#23398;&#20064;&#33539;&#24335;&#21644;FL&#30340;&#24322;&#36136;&#24615;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#25915;&#20987;&#38754;&#12290;&#29616;&#26377;&#30340;FL&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#36890;&#24120;&#20250;&#20851;&#27880;&#25972;&#20010;&#27169;&#22411;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#26041;&#27861;&#24847;&#35782;&#21040;&#21518;&#38376;&#20851;&#38190;&#65288;BC&#65289;&#23618;&#30340;&#23384;&#22312;&#65292;&#21518;&#38376;&#20851;&#38190;&#23618;&#26159;&#25351;&#25511;&#21046;&#27169;&#22411;&#28431;&#27934;&#30340;&#19968;&#23567;&#37096;&#20998;&#23618;&#12290;&#25915;&#20987;BC&#23618;&#21487;&#20197;&#36798;&#21040;&#25915;&#20987;&#25972;&#20010;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20294;&#34987;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25163;&#27573;&#21457;&#29616;&#30340;&#26426;&#20250;&#35201;&#23567;&#24471;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25915;&#20987;&#32773;&#30340;&#35282;&#24230;&#35782;&#21035;&#21644;&#39564;&#35777;BC&#23618;&#30340;&#26222;&#36866;&#24615;&#26041;&#27861;&#12290;&#22522;&#20110;&#35782;&#21035;&#20986;&#30340;BC&#23618;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#38450;&#24481;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#23547;&#27714;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32500;&#24230;&#20381;&#36182;&#20248;&#21270;&#24230;&#20026;$O(d\delta^{-1}\epsilon^{-3})$&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#38750;&#20984;&#38543;&#26426;&#38646;&#38454;&#35774;&#32622;&#20013;&#38750;&#20809;&#28369;&#20248;&#21270;&#19982;&#20809;&#28369;&#20248;&#21270;&#30340;&#19968;&#26679;&#23481;&#26131;&#12290;</title><link>http://arxiv.org/abs/2307.04504</link><description>&lt;p&gt;
&#38646;&#38454;&#38750;&#20809;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2307.04504v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04504
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32500;&#24230;&#20381;&#36182;&#20248;&#21270;&#24230;&#20026;$O(d\delta^{-1}\epsilon^{-3})$&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#38750;&#20984;&#38543;&#26426;&#38646;&#38454;&#35774;&#32622;&#20013;&#38750;&#20809;&#28369;&#20248;&#21270;&#19982;&#20809;&#28369;&#20248;&#21270;&#30340;&#19968;&#26679;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20165;&#26377;&#22024;&#26434;&#20989;&#25968;&#35780;&#20272;&#26469;&#20135;&#29983;Lipschitz&#30446;&#26631;&#30340;$(\delta, \epsilon)$-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#30446;&#26631;&#21487;&#33021;&#26082;&#19981;&#20809;&#28369;&#20063;&#19981;&#20984;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#30340;&#38543;&#26426;&#38646;&#38454;&#31639;&#27861;&#65292;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#21463;&#21040;&#20102;$\Omega(d^{3/2})$&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#22256;&#25200;&#65292;&#20854;&#20013;$d$&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65292;&#36825;&#34987;&#25512;&#27979;&#20026;&#26368;&#20248;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26356;&#24555;&#30340;&#31639;&#27861;&#26469;&#39539;&#26021;&#36825;&#20010;&#29468;&#24819;&#65292;&#35813;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#20026;$O(d\delta^{-1}\epsilon^{-3})$&#65292;&#36825;&#26159;&#20851;&#20110;$d$&#30340;&#26368;&#20248;&#65288;&#22312;&#25968;&#20540;&#24120;&#25968;&#19978;&#65289;&#65292;&#23545;&#20110;&#31934;&#24230;&#21442;&#25968;$\delta, \epsilon$&#20063;&#26159;&#26368;&#20248;&#30340;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;Lin&#31561;&#20154;&#30041;&#19979;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65288;NeurIPS'22&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#23454;&#29616;&#30340;&#25910;&#25947;&#36895;&#24230;&#23545;&#20110;&#20809;&#28369;&#30446;&#26631;&#20063;&#26159;&#26368;&#20248;&#30340;&#65292;&#35777;&#26126;&#22312;&#38750;&#20984;&#38543;&#26426;&#38646;&#38454;&#35774;&#32622;&#20013;&#65292;&#38750;&#20809;&#28369;&#20248;&#21270;&#19982;&#20809;&#28369;&#20248;&#21270;&#19968;&#26679;&#23481;&#26131;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29616;&#19978;&#36848;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of producing $(\delta,\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\delta^{-1}\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\delta,\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#22788;&#29702;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#30340;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;SLQR&#21644;OLQR&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#20026;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.03590</link><description>&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#21152;&#36895;&#20248;&#21270;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Accelerated Optimization Landscape of Linear-Quadratic Regulator. (arXiv:2307.03590v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#22788;&#29702;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#30340;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;SLQR&#21644;OLQR&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#20026;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#26159;&#26368;&#20248;&#25511;&#21046;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22788;&#29702;LQR&#38382;&#39064;&#30340;&#19968;&#38454;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#20102;SLQR&#21644;OLQR&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#36825;&#23545;&#20110;&#24212;&#29992;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23545;&#20110;SLQR&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#28151;&#21512;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20854;&#35299;&#36712;&#36857;&#25351;&#25968;&#32423;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear-quadratic regulator (LQR) is a landmark problem in the field of optimal control, which is the concern of this paper. Generally, LQR is classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based on whether the full state is obtained. It has been suggested in existing literature that both the SLQR and the OLQR could be viewed as \textit{constrained nonconvex matrix optimization} problems in which the only variable to be optimized is the feedback gain matrix. In this paper, we introduce a first-order accelerated optimization framework of handling the LQR problem, and give its convergence analysis for the cases of SLQR and OLQR, respectively.  Specifically, a Lipschiz Hessian property of LQR performance criterion is presented, which turns out to be a crucial property for the application of modern optimization techniques. For the SLQR problem, a continuous-time hybrid dynamic system is introduced, whose solution trajectory is shown to converge exponentially to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#38381;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;RANS&#27169;&#25311;&#20013;&#32771;&#34385;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#21442;&#25968;&#21270;&#37096;&#20998;&#21644;&#38543;&#26426;&#21464;&#37327;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20844;&#24335;&#21644;&#31232;&#30095;&#20808;&#39564;&#26469;&#35782;&#21035;&#27169;&#22411;&#19981;&#36275;&#30340;&#21306;&#22495;&#65292;&#20197;&#36827;&#34892;&#20462;&#27491;&#12290;&#35757;&#32451;&#20351;&#29992;&#38388;&#25509;&#31232;&#30095;&#25968;&#25454;&#65292;&#25512;&#26029;&#21644;&#23398;&#20064;&#20351;&#29992;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.02432</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;RANS&#27169;&#25311;&#30340;&#38381;&#21512;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
A probabilistic, data-driven closure model for RANS simulations with aleatoric, model uncertainty. (arXiv:2307.02432v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#38381;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;RANS&#27169;&#25311;&#20013;&#32771;&#34385;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#21442;&#25968;&#21270;&#37096;&#20998;&#21644;&#38543;&#26426;&#21464;&#37327;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20844;&#24335;&#21644;&#31232;&#30095;&#20808;&#39564;&#26469;&#35782;&#21035;&#27169;&#22411;&#19981;&#36275;&#30340;&#21306;&#22495;&#65292;&#20197;&#36827;&#34892;&#20462;&#27491;&#12290;&#35757;&#32451;&#20351;&#29992;&#38388;&#25509;&#31232;&#30095;&#25968;&#25454;&#65292;&#25512;&#26029;&#21644;&#23398;&#20064;&#20351;&#29992;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#38381;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;Reynolds&#24179;&#22343;Navier-Stokes (RANS)&#27169;&#25311;&#20013;&#32771;&#34385;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#38381;&#21512;&#27169;&#22411;&#21253;&#25324;&#20004;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24352;&#37327;&#22522;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#20381;&#36182;&#20110;&#24212;&#21464;&#29575;&#21644;&#26059;&#36716;&#24352;&#37327;&#30340;&#19981;&#21464;&#37327;&#12290;&#31532;&#20108;&#37096;&#20998;&#21017;&#26159;&#38543;&#26426;&#21464;&#37327;&#65292;&#29992;&#20110;&#32771;&#34385;&#27169;&#22411;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#30340;&#20844;&#24335;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#31181;&#31232;&#30095;&#20808;&#39564;&#65292;&#20197;&#35782;&#21035;&#38382;&#39064;&#39046;&#22495;&#20013;&#21442;&#25968;&#21270;&#38381;&#21512;&#27169;&#22411;&#19981;&#36275;&#30340;&#22320;&#26041;&#65292;&#36827;&#32780;&#38656;&#35201;&#23545;&#38647;&#35834;&#24212;&#21147;&#24352;&#37327;&#36827;&#34892;&#38543;&#26426;&#20462;&#27491;&#12290;&#35757;&#32451;&#20351;&#29992;&#38388;&#25509;&#31232;&#30095;&#25968;&#25454;&#65292;&#22914;&#24179;&#22343;&#36895;&#24230;&#21644;&#21387;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#30452;&#25509;&#30340;&#38647;&#35834;&#24212;&#21147;&#25968;&#25454;&#65292;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#12290;&#20026;&#20102;&#25512;&#26029;&#21644;&#23398;&#20064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a data-driven, closure model for Reynolds-averaged Navier-Stokes (RANS) simulations that incorporates aleatoric, model uncertainty. The proposed closure consists of two parts. A parametric one, which utilizes previously proposed, neural-network-based tensor basis functions dependent on the rate of strain and rotation tensor invariants. This is complemented by latent, random variables which account for aleatoric model errors. A fully Bayesian formulation is proposed, combined with a sparsity-inducing prior in order to identify regions in the problem domain where the parametric closure is insufficient and where stochastic corrections to the Reynolds stress tensor are needed. Training is performed using sparse, indirect data, such as mean velocities and pressures, in contrast to the majority of alternatives that require direct Reynolds stress data. For inference and learning, a Stochastic Variational Inference scheme is employed, which is based on Monte Carlo estimates of the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00185</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#24212;&#29992;&#30340;&#26500;&#36896;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;(IRWNNs)&#30001;&#20110;&#26131;&#20110;&#23454;&#29616;&#21644;&#24555;&#36895;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;IRWNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#26159;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#65288;&#33410;&#28857;&#65289;&#19982;&#27531;&#24046;&#35823;&#24046;&#65288;&#27169;&#22411;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#30340;&#21487;&#35299;&#37322;&#30340;&#26500;&#36896;&#31639;&#27861;(ICA)&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#26469;&#38543;&#26426;&#20998;&#37197;&#38544;&#34255;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#33410;&#28857;&#27744;&#31574;&#30053;&#33719;&#21462;&#26356;&#26377;&#21033;&#20110;&#25910;&#25947;&#30340;&#38544;&#34255;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#35777;&#26126;&#20102;ICA&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;ICA&#30340;&#36731;&#37327;&#32423;&#29256;&#26412;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04891</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20196;&#20154;&#24778;&#35766;&#19988;&#26377;&#29992;&#30340;&#29305;&#24615;&#20043;&#19968;&#12290;&#23427;&#30340;&#24037;&#20316;&#21407;&#29702;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36817;&#26399;&#65292;&#20154;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#39118;&#26684;&#21270;&#30340;&#31867;&#20803;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#23427;&#20204;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#20989;&#25968;&#23545;&#26469;&#33258;&#20989;&#25968;&#31867;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;$(x, f(x))$ &#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35266;&#23519;&#27169;&#22411;&#23545;&#21516;&#19968;&#31867;&#20013;&#26410;&#35265;&#36807;&#30340;&#20989;&#25968;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#19968;&#30740;&#31350;&#32447;&#36335;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#23545;&#20110;&#35832;&#22914;&#32447;&#24615;&#22238;&#24402;&#31561;&#20960;&#20010;&#38382;&#39064;&#65292;&#35757;&#32451;&#22909;&#30340; Transformer &#23398;&#20064;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#36825;&#31181;&#34892;&#20026;&#30340;&#24402;&#32435;&#20559;&#24046;&#24182;&#19981;&#28165;&#26970;&#12290;&#25317;&#26377;&#26080;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#27169;&#22411;&#26159;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#65306;&#23427;&#23398;&#20064;&#20102;&#39044;&#35757;&#32451;&#20998;&#24067;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#39640;&#23481;&#37327;&#30340; Transformer &#27169;&#22411;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#27169;&#25311;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#29702;&#24819;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#21253;&#25324;&#22806;&#25512;&#21644;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21512;&#29702;&#20989;&#25968;&#30340;&#20808;&#39564;&#27010;&#29575;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#23567;&#21270;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#36827;&#19968;&#27493;&#25506;&#31350;&#36825;&#31181;&#32852;&#31995;&#65292;&#35777;&#26126;&#20351;&#29992;&#30495;&#23454;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#20351;&#29992;&#22266;&#23450;&#20808;&#39564;&#25110;&#27809;&#26377;&#20808;&#39564;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs $(x, f(x))$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21442;&#19982;&#29575;&#30340;&#23458;&#25143;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03401</link><description>&lt;p&gt;
&#22788;&#29702;&#32852;&#37030;&#24179;&#22343;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging. (arXiv:2306.03401v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21442;&#19982;&#29575;&#30340;&#23458;&#25143;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26410;&#30693;&#21442;&#19982;&#27010;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#24120;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#30340;&#19981;&#21516;&#21442;&#19982;&#29575;&#65292;&#22914;&#26524;&#19981;&#36866;&#24403;&#22788;&#29702;&#65292;&#21017;&#21487;&#33021;&#20250;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#36896;&#25104;&#37325;&#22823;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#20840;&#23616;&#26041;&#24046;&#32553;&#20943;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#30340;&#20869;&#23384;&#65292;&#20854;&#20056;&#27861;&#22240;&#23376;&#31561;&#20110;&#23458;&#25143;&#24635;&#25968;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#26159;&#25214;&#21040;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#22791;&#19981;&#21516;&#21442;&#19982;&#29575;&#23458;&#25143;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#30340;&#21442;&#19982;&#21382;&#21490;&#26469;&#35843;&#25972;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#20013;&#30340;&#32858;&#21512;&#26435;&#37325;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#21442;&#19982;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#38750;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#30340;FedAvg&#21487;&#33021;&#20250;&#20174;&#21407;&#22987;FL&#30446;&#26631;&#30340;&#26368;&#20248;&#35299;&#20559;&#31163;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#25214;&#21040;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#24403;&#21442;&#19982;&#27010;&#29575;&#19981;&#21487;&#30693;&#26102;&#35745;&#31639;&#26368;&#20248;&#26435;&#37325;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), clients usually have diverse participation probabilities that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation probabilities, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the part
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03116</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#21270;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20174;&#20247;&#21253;&#26381;&#21153;&#20013;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#27599;&#20010;&#27880;&#37322;&#32773;&#37117;&#23436;&#25104;&#33258;&#24049;&#30340;&#23567;&#37096;&#20998;&#27880;&#37322;&#65292;&#19981;&#21516;&#27880;&#37322;&#32773;&#30340;&#26631;&#27880;&#38169;&#35823;&#24448;&#24448;&#19981;&#21516;&#12290;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#36716;&#31227;&#30697;&#38453;&#26469;&#24314;&#27169;&#22122;&#22768;&#20135;&#29983;&#36807;&#31243;&#26159;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#23454;&#38469;&#20247;&#21253;&#27169;&#22411;&#20013;&#65292;&#36716;&#31227;&#30697;&#38453;&#26082;&#30001;&#27880;&#37322;&#32773;&#20381;&#36182;&#65292;&#20063;&#30001;&#23454;&#20363;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32773;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#36716;&#31227;&#30697;&#38453;(AIDTM)&#20855;&#26377;&#39640;&#22797;&#26434;&#24230;&#65292;&#32780;&#23454;&#38469;&#27880;&#37322;&#24448;&#24448;&#28041;&#21450;&#27880;&#37322;&#31232;&#30095;&#24615;&#65292;&#36825;&#20351;&#24471;&#24314;&#31435;AIDTM&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26082;&#35201;&#20445;&#25345;&#24314;&#27169;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#33021;&#26356;&#30495;&#23454;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#31639;AIDTM&#21644;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2306.03013</link><description>&lt;p&gt;
&#26126;&#34255;&#26263;&#31363;&#65306;&#20266;&#35013;&#25968;&#25454;&#31363;&#21462;&#25915;&#20987;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning. (arXiv:2306.03013v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#24050;&#32463;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31363;&#21462;&#22312;&#22823;&#25209;&#37327;&#21644;&#23433;&#20840;&#32858;&#21512;&#31561;&#20043;&#21069;&#34987;&#35270;&#20026;&#31169;&#23494;&#30340;&#35774;&#32622;&#20013;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20851;&#20110;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#23458;&#25143;&#31471;&#20390;&#27979;&#24615;&#30340;&#30097;&#34385;&#34987;&#25552;&#20986;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#34987;&#20844;&#24320;&#30693;&#26195;&#21518;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20390;&#27979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35768;&#22810;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#21407;&#21017;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#37117;&#21487;&#20197;&#36890;&#36807;&#21512;&#29702;&#30340;&#23458;&#25143;&#31471;&#26816;&#26597;&#26469;&#26816;&#27979;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#29702;&#24819;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;SEER&#25915;&#20987;&#26694;&#26550;&#65292;&#23427;&#28385;&#36275;&#25152;&#26377;&#29702;&#24819;&#35201;&#27714;&#65292;&#21487;&#20197;&#20174;&#29616;&#23454;&#32593;&#32476;&#30340;&#26799;&#24230;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#21363;&#20351;&#26159;&#22312;&#22823;&#25209;&#37327;(&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#26368;&#22823;&#21487;&#36798;512)&#21644;&#23433;&#20840;&#32858;&#21512;&#30340;&#24773;&#20917;&#19979;&#12290;SEER&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#20351;&#29992;&#31192;&#23494;&#35299;&#30721;&#22120;&#65292;&#22312;&#20849;&#20139;&#27169;&#22411;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#36808;&#21521;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#26377;&#21069;&#36884;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability.We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01158</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#30693;&#35782;&#30340;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#23398;&#32773;&#20204;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#20197;&#34893;&#29983;&#20986;&#21487;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#27169;&#22359;&#36890;&#24120;&#26159;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#65292;&#20063;&#20801;&#35768;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#22788;&#29702;&#21644;&#25972;&#21512;&#22810;&#31181;&#31867;&#22411;&#20449;&#24687;&#65288;&#30693;&#35782;&#65289;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21017;&#65292;&#23376;&#30446;&#26631;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26032;&#30340;&#26694;&#26550;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#21363;&#22686;&#24378;&#35760;&#24518;&#30340;&#20210;&#35009;&#22120;&#65292;&#23427;&#22686;&#21152;&#20102;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24050;&#26377;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#20063;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;hinge-Wasserstein&#65292;&#29992;&#20110;&#32531;&#35299;&#22238;&#24402;&#20219;&#21153;&#20013;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#26377;&#25928;&#25552;&#39640;&#20102;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.00560</link><description>&lt;p&gt;
Hinge-Wasserstein: &#36890;&#36807;&#20998;&#31867;&#36991;&#20813;&#22238;&#24402;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;
&lt;/p&gt;
&lt;p&gt;
Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification. (arXiv:2306.00560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;hinge-Wasserstein&#65292;&#29992;&#20110;&#32531;&#35299;&#22238;&#24402;&#20219;&#21153;&#20013;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#26377;&#25928;&#25552;&#39640;&#20102;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24615;&#33021;&#26041;&#38754;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#12290;&#22312;&#27169;&#31946;&#29978;&#33267;&#19981;&#21487;&#39044;&#27979;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#36807;&#24230;&#33258;&#20449;&#21487;&#33021;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#65292;&#37319;&#29992;&#22238;&#24402;-&#20998;&#31867;&#26041;&#27861;&#26377;&#28508;&#21147;&#32531;&#35299;&#36825;&#20123;&#27495;&#20041;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#25152;&#38656;&#36755;&#20986;&#30340;&#31163;&#25955;&#27010;&#29575;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#23494;&#24230;&#20272;&#35745;&#20173;&#28982;&#20542;&#21521;&#20110;&#36807;&#24230;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#24120;&#35265;&#30340;NLL&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#26102;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;hinge-Wasserstein&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#27492;&#25439;&#22833;&#26174;&#30528;&#25552;&#39640;&#20102;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#65306; aleatoric&#19981;&#30830;&#23450;&#24615;&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26032;&#25439;&#22833;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20998;&#21035;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#28436;&#31034;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks are prone to being overconfident despite their drastically improved performance. In ambiguous or even unpredictable real-world scenarios, this overconfidence can pose a major risk to the safety of applications. For regression tasks, the regression-by-classification approach has the potential to alleviate these ambiguities by instead predicting a discrete probability density over the desired output. However, a density estimator still tends to be overconfident when trained with the common NLL loss. To mitigate the overconfidence problem, we propose a loss function, hinge-Wasserstein, based on the Wasserstein Distance. This loss significantly improves the quality of both aleatoric and epistemic uncertainty, compared to previous work. We demonstrate the capabilities of the new loss on a synthetic dataset, where both types of uncertainty are controlled separately. Moreover, as a demonstration for real-world scenarios, we evaluate our approach on the benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18436</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#23454;&#29616;&#26368;&#20248;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#22343;&#20540;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#26494;&#24347;&#26368;&#36817;&#34987;&#25552;&#20986;&#29992;&#20110;&#35299;&#20915;K&#22343;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#20294;&#23454;&#29616;SDP&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#36825;&#20123;&#20445;&#35777;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#34987;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#20047;&#22362;&#23454;&#30340;&#32479;&#35745;&#22522;&#30784;&#25110;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;NMF&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#20984;Burer-Monteiro&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#21322;&#23450;&#35268;&#21010;&#26494;&#24347;&#30340;K&#22343;&#20540;&#20844;&#24335;&#30340;&#38750;&#36127;&#20302;&#31209;&#38480;&#21046;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#65292;&#21516;&#26102;&#20063;&#20139;&#26377;&#19982;SDP&#30456;&#21516;&#30340;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;NMF&#31639;&#27861;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;SDP&#27714;&#35299;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
$K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;</title><link>http://arxiv.org/abs/2305.17493</link><description>&lt;p&gt;
&#36882;&#24402;&#30340;&#35781;&#21650;&#65306;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#35753;&#27169;&#22411;&#24536;&#35760;
&lt;/p&gt;
&lt;p&gt;
The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17493
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#20174;&#25551;&#36848;&#24615;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;GPT-2&#12289;GPT-3(.5)&#21644;GPT-4&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#24778;&#20154;&#12290;ChatGPT&#23558;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#22823;&#20247;&#35270;&#37326;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#19981;&#21487;&#36991;&#20813;&#24182;&#23558;&#24443;&#24213;&#25913;&#21464;&#22312;&#32447;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20010;&#29983;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26410;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#12290;&#24403;LLMs&#21344;&#25454;&#20102;&#22312;&#32447;&#35821;&#35328;&#30340;&#22823;&#37096;&#20998;&#26102;&#65292;GPT-{n}&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20250;&#23548;&#33268;&#25152;&#24471;&#27169;&#22411;&#20013;&#19981;&#21487;&#36870;&#32570;&#38519;&#65292;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#21457;&#29983;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;LLMs&#20013;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#35937;&#32972;&#21518;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23601;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#21152;&#36895;&#20248;&#21270;&#24182;&#22312;&#24635;&#20307;&#19978;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13404</link><description>&lt;p&gt;
&#21033;&#29992;&#21442;&#25968;&#23545;&#31216;&#24615;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Convergence and Generalization Using Parameter Symmetries. (arXiv:2305.13404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#21152;&#36895;&#20248;&#21270;&#24182;&#22312;&#24635;&#20307;&#19978;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#65292;&#21442;&#25968;&#30340;&#19981;&#21516;&#20540;&#21487;&#33021;&#23548;&#33268;&#30456;&#21516;&#30340;&#25439;&#22833;&#20540;&#12290;&#21442;&#25968;&#31354;&#38388;&#23545;&#31216;&#24615;&#26159;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#32780;&#20445;&#25345;&#25439;&#22833;&#19981;&#21464;&#30340;&#21464;&#25442;&#12290;&#20256;&#36865;&#24212;&#29992;&#36825;&#26679;&#30340;&#21464;&#25442;&#26469;&#21152;&#36895;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31639;&#27861;&#25104;&#21151;&#30340;&#30830;&#20999;&#26426;&#21046;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#22312;&#30701;&#26399;&#20869;&#21152;&#36895;&#20248;&#21270;&#65292;&#32780;&#19988;&#21487;&#20197;&#20351;&#24635;&#20307;&#25910;&#25947;&#26102;&#38388;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26368;&#23567;&#20540;&#26354;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20256;&#36865;&#38598;&#25104;&#21040;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#20013;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In overparametrized models, different values of the parameters may result in the same loss value. Parameter space symmetries are transformations that change the model parameters but leave the loss invariant. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind this algorithm's success is not well understood. In this paper, we show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence. Additionally, we show that teleporting to minima with different curvatures improves generalization and provide insights on the connection between the curvature of the minima and generalization ability. Finally, we show that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22312;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#20844;&#20849;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#38544;&#31169;&#21644;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#25552;&#39640;&#20844;&#20849;&#25968;&#25454;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#65292;&#20026;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12132</link><description>&lt;p&gt;
&#20844;&#20849;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#24110;&#21161;&#31169;&#26377;&#20132;&#21449;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Public Large Language Models Help Private Cross-device Federated Learning?. (arXiv:2305.12132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22312;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#20844;&#20849;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#38544;&#31169;&#21644;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#25552;&#39640;&#20844;&#20849;&#25968;&#25454;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#65292;&#20026;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#65288;&#24046;&#20998;&#65289;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20132;&#21449;&#35774;&#22791;FL&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#36739;&#23567;&#65292;&#22312;&#35757;&#32451;&#20013;&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#21442;&#19982;&#19979;&#21487;&#20197;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;&#21270;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#35777;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#20844;&#20849;&#25968;&#25454;&#24050;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#21644;LLMs&#26469;&#24110;&#21161;&#35774;&#22791;&#19978;&#30340;FL&#27169;&#22411;&#36827;&#34892;&#24046;&#20998;&#31169;&#26377;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33976;&#39311;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#29702;&#35770;&#22522;&#30784;&#23545;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#25509;&#36817;&#31169;&#26377;&#25968;&#25454;&#20998;&#24067;&#30340;&#37319;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#65288;&#39044;&#65289;&#35757;&#32451;&#20844;&#20849;&#25968;&#25454;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study (differentially) private federated learning (FL) of language models. The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users. Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models. In this work, we provide a systematic study of using large-scale public data and LLMs to help differentially private training of on-device FL models, and further improve the privacy-utility tradeoff by techniques of distillation. Moreover, we propose a novel distribution matching algorithm with theoretical grounding to sample public data close to private data distribution, which significantly improves the sample efficiency of (pre-)training on public data. The proposed method is efficient and effective for training private model by taking advantage 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.12118</link><description>&lt;p&gt;
&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;&#25913;&#36827;&#20102;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Annealing Self-Distillation Rectification Improves Adversarial Training. (arXiv:2305.12118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#27169;&#22411;&#34987;&#20248;&#21270;&#20197;&#36866;&#24212;&#21487;&#25509;&#21463;&#30340;&#23545;&#25239;&#25200;&#21160;&#39044;&#31639;&#20869;&#30340;&#19968;&#28909;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#30001;&#25200;&#21160;&#24102;&#26469;&#30340;&#22522;&#30784;&#20998;&#24067;&#21464;&#21270;&#65292;&#23548;&#33268;&#20102;&#24378;&#20581;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22686;&#24378;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24378;&#20581;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#24182;&#30830;&#23450;&#24378;&#20581;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#26356;&#24179;&#28369;&#21644;&#26356;&#33391;&#22909;&#26657;&#20934;&#30340;&#36755;&#20986;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#36719;&#26631;&#31614;&#20316;&#20026;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#33021;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;ADR&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20462;&#27491;&#30340;&#20998;&#24067;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#39069;&#22806;&#30340;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26367;&#25442;&#21367;&#31215;&#23618;&#20197;&#23454;&#29616;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by repl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06061</link><description>&lt;p&gt;
&#35270;&#35273;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Visual Tuning. (arXiv:2305.06061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#24191;&#27867;&#35777;&#26126;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#34920;&#29616;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#24778;&#20154;&#21457;&#23637;&#65292;&#35270;&#35273;&#35843;&#25972;&#36339;&#20986;&#20102;&#26631;&#20934;&#30340;&#27169;&#24335;&#25805;&#20316;&#65292;&#21363;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#20165;&#24494;&#35843;&#23436;&#20840;&#36830;&#25509;&#23618;&#12290;&#30456;&#21453;&#65292;&#36817;&#26399;&#30340;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#26356;&#26032;&#26356;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#27604;&#20840;&#38754;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#21442;&#25968;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#21644;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#37096;&#32626;&#22312;&#20113;&#31471;&#30340;&#26085;&#30410;&#24222;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20840;&#38754;&#20102;&#35299;&#35270;&#35273;&#35843;&#25972;&#30340;&#20840;&#35980;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#26412;&#32508;&#36848;&#25551;&#32472;&#20102;&#22823;&#37327;&#30340;&#36817;&#26399;&#30740;&#31350;&#20316;&#21697;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#24037;&#20316;&#21644;&#27169;&#22411;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25552;&#20379;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#35814;&#32454;&#32972;&#26223;&#65292;&#24182;&#23558;&#26368;&#36817;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#32452;&#65306;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20316;&#29289;&#20998;&#21106;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#28304;&#22495;&#30340;&#27169;&#22411;&#38598;&#21512;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#20316;&#29289;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#27867;&#21270;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.01029</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;&#20316;&#29289;&#20998;&#21106;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Crop Segmentation with Knowledge Distillation. (arXiv:2304.01029v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20316;&#29289;&#20998;&#21106;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#28304;&#22495;&#30340;&#27169;&#22411;&#38598;&#21512;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#20316;&#29289;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#27867;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31934;&#30830;&#20892;&#19994;&#36880;&#28176;&#26397;&#30528;&#33258;&#21160;&#21270;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#20197;&#25903;&#25345;&#19982;&#30000;&#38388;&#31649;&#29702;&#30456;&#20851;&#30340;&#25152;&#26377;&#27963;&#21160;&#12290;&#26381;&#21153;&#26426;&#22120;&#20154;&#22312;&#36825;&#19968;&#28436;&#21464;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#37096;&#32626;&#33021;&#22815;&#22312;&#30000;&#38388;&#23548;&#33322;&#24182;&#22312;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#30340;&#33258;&#20027;&#20195;&#29702;&#65292;&#20363;&#22914;&#30417;&#27979;&#12289;&#21943;&#27922;&#21644;&#25910;&#33719;&#12290;&#20026;&#20102;&#25191;&#34892;&#36825;&#20123;&#31934;&#30830;&#30340;&#21160;&#20316;&#65292;&#31227;&#21160;&#26426;&#22120;&#20154;&#38656;&#35201;&#19968;&#20010;&#23454;&#26102;&#24863;&#30693;&#31995;&#32479;&#65292;&#33021;&#22815;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#24182;&#22312;&#37326;&#22806;&#35782;&#21035;&#30446;&#26631;&#12290;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#65292;&#23545;&#26032;&#30340;&#20316;&#29289;&#21644;&#29615;&#22659;&#26465;&#20214;&#36827;&#34892;&#27867;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#24456;&#23569;&#26377;&#26631;&#35760;&#26679;&#26412;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20316;&#29289;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;&#22495;&#27867;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#28304;&#22495;&#19978;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#38598;&#21512;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#26410;&#35265;&#30446;&#26631;&#22495;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, precision agriculture has gradually oriented farming closer to automation processes to support all the activities related to field management. Service robotics plays a predominant role in this evolution by deploying autonomous agents that can navigate fields while performing tasks without human intervention, such as monitoring, spraying, and harvesting. To execute these precise actions, mobile robots need a real-time perception system that understands their surroundings and identifies their targets in the wild. Generalizing to new crops and environmental conditions is critical for practical applications, as labeled samples are rarely available. In this paper, we investigate the problem of crop segmentation and propose a novel approach to enhance domain generalization using knowledge distillation. In the proposed framework, we transfer knowledge from an ensemble of models individually trained on source domains to a student model that can adapt to unseen target domains. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;</title><link>http://arxiv.org/abs/2304.00195</link><description>&lt;p&gt;
&#25277;&#35937;&#22120;&#65306;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20851;&#31995;&#23398;&#20064;&#36716;&#21270;&#20026;Transformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.03364</link><description>&lt;p&gt;
&#36208;&#21521;AI-enabled&#36830;&#25509;&#20135;&#19994;: AGV&#36890;&#20449;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24037;&#19994;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;: &#24037;&#19994;&#36710;&#36742;&#38388;&#36890;&#20449;(iV2V)&#21644;&#24037;&#19994;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#21152;&#20256;&#24863;&#22120;(iV2I+)&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20004;&#20010;&#25429;&#33719;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;iV2V&#28085;&#30422;&#20102;&#33258;&#21160;&#24341;&#23548;&#36710;(AGVs)&#20043;&#38388;&#30340;&#20391;&#21521;&#38142;&#36335;&#36890;&#20449;&#22330;&#26223;&#65292;&#32780;iV2I+&#21017;&#26159;&#22312;&#24037;&#19994;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#33258;&#20027;&#28165;&#27905;&#26426;&#22120;&#20154;&#36830;&#25509;&#21040;&#31169;&#26377;&#34562;&#31389;&#32593;&#32476;&#12290;&#19981;&#21516;&#36890;&#20449;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#36830;&#21516;&#20849;&#21516;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;(ML)&#21487;&#20197;&#21033;&#29992;&#30340;&#27934;&#23519;&#21147;&#65292;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#24050;&#26631;&#35760;&#21644;&#39044;&#36807;&#28388;&#65292;&#20197;&#20415;&#24555;&#36895;&#21551;&#21160;&#21644;&#24212;&#29992;&#12290;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#27979;&#35797;&#24179;&#21488;&#21644;&#27979;&#37327;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.05256</link><description>&lt;p&gt;
CALIME: &#22240;&#26524;&#24863;&#30693;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;-&#26080;&#20851;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations. (arXiv:2212.05256v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#20551;&#35774;&#29305;&#24449;&#29420;&#31435;&#24615;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#22686;&#21152;&#20449;&#20219;&#24182;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#22312;&#22260;&#32469;&#36755;&#20837;&#23454;&#20363;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#32534;&#30721;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#37322;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#20223;&#40657;&#30418;&#23376;&#30340;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#27604;&#21021;&#22987;&#26041;&#27861;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant drawback of eXplainable Artificial Intelligence (XAI) approaches is the assumption of feature independence. This paper focuses on integrating causal knowledge in XAI methods to increase trust and help users assess explanations' quality. We propose a novel extension to a widely used local and model-agnostic explainer that explicitly encodes causal relationships in the data generated around the input instance to explain. Extensive experiments show that our method achieves superior performance comparing the initial one for both the fidelity in mimicking the black-box and the stability of the explanations.
&lt;/p&gt;</description></item><item><title>ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13700</link><description>&lt;p&gt;
ES-GNN: &#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;
&lt;/p&gt;
&lt;p&gt;
ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting. (arXiv:2205.13700v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13700
&lt;/p&gt;
&lt;p&gt;
ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#29616;&#20195;&#21464;&#20307;&#20027;&#35201;&#20381;&#36182;&#20110;&#21516;&#36136;&#24615;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#36890;&#24120;&#21516;&#26102;&#26174;&#31034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#38142;&#25509;&#27169;&#24335;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;GNN&#22312;&#25972;&#20307;&#19978;&#24179;&#28369;&#33410;&#28857;&#25509;&#36817;&#24615;&#21487;&#33021;&#20250;&#32858;&#21512;&#20219;&#21153;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#65288;&#29978;&#33267;&#26377;&#23475;&#65289;&#30340;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36793;&#20998;&#21106;GNN&#65288;ES-GNN&#65289;&#26694;&#26550;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#33410;&#28857;&#38598;&#20294;&#20855;&#26377;&#29420;&#21344;&#36793;&#38598;&#30340;&#23376;&#22270;&#12290;&#22312;&#36825;&#20004;&#20010;&#23376;&#22270;&#19978;&#20998;&#21035;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#65292;&#20174;&#32780;&#20351;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#20132;&#26367;&#36827;&#34892;&#65292;&#23454;&#29616;&#20102;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have achieved enormous success in multiple graph analytical tasks, modern variants mostly rely on the strong inductive bias of homophily. However, real-world networks typically exhibit both homophilic and heterophilic linking patterns, wherein adjacent nodes may share dissimilar attributes and distinct labels. Therefore, GNNs smoothing node proximity holistically may aggregate both task-relevant and irrelevant (even harmful) information, limiting their ability to generalize to heterophilic graphs and potentially causing non-robustness. In this work, we propose a novel edge splitting GNN (ES-GNN) framework to adaptively distinguish between graph edges either relevant or irrelevant to learning tasks. This essentially transfers the original graph into two subgraphs with the same node set but exclusive edge sets dynamically. Given that, information propagation separately on these subgraphs and edge splitting are alternatively conducted, thus disentangling
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#19988;&#20801;&#35768;&#33258;&#36866;&#24212;&#36873;&#25321;&#38543;&#26426;&#27493;&#38271;&#21644;&#20351;&#29992;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2205.13687</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#23545;&#32422;&#26463;&#30340;&#38543;&#26426;&#20248;&#21270;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference of Constrained Stochastic Optimization via Sketched Sequential Quadratic Programming. (arXiv:2205.13687v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#19988;&#20801;&#35768;&#33258;&#36866;&#24212;&#36873;&#25321;&#38543;&#26426;&#27493;&#38271;&#21644;&#20351;&#29992;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23545;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#22312;&#32447;&#38543;&#26426;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65288;&#21363;KKT&#26465;&#20214;&#65289;&#12290;&#21463;&#26368;&#36817;&#25968;&#20540;&#20108;&#38454;&#26041;&#27861;&#35774;&#35745;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20801;&#35768;StoSQP&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#20219;&#24847;&#38543;&#26426;&#27493;&#38271;$ \bar {\ alpha} _t $&#65292;&#21482;&#35201;$ \ beta _t \ leq \ bar {\ alpha} _t \ leq \ beta _t + \ chi _t $&#65292;&#20854;&#20013; $ \ beta_t $ &#21644; $ \ chi_t = o(\beta_t) $ &#26159;&#26576;&#20123;&#25511;&#21046;&#24207;&#21015;&#12290;&#20026;&#20102;&#38477;&#20302;&#20108;&#38454;&#26041;&#27861;&#30340;&#20027;&#35201;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#36824;&#20801;&#35768;StoSQP&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#30340;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#19981;&#31934;&#30830;&#22320;&#35299;&#20915;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#35201;&#27714;&#36924;&#36817;&#35823;&#24046;&#38543;&#30528;&#36845;&#20195;&#30340;&#36827;&#34892;&#32780;&#20943;&#23567;&#12290;&#23545;&#20110;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#65288;i&#65289;&#19979;&#65292;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26368;&#22810;&#20026;$ O(1 / \ ep&#65289;$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider statistical inference of equality-constrained stochastic nonlinear optimization problems. We develop a fully online stochastic sequential quadratic programming (StoSQP) method to solve the problems, which can be regarded as applying Newton's method to the first-order optimality conditions (i.e., the KKT conditions). Motivated by recent designs of numerical second-order methods, we allow StoSQP to adaptively select any random stepsize $\bar{\alpha}_t$, as long as $\beta_t\leq \bar{\alpha}_t \leq \beta_t+\chi_t$, for some control sequences $\beta_t$ and $\chi_t=o(\beta_t)$. To reduce the dominant computational cost of second-order methods, we additionally allow StoSQP to inexactly solve quadratic programs via efficient randomized iterative solvers that utilize sketching techniques. Notably, we do not require the approximation error to diminish as iteration proceeds. For the developed method, we show that under mild assumptions (i) computationally, it can take at most $O(1/\ep
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#23545;&#20110;&#22810;&#35270;&#35282;&#22534;&#21472;&#20013;&#30340;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#38750;&#36127;&#22871;&#32034;&#12289;&#38750;&#36127;&#33258;&#36866;&#24212;&#22871;&#32034;&#21644;&#38750;&#36127;&#24377;&#24615;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#26368;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2010.16271</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#22534;&#21472;&#20013;&#30340;&#35270;&#22270;&#36873;&#25321;&#65306;&#36873;&#25321;&#20803;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
View selection in multi-view stacking: Choosing the meta-learner. (arXiv:2010.16271v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.16271
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#23545;&#20110;&#22810;&#35270;&#35282;&#22534;&#21472;&#20013;&#30340;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#38750;&#36127;&#22871;&#32034;&#12289;&#38750;&#36127;&#33258;&#36866;&#24212;&#22871;&#32034;&#21644;&#38750;&#36127;&#24377;&#24615;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#26368;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#22534;&#21472;&#26159;&#19968;&#31181;&#23558;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#65288;&#21363;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#65289;&#25551;&#36848;&#30456;&#21516;&#23545;&#35937;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#22522;&#23398;&#20064;&#31639;&#27861;&#20998;&#21035;&#22312;&#27599;&#20010;&#35270;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#32467;&#26524;&#30001;&#20803;&#23398;&#20064;&#31639;&#27861;&#32452;&#21512;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22534;&#21472;&#30340;&#32602;&#20998;&#36923;&#36753;&#22238;&#24402;&#65292;&#20316;&#20026;&#22810;&#35270;&#35282;&#22534;&#21472;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35782;&#21035;&#23545;&#39044;&#27979;&#26368;&#37325;&#35201;&#30340;&#35270;&#22270;&#26041;&#38754;&#26159;&#26377;&#29992;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19971;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#20004;&#20010;&#30495;&#23454;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#24615;&#33021;&#65292;&#25193;&#23637;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#35270;&#22270;&#36873;&#25321;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#23545;&#30740;&#31350;&#37117;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#38750;&#36127;&#22871;&#32034;&#12289;&#38750;&#36127;&#33258;&#36866;&#24212;&#22871;&#32034;&#21644;&#38750;&#36127;&#24377;&#24615;&#32593;&#32476;&#37117;&#26159;&#21512;&#36866;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;&#20855;&#20307;&#22312;&#36825;&#19977;&#31181;&#26041;&#27861;&#20013;&#35813;&#36873;&#25321;&#21738;&#19968;&#31181;&#21462;&#20915;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Multi-view stacking is a framework for combining information from different views (i.e. different feature sets) describing the same set of objects. In this framework, a base-learner algorithm is trained on each view separately, and their predictions are then combined by a meta-learner algorithm. In a previous study, stacked penalized logistic regression, a special case of multi-view stacking, has been shown to be useful in identifying which views are most important for prediction. In this article we expand this research by considering seven different algorithms to use as the meta-learner, and evaluating their view selection and classification performance in simulations and two applications on real gene-expression data sets. Our results suggest that if both view selection and classification accuracy are important to the research at hand, then the nonnegative lasso, nonnegative adaptive lasso and nonnegative elastic net are suitable meta-learners. Exactly which among these three is to be
&lt;/p&gt;</description></item></channel></rss>