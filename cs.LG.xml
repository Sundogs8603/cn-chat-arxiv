<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#30340;&#20004;&#20154;&#38646;&#21644;&#30697;&#38453;&#28216;&#25103;&#20013;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#20915;&#26007;&#36172;&#21330;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16252</link><description>&lt;p&gt;
Matrix Games&#20013;&#30340;&#36817;&#26368;&#20248;&#32431;&#25506;&#32034;&#65306;&#38543;&#26426;&#36172;&#24466;&#19982;&#20915;&#26007;&#36172;&#24466;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits &amp; Dueling Bandits. (arXiv:2310.16252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#30340;&#20004;&#20154;&#38646;&#21644;&#30697;&#38453;&#28216;&#25103;&#20013;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#20915;&#26007;&#36172;&#21330;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20855;&#26377;&#22122;&#22768;&#30340;&#20004;&#20154;&#38646;&#21644;&#30697;&#38453;&#28216;&#25103;&#20013;&#65292;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#65288;PSNE&#65289;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#27169;&#22411;&#20013;&#65292;&#20219;&#20309;&#23398;&#20064;&#22120;&#21487;&#20197;&#23545;&#36755;&#20837;&#30697;&#38453;A&#30340;&#26576;&#20010;&#20803;&#32032;&#65288;i&#65292;j&#65289;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#35266;&#23519;&#21040;A_{i&#65292;j} + \eta&#30340;&#20540;&#65292;&#20854;&#20013;\eta&#26159;&#19968;&#20010;&#38646;&#22343;&#20540;&#30340;1-&#23376;&#39640;&#26031;&#22122;&#22768;&#12290;&#23398;&#20064;&#22120;&#30340;&#30446;&#26631;&#26159;&#22312;&#23613;&#21487;&#33021;&#23569;&#30340;&#37319;&#26679;&#27425;&#25968;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#30830;&#23450;A&#30340;PSNE&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#21482;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#36317;&#12290;&#30830;&#23450;PSNE&#30340;&#38382;&#39064;&#20063;&#25512;&#24191;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#20915;&#26007;&#36172;&#24466;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#36825;&#20004;&#20010;&#35774;&#32622;&#20013;&#19982;&#26368;&#20248;&#30028;&#38480;&#30456;&#21305;&#37197;&#65292;&#21482;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of identifying the pure strategy Nash equilibrium (PSNE) in a two-player zero-sum matrix game with noise. Formally, we are given a stochastic model where any learner can sample an entry $(i,j)$ of the input matrix $A\in[-1,1]^{n\times m}$ and observe $A_{i,j}+\eta$ where $\eta$ is a zero-mean 1-sub-Gaussian noise. The aim of the learner is to identify the PSNE of $A$, whenever it exists, with high probability while taking as few samples as possible. Zhou et al. (2017) presents an instance-dependent sample complexity lower bound that depends only on the entries in the row and column in which the PSNE lies. We design a near-optimal algorithm whose sample complexity matches the lower bound, up to log factors. The problem of identifying the PSNE also generalizes the problem of pure exploration in stochastic multi-armed bandits and dueling bandits, and our result matches the optimal bounds, up to log factors, in both the settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13164</link><description>&lt;p&gt;
&#20960;&#20046;&#31561;&#21464;&#24615;&#36890;&#36807;&#26446;&#20195;&#25968;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#29616;&#26377;&#23450;&#20041;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#30456;&#23545;&#20110;&#32676;&#20316;&#29992;&#30340;&#31561;&#21464;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#36171;&#20104;&#19968;&#20010;&#26550;&#26500;&#20855;&#20307;&#30340;&#32676;&#31561;&#21464;&#24615;&#23545;&#27169;&#22411;&#25152;&#26399;&#26395;&#30475;&#21040;&#30340;&#25968;&#25454;&#21464;&#25442;&#31867;&#22411;&#26045;&#21152;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#12290;&#20005;&#26684;&#31561;&#21464;&#27169;&#22411;&#24378;&#21046;&#25191;&#34892;&#23545;&#31216;&#24615;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#36825;&#26679;&#30340;&#20005;&#26684;&#31561;&#21464;&#24615;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25110;&#20165;&#32534;&#30721;&#20102;&#36817;&#20284;&#25110;&#37096;&#20998;&#23545;&#31216;&#24615;&#30340;&#28508;&#22312;&#29289;&#29702;&#23450;&#24459;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20005;&#26684;&#31561;&#21464;&#24615;&#30340;&#20808;&#39564;&#23454;&#38469;&#19978;&#21487;&#33021;&#36807;&#20110;&#24378;&#22823;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#21363;&#20960;&#20046;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#29616;&#26377;&#23450;&#20041;&#19981;&#21516;&#30340;&#20960;&#20046;&#31561;&#21464;&#24615;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#32473;&#20986;&#20102;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20960;&#20046;&#31561;&#21464;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
&lt;/p&gt;</description></item><item><title>PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11676</link><description>&lt;p&gt;
PREM:&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11676
&lt;/p&gt;
&lt;p&gt;
PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#22312;&#35782;&#21035;&#21307;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26631;&#27880;&#25968;&#25454;&#30340;&#21294;&#20047;&#65292;&#24050;&#26377;&#30340;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#24448;&#24448;&#22312;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#32321;&#29712;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PREprocessing and Matching&#65288;&#31616;&#31216;PREM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;PREM&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#39044;&#22788;&#29702;&#27169;&#22359;&#21644;&#37051;&#23621;&#21305;&#37197;&#27169;&#22359;&#12290;PREM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Swin Transformer&#27169;&#22411;&#20174;CT&#22270;&#20687;&#20013;&#35786;&#26029;COVID-19&#65292;&#26041;&#27861;&#22312;&#24739;&#32773;&#32423;&#21035;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26041;&#27861;&#65292;&#20026;&#20934;&#30830;&#35786;&#26029;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.08165</link><description>&lt;p&gt;
&#21033;&#29992;Swin Transformer&#27169;&#22411;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19
&lt;/p&gt;
&lt;p&gt;
COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images. (arXiv:2310.08165v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08165
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Swin Transformer&#27169;&#22411;&#20174;CT&#22270;&#20687;&#20013;&#35786;&#26029;COVID-19&#65292;&#26041;&#27861;&#22312;&#24739;&#32773;&#32423;&#21035;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26041;&#27861;&#65292;&#20026;&#20934;&#30830;&#35786;&#26029;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#22320;&#35786;&#26029;COVID-19&#23545;&#20110;&#22823;&#35268;&#27169;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#39044;&#21360;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CT&#22270;&#20687;&#36827;&#34892;COVID-19&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26368;&#20808;&#36827;&#30340;Swin Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#31181;&#31995;&#32479;&#30340;&#24739;&#32773;&#32423;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#27599;&#20010;CT&#20999;&#29255;&#20998;&#31867;&#20026;COVID-19&#25110;&#38750;COVID-19&#65292;&#24182;&#36890;&#36807;&#22810;&#25968;&#34920;&#20915;&#30830;&#23450;&#24739;&#32773;&#30340;&#25972;&#20307;&#35786;&#26029;&#32467;&#26524;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#24212;&#29992;Swin Transformer&#32467;&#26524;&#34920;&#29616;&#20986;&#24322;&#24120;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#35768;&#22810;&#31454;&#20105;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;COVID-19&#35786;&#26029;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#27169;&#22411;&#36798;&#21040;&#30340;&#23439;F1&#20998;&#25968;&#36229;&#36807;&#22522;&#32447;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20934;&#30830;&#35786;&#26029;&#30340;&#24378;&#26377;&#21147;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate and efficient diagnosis of COVID-19 is of paramount importance, particularly in the context of large-scale medical imaging datasets. In this preprint paper, we propose a novel approach for COVID-19 diagnosis using CT images that leverages the power of Swin Transformer models, state-of-the-art solutions in computer vision tasks. Our method includes a systematic approach for patient-level predictions, where individual CT slices are classified as COVID-19 or non-COVID, and the patient's overall diagnosis is determined through majority voting. The application of the Swin Transformer in this context results in patient-level predictions that demonstrate exceptional diagnostic accuracy. In terms of evaluation metrics, our approach consistently outperforms the baseline, as well as numerous competing methods, showcasing its effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model exceeds the baseline and offers a robust solution for accurate diagnosis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08164</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#21644;&#32463;&#36807;RLHF&#35843;&#25972;&#30340;&#29256;&#26412;&#30340;&#28608;&#27963;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#21453;&#26144;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24773;&#26223;&#65292;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20196;&#29260;-&#22870;&#21169;&#26144;&#23556;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36825;&#26159;&#39318;&#27425;&#24212;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#37322;&#23398;&#20064;&#22870;&#21169;&#21644;&#24191;&#27867;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#65292;&#36825;&#20026;&#30830;&#20445;&#25351;&#23450;&#30446;&#26631;&#21644;&#27169;&#22411;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#65288;ARC&#65289;&#26469;&#23450;&#20041;&#36866;&#24403;&#30340;&#23545;&#20934;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#22312;WiFi&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;CSI&#25968;&#25454;&#19978;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06328</link><description>&lt;p&gt;
&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#23450;&#20041;CSI&#25968;&#25454;&#30340;&#23545;&#20934;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Exploit the antenna response consistency to define the alignment criteria for CSI data. (arXiv:2310.06328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#65288;ARC&#65289;&#26469;&#23450;&#20041;&#36866;&#24403;&#30340;&#23545;&#20934;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#22312;WiFi&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;CSI&#25968;&#25454;&#19978;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#29992;&#20110;&#22522;&#20110;WiFi&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30001;&#20110;&#33021;&#22815;&#35299;&#20915;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#32780;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;SSL&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#27604;&#23398;&#20064;&#65292;&#31227;&#26893;&#21040;CSI&#25968;&#25454;&#19978;&#24448;&#24448;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#23545;&#20934;&#26631;&#20934;&#19981;&#24403;&#65292;&#36825;&#30772;&#22351;&#20102;&#29305;&#24449;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;``Anetenna Response Consistency (ARC)''&#20316;&#20026;&#23450;&#20041;&#21512;&#36866;&#23545;&#20934;&#26631;&#20934;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;ARC&#30340;&#35774;&#35745;&#22312;&#20445;&#30041;&#36755;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#23545;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20174;CSI&#25968;&#25454;&#32467;&#26500;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;ARC&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26368;&#20248;&#35299;&#23548;&#33268;&#20102;&#20174;&#36755;&#20837;CSI&#25968;&#25454;&#21040;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#21160;&#20316;&#21521;&#37327;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \textbf{A}netenna \textbf{R}esponse \textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensi
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#24320;&#21457;&#20102;&#28023;&#27915;&#23376;&#21306;&#23610;&#24230;&#31283;&#20581;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#20026;&#35299;&#20915;&#27668;&#20505;&#27169;&#25311;&#20013;&#38271;&#26399;&#39044;&#27979;&#35823;&#24046;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.02691</link><description>&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#28023;&#27915;&#23376;&#21306;&#23610;&#24230;&#31283;&#20581;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators. (arXiv:2310.02691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#24320;&#21457;&#20102;&#28023;&#27915;&#23376;&#21306;&#23610;&#24230;&#31283;&#20581;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#20026;&#35299;&#20915;&#27668;&#20505;&#27169;&#25311;&#20013;&#38271;&#26399;&#39044;&#27979;&#35823;&#24046;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#27169;&#25311;&#20013;&#65292;&#23567;&#23610;&#24230;&#36807;&#31243;&#22609;&#36896;&#20102;&#28023;&#27915;&#21160;&#21147;&#23398;&#65292;&#20294;&#30452;&#25509;&#35299;&#20915;&#36825;&#20123;&#36807;&#31243;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#24120;&#24120;&#20351;&#29992;&#32463;&#39564;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#36129;&#29486;&#65292;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35823;&#24046;&#12290;&#26412;&#25991;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#24320;&#21457;&#20102;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#30340;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#39057;&#22495;&#20013;&#36816;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In climate simulations, small-scale processes shape ocean dynamics but remain computationally expensive to resolve directly. For this reason, their contributions are commonly approximated using empirical parameterizations, which lead to significant errors in long-term projections. In this work, we develop parameterizations based on Fourier Neural Operators, showcasing their accuracy and generalizability in comparison to other approaches. Finally, we discuss the potential and limitations of neural networks operating in the frequency domain, paving the way for future investigation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.01837</link><description>&lt;p&gt;
&#25193;&#23637;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#26080;&#27861;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;/&#25512;&#29702;&#25805;&#20316;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21482;&#33021;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#19987;&#23478;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#34892;&#20026;&#21644;&#22522;&#30784;&#20915;&#31574;&#36807;&#31243;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#30830;&#20445;AI&#27169;&#22411;&#31283;&#20581;&#12289;&#23454;&#29992;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#30340;&#25163;&#27573;&#12290;&#24050;&#26377;&#19968;&#20123;XAI&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#35299;&#37322;&#21017;&#22522;&#26412;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#26368;&#26032;&#30340;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#20854;&#36866;&#29992;&#20110;&#22810;&#31867;&#22270;&#20687;&#20998;&#21106;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.16064</link><description>&lt;p&gt;
&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#32454;&#32990;&#24418;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Masked autoencoders are scalable learners of cellular morphology. (arXiv:2309.16064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20869;&#23481;&#26174;&#24494;&#38236;&#26816;&#26597;&#20013;&#20174;&#32454;&#32990;&#34920;&#22411;&#20013;&#25512;&#26029;&#29983;&#29289;&#20851;&#31995;&#22312;&#29983;&#29289;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#27604;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#24449;&#26356;&#33021;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#26368;&#39640;&#23610;&#24230;&#19978;&#65292;&#19968;&#20010;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35206;&#30422;&#36229;&#36807;35&#20159;&#20010;&#21807;&#19968;&#21098;&#35009;&#22270;&#20687;&#30340;ViT-L/8&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#24050;&#30693;&#29983;&#29289;&#20851;&#31995;&#26102;&#30456;&#23545;&#25913;&#36827;&#39640;&#36798;28%&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how weakly supervised and self-supervised deep learning approaches scale when training larger models on larger datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 95-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised models at inferring known biological relationships curated from public databases.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#30028;&#38480;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26080;&#27861;&#25214;&#21040;&#32039;&#33268;&#30340;&#30028;&#38480;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13658</link><description>&lt;p&gt;
&#26080;&#27861;&#25214;&#21040;&#20986;&#33394;&#30340;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fantastic Generalization Measures are Nowhere to be Found. (arXiv:2309.13658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#30028;&#38480;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26080;&#27861;&#25214;&#21040;&#32039;&#33268;&#30340;&#30028;&#38480;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#27867;&#21270;&#30028;&#38480;&#20316;&#20026;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30028;&#38480;&#37117;&#19981;&#26159;&#32039;&#33268;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#20182;&#20204;&#30340;&#35770;&#25991;&#8220;Fantastic Generalization Measures and Where to Find Them&#8221;&#20013;&#65292;Jiang&#31561;&#20154;&#65288;2020&#65289;&#26816;&#26597;&#20102;&#21313;&#20960;&#20010;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#26377;&#21487;&#33021;&#25214;&#21040;&#32039;&#33268;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#20004;&#31181;&#27867;&#21270;&#30028;&#38480;&#65306;&#65288;1&#65289;&#20381;&#36182;&#20110;&#35757;&#32451;&#38598;&#21644;&#23398;&#20064;&#31639;&#27861;&#36755;&#20986;&#30340;&#30028;&#38480;&#12290;&#25991;&#29486;&#20013;&#26377;&#22810;&#20010;&#36825;&#31181;&#31867;&#22411;&#30340;&#30028;&#38480;&#65288;&#20363;&#22914;&#22522;&#20110;&#33539;&#25968;&#21644;&#22522;&#20110;&#38388;&#38548;&#30340;&#30028;&#38480;&#65289;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#30028;&#38480;&#33021;&#22815;&#19968;&#33268;&#22320;&#32039;&#33268;&#65307;&#65288;2&#65289;&#20381;&#36182;&#20110;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, none of these bounds are tight. For instance, in their paper ``Fantastic Generalization Measures and Where to Find Them'', Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them imply guarantees that can explain the remarkable performance of neural networks. This raises the question of whether tight generalization bounds are at all possible. We consider two types of generalization bounds common in the literature: (1) bounds that depend on the training set and the output of the learning algorithm. There are multiple bounds of this type in the literature (e.g., norm-based and margin-based bounds), but we prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that depend on the training set and on the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;PDMs&#65289;&#29983;&#25104;&#20102;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;PDMs&#25104;&#21151;&#29983;&#25104;&#20102;&#22797;&#26434;&#21644;&#36924;&#30495;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#20294;&#37319;&#26679;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20182;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#12289;&#37319;&#26679;&#21644;&#35780;&#20272;PDMs&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.16847</link><description>&lt;p&gt;
&#24178;&#28041;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Interferometric Satellite Aperture Radar. (arXiv:2308.16847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16847
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;PDMs&#65289;&#29983;&#25104;&#20102;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;PDMs&#25104;&#21151;&#29983;&#25104;&#20102;&#22797;&#26434;&#21644;&#36924;&#30495;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#20294;&#37319;&#26679;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20182;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#12289;&#37319;&#26679;&#21644;&#35780;&#20272;PDMs&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;PDMs&#65289;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;&#19968;&#31867;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#38750;&#33258;&#28982;&#22270;&#20687;&#65288;&#22914;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#25968;&#25454;&#65289;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#36824;&#26159;&#22823;&#37096;&#20998;&#26410;&#30693;&#30340;&#12290;&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#65288;&#23588;&#20854;&#26159;&#26631;&#35760;&#30340;&#65289;&#21355;&#26143;&#25968;&#25454;&#23545;&#20110;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#21644;&#20998;&#26512;&#65288;&#24178;&#28041;&#65289;&#21355;&#26143;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;PDMs&#29983;&#25104;&#20102;&#20960;&#20010;&#22522;&#20110;&#38647;&#36798;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PDMs&#25104;&#21151;&#29983;&#25104;&#20102;&#20855;&#26377;&#22797;&#26434;&#21644;&#36924;&#30495;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#20294;&#37319;&#26679;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20107;&#23454;&#19978;&#65292;&#21152;&#36895;&#37319;&#26679;&#31574;&#30053;&#22312;&#31616;&#21333;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#65289;&#19978;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#22833;&#36133;&#20102;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#24320;&#28304;&#24037;&#20855;https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#12289;&#37319;&#26679;&#21644;&#35780;&#20272;PDMs&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Diffusion Models (PDMs) have recently emerged as a very promising class of generative models, achieving high performance in natural image generation. However, their performance relative to non-natural images, like radar-based satellite data, remains largely unknown. Generating large amounts of synthetic (and especially labelled) satellite data is crucial to implement deep-learning approaches for the processing and analysis of (interferometric) satellite aperture radar data. Here, we leverage PDMs to generate several radar-based satellite image datasets. We show that PDMs succeed in generating images with complex and realistic structures, but that sampling time remains an issue. Indeed, accelerated sampling strategies, which work well on simple image datasets like MNIST, fail on our radar datasets. We provide a simple and versatile open-source https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and evaluate PDMs using any dataset on a single GPU.
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10364</link><description>&lt;p&gt;
SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;
&lt;/p&gt;
&lt;p&gt;
SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#33021;&#22815;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#20351;&#20854;&#25104;&#20026;&#29289;&#29702;&#31995;&#32479;&#27010;&#29575;&#24314;&#27169;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#32806;&#21512;&#26550;&#26500;&#26080;&#27861;&#36171;&#20104;&#25805;&#20316;&#21407;&#23376;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#27969;SE(3)&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27839;&#38468;&#21152;&#22686;&#24378;&#32500;&#24230;&#36827;&#34892;&#22352;&#26631;&#20998;&#21106;&#30340;&#32806;&#21512;&#27969;&#65292;&#20197;&#20445;&#25345;SE(3)&#21644;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;&#22312;&#27599;&#19968;&#23618;&#20013;&#65292;&#27969;&#23558;&#21407;&#23376;&#30340;&#20301;&#32622;&#26144;&#23556;&#21040;&#23398;&#20064;&#24471;&#21040;&#30340;SE(3)&#19981;&#21464;&#22522;&#19978;&#65292;&#25105;&#20204;&#22312;&#36820;&#22238;&#21040;&#21407;&#22987;&#22522;&#20043;&#21069;&#24212;&#29992;&#26631;&#20934;&#27969;&#21464;&#25442;&#65292;&#22914;&#21333;&#35843;&#20998;&#23376;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#27969;&#20445;&#25345;&#20102;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20135;&#29983;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#26399;&#26395;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#22312;DW4&#12289;LJ13&#21644;QM9&#20301;&#32622;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27969;&#19982;&#31561;&#21464;&#27969;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#31614;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#26469;&#35843;&#25972;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#26032;&#31867;&#21035;&#25110;&#25913;&#21892;&#38646;&#26679;&#26412;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.12226</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20960;&#20309;&#24863;&#30693;&#33258;&#36866;&#24212;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Adaptation for Pretrained Models. (arXiv:2307.12226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#31614;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#26469;&#35843;&#25972;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#26032;&#31867;&#21035;&#25110;&#25913;&#21892;&#38646;&#26679;&#26412;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#20165;&#20855;&#26377;&#36739;&#23567;&#27604;&#20363;&#26631;&#31614;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#26631;&#31614;&#31354;&#38388;&#36890;&#24120;&#20351;&#29992;&#24230;&#37327;&#26469;&#34913;&#37327;&#26631;&#31614;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#23558;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#35843;&#25972;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#26032;&#31867;&#21035;&#65292;&#25110;&#32773;&#22312;&#38646;&#26679;&#26412;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#26631;&#20934;&#39044;&#27979;&#35268;&#21017;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#20854;&#20013;&#23558;argmax&#26367;&#25442;&#20026;Fr&#233;chet&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#20026;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#65288;i&#65289;&#23398;&#20064;&#29702;&#35770;&#32467;&#26524;&#65292;&#26435;&#34913;&#26631;&#31614;&#31354;&#38388;&#30452;&#24452;&#12289;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#32500;&#24230;&#65292;&#65288;ii&#65289;&#34920;&#24449;&#21487;&#33021;&#39044;&#27979;&#20219;&#20309;&#26410;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#30340;&#25152;&#26377;&#24773;&#26223;&#30340;&#29305;&#24449;&#65292;&#65288;iii&#65289;&#19968;&#31181;&#26368;&#20248;&#30340;&#20027;&#21160;&#23398;&#20064;&#24335;&#19979;&#19968;&#31867;&#21035;&#36873;&#25321;&#36807;&#31243;&#65292;&#20197;&#33719;&#21462;&#26368;&#20339;&#30340;&#35757;&#32451;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#20301;&#26080;&#27169;&#22411;&#20248;&#21270;&#23454;&#29616;&#39640;&#24615;&#33021;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#39640;&#36895;&#21644;&#20302;&#33021;&#32791;&#30340;&#25968;&#25454;&#22788;&#29702;&#65292;&#20294;&#22312;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#25442;&#20013;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#30340;&#36731;&#37327;&#32423;&#21407;&#20301;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#31995;&#32479;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#30452;&#25509;&#23558;&#25439;&#22833;&#21453;&#21521;&#20256;&#25773;&#21040;&#20809;&#23398;&#26435;&#37325;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#21644;&#26377;&#20559;&#35265;&#30340;&#31995;&#32479;&#27169;&#25311;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#22312;&#21333;&#23618;&#34893;&#23556;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;FMNIST&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26080;&#22270;&#29255;&#21644;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#65292;&#32467;&#21512;&#20854;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#20302;&#38656;&#27714;&#65292;&#21152;&#36895;&#20102;&#20809;&#23398;&#35745;&#31639;&#20174;&#23454;&#39564;&#23460;&#28436;&#31034;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20102;&#21160;&#24577;&#22270;&#20013;&#36793;&#30340;&#26356;&#26032;&#65292;&#35299;&#20915;&#20102;&#35774;&#35745;&#21160;&#24577;&#31639;&#27861;&#20013;&#30340;&#26410;&#30693;&#26356;&#26032;&#24207;&#21015;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36825;&#19968;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#65292;&#22312;&#29702;&#35770;&#19978;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.08890</link><description>&lt;p&gt;
&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65306;&#20805;&#20998;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#65292;&#23454;&#29616;&#38646;&#25104;&#26412;&#65288;arXiv:2307.08890v1 [cs.DS]&#65289;
&lt;/p&gt;
&lt;p&gt;
The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free. (arXiv:2307.08890v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20102;&#21160;&#24577;&#22270;&#20013;&#36793;&#30340;&#26356;&#26032;&#65292;&#35299;&#20915;&#20102;&#35774;&#35745;&#21160;&#24577;&#31639;&#27861;&#20013;&#30340;&#26410;&#30693;&#26356;&#26032;&#24207;&#21015;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36825;&#19968;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#65292;&#22312;&#29702;&#35770;&#19978;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#39640;&#25928;&#30340;&#21160;&#24577;&#31639;&#27861;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#26356;&#26032;&#24207;&#21015;&#30340;&#26410;&#30693;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;3-&#39030;&#28857;&#36830;&#36890;&#24615;&#12289;&#24179;&#38754;&#26377;&#21521;&#22270;&#25152;&#26377;&#28857;&#23545;&#26368;&#30701;&#36335;&#24452;&#31561;&#65292;&#26368;&#20339;&#30340;&#37096;&#20998;&#21160;&#24577;&#35299;&#21644;&#26368;&#20339;&#30340;&#20840;&#21160;&#24577;&#35299;&#20043;&#38388;&#30340;&#36816;&#34892;&#26102;&#38388;&#24046;&#24322;&#26159;&#22810;&#39033;&#24335;&#30340;&#65292;&#29978;&#33267;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#27979;&#21024;&#38500;&#21160;&#24577;&#27169;&#22411;&#65292;&#21463;&#21040;&#36817;&#26399;&#20851;&#20110;&#39044;&#27979;&#21160;&#24577;&#22270;&#20013;&#36793;&#26356;&#26032;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#21551;&#21457;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#36793;&#22312;&#32447;&#19978;&#34987;&#25554;&#20837;&#21644;&#21024;&#38500;&#65292;&#24182;&#19988;&#24403;&#19968;&#26465;&#36793;&#34987;&#25554;&#20837;&#26102;&#65292;&#23427;&#24102;&#26377;&#19968;&#20010;&#23427;&#21024;&#38500;&#26102;&#38388;&#30340;&#8220;&#39044;&#27979;&#8221;&#12290;&#35813;&#27169;&#22411;&#21453;&#26144;&#20102;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#19968;&#20123;&#24773;&#26223;&#65292;&#20854;&#20013;&#26381;&#21153;&#21487;&#20197;&#35775;&#38382;&#21382;&#21490;&#25968;&#25454;&#25110;&#20854;&#20182;&#20851;&#20110;&#36755;&#20837;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#25454;&#27492;&#23545;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#20063;&#26377;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#25554;&#20540;&#20102;&#37096;&#20998;&#21160;&#24577;&#35299;&#21644;&#20840;&#21160;&#24577;&#35299;&#20043;&#38388;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main bottleneck in designing efficient dynamic algorithms is the unknown nature of the update sequence. In particular, there are some problems, like 3-vertex connectivity, planar digraph all pairs shortest paths, and others, where the separation in runtime between the best partially dynamic solutions and the best fully dynamic solutions is polynomial, sometimes even exponential.  In this paper, we formulate the predicted-deletion dynamic model, motivated by a recent line of empirical work about predicting edge updates in dynamic graphs. In this model, edges are inserted and deleted online, and when an edge is inserted, it is accompanied by a "prediction" of its deletion time. This models real world settings where services may have access to historical data or other information about an input and can subsequently use such information make predictions about user behavior. The model is also of theoretical interest, as it interpolates between the partially dynamic and fully dynamic set
&lt;/p&gt;</description></item><item><title>&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04870</link><description>&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#65306;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04870
&lt;/p&gt;
&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;(OUA)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;OUA&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#25110;&#24369;&#20449;&#21495;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#29992;&#20110;&#27809;&#26377;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#30001;&#24369;&#20449;&#21495;&#25152;&#26500;&#25104;&#30340;&#31354;&#38388;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;OUA&#22312;&#19968;&#33324;&#30340;&#24369;&#20449;&#21495;&#38598;&#21512;&#19979;&#20855;&#26377;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#65292;OUA&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.00154</link><description>&lt;p&gt;
Stitched ViTs&#26159;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#65292;&#25552;&#20986;&#20102;SN-Netv2&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#26222;&#36890;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;ViTs&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#37319;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;ViTs&#38656;&#35201;&#21333;&#29420;&#35757;&#32451;&#65292;&#24182;&#21463;&#21040;&#22266;&#23450;&#30340;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#21487;&#25340;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25340;&#25509;&#39044;&#35757;&#32451;&#27169;&#22411;&#26063;&#32676;&#26469;&#24555;&#36895;&#29983;&#25104;&#28085;&#30422;&#20016;&#23500;&#23376;&#32593;&#32476;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#25903;&#25345;&#22312;&#36816;&#34892;&#26102;&#30340;&#22810;&#26679;&#24615;&#24615;&#33021;-&#25928;&#29575;&#26435;&#34913;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SN-Netv2&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#25913;&#36827;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#20419;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#25340;&#25509;&#26041;&#26696;&#26469;&#25193;&#22823;&#25340;&#25509;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32771;&#34385;&#31354;&#38388;&#20013;&#24213;&#23618;FLOPs&#20998;&#24067;&#30340;&#36164;&#28304;&#21463;&#38480;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#37319;&#26679;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;SN-Netv2&#36827;&#34892;&#20102;&#32454;&#24494;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained plain vision Transformers (ViTs) have been the workhorse for many downstream tasks. However, existing works utilizing off-the-shelf ViTs are inefficient in terms of training and deployment, because adopting ViTs with individual sizes requires separate training and is restricted by fixed performance-efficiency trade-offs. In this paper, we are inspired by stitchable neural networks, which is a new framework that cheaply produces a single model that covers rich subnetworks by stitching pretrained model families, supporting diverse performance-efficiency trade-offs at runtime. Building upon this foundation, we introduce SN-Netv2, a systematically improved model stitching framework to facilitate downstream task adaptation. Specifically, we first propose a Two-way stitching scheme to enlarge the stitching space. We then design a resource-constrained sampling strategy that takes into account the underlying FLOPs distributions in the space for improved sampling. Finally, we o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#65288;GraSS&#65289;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#28151;&#28102;&#21644;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15868</link><description>&lt;p&gt;
GraSS:&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation. (arXiv:2306.15868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15868
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#65288;GraSS&#65289;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#27491;&#26679;&#26412;&#28151;&#28102;&#21644;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SSCL&#65289;&#22312;&#36965;&#24863;&#22270;&#20687;&#65288;RSI&#65289;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#37324;&#31243;&#30865;&#12290;&#20854;&#26680;&#24515;&#22312;&#20110;&#35774;&#35745;&#19968;&#31181;&#26080;&#30417;&#30563;&#23454;&#20363;&#21306;&#20998;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#21033;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;SSCL&#22312;&#24212;&#29992;&#20110;RSI&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#26102;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;1&#65289;&#27491;&#26679;&#26412;&#28151;&#28102;&#38382;&#39064;&#65307;2&#65289;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#12290;&#22312;&#38656;&#35201;&#20687;&#32032;&#32423;&#25110;&#30446;&#26631;&#32423;&#29305;&#24449;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#23427;&#24341;&#20837;&#20102;&#29305;&#24449;&#36866;&#24212;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#37492;&#21035;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30340;&#26799;&#24230;&#26144;&#23556;&#21040;RSI&#30340;&#29305;&#23450;&#21306;&#22495;&#65292;&#36825;&#20123;&#29305;&#23450;&#21306;&#22495;&#24448;&#24448;&#21253;&#21547;&#29305;&#27530;&#30340;&#22320;&#38754;&#23545;&#35937;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#31574;&#30053;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GraSS&#65289;&#29992;&#20110;RSI&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised contrastive learning (SSCL) has achieved significant milestones in remote sensing image (RSI) understanding. Its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. However, existing instance discrimination based SSCL suffer from two limitations when applied to the RSI semantic segmentation task: 1) Positive sample confounding issue; 2) Feature adaptation bias. It introduces a feature adaptation bias when applied to semantic segmentation tasks that require pixel-level or object-level features. In this study, We observed that the discrimination information can be mapped to specific regions in RSI through the gradient of unsupervised contrastive loss, these specific regions tend to contain singular ground objects. Based on this, we propose contrastive learning with Gradient guided Sampling Strategy (GraSS) for RSI semantic segmentation. Gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377; $S$ &#20010;&#19978;&#19979;&#25991;&#21644; $A$ &#31181;&#34892;&#21160;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#32452;&#30340;&#38543;&#26426;&#36172;&#24466;&#38382;&#39064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13053</link><description>&lt;p&gt;
&#21487;&#20998;&#32452;&#30340;&#38543;&#26426;&#36172;&#24466;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Context-lumpable stochastic bandits. (arXiv:2306.13053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377; $S$ &#20010;&#19978;&#19979;&#25991;&#21644; $A$ &#31181;&#34892;&#21160;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#32452;&#30340;&#38543;&#26426;&#36172;&#24466;&#38382;&#39064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377; $S$ &#20010;&#19978;&#19979;&#25991;&#21644; $A$ &#31181;&#34892;&#21160;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#12290;&#22312;&#27599;&#19968;&#36718; $t=1,2,\dots$ &#20013;&#65292;&#23398;&#20064;&#32773;&#35266;&#23519;&#19968;&#20010;&#38543;&#26426;&#19978;&#19979;&#25991;&#65292;&#24182;&#26681;&#25454;&#20854;&#20197;&#24448;&#30340;&#32463;&#39564;&#36873;&#25321;&#19968;&#20010;&#34892;&#21160;&#12290;&#28982;&#21518;&#65292;&#23398;&#20064;&#32773;&#35266;&#23519;&#19968;&#20010;&#38543;&#26426;&#22870;&#21169;&#65292;&#20854;&#24179;&#22343;&#20540;&#26159;&#35813;&#36718;&#19978;&#19979;&#25991;&#21644;&#34892;&#21160;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#22312;&#20551;&#35774;&#19978;&#19979;&#25991;&#21487;&#20197;&#20998;&#20026; $r\leq \min\{S,A\}$ &#32452;&#65292;&#20351;&#24471;&#20219;&#24847;&#20004;&#20010;&#22312;&#21516;&#19968;&#32452;&#20869;&#30340;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#34892;&#21160;&#30340;&#24179;&#22343;&#22870;&#21169;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#22312;&#20351;&#29992; $\widetilde O(r(S + A)/\epsilon^2)$ &#20010;&#26679;&#26412;&#21518;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010; $\epsilon$-&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#32622;&#20449;&#24230;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340; $\widetilde\Omega(r (S + A )/\epsilon^2)$ &#19979;&#38480;&#12290;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20854;&#32047;&#31215;&#36951;&#25022;&#22312;&#26102;&#38388; $T$ &#20869;&#26377;&#30028;&#65292;&#21363; $\widetilde O(\sqrt{r^3(S +A)T})$&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23637;&#31034;&#22312;&#21487;&#36817;&#20284;&#27491;&#30830;&#35774;&#32622;&#20013;&#25509;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#30340; $\widetilde O(\sqrt{r^3(S +A)T})$ &#32047;&#31215;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a contextual bandit problem with $S $ contexts and $A $ actions. In each round $t=1,2,\dots$ the learner observes a random context and chooses an action based on its past experience. The learner then observes a random reward whose mean is a function of the context and the action for the round. Under the assumption that the contexts can be lumped into $r\le \min\{S ,A \}$ groups such that the mean reward for the various actions is the same for any two contexts that are in the same group, we give an algorithm that outputs an $\epsilon$-optimal policy after using at most $\widetilde O(r (S +A )/\epsilon^2)$ samples with high probability and provide a matching $\widetilde\Omega(r (S +A )/\epsilon^2)$ lower bound. In the regret minimization setting, we give an algorithm whose cumulative regret up to time $T$ is bounded by $\widetilde O(\sqrt{r^3(S +A )T})$. To the best of our knowledge, we are the first to show the near-optimal sample complexity in the PAC setting and $\widetild
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#21644;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#65288;DOF-ID&#65289;&#26550;&#26500;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#29992;&#20110;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20174;&#20854;&#20182;&#32593;&#32476;&#31995;&#32479;&#20013;&#33719;&#24471;&#32463;&#39564;&#20197;&#21450;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#32780;&#19981;&#36829;&#21453;&#20854;&#20182;&#31995;&#32479;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27491;&#22312;&#21327;&#20316;&#30340;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13029</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22312;&#32447;&#32852;&#37030; G &#32593;&#32476;&#23398;&#20064;&#29992;&#20110;&#36731;&#37327;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Decentralized Online Federated G-Network Learning for Lightweight Intrusion Detection. (arXiv:2306.13029v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#21644;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#65288;DOF-ID&#65289;&#26550;&#26500;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#29992;&#20110;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20174;&#20854;&#20182;&#32593;&#32476;&#31995;&#32479;&#20013;&#33719;&#24471;&#32463;&#39564;&#20197;&#21450;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#32780;&#19981;&#36829;&#21453;&#20854;&#20182;&#31995;&#32479;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27491;&#22312;&#21327;&#20316;&#30340;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#22411;&#26410;&#30693;&#65288;&#38646;&#26085;&#65289;&#25915;&#20987;&#30340;&#20986;&#29616;&#21644;&#26131;&#21463;&#25915;&#20987;&#30340;&#35774;&#22791;&#30340;&#20852;&#36215;&#65292;&#32593;&#32476;&#31995;&#32479;&#38754;&#20020;&#30528;&#26085;&#30410;&#22686;&#21152;&#30340;&#32593;&#32476;&#25915;&#20987;&#23041;&#32961;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#22312;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#23398;&#20064;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#32463;&#24120;&#38480;&#21046;&#20102;&#20165;&#26377;&#31169;&#26377;&#26412;&#22320;&#25968;&#25454;&#35775;&#38382;&#30340;&#32593;&#32476;&#31995;&#32479;&#24212;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#21644;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#65288;DOF-ID&#65289;&#26550;&#26500;&#12290;DOF-ID &#26159;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#31995;&#32479;&#65292;&#20801;&#35768;&#27599;&#20010;&#29992;&#20110;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20174;&#20854;&#20182;&#32593;&#32476;&#31995;&#32479;&#20013;&#33719;&#24471;&#30340;&#32463;&#39564;&#20197;&#21450;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#32780;&#19981;&#36829;&#21453;&#20854;&#20182;&#31995;&#32479;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#36890;&#36807;&#20351;&#29992;&#20844;&#20849; Kitsune &#21644; Bot-IoT &#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;DOF-ID &#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27491;&#22312;&#21327;&#20316;&#30340;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyberattacks are increasingly threatening networked systems, often with the emergence of new types of unknown (zero-day) attacks and the rise of vulnerable devices. While Machine Learning (ML)-based Intrusion Detection Systems (IDSs) have been shown to be extremely promising in detecting these attacks, the need to learn large amounts of labelled data often limits the applicability of ML-based IDSs to cybersystems that only have access to private local data. To address this issue, this paper proposes a novel Decentralized and Online Federated Learning Intrusion Detection (DOF-ID) architecture. DOF-ID is a collaborative learning system that allows each IDS used for a cybersystem to learn from experience gained in other cybersystems in addition to its own local data without violating the data privacy of other systems. As the performance evaluation results using public Kitsune and Bot-IoT datasets show, DOF-ID significantly improves the intrusion detection performance in all collaborating 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.11363</link><description>&lt;p&gt;
&#21463;&#36974;&#34109;&#25193;&#25955;&#27169;&#22411;&#26159;&#24555;&#36895;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#20107;&#23454;&#19978;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#35813;&#25216;&#26415;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#36974;&#34109;&#36755;&#20837;&#22270;&#20687;&#30340;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;&#39640;&#36798;90&#65285;&#65289;&#65292;&#24182;&#21033;&#29992;&#36974;&#34109;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#26469;&#21435;&#22122;&#21487;&#35265;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#26174;&#33879;&#30340;&#29305;&#24449;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#36974;&#34109;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;CelebA-HQ $256 \times 256$&#20687;&#32032;&#31354;&#38388;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#20102;&#22522;&#20110;ViT&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;4&#20493;&#21152;&#36895;&#65292;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#19982;&#21435;&#22122;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.07745</link><description>&lt;p&gt;
&#26680;&#21270;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#20855;&#26377;&#22797;&#26434;&#27169;&#22411;&#21644;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#35777;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#32467;&#26524;&#36890;&#24120;&#38598;&#20013;&#20110;&#20855;&#26377;&#23569;&#37327;&#29366;&#24577;-&#34892;&#20026;&#25110;&#31616;&#21333;&#27169;&#22411;&#65288;&#20363;&#22914;&#32447;&#24615;&#24314;&#27169;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#65289;&#30340;&#35774;&#32622;&#12290; &#20026;&#20102;&#25512;&#23548;&#26377;&#25928;&#22788;&#29702;&#26356;&#24191;&#27867;&#20540;&#20989;&#25968;&#30340;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;RL&#31574;&#30053;&#65292;&#19968;&#20123;&#26368;&#26032;&#24037;&#20316;&#32771;&#34385;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;$\pi$-KRVI&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#19968;&#31181;&#20048;&#35266;&#20462;&#25913;&#65292;&#24403;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#30001;RKHS&#34920;&#31034;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#39640;&#24230;&#38750;&#20809;&#28369;&#20869;&#26680;&#65288;&#20363;&#22914;&#31070;&#32463;&#20999;&#21521;&#20869;&#26680;&#25110;&#26576;&#20123;Mat\'ern&#20869;&#26680;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18766</link><description>&lt;p&gt;
HiFA: &#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#21450;&#20854;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#22312;&#25552;&#21319;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#25552;&#20379;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#30340;2D&#28210;&#26579;&#24471;&#20998;&#24182;&#29992;&#20110;&#20248;&#21270;NeRFs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;3D&#20960;&#20309;&#30340;&#26377;&#38480;&#29702;&#35299;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36935;&#21040;&#22810;&#20010;&#35270;&#35282;&#19978;&#30340;&#20266;&#24433;&#21644;&#19981;&#19968;&#33268;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#37325;&#26032;&#21046;&#23450;&#20248;&#21270;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#37322;&#25918;&#20102;&#25193;&#25955;&#20808;&#39564;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#65292;&#25105;&#20204;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#65292;&#24182;&#35268;&#33539;&#21270;NeRF&#30340;&#23494;&#24230;&#22330;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#24212;&#23545;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#26131;&#21463;&#20849;&#38169;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24809;&#32602;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11475</link><description>&lt;p&gt;
&#26354;&#32447;&#19978;&#25196;&#65306;&#22312;&#21487;&#24494;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#20013;&#30340;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models. (arXiv:2305.11475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#24212;&#23545;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#26131;&#21463;&#20849;&#38169;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24809;&#32602;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;GAM&#65289;&#21487;&#34920;&#36798;&#30446;&#26631;&#21464;&#37327;&#20026;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#21644;&#35299;&#37322;&#24615;&#65292;&#20854;&#20877;&#27425;&#21463;&#21040;&#27426;&#36814;&#12290;&#23613;&#31649;GAM&#30446;&#21069;&#22791;&#21463;&#28909;&#25447;&#65292;&#20294;&#20854;&#26131;&#21463;&#20849;&#38169;&#24615;&#65292;&#21363;&#29305;&#24449;&#20043;&#38388;&#30340;&#65288;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65289;&#20381;&#36182;&#24615;&#36804;&#20170;&#20026;&#27490;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20849;&#38169;&#24615;&#22914;&#20309;&#20005;&#37325;&#30772;&#22351;GAM&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#27861;&#65306;&#19968;&#20010;&#22312;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#19978;&#36827;&#34892;&#24809;&#32602;&#30340;&#27010;&#24565;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#35813;&#36807;&#31243;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#30340;&#21152;&#24615;&#27169;&#22411;&#65292;&#22914;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#25110;&#31070;&#32463;&#39044;&#35328;&#12290;&#24182;&#19988;&#36890;&#36807;&#28040;&#38500;&#33258;&#25105;&#25269;&#28040;&#30340;&#29305;&#24449;&#36129;&#29486;&#30340;&#27495;&#20041;&#65292;&#22686;&#24378;&#20102;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity - i.e., (possibly non-linear) dependencies between the features - has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;Dir-GNN&#65292;&#24182;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10498</link><description>&lt;p&gt;
&#36793;&#26041;&#21521;&#24615;&#25552;&#39640;&#20102;&#24322;&#36136;&#22270;&#19978;&#30340;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Edge Directionality Improves Learning on Heterophilic Graphs. (arXiv:2305.10498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;Dir-GNN&#65292;&#24182;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#24314;&#27169;&#20851;&#31995;&#25968;&#25454;&#30340;&#20107;&#23454;&#26631;&#20934;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#26159;&#26377;&#21521;&#30340;&#65292;&#20294;&#20170;&#22825;&#22823;&#22810;&#25968;GNN&#27169;&#22411;&#37117;&#36890;&#36807;&#20351;&#22270;&#25104;&#20026;&#26080;&#21521;&#22270;&#26469;&#23436;&#20840;&#24573;&#30053;&#36825;&#20123;&#20449;&#24687;&#12290;&#36825;&#26679;&#20570;&#30340;&#21407;&#22240;&#26159;&#21382;&#21490;&#24615;&#30340;&#65306;1&#65289;&#35768;&#22810;&#26089;&#26399;&#30340;&#35889;GNN&#21464;&#20307;&#26126;&#30830;&#35201;&#27714;&#22270;&#26159;&#26080;&#21521;&#30340;&#65292;2&#65289;&#20851;&#20110;&#21516;&#31867;&#22270;&#30340;&#31532;&#19968;&#25209;&#22522;&#20934;&#27979;&#35797;&#24182;&#26410;&#21457;&#29616;&#20351;&#29992;&#26041;&#21521;&#24615;&#26377;&#26126;&#26174;&#30340;&#22686;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24322;&#31867;&#35774;&#32622;&#20013;&#65292;&#23558;&#22270;&#24418;&#35270;&#20026;&#26377;&#21521;&#22270;&#21487;&#20197;&#22686;&#21152;&#22270;&#30340;&#20869;&#22312;&#21516;&#36136;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#20174;&#27491;&#30830;&#20351;&#29992;&#26041;&#21521;&#24615;&#20449;&#24687;&#20013;&#21487;&#33021;&#24471;&#21040;&#30340;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Directed Graph Neural Network&#65288;Dir-GNN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#38754;&#21521;&#26377;&#21521;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#12290;Dir-GNN&#21487;&#20197;&#29992;&#20110;&#25193;&#23637;&#20219;&#20309;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#20197;&#36890;&#36807;&#23545;&#27599;&#20010;&#33410;&#28857;&#25191;&#34892;&#21333;&#29420;&#30340;&#36827;&#20986;&#28040;&#24687;&#32858;&#21512;&#26469;&#32771;&#34385;&#36793;&#26041;&#21521;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#35780;&#20272;&#20102;Dir-GNN&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26041;&#21521;&#24615;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;Dir-GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are directed, the majority of today's GNN models discard this information altogether by simply making the graph undirected. The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. In this paper, we show that in heterophilic settings, treating the graph as directed increases the effective homophily of the graph, suggesting a potential gain from the correct use of directionality information. To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel general framework for deep learning on directed graphs. Dir-GNN can be used to extend any Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the incoming and outg
&lt;/p&gt;</description></item><item><title>Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.08415</link><description>&lt;p&gt;
Marsellus: &#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC
&lt;/p&gt;
&lt;p&gt;
Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08415
&lt;/p&gt;
&lt;p&gt;
Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#20114;&#32852;&#29289;&#32852;&#32593;&#65288;AI-IoT&#65289;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#38656;&#35201;&#22312;&#33539;&#22260;&#24191;&#27867;&#30340;&#24037;&#20316;&#26465;&#20214;&#19979;&#65292;&#22312;&#20960;&#21313;&#27627;&#29926;&#30340;&#21151;&#32791;&#38480;&#21046;&#19979;&#36816;&#34892;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#23494;&#38598;&#22411;&#20294;&#24378;&#37327;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#20197;&#21450;&#38656;&#35201;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Marsellus&#65292;&#19968;&#20010;&#22312;GlobalFoundries 22nm FDX&#19978;&#21046;&#36896;&#30340;&#20840;&#25968;&#23383;&#24322;&#26500;SoC&#65292;&#29992;&#20110;AI-IoT&#26411;&#31471;&#33410;&#28857;&#65292;&#23427;&#32467;&#21512;&#20102;&#65306;1&#65289;&#19968;&#20010;16&#20010;RISC-V&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#26680;&#24515;&#30340;&#36890;&#29992;&#38598;&#32676;&#65292;&#29992;&#20110;&#25191;&#34892;&#21508;&#31181;&#25903;&#25345;4&#20301;&#21644;2&#20301;&#31639;&#26415;&#25193;&#23637;&#65288;XpulpNN&#65289;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#34701;&#21512;&#30340;MAC&#21644;LOAD&#25805;&#20316;&#21644;&#28014;&#28857;&#25903;&#25345;&#65307;2&#65289;&#19968;&#20010;2-8&#20301;&#21487;&#37325;&#26500;&#20108;&#36827;&#21046;&#24341;&#25806;&#65288;RBE&#65289;&#65292;&#29992;&#20110;&#21152;&#36895;DNN&#20013;&#30340;3x3&#21644;1x1&#65288;&#36880;&#28857;&#65289;&#21367;&#31215;&#65307;3&#65289;&#19968;&#32452;&#36830;&#25509;&#21040;&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#29255;&#19978;&#30417;&#35270;&#65288;OCM&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT) System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and nano-robotics need to run many diverse tasks within a power envelope of a few tens of mW over a wide range of operating conditions: compute-intensive but strongly quantized Deep Neural Network (DNN) inference, as well as signal processing and control requiring high-precision floating-point. We present Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16 RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions (XpulpNN), combined with fused MAC&amp;LOAD operations and floating-point support; 2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1 (pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks connected to an Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.18158</link><description>&lt;p&gt;
&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#31209;&#19968;&#20989;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#28041;&#21450;&#21040;&#36890;&#36807;&#32422;&#26463;&#26469;&#24314;&#27169;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#37319;&#29992;&#25351;&#26631;&#21464;&#37327;&#26469;&#35782;&#21035;&#36830;&#32493;&#21464;&#37327;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#36890;&#36807;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25903;&#25345;&#20989;&#25968;&#21442;&#25968;&#21644;&#31163;&#25955;&#35268;&#21010;&#25216;&#26415;&#20197;&#25552;&#20379;&#20984;&#21253;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26041;&#27861;&#65292;&#21033;&#29992;&#36879;&#35270;&#20989;&#25968;&#24341;&#36215;&#30340;&#38544;&#34255;&#22278;&#38181;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#27599;&#20010;&#22278;&#38181;&#32422;&#26463;&#28041;&#21450;&#29420;&#31435;&#36830;&#32493;&#21464;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#21644;&#19968;&#32452;&#20108;&#20803;&#21464;&#37327;&#30340;&#19968;&#33324;&#22278;&#38181;&#28151;&#21512;&#20108;&#36827;&#21046;&#38598;&#21512;&#24314;&#31435;&#20102;&#19968;&#20010;&#20984;&#21253;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#24212;&#23545;epi&#30456;&#20851;&#30340;&#38598;&#21512;&#30340;&#25193;&#23637;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving minimization of a rank-one convex function over constraints modeling restrictions on the support of the decision variables emerge in various machine learning applications. These problems are often modeled with indicator variables for identifying the support of the continuous variables. In this paper we investigate compact extended formulations for such problems through perspective reformulation techniques. In contrast to the majority of previous work that relies on support function arguments and disjunctive programming techniques to provide convex hull results, we propose a constructive approach that exploits a hidden conic structure induced by perspective functions. To this end, we first establish a convex hull result for a general conic mixed-binary set in which each conic constraint involves a linear function of independent continuous variables and a set of binary variables. We then demonstrate that extended representations of sets associated with epi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09373</link><description>&lt;p&gt;
3D&#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#29992;&#20110;&#24322;&#26500;&#23156;&#20799;&#33041; MRI &#39046;&#22495;&#38388;&#36866;&#24212;&#24615;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#33041; MRI &#22312;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#12289;&#36328;&#22330;&#26223;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#20998;&#21106;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#33945;&#29256;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#26469;&#23545;&#23156;&#20799;&#33041;MRI&#30340;&#19981;&#21516;&#20122;&#30382;&#36136;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#26631;&#35760;&#28304;&#22495;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
&lt;/p&gt;</description></item><item><title>FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.01928</link><description>&lt;p&gt;
&#22522;&#20110;Shapley&#20540;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#25968;&#25454;&#20877;&#21152;&#26435;&#26041;&#27861;FairShap
&lt;/p&gt;
&lt;p&gt;
FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01928
&lt;/p&gt;
&lt;p&gt;
FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#26497;&#20854;&#37325;&#35201;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#28982;&#32780;&#24403;&#21069;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#35201;&#27714;&#20351;&#29992;&#36890;&#24120;&#23384;&#22312;&#20559;&#24046;&#30340;&#28023;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19987;&#27880;&#20110;&#24314;&#27169;&#21644;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Shapley&#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#30340;&#39044;&#22788;&#29702;&#65288;&#20877;&#21152;&#26435;&#65289;&#26041;&#27861;FairShap&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#34913;&#37327;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#39044;&#23450;&#20041;&#30340;&#20844;&#24179;&#25351;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#36136;&#65292;&#26377;&#21508;&#31181;&#22521;&#35757;&#22330;&#26223;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20135;&#29983;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#24182;&#19988;&#20934;&#30830;&#24230;&#26356;&#39640;&#25110;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30452;&#26041;&#22270;&#21644;&#28508;&#31354;&#38388;&#21487;&#35270;&#21270;&#26469;&#35828;&#26126;FairShap&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23545;&#20110;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness is of utmost societal importance, yet the current trend in large-scale machine learning models requires training with massive datasets that are typically biased. In this context, pre-processing methods that focus on modeling and correcting bias in the data emerge as valuable approaches. In this paper, we propose FairShap, a novel pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation by means of Shapley Values. Our approach is model agnostic and easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with a variety of training scenarios and models and show how it outperforms other methods, yielding fairer models with higher or similar levels of accuracy. We also illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe that this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.08892</link><description>&lt;p&gt;
D3G: &#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
D3G: Learning Multi-robot Coordination from Demonstrations. (arXiv:2207.08892v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21487;&#24494;&#21160;&#24577;&#28216;&#25103;&#65288;D3G&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#25105;&#20204;&#23558;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#34920;&#31034;&#20026;&#19968;&#20010;&#21160;&#24577;&#28216;&#25103;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#21463;&#20854;&#33258;&#36523;&#21160;&#24577;&#21644;&#30446;&#26631;&#30340;&#25511;&#21046;&#65292;&#21516;&#26102;&#20063;&#21462;&#20915;&#20110;&#20854;&#20182;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35843;&#25972;&#27599;&#20010;&#26426;&#22120;&#20154;&#30340;&#30446;&#26631;&#21644;&#21160;&#24577;&#65292;&#21487;&#20197;&#36866;&#24212;&#21327;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;D3G&#20351;&#27599;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#22312;&#20998;&#24067;&#24335;&#26041;&#24335;&#19979;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#20855;&#26377;&#26032;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#65292;&#25152;&#26377;&#26426;&#22120;&#20154;&#21512;&#20316;&#23547;&#25214;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#20197;&#21450;&#19968;&#20010;&#21453;&#21521;&#20256;&#36882;&#65292;&#22312;&#36890;&#20449;&#22270;&#20013;&#20256;&#25773;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#27979;&#35797;&#20102;D3G&#65292;&#24182;&#32473;&#20986;&#20102;&#19981;&#21516;&#20219;&#21153;&#37197;&#32622;&#30340;&#20004;&#31181;&#26426;&#22120;&#20154;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;D3G&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a Distributed Differentiable Dynamic Game (D3G) framework, which enables learning multi-robot coordination from demonstrations. We represent multi-robot coordination as a dynamic game, where the behavior of a robot is dictated by its own dynamics and objective that also depends on others' behavior. The coordination thus can be adapted by tuning the objective and dynamics of each robot. The proposed D3G enables each robot to automatically tune its individual dynamics and objectives in a distributed manner by minimizing the mismatch between its trajectory and demonstrations. This learning framework features a new design, including a forward-pass, where all robots collaboratively seek Nash equilibrium of a game, and a backward-pass, where gradients are propagated via the communication graph. We test the D3G in simulation with two types of robots given different task configurations. The results validate the capability of D3G for learning multi-robot coordination from de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#32622;&#20449;&#21306;&#38388;&#19978;&#38480;&#25277;&#26679;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20551;&#35774;&#20855;&#26377;&#20132;&#25442;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#25628;&#32034;&#27169;&#22411;&#26550;&#26500;&#36873;&#25321;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.03017</link><description>&lt;p&gt;
&#24182;&#34892;&#30340;&#19968;&#33268;&#32622;&#20449;&#21306;&#38388;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parallel Conformal Hyperparameter Optimization. (arXiv:2207.03017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#32622;&#20449;&#21306;&#38388;&#19978;&#38480;&#25277;&#26679;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20551;&#35774;&#20855;&#26377;&#20132;&#25442;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#25628;&#32034;&#27169;&#22411;&#26550;&#26500;&#36873;&#25321;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20986;&#29616;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#26694;&#26550;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#20381;&#36182;&#20110;&#20005;&#26684;&#30340;&#12289;&#36890;&#24120;&#26159;&#27491;&#24577;&#20998;&#24067;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;&#25628;&#32034;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#21306;&#38388;&#19968;&#33268;&#24615;&#19978;&#38480;&#25277;&#26679;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20132;&#25442;&#24615;&#20551;&#35774;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#25628;&#32034;&#27169;&#22411;&#26550;&#26500;&#36873;&#25321;&#12290;&#23545;&#36229;&#21442;&#20248;&#21270;&#30340;&#22810;&#20010;&#26550;&#26500;&#36827;&#34892;&#20102;&#25506;&#32034;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#23494;&#38598;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#38543;&#26426;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several novel frameworks for hyperparameter search have emerged in the last decade, but most rely on strict, often normal, distributional assumptions, limiting search model flexibility. This paper proposes a novel optimization framework based on upper confidence bound sampling of conformal confidence intervals, whose assumption of exchangeability enables greater choice of search model architectures. Several such architectures were explored and benchmarked on hyperparameter tuning of both dense and convolutional neural networks, displaying superior performance to random search.
&lt;/p&gt;</description></item></channel></rss>