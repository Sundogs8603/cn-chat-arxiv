<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#38024;&#23545;&#21098;&#26525;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25512;&#23548;&#20986;&#20102;&#27844;&#38706;&#20449;&#24687;&#30340;&#19978;&#38480;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.19958</link><description>&lt;p&gt;
PriPrune: &#22312;&#21098;&#26525;&#32852;&#37030;&#23398;&#20064;&#20013;&#37327;&#21270;&#21644;&#20445;&#25252;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21098;&#26525;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25512;&#23548;&#20986;&#20102;&#27844;&#38706;&#20449;&#24687;&#30340;&#19978;&#38480;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#36890;&#36807;&#20165;&#20132;&#25442;&#27169;&#22411;&#26356;&#26032;&#32780;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#33539;&#20363;&#65292;&#32780;&#19981;&#38656;&#35201;&#35774;&#22791;&#20849;&#20139;&#20182;&#20204;&#30340;&#23616;&#37096;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#35774;&#22791;&#22312;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#19968;&#27493;&#20174;&#27169;&#22411;&#21098;&#26525;&#20013;&#21463;&#30410; - &#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#33539;&#20363;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36890;&#36807;&#20351;&#23616;&#37096;&#27169;&#22411;&#26356;&#31895;&#31961;&#65292;&#21098;&#26525;&#39044;&#35745;&#22312;FL&#29615;&#22659;&#20013;&#20063;&#20250;&#25552;&#20379;&#19968;&#23450;&#30340;&#38544;&#31169;&#25915;&#20987;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#27492;&#20445;&#25252;&#36827;&#34892;&#27491;&#24335;&#25110;&#23454;&#39564;&#24615;&#30340;&#29305;&#24449;&#21270;&#65292;&#24182;&#19988;&#19981;&#28165;&#26970;&#23427;&#26159;&#21542;&#36275;&#20197;&#25269;&#24481;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21098;&#26525;FL&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#38556;&#36827;&#34892;&#20102;&#39318;&#27425;&#35843;&#26597;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#21098;&#26525;FL&#27169;&#22411;&#27844;&#38706;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21457;&#29616;&#36827;&#34892;&#20102;&#34917;&#20805;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.  In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with compreh
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#29615;&#35856;&#25391;&#22120;&#20013;&#33108;&#20307;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#25439;&#32791;&#23545;&#24211;&#20177;&#35745;&#31639;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#20010;&#21306;&#22495;&#65292;&#20854;&#20013;&#19968;&#20010;&#21306;&#22495;&#22312;&#20302;&#36755;&#20837;&#21151;&#29575;&#21644;&#33410;&#28857;&#25968;&#19979;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35823;&#24046;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25913;&#36827;&#26102;&#38388;&#24310;&#36831;&#24211;&#20177;&#35745;&#31639;&#30340;&#39044;&#27979;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#25351;&#23548;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.09433</link><description>&lt;p&gt;
&#22522;&#20110;&#30789;&#24494;&#29615;&#30340;&#24211;&#20177;&#35745;&#31639;&#20013;&#33108;&#20307;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#25439;&#32791;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing. (arXiv:2310.09433v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#29615;&#35856;&#25391;&#22120;&#20013;&#33108;&#20307;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#25439;&#32791;&#23545;&#24211;&#20177;&#35745;&#31639;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#20010;&#21306;&#22495;&#65292;&#20854;&#20013;&#19968;&#20010;&#21306;&#22495;&#22312;&#20302;&#36755;&#20837;&#21151;&#29575;&#21644;&#33410;&#28857;&#25968;&#19979;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35823;&#24046;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25913;&#36827;&#26102;&#38388;&#24310;&#36831;&#24211;&#20177;&#35745;&#31639;&#30340;&#39044;&#27979;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#25351;&#23548;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#29615;&#35856;&#25391;&#22120;&#26159;&#26102;&#38388;&#24310;&#36831;&#20809;&#23376;&#24211;&#20177;&#35745;&#31639;&#30340;&#26377;&#24076;&#26395;&#30340;&#22120;&#20214;&#65292;&#20294;&#26159;&#24494;&#29615;&#35856;&#25391;&#22120;&#20013;&#19981;&#21516;&#29289;&#29702;&#25928;&#24212;&#23545;&#24211;&#20177;&#35745;&#31639;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#20998;&#26512;&#32447;&#24615;&#25439;&#32791;&#12289;&#28909;&#20809;&#21644;&#33258;&#30001;&#36733;&#27969;&#23376;&#25928;&#24212;&#26494;&#24347;&#26102;&#38388;&#23545;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153; NARMA-10 &#30340;&#39044;&#27979;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19977;&#20010;&#21306;&#22495;&#65292;&#30001;&#36755;&#20837;&#21151;&#29575;&#21644;&#20809;&#28304;&#19982;&#24494;&#29615;&#35856;&#25391;&#20043;&#38388;&#30340;&#39057;&#29575;&#22833;&#35856;&#23450;&#20041;&#65292;&#36825;&#20123;&#21306;&#22495;&#25581;&#31034;&#20102;&#33108;&#20307;&#20174;&#32447;&#24615;&#21040;&#38750;&#32447;&#24615;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#20854;&#20013;&#19968;&#20010;&#21306;&#22495;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#36755;&#20837;&#21151;&#29575;&#21644;&#33410;&#28857;&#25968;&#19979;&#25552;&#20379;&#20102;&#38750;&#24120;&#20302;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35823;&#24046;&#65292;&#32780;&#20854;&#20182;&#21306;&#22495;&#35201;&#20040;&#32570;&#20047;&#38750;&#32447;&#24615;&#65292;&#35201;&#20040;&#21464;&#24471;&#19981;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#20854;&#29289;&#29702;&#29305;&#24615;&#20197;&#25552;&#39640;&#26102;&#38388;&#24310;&#36831;&#24211;&#20177;&#35745;&#31639;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microring resonators (MRRs) are promising devices for time-delay photonic reservoir computing, but the impact of the different physical effects taking place in the MRRs on the reservoir computing performance is yet to be fully understood. We numerically analyze the impact of linear losses as well as thermo-optic and free-carrier effects relaxation times on the prediction error of the time-series task NARMA-10. We demonstrate the existence of three regions, defined by the input power and the frequency detuning between the optical source and the microring resonance, that reveal the cavity transition from linear to nonlinear regimes. One of these regions offers very low error in time-series prediction under relatively low input power and number of nodes while the other regions either lack nonlinearity or become unstable. This study provides insight into the design of the MRR and the optimization of its physical properties for improving the prediction performance of time-delay reservoir co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.01438</link><description>&lt;p&gt;
&#26500;&#24314;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#37319;&#38598;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#24322;&#36136;&#21307;&#23398;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#23558;&#25918;&#23556;&#23398;&#25195;&#25551;&#12289;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21644;&#20998;&#23376;&#20449;&#24687;&#19982;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#26159;&#24320;&#21457;&#23545;&#30142;&#30149;&#26377;&#20840;&#38754;&#29702;&#35299;&#21644;&#20248;&#21270;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#22797;&#26434;&#30142;&#30149;&#65288;&#22914;&#30284;&#30151;&#65289;&#20013;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#36827;&#34892;&#25972;&#21512;&#30340;&#38656;&#27714;&#26356;&#21152;&#31361;&#20986;&#65292;&#20197;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#32959;&#30244;&#25968;&#25454;&#31995;&#32479;&#65288;MINDS&#65289;-&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#32463;&#27982;&#39640;&#25928;&#30340;&#20803;&#25968;&#25454;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26469;&#33258;&#20844;&#20849;&#26469;&#28304;&#65288;&#22914;&#30284;&#30151;&#30740;&#31350;&#25968;&#25454;&#20849;&#20139;&#24211;&#65289;&#30340;&#24322;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#19988;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#20013;&#12290;MINDS&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#25506;&#32034;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#24182;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;&#21327;&#35843;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;MINDS&#26088;&#22312;&#23454;&#29616;&#20419;&#36827;&#30740;&#31350;&#21019;&#26032;&#12289;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancements in data acquisition, storage, and processing techniques have resulted in the rapid growth of heterogeneous medical data. Integrating radiological scans, histopathology images, and molecular information with clinical data is essential for developing a holistic understanding of the disease and optimizing treatment. The need for integrating data from multiple sources is further pronounced in complex diseases such as cancer for enabling precision medicine and personalized treatments. This work proposes Multimodal Integration of Oncology Data System (MINDS) - a flexible, scalable, and cost-effective metadata framework for efficiently fusing disparate data from public sources such as the Cancer Research Data Commons (CRDC) into an interconnected, patient-centric framework. MINDS offers an interface for exploring relationships across data types and building cohorts for developing large-scale multimodal machine learning models. By harmonizing multimodal data, MINDS aims to pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#30340;&#26041;&#27861;&#65288;VaSSO&#65289;&#22686;&#24378;&#20102;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#21644;&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.15639</link><description>&lt;p&gt;
&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#22686;&#24378;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#30340;&#26041;&#27861;&#65288;VaSSO&#65289;&#22686;&#24378;&#20102;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#21644;&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26377;&#30528;&#33391;&#22909;&#30340;&#35760;&#24405;&#65292;&#21363;&#20351;&#27809;&#26377;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;SAM&#20511;&#21161;&#25439;&#22833;&#20989;&#25968;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#37051;&#22495;&#20869;&#21442;&#25968;&#23545;&#25932;&#23545;&#25200;&#21160;&#24341;&#36215;&#30340;&#26368;&#22823;&#25439;&#22833;&#65292;&#23547;&#25214;&#8220;&#24179;&#22374;&#26368;&#23567;&#20540;&#8221;&#25152;&#22312;&#30340;&#8220;&#24179;&#22374;&#23665;&#35895;&#8221;&#65292;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#32771;&#34385;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#38160;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#36825;&#31181;&#8220;&#36807;&#20110;&#21451;&#22909;&#30340;&#25932;&#23545;&#32773;&#8221;&#21487;&#33021;&#20250;&#38480;&#21046;&#27867;&#21270;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;&#26412;&#25991;&#30340;&#26032;&#26041;&#27861;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#65288;VaSSO&#65289;&#26469;&#31283;&#23450;&#25932;&#23545;&#32773;&#65292;&#36991;&#20813;&#36825;&#31181;&#21451;&#22909;&#24615;&#12290; VaSSO&#30340;&#31283;&#23450;&#24615;&#21487;&#35777;&#26126;&#65292;&#24182;&#22312;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#20013;&#65288;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#30456;&#23545;&#20110;SAM&#26377;&#30528;&#25968;&#20540;&#19978;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#35777;&#23454;VaSSO&#36171;&#20104;SAM&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an 'over-friendly adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through variance suppression (VaSSO) to avoid such friendliness. VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise.
&lt;/p&gt;</description></item><item><title>PrNet&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;Android&#21407;&#22987;GNSS&#27979;&#37327;&#23450;&#20301;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26657;&#27491;&#20266;&#36317;&#20559;&#24046;&#26469;&#25552;&#39640;&#23450;&#20301;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#22788;&#29702;&#20266;&#36317;&#20559;&#24046;&#21644;&#22122;&#22768;&#30340;&#28151;&#21512;&#31649;&#36947;&#12290;</title><link>http://arxiv.org/abs/2309.12204</link><description>&lt;p&gt;
PrNet&#65306;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;Android&#21407;&#22987;GNSS&#27979;&#37327;&#23450;&#20301;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26657;&#27491;&#20266;&#36317;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements. (arXiv:2309.12204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12204
&lt;/p&gt;
&lt;p&gt;
PrNet&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;Android&#21407;&#22987;GNSS&#27979;&#37327;&#23450;&#20301;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26657;&#27491;&#20266;&#36317;&#20559;&#24046;&#26469;&#25552;&#39640;&#23450;&#20301;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#22788;&#29702;&#20266;&#36317;&#20559;&#24046;&#21644;&#22122;&#22768;&#30340;&#28151;&#21512;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#36731;&#20266;&#36317;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#20174;&#23433;&#21331;&#26234;&#33021;&#25163;&#26426;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#23450;&#20301;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23454;&#29992;&#30340;&#22522;&#20110;&#21355;&#26143;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#34920;&#31034;&#20266;&#36317;&#20559;&#24046;&#65292;&#20854;&#36755;&#20837;&#26159;&#20174;Android&#21407;&#22987;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#27979;&#37327;&#20013;&#24471;&#20986;&#30340;&#19982;&#21355;&#26143;-&#25509;&#25910;&#22120;-&#29615;&#22659;&#30456;&#20851;&#30340;&#20845;&#20010;&#29305;&#24449;&#12290;&#20026;&#20102;&#30417;&#30563;&#35757;&#32451;&#36807;&#31243;&#65292;&#25105;&#20204;&#20180;&#32454;&#35745;&#31639;&#20266;&#36317;&#20559;&#24046;&#30340;&#30446;&#26631;&#20540;&#65292;&#24182;&#21033;&#29992;&#22320;&#29702;&#30495;&#23454;&#20301;&#32622;&#21644;&#24179;&#28369;&#25216;&#26415;&#20248;&#21270;&#20102;&#19968;&#20010;&#21253;&#21547;&#26234;&#33021;&#25163;&#26426;&#26102;&#38047;&#20559;&#24046;&#20272;&#35745;&#27531;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#23450;&#20301;&#24341;&#25806;&#26657;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#36317;&#35745;&#31639;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#28151;&#21512;&#31649;&#36947;&#21487;&#20197;&#22788;&#29702;&#20266;&#36317;&#20559;&#24046;&#21644;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#65292;&#24182;&#32771;&#34385;&#20102;&#22235;&#20010;&#24212;&#29992;&#22330;&#26223;&#26469;&#30740;&#31350;&#25351;&#32441;&#23450;&#20301;&#21644;&#20132;&#21449;&#36319;&#36394;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network for mitigating pseudorange bias to improve localization performance with data collected from Android smartphones. We represent pseudorange bias using a pragmatic satellite-wise Multiple Layer Perceptron (MLP), the inputs of which are six satellite-receiver-context-related features derived from Android raw Global Navigation Satellite System (GNSS) measurements. To supervise the training process, we carefully calculate the target values of pseudorange bias using location ground truth and smoothing techniques and optimize a loss function containing the estimation residuals of smartphone clock bias. During the inference process, we employ model-based localization engines to compute locations with pseudoranges corrected by the neural network. Consequently, this hybrid pipeline can attend to both pseudorange bias and noise. We evaluate the framework on an open dataset and consider four application scenarios for investigating fingerprinting and cross-trace localiza
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DeCoAR&#27169;&#22411;&#22312;&#31934;&#32454;&#35757;&#32451;&#26041;&#26696;&#20013;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;1.81\%&#21644;&#32422;4.56\%&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.01108</link><description>&lt;p&gt;
&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;: &#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#26159;&#21542;&#26377;&#21033;&#65311;
&lt;/p&gt;
&lt;p&gt;
Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DeCoAR&#27169;&#22411;&#22312;&#31934;&#32454;&#35757;&#32451;&#26041;&#26696;&#20013;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;1.81\%&#21644;&#32422;4.56\%&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;(ACI)&#28041;&#21450;&#20174;&#22768;&#23398;&#31354;&#38388;&#26144;&#23556;&#21040;&#21457;&#38899;&#31354;&#38388;&#12290;&#20449;&#21495;&#22788;&#29702;&#29305;&#24449;&#22914;MFCCs&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;ACI&#20219;&#21153;&#12290;&#23545;&#20110;&#26377;&#35821;&#38899;&#38556;&#30861;&#30340;&#24739;&#32773;&#65292;&#30001;&#20110;&#19981;&#20934;&#30830;&#21644;&#19981;&#28165;&#26224;&#30340;&#21457;&#38899;&#65292;ACI&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#30340;ACI&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#29305;&#24449;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;ACI&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;x-vectors&#19982;&#25552;&#21462;&#30340;SSL&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;BLSTM&#32593;&#32476;&#12290;&#22312;&#24050;&#30693;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;ACI&#35757;&#32451;&#26041;&#26696;&#65288;&#20027;&#39064;&#29305;&#23450;&#65292;&#32858;&#21512;&#21644;&#24494;&#35843;&#65289;&#12290;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;DeCoAR&#22312;&#24494;&#35843;&#26041;&#26696;&#20013;&#65292;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#24133;&#24230;&#20998;&#21035;&#20026;&#32422;1.81\%&#21644;&#32422;4.56\%&#12290;
&lt;/p&gt;
&lt;p&gt;
$ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#23450;&#37327;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#21644;&#27169;&#22411;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32622;&#20449;&#24230;&#12290;&#21516;&#26102;&#65292;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11197</link><description>&lt;p&gt;
&#22312;&#35821;&#38899;&#12289;&#35821;&#35328;&#21644;&#21548;&#21147;&#31185;&#23398;&#20013;&#24314;&#31435;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#21151;&#25928;&#20998;&#26512;&#21644;&#26679;&#26412;&#23481;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation. (arXiv:2308.11197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#23450;&#37327;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#21644;&#27169;&#22411;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32622;&#20449;&#24230;&#12290;&#21516;&#26102;&#65292;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#31532;&#19968;&#20010;&#30446;&#30340;&#26159;&#25552;&#20379;&#23450;&#37327;&#35777;&#25454;&#65292;&#20197;&#28608;&#21169;&#30740;&#31350;&#20154;&#21592;&#25913;&#29992;&#26356;&#20581;&#22766;&#30340;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#30446;&#30340;&#26159;&#22312;&#30740;&#31350;&#35774;&#35745;&#36807;&#31243;&#20013;&#25552;&#20986;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#30340;&#21151;&#25928;&#20998;&#26512;&#26041;&#27861;&#21644;MATLAB&#20195;&#30721;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#37327;&#21270;&#20102;&#25152;&#37319;&#29992;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#30340;&#21028;&#21035;&#21147;&#12289;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#21644;&#27169;&#22411;&#30340;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32479;&#35745;&#32622;&#20449;&#24230;&#65292;&#27604;&#36739;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65288;&#21333;&#19968;&#30041;&#20986;&#27861;&#12289;10&#25240;&#20132;&#21449;&#39564;&#35777;&#12289;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#27861;&#21644;&#23884;&#22871;10&#25240;&#20132;&#21449;&#39564;&#35777;&#65289;&#12290;&#21033;&#29992;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#30340;&#20998;&#24067;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#65288;&#945;=0.05&#65292;1-&#946;=0.8&#65289;&#12290;&#27169;&#22411;&#30340;&#32479;&#35745;&#32622;&#20449;&#24230;&#34987;&#23450;&#20041;&#20026;&#27491;&#30830;&#29305;&#24449;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.04119</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#23450;&#21046;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Constructing Custom Thermodynamics Using Deep Learning. (arXiv:2308.04119v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#26159;&#22522;&#20110;&#20808;&#21069;&#31215;&#32047;&#30340;&#25968;&#25454;&#20197;&#21450;&#24050;&#30693;&#30340;&#29289;&#29702;&#21407;&#29702;&#65288;&#21253;&#25324;&#23545;&#31216;&#24615;&#21644;&#23432;&#24658;&#23450;&#24459;&#65289;&#25552;&#20379;&#30340;&#38480;&#21046;&#65292;&#36827;&#34892;&#33258;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#26679;&#30340;&#33258;&#21160;&#20551;&#35774;&#21019;&#24314;&#21644;&#39564;&#35777;&#21487;&#20197;&#24110;&#21161;&#31185;&#23398;&#23478;&#30740;&#31350;&#22797;&#26434;&#30340;&#29616;&#35937;&#65292;&#20256;&#32479;&#30340;&#29289;&#29702;&#30452;&#35273;&#21487;&#33021;&#26080;&#27861;&#24212;&#23545;&#12290;&#23588;&#20854;&#37325;&#35201;&#30340;&#26159;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20854;&#26102;&#38388;&#28436;&#21464;&#21463;&#21040;&#21464;&#21270;&#30340;&#22806;&#37096;&#21442;&#25968;&#30340;&#24378;&#28872;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#30452;&#25509;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#37027;&#20123;&#24494;&#35266;&#25551;&#36848;&#23436;&#25972;&#19981;&#20999;&#23454;&#38469;&#12289;&#26500;&#24314;&#29702;&#35770;&#23439;&#35266;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#30693;&#35782;&#25110;&#35797;&#38169;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most exciting applications of AI is automated scientific discovery based on previously amassed data, coupled with restrictions provided by the known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Of particular importance are complex dynamic systems where their time evolution is strongly influenced by varying external parameters. In this paper we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes render complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by sim
&lt;/p&gt;</description></item><item><title>UnIVAL&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.16184</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Model for Image, Video, Audio and Language Tasks. (arXiv:2307.16184v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16184
&lt;/p&gt;
&lt;p&gt;
UnIVAL&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#24471;&#24314;&#31435;&#36890;&#29992;&#20195;&#29702;&#21464;&#24471;&#19981;&#20877;&#26159;&#24187;&#24819;&#12290;&#26500;&#24314;&#36825;&#31181;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38590;&#39064;&#26159;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#25903;&#25345;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#12290;&#34429;&#28982;&#19968;&#20123;&#22823;&#22411;&#27169;&#22411;&#65288;&#20363;&#22914;Flameigno&#65289;&#32463;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;&#20004;&#20010;&#27169;&#24577;&#65292;&#20294;&#30446;&#21069;&#23567;&#21040;&#20013;&#22411;&#30340;&#32479;&#19968;&#27169;&#22411;&#20173;&#28982;&#23616;&#38480;&#20110;&#20004;&#20010;&#27169;&#24577;&#65292;&#36890;&#24120;&#26159;&#22270;&#20687;-&#25991;&#26412;&#25110;&#35270;&#39057;-&#25991;&#26412;&#12290;&#25105;&#20204;&#35201;&#25552;&#20986;&#30340;&#38382;&#39064;&#26159;&#65306;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#25903;&#25345;&#25152;&#26377;&#27169;&#24577;&#30340;&#32479;&#19968;&#27169;&#22411;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UnIVAL&#65292;&#36825;&#26159;&#23545;&#36825;&#20010;&#38596;&#24515;&#21187;&#21187;&#30446;&#26631;&#36808;&#20986;&#30340;&#19968;&#27493;&#12290;UnIVAL&#27169;&#22411;&#25317;&#26377;&#32422;0.25&#20159;&#20010;&#21442;&#25968;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model 
&lt;/p&gt;</description></item><item><title>Dataset Grouper&#26159;&#19968;&#20010;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#32676;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#20811;&#26381;&#20102;&#20869;&#23384;&#38480;&#21046;&#12289;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#19982;&#19981;&#21516;&#30340;&#36719;&#20214;&#26694;&#26550;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#27604;&#20197;&#21069;&#26356;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#35821;&#35328;&#24314;&#27169;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2307.09619</link><description>&lt;p&gt;
&#38754;&#21521;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#38598;&#27969;&#27700;&#32447;&#65306;&#29992;&#20110;&#32676;&#32452;&#32467;&#26500;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning. (arXiv:2307.09619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09619
&lt;/p&gt;
&lt;p&gt;
Dataset Grouper&#26159;&#19968;&#20010;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#32676;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#20811;&#26381;&#20102;&#20869;&#23384;&#38480;&#21046;&#12289;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#19982;&#19981;&#21516;&#30340;&#36719;&#20214;&#26694;&#26550;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#27604;&#20197;&#21069;&#26356;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#35821;&#35328;&#24314;&#27169;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Dataset Grouper&#30340;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#30340;&#32676;&#32452;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;&#32852;&#37030;&#65289;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#22522;&#30784;&#27169;&#22411;&#35268;&#27169;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#25311;&#12290;&#35813;&#24211;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#20998;&#21306;&#21019;&#24314;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#32676;&#32452;&#32467;&#26500;&#21270;&#29256;&#26412;&#65292;&#24182;&#30452;&#25509;&#23548;&#33268;&#21508;&#31181;&#26377;&#29992;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25554;&#20837;&#29616;&#26377;&#30340;&#36719;&#20214;&#26694;&#26550;&#20013;&#12290;Dataset Grouper&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#21363;&#20351;&#21333;&#20010;&#32676;&#32452;&#30340;&#25968;&#25454;&#38598;&#20063;&#22826;&#22823;&#26080;&#27861;&#25918;&#20837;&#20869;&#23384;&#30340;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#23427;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#26082;&#21487;&#20197;&#36873;&#25321;&#22522;&#30784;&#65288;&#38750;&#20998;&#21306;&#65289;&#25968;&#25454;&#38598;&#65292;&#20063;&#21487;&#20197;&#23450;&#20041;&#20998;&#21306;&#12290;&#26368;&#21518;&#65292;&#23427;&#19982;&#26694;&#26550;&#26080;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;Dataset Grouper&#20801;&#35768;&#22312;&#27604;&#20808;&#21069;&#24037;&#20316;&#20013;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#32852;&#37030;&#35821;&#35328;&#24314;&#27169;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;FedAvg&#36825;&#26679;&#30340;&#31639;&#27861;&#26356;&#20687;&#20803;&#23398;&#20064;&#26041;&#27861;&#32780;&#19981;&#26159;&#32463;&#39564;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a library, Dataset Grouper, to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library allows the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper allows for large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15774</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19979;&#19968;&#27493;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#22797;&#36328;&#23398;&#31185;&#35752;&#35770;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#35282;&#24230;&#20026;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#19979;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#36335;&#32447;&#22270;&#65292;&#27010;&#36848;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19977;&#20010;&#23618;&#38754;&#19978;&#30340;&#26410;&#26469;&#26041;&#21521;&#65306;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65307;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#65307;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#35813;&#36335;&#32447;&#22270;&#26088;&#22312;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#35752;&#35770;&#65292;&#21516;&#26102;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI) from a technical perspective. We contribute a roadmap that lays out future directions of Generative AI spanning three levels: Aligning with human values; Accommodating humans' expression of intents; and Augmenting humans' abilities in a collaborative workflow. This roadmap intends to draw interdisciplinary research teams to a comprehensive list of emergent ideas in HGAI, identifying their interested topics while maintaining a coherent big picture of the future work landscape.
&lt;/p&gt;</description></item><item><title>MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.11758</link><description>&lt;p&gt;
MRFI&#65306;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#30340;&#24320;&#28304;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11758
&lt;/p&gt;
&lt;p&gt;
MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#21363;&#20351;&#22312;&#19981;&#21487;&#38752;&#30340;&#30828;&#20214;&#19978;&#20063;&#33021;&#36827;&#34892;&#26377;&#24377;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#36827;&#34892;&#21508;&#31181;&#30828;&#20214;&#25925;&#38556;&#30340;&#20840;&#38754;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#30340;&#38169;&#35823;&#27880;&#20837;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#28982;&#23616;&#38480;&#20110;&#23545;&#31070;&#32463;&#20803;&#30340;&#22522;&#26412;&#25925;&#38556;&#27880;&#20837;&#65292;&#24182;&#26410;&#25552;&#20379;&#32454;&#31890;&#24230;&#28431;&#27934;&#20998;&#26512;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#38656;&#35201;&#26356;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20351;&#25925;&#38556;&#27880;&#20837;&#19982;&#27491;&#24120;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32039;&#23494;&#32806;&#21512;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#30340;&#20351;&#29992;&#38590;&#24230;&#24182;&#20943;&#24930;&#20102;&#25925;&#38556;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;MRFI&#12290;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIBA&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23384;&#22312;&#30340;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06209</link><description>&lt;p&gt;
&#31232;&#30095;&#38544;&#24418;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIBA&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23384;&#22312;&#30340;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#22312;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#24471;&#21463;&#23475;&#30340;&#27169;&#22411;&#23545;&#27491;&#24120;&#26679;&#26412;&#26377;&#27491;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#23558;&#24102;&#26377;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#24402;&#31867;&#20026;&#30446;&#26631;&#20998;&#31867;&#12290;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#20852;&#32780;&#21448;&#21361;&#38505;&#30340;&#35757;&#32451;&#38454;&#27573;&#23041;&#32961;&#65292;&#23545;DNN&#24212;&#29992;&#24102;&#26469;&#20005;&#37325;&#39118;&#38505;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#65292;&#22240;&#27492;&#19981;&#22815;&#38544;&#31192;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#35774;&#35745;&#26377;&#25928;&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#19981;&#33021;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#35302;&#21457;&#22120;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#31232;&#30095;&#24615;&#21644;&#38544;&#31192;&#24615;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#31216;&#20026;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#65288;SIBA&#65289;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the adversary manipulates a small portion of training data such that the victim model predicts normally on the benign samples but classifies the triggered samples as the target class. The backdoor attack is an emerging yet threatening training-phase threat, leading to serious risks in DNN-based applications. In this paper, we revisit the trigger patterns of existing backdoor attacks. We reveal that they are either visible or not sparse and therefore are not stealthy enough. More importantly, it is not feasible to simply combine existing methods to design an effective sparse and invisible backdoor attack. To address this problem, we formulate the trigger generation as a bi-level optimization problem with sparsity and invisibility constraints and propose an effective method to solve it. The proposed method is dubbed sparse and invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark datasets unde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D-DenseUNet&#30340;&#26032;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#22359;&#21644;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#35299;&#20915;&#21322;&#30417;&#30563;&#25216;&#26415;&#20013;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#22788;&#29702;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#33041;&#37096;&#20998;&#26512;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05745</link><description>&lt;p&gt;
&#20004;&#20010;&#29420;&#31435;&#35757;&#32451;&#22120;&#26159;&#26356;&#22909;&#30340;&#27169;&#33539;
&lt;/p&gt;
&lt;p&gt;
Two Independent Teachers are Better Role Model. (arXiv:2306.05745v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05745
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D-DenseUNet&#30340;&#26032;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#22359;&#21644;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#35299;&#20915;&#21322;&#30417;&#30563;&#25216;&#26415;&#20013;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#22788;&#29702;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#33041;&#37096;&#20998;&#26512;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23156;&#20799;&#33041;&#37096;&#20998;&#26512;&#20013;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21322;&#30417;&#30563;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;&#38598;&#25104;&#65292;&#24179;&#22343;&#25945;&#24072;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#29992;&#22534;&#21472;&#30340;&#23616;&#37096;&#36816;&#31639;&#31526;&#26469;&#25910;&#38598;&#36828;&#31243;&#20449;&#24687;&#65292;&#32780;&#23616;&#37096;&#36816;&#31639;&#31526;&#38480;&#21046;&#20102;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;MRI&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65288;TPs&#65289;&#65292;&#20363;&#22914;T1&#21644;T2&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#65292;&#23427;&#20204;&#23558;&#20004;&#31181;&#25968;&#25454;&#37117;&#20316;&#20026;&#36755;&#20837;&#29992;&#20110;&#20998;&#21106;&#36807;&#31243;&#65292;&#21363;&#27169;&#22411;&#19968;&#27425;&#35757;&#32451;&#20110;&#25968;&#25454;&#38598;&#65292;&#25512;&#26029;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21517;&#20026;3D-DenseUNet&#30340;&#26032;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#20854;&#20316;&#20026;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#22359;&#22312;&#19979;&#37319;&#26679;&#20013;&#24037;&#20316;&#20197;&#35299;&#20915;&#31354;&#38388;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;&#33258;&#27880;&#24847;&#27169;&#22359;&#23558;&#19979;&#37319;&#26679;&#23618;&#19982;&#19978;&#37319;&#26679;&#23618;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#25910;&#38598;&#36828;&#31243;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#22788;&#29702;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65292;&#24182;&#21462;&#24471;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent deep learning models have attracted substantial attention in infant brain analysis. These models have performed state-of-the-art performance, such as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher). However, these models depend on an encoder-decoder structure with stacked local operators to gather long-range information, and the local operators limit the efficiency and effectiveness. Besides, the $MRI$ data contain different tissue properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models is that they use both data as inputs to the segment process, i.e., the models are trained on the dataset once, and it requires much computational and memory requirements during inference. In this work, we address the above limitations by designing a new deep-learning model, called 3D-DenseUNet, which works as adaptable global aggregation blocks in down-sampling to solve the issue of spatial information loss. The self-attention module connects the down-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#39640;&#26031;&#21464;&#20998;&#26063;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#25552;&#20379;&#20102;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.03638</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#39640;&#26031;&#21464;&#20998;&#26063;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#25552;&#20379;&#20102;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#27809;&#26377;&#35777;&#26126;&#20854;&#38543;&#26426;&#20248;&#21270;&#25104;&#21151;&#30340;&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#26159;&#29616;&#26377;&#38543;&#26426;&#20248;&#21270;&#35777;&#26126;&#20013;&#30340;&#29702;&#35770;&#24046;&#36317;&#65292;&#21363;&#20855;&#26377;&#24322;&#24120;&#22122;&#22768;&#36793;&#30028;&#21644;&#22797;&#21512;&#38750;&#24179;&#28369;&#30446;&#26631;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#23494;&#38598;&#30340;&#39640;&#26031;&#21464;&#20998;&#26063;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;&#20877;&#21442;&#25968;&#21270;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#28385;&#36275;&#20108;&#27425;&#22122;&#22768;&#30028;&#65292;&#24182;&#20026;&#20351;&#29992;&#35813;&#30028;&#38480;&#30340;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25552;&#20379;&#26032;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#36825;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#20005;&#26684;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15930</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#31070;&#32463;&#36807;&#31243;&#30340;&#31471;&#21040;&#31471;Meta-Bayesian&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;Meta-Bayesian optimization&#65292;Meta-BO&#65289;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#29420;&#31435;&#20803;&#23398;&#20064;&#36807;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#65292;&#20294;&#26159;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#20010;&#32452;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;Meta-BO&#26694;&#26550;&#65292;&#36890;&#36807;Transformer&#20307;&#31995;&#32467;&#26500;&#23558;&#31070;&#32463;&#36807;&#31243;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20351;&#36825;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#20855;&#26377;&#22788;&#29702;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05400</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#25506;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#33104;&#36133;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#20581;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#21644;&#24418;&#24335;&#31283;&#20581;&#24615;&#39564;&#35777;&#39046;&#22495;&#20013;&#65292;&#31283;&#20581;&#24615;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#22312;Lp&#33539;&#25968;&#36317;&#31163;&#20869;&#23545;&#25152;&#26377;&#36755;&#20837;&#21464;&#21270;&#30340;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#36890;&#24120;&#36890;&#36807;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#26469;&#25913;&#36827;&#21644;&#35780;&#20272;&#65292;&#32780;&#24456;&#23569;&#32771;&#34385;&#25968;&#23398;&#23450;&#20041;&#30340;Lp&#33539;&#25968;&#22833;&#30495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#26469;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#25239;&#31283;&#20581;&#24615;&#39046;&#22495;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#21644;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;Lp&#33539;&#25968;&#20043;&#38388;&#31283;&#20581;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21738;&#20123;Lp&#33539;&#25968;&#30340;&#22833;&#30495;&#24212;&#35813;&#29992;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoNeRF&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#20027;&#20307;&#20195;&#29702;&#25910;&#38598;&#35757;&#32451;NeRF&#25152;&#38656;&#25968;&#25454;&#65292;&#35757;&#32451;NeRF&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2304.11241</link><description>&lt;p&gt;
AutoNeRF: &#33258;&#20027;&#20195;&#29702;&#35757;&#32451;&#38544;&#24335;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AutoNeRF: Training Implicit Scene Representations with Autonomous Agents. (arXiv:2304.11241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoNeRF&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#20027;&#20307;&#20195;&#29702;&#25910;&#38598;&#35757;&#32451;NeRF&#25152;&#38656;&#25968;&#25454;&#65292;&#35757;&#32451;NeRF&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#34920;&#31034;&#65292;&#22914;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#26032;&#35270;&#35282;&#32508;&#21512;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#25910;&#38598;&#25968;&#25454;&#24182;&#36827;&#34892;&#32454;&#33268;&#30340;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoNeRF&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#20027;&#20307;&#20195;&#29702;&#25910;&#38598;&#35757;&#32451;NeRF&#25152;&#38656;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#26377;&#25928;&#22320;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#24182;&#20351;&#29992;&#32463;&#39564;&#33258;&#20027;&#22320;&#26500;&#24314;&#30456;&#24212;&#30340;&#38544;&#24335;&#22320;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#22522;&#20110;&#21069;&#27839;&#30340;&#25506;&#32034;&#21644;&#30001;&#32463;&#36807;&#35757;&#32451;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#21644;&#32463;&#20856;&#30340;&#20302;&#32423;&#36335;&#24452;&#36861;&#36394;&#22120;&#32452;&#25104;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65306;&#32463;&#20856;&#35270;&#35282;&#28210;&#26579;&#12289;&#22320;&#22270;&#37325;&#24314;&#12289;&#35268;&#21010;&#21644;&#23039;&#24577;&#24494;&#35843;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#20027;&#20195;&#29702;AutoNeRF&#21487;&#20197;&#25104;&#21151;&#22320;&#35757;&#32451;NeRF&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit representations such as Neural Radiance Fields (NeRF) have been shown to be very effective at novel view synthesis. However, these models typically require manual and careful human data collection for training. In this paper, we present AutoNeRF, a method to collect data required to train NeRFs using autonomous embodied agents. Our method allows an agent to explore an unseen environment efficiently and use the experience to build an implicit map representation autonomously. We compare the impact of different exploration strategies including handcrafted frontier-based exploration and modular approaches composed of trained high-level planners and classical low-level path followers. We train these models with different reward functions tailored to this problem and evaluate the quality of the learned representations on four different downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. Empirical results show that NeRFs can be trained 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Stackelberg&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#32858;&#21512;&#22120;&#21644;&#20013;&#38388;&#21830;&#20043;&#38388;&#21452;&#21521;&#20132;&#26131;&#33021;&#28304;&#65292;&#35299;&#20915;&#26041;&#26696;&#21487;&#25193;&#23637;&#65292;&#19988;&#20445;&#35777;&#28385;&#36275;&#20013;&#38388;&#21830;&#27599;&#26085;&#33021;&#28304;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.02086</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#23398;&#20064;&#36817;&#20284;Stackelberg&#35299;&#20915;&#26041;&#26696;&#22312;&#24102;&#26377;&#38656;&#27714;&#21709;&#24212;&#32858;&#21512;&#22120;&#30340;&#33021;&#28304;&#20132;&#26131;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scalable Online Learning of Approximate Stackelberg Solutions in Energy Trading Games with Demand Response Aggregators. (arXiv:2304.02086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Stackelberg&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#32858;&#21512;&#22120;&#21644;&#20013;&#38388;&#21830;&#20043;&#38388;&#21452;&#21521;&#20132;&#26131;&#33021;&#28304;&#65292;&#35299;&#20915;&#26041;&#26696;&#21487;&#25193;&#23637;&#65292;&#19988;&#20445;&#35777;&#28385;&#36275;&#20013;&#38388;&#21830;&#27599;&#26085;&#33021;&#28304;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Stackelberg&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#32858;&#21512;&#22120;&#21644;&#20013;&#38388;&#21830;&#20043;&#38388;&#21452;&#21521;&#20132;&#26131;&#33021;&#28304;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#28789;&#27963;&#30340;&#33021;&#28304;&#22871;&#21033;&#21644;&#39069;&#22806;&#30340;&#36135;&#24065;&#22870;&#21169;&#65292;&#21516;&#26102;&#30830;&#20445;&#28385;&#36275;&#20013;&#38388;&#21830;&#25152;&#38656;&#30340;&#27599;&#26085;&#33021;&#28304;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#65288;&#38543;&#30528;&#20013;&#38388;&#21830;&#25968;&#37327;&#22686;&#21152;&#65289;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#22312;&#32447;&#37319;&#26679;&#21644;&#23398;&#20064;&#20013;&#38388;&#21830;&#30340;&#32047;&#31215;&#26368;&#20339;&#21709;&#24212;&#65292;&#23547;&#25214;&#36817;&#20284;&#30340;&#22343;&#34913;&#35299;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#36817;&#20284;&#22343;&#34913;&#35299;&#36136;&#37327;&#30340;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#26469;&#33258;&#21152;&#21033;&#31119;&#23612;&#20122;&#26085;&#21069;&#33021;&#28304;&#24066;&#22330;&#21644;&#21152;&#24030;&#22823;&#23398;&#25140;&#32500;&#26031;&#20998;&#26657;&#24314;&#31569;&#33021;&#28304;&#38656;&#27714;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21644;&#22312;&#32447;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a Stackelberg game theoretic framework is proposed for trading energy bidirectionally between the demand-response (DR) aggregator and the prosumers. This formulation allows for flexible energy arbitrage and additional monetary rewards while ensuring that the prosumers' desired daily energy demand is met. Then, a scalable (with the number of prosumers) approach is proposed to find approximate equilibria based on online sampling and learning of the prosumers' cumulative best response. Moreover, bounds are provided on the quality of the approximate equilibrium solution. Last, real-world data from the California day-ahead energy market and the University of California at Davis building energy demands are utilized to demonstrate the efficacy of the proposed framework and the online scalable solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31639;&#27861;IDBM&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#27599;&#19968;&#27493;&#26377;&#25928;&#22320;&#32806;&#21512;&#30446;&#26631;&#24230;&#37327;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#26469;&#23450;&#20041;&#19968;&#20010;&#36817;&#20284;&#20256;&#36755;&#31616;&#21333;&#20998;&#24067;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#29983;&#25104;&#36807;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00917</link><description>&lt;p&gt;
&#25193;&#25955;&#26725;&#28151;&#21512;&#20256;&#36755;&#12289;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#21644;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Diffusion Bridge Mixture Transports, Schr\"odinger Bridge Problems and Generative Modeling. (arXiv:2304.00917v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31639;&#27861;IDBM&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#27599;&#19968;&#27493;&#26377;&#25928;&#22320;&#32806;&#21512;&#30446;&#26631;&#24230;&#37327;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#26469;&#23450;&#20041;&#19968;&#20010;&#36817;&#20284;&#20256;&#36755;&#31616;&#21333;&#20998;&#24067;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#29983;&#25104;&#36807;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#23547;&#27714;&#23450;&#20041;&#22312;&#20004;&#20010;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#20256;&#36755;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#21516;&#26102;&#26368;&#20248;&#22320;&#28385;&#36275;&#26368;&#25509;&#36817;&#21442;&#32771;&#36807;&#31243;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#20934;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#21363;&#36845;&#20195;&#25193;&#25955;&#26725;&#28151;&#21512;&#20256;&#36755;&#65288;IDBM&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#21160;&#24577;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#12290;IDBM&#36807;&#31243;&#34920;&#29616;&#20986;&#22312;&#27599;&#19968;&#27493;&#23454;&#29616;&#30446;&#26631;&#24230;&#37327;&#20043;&#38388;&#30340;&#26377;&#25928;&#32806;&#21512;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;IDBM&#36807;&#31243;&#30340;&#21021;&#22987;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#29702;&#35770;&#21457;&#29616;&#36890;&#36807;&#35768;&#22810;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;IDBM&#36807;&#31243;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#26469;&#23450;&#20041;&#19968;&#20010;&#36817;&#20284;&#20256;&#36755;&#31616;&#21333;&#20998;&#24067;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#36845;&#20195;&#25193;&#25955;&#26725;&#28151;&#21512;&#20256;&#36755;&#65288;IDBM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#34203;&#23450;&#35860;&#26725;&#38382;&#39064;&#12290;IDBM&#22312;&#27599;&#19968;&#27493;&#23454;&#29616;&#30446;&#26631;&#24230;&#37327;&#20043;&#38388;&#30340;&#26377;&#25928;&#32806;&#21512;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#35777;&#26126;&#20102;IDBM&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#35768;&#22810;&#25968;&#20540;&#23454;&#39564;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23427;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#26469;&#36817;&#20284;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic Schr\"odinger bridge problem seeks a stochastic process that defines a transport between two target probability measures, while optimally satisfying the criteria of being closest, in terms of Kullback-Leibler divergence, to a reference process.  We propose a novel sampling-based iterative algorithm, the iterated diffusion bridge mixture transport (IDBM), aimed at solving the dynamic Schr\"odinger bridge problem. The IDBM procedure exhibits the attractive property of realizing a valid coupling between the target measures at each step. We perform an initial theoretical investigation of the IDBM procedure, establishing its convergence properties. The theoretical findings are complemented by numerous numerical experiments illustrating the competitive performance of the IDBM procedure across various applications.  Recent advancements in generative modeling employ the time-reversal of a diffusion process to define a generative process that approximately transports a simple distri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;CPPI&#21644;TIPP&#31574;&#30053;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29305;&#23450;&#35774;&#35745;&#30340;MARL&#26041;&#27861;CPPI-MADDPG&#21644;TIPP-MADDPG&#65292;&#29992;&#20110;&#37327;&#21270;&#24066;&#22330;&#30340;&#25112;&#30053;&#20132;&#26131;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11959</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#20132;&#26131;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Strategic Trading in Quantitative Markets through Multi-Agent Reinforcement Learning. (arXiv:2303.11959v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;CPPI&#21644;TIPP&#31574;&#30053;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29305;&#23450;&#35774;&#35745;&#30340;MARL&#26041;&#27861;CPPI-MADDPG&#21644;TIPP-MADDPG&#65292;&#29992;&#20110;&#37327;&#21270;&#24066;&#22330;&#30340;&#25112;&#30053;&#20132;&#26131;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#24066;&#22330;&#20013;&#65292;&#30001;&#20110;&#24066;&#22330;&#21160;&#24577;&#24555;&#36895;&#21464;&#21270;&#21644;&#22823;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22914;&#20309;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#21033;&#28070;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#38754;&#21521;&#22870;&#21169;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#22797;&#26434;&#30340;&#37329;&#34701;&#22330;&#26223;&#20013;&#24050;&#25104;&#20026;&#35299;&#20915;&#31574;&#30053;&#20915;&#31574;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#20004;&#31181;&#20808;&#21069;&#30340;&#37329;&#34701;&#20132;&#26131;&#31574;&#30053;&#65288;&#24658;&#23450;&#27604;&#20363;&#32452;&#21512;&#20445;&#38505;&#65288;CPPI&#65289;&#21644;&#26102;&#38388;&#19981;&#21464;&#32452;&#21512;&#20445;&#25252;&#65288;TIPP&#65289;&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MADDPG&#65289;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65306;CPPI-MADDPG&#21644;TIPP-MADDPG&#30740;&#31350;&#37327;&#21270;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#20132;&#26131;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#23454;&#38469;&#37329;&#34701;&#24066;&#22330;&#19978;&#30340;100&#31181;&#19981;&#21516;&#32929;&#31080;&#26469;&#27979;&#35797;&#36825;&#20123;&#29305;&#21035;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPPI-MADDPG&#21644;TIPP-MADDPG&#26041;&#27861;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rapid dynamics and a mass of uncertainties in the quantitative markets, the issue of how to take appropriate actions to make profits in stock trading remains a challenging one. Reinforcement learning (RL), as a reward-oriented approach for optimal control, has emerged as a promising method to tackle this strategic decision-making problem in such a complex financial scenario. In this paper, we integrated two prior financial trading strategies named constant proportion portfolio insurance (CPPI) and time-invariant portfolio protection (TIPP) into multi-agent deep deterministic policy gradient (MADDPG) and proposed two specifically designed multi-agent RL (MARL) methods: CPPI-MADDPG and TIPP-MADDPG for investigating strategic trading in quantitative markets. Afterward, we selected 100 different shares in the real financial market to test these specifically proposed approaches. The experiment results show that CPPI-MADDPG and TIPP-MADDPG approaches generally outperform the conve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CD-GraB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;CD-GraB&#23637;&#29616;&#20986;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#24182;&#19988;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.00845</link><description>&lt;p&gt;
CD-GraB&#65306;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#35777;&#26126;&#21152;&#36895;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training. (arXiv:2302.00845v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CD-GraB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;CD-GraB&#23637;&#29616;&#20986;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#24182;&#19988;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#22312;&#32447;&#26799;&#24230;&#24179;&#34913;&#65288;GraB&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#22522;&#20110;&#32622;&#25442;&#30340;&#31034;&#20363;&#25490;&#24207;&#21487;&#20197;&#20445;&#35777;&#20248;&#20110;&#38543;&#26426;&#37325;&#25490;&#65288;RR&#65289;&#12290;&#32780;RR&#20250;&#20219;&#24847;&#25490;&#21015;&#35757;&#32451;&#31034;&#20363;&#65292;GraB&#21033;&#29992;&#20808;&#21069;&#26102;&#26399;&#30340;&#38472;&#26087;&#26799;&#24230;&#23545;&#31034;&#20363;&#36827;&#34892;&#25490;&#24207;--&#23454;&#29616;&#27604;RR&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#20294;&#26159;&#65292;GraB&#22312;&#35774;&#35745;&#19978;&#23384;&#22312;&#38480;&#21046;&#65306;&#34429;&#28982;&#23427;&#23637;&#31034;&#20102;&#22312;&#38598;&#20013;&#25968;&#25454;&#19978;&#25193;&#23637;&#35757;&#32451;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#24182;&#19981;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#29616;&#20195;&#20998;&#24067;&#24335;ML&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#35843;&#20998;&#24067;&#24335;GraB&#65288;CD-GraB&#65289;&#65292;&#23427;&#21033;&#29992;&#20808;&#21069;&#20851;&#20110;&#20869;&#26680;&#31232;&#30095;&#21270;&#24037;&#20316;&#30340;&#27934;&#23519;&#21147;&#65292;&#23558;&#32622;&#25442;&#25490;&#24207;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#30340;&#20248;&#21183;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#35774;&#32622;&#12290;CD-GraB&#20855;&#26377;&#21487;&#24573;&#30053;&#30340;&#24320;&#38144;&#65292;&#22312;&#20013;&#22830;&#38598;&#26435;GraB&#19978;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#20248;&#20110;&#20998;&#24067;&#24335;RR&#31561;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages stale gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: While it demonstrates an impressive ability to scale-up training on centralized data, it does not naturally extend to modern distributed ML workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms baselines empirically, including distributed RR, on a variety of benchmark tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;ODE&#20013;&#35757;&#32451;&#21644;&#35777;&#26126;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#36830;&#32493;&#25511;&#21046;&#21644;&#22270;&#20687;&#20998;&#31867;&#65292;&#20855;&#26377;&#38750;&#34394;&#20551;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.16940</link><description>&lt;p&gt;
FI-ODE: &#31070;&#32463;ODE&#20013;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs. (arXiv:2210.16940v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;ODE&#20013;&#35757;&#32451;&#21644;&#35777;&#26126;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#36830;&#32493;&#25511;&#21046;&#21644;&#22270;&#20687;&#20998;&#31867;&#65292;&#20855;&#26377;&#38750;&#34394;&#20551;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#19981;&#21464;&#24615;&#26159;&#25511;&#21046;&#29702;&#35770;&#20013;&#38271;&#26399;&#30740;&#31350;&#30340;&#24615;&#36136;&#65292;&#29992;&#20110;&#35777;&#26126;&#21160;&#24577;&#31995;&#32479;&#22312;&#25152;&#26377;&#26102;&#38388;&#20869;&#20445;&#25345;&#22312;&#19968;&#20123;&#39044;&#23450;&#29366;&#24577;&#38598;&#21512;&#20869;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#22312;&#25200;&#21160;&#19979;&#20445;&#25345;&#35777;&#20070;&#26377;&#25928;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#21487;&#35777;&#26126;&#35777;&#23454;&#31070;&#32463;ODE&#20013;&#30340;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22330;&#26223;&#20013;&#24212;&#29992;&#20102;&#36825;&#20010;&#26694;&#26550;&#65306;&#40065;&#26834;&#36830;&#32493;&#25511;&#21046;&#20013;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#24615;&#65292;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#38750;&#34394;&#20551;&#20445;&#35777;&#30340;&#35757;&#32451;NODE&#31574;&#30053;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forward invariance is a long-studied property in control theory that is used to certify that a dynamical system stays within some pre-specified set of states for all time, and also admits robustness guarantees (e.g., the certificate holds under perturbations). We propose a general framework for training and provably certifying robust forward invariance in Neural ODEs. We apply this framework in two settings: certified safety in robust continuous control, and certified adversarial robustness for image classification. To our knowledge, this is the first instance of training NODE policies with such non-vacuous certified guarantees.
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26159;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#65292;AEAIL&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;&#20110;&#22122;&#22768;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.11004</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Auto-Encoding Adversarial Imitation Learning. (arXiv:2206.11004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11004
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26159;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#65292;AEAIL&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;&#20110;&#22122;&#22768;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#20854;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#25581;&#31034;&#20102;&#22312;&#27809;&#26377;&#26469;&#33258;&#29615;&#22659;&#30340;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#33719;&#21462;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#20174;&#31034;&#33539;&#20013;&#25512;&#23548;&#20986;&#19987;&#23478;&#31574;&#30053;&#65292;AEAIL&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36825;&#27604;&#20043;&#21069;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22810;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;&#30446;&#26631;&#20989;&#25968;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;AEAIL&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#22122;&#22768;&#26102;&#65292;AEAIL&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) provides a powerful framework for decision-making, but its application in practice often requires a carefully designed reward function. Adversarial Imitation Learning (AIL) sheds light on automatic policy acquisition without access to the reward signal from the environment. In this work, we propose Auto-Encoding Adversarial Imitation Learning (AEAIL), a robust and scalable AIL framework. To induce expert policies from demonstrations, AEAIL utilizes the reconstruction error of an auto-encoder as a reward signal, which provides more information for optimizing policies than the prior discriminator-based ones. Subsequently, we use the derived objective functions to train the auto-encoder and the agent policy. Experiments show that our AEAIL performs superior compared to state-of-the-art methods on both state and image based environments. More importantly, AEAIL shows much better robustness when the expert demonstrations are noisy.
&lt;/p&gt;</description></item></channel></rss>