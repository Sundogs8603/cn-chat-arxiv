<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>DACR&#25552;&#20986;&#20102;&#20998;&#24067;&#22686;&#24378;&#23545;&#27604;&#37325;&#24314;&#65288;DACR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#39069;&#22806;&#30340;&#25968;&#25454;&#26469;&#21387;&#32553;&#27491;&#24120;&#25968;&#25454;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2401.11271</link><description>&lt;p&gt;
DACR&#65306;&#20998;&#24067;&#22686;&#24378;&#23545;&#27604;&#37325;&#24314;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DACR: Distribution-Augmented Contrastive Reconstruction for Time-Series Anomaly Detection. (arXiv:2401.11271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11271
&lt;/p&gt;
&lt;p&gt;
DACR&#25552;&#20986;&#20102;&#20998;&#24067;&#22686;&#24378;&#23545;&#27604;&#37325;&#24314;&#65288;DACR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#39069;&#22806;&#30340;&#25968;&#25454;&#26469;&#21387;&#32553;&#27491;&#24120;&#25968;&#25454;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#35782;&#21035;&#25925;&#38556;&#12289;&#22833;&#36133;&#12289;&#23041;&#32961;&#21644;&#24322;&#24120;&#20540;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#34987;&#24212;&#29992;&#20110;&#27492;&#20027;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#21644;&#39640;&#24230;&#21160;&#24577;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#24448;&#24448;&#38754;&#20020;&#22256;&#38590;&#65292;&#20363;&#22914;&#27491;&#24120;&#25968;&#25454;&#21487;&#33021;&#30001;&#22810;&#20010;&#20998;&#24067;&#32452;&#25104;&#65292;&#19981;&#21516;&#31243;&#24230;&#30340;&#24322;&#24120;&#21487;&#33021;&#19982;&#27491;&#24120;&#25968;&#25454;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#22686;&#24378;&#23545;&#27604;&#37325;&#24314;&#65288;DACR&#65289;&#12290;DACR&#29983;&#25104;&#19982;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#19981;&#30456;&#20132;&#30340;&#39069;&#22806;&#25968;&#25454;&#26469;&#21387;&#32553;&#27491;&#24120;&#25968;&#25454;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;DACR&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#37325;&#24314;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in time-series data is crucial for identifying faults, failures, threats, and outliers across a range of applications. Recently, deep learning techniques have been applied to this topic, but they often struggle in real-world scenarios that are complex and highly dynamic, e.g., the normal data may consist of multiple distributions, and various types of anomalies may differ from the normal data to different degrees. In this work, to tackle these challenges, we propose Distribution-Augmented Contrastive Reconstruction (DACR). DACR generates extra data disjoint from the normal data distribution to compress the normal data's representation space, and enhances the feature extractor through contrastive learning to better capture the intrinsic semantics from time-series data. Furthermore, DACR employs an attention mechanism to model the semantic dependencies among multivariate time-series features, thereby achieving more robust reconstruction for anomaly detection. Extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#26465;&#20214;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#33021;&#22815;&#20135;&#29983;&#26356;&#23569;&#30340;&#32570;&#38519;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.11261</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#36127;&#39640;&#26031;&#28151;&#21512;&#26799;&#24230;&#30340;&#25193;&#25955;&#27169;&#22411;&#26465;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#26465;&#20214;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#33021;&#22815;&#20135;&#29983;&#26356;&#23569;&#30340;&#32570;&#38519;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#19968;&#31181;&#23545;&#22270;&#20687;&#21512;&#25104;&#21644;&#20854;&#20182;&#39046;&#22495;&#26377;&#24040;&#22823;&#24433;&#21709;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#20204;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#22810;&#26679;&#30340;&#26465;&#20214;&#36755;&#20837;&#65292;&#22914;&#25991;&#26412;&#25110;&#36793;&#30028;&#26694;&#65292;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#29305;&#24449;&#26465;&#20214;&#26469;&#24341;&#23548;&#21435;&#22122;&#36807;&#31243;&#30340;&#26465;&#20214;&#26426;&#21046;&#12290;&#22522;&#20110;&#38598;&#21512;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#22522;&#20110;&#29305;&#24449;&#21644;&#31867;&#21035;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#22240;&#27492;&#22522;&#20110;&#29305;&#24449;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#27604;&#22522;&#20110;&#31867;&#21035;&#30340;&#26465;&#20214;&#28508;&#22312;&#20998;&#24067;&#20135;&#29983;&#26356;&#23569;&#30340;&#32570;&#38519;&#29983;&#25104;&#12290;&#20998;&#21035;&#35757;&#32451;&#20102;&#20004;&#20010;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20989;&#25968;&#65292;&#31216;&#20026;&#36127;&#39640;&#26031;&#28151;&#21512;&#26799;&#24230;&#65288;NGMG&#65289;&#65292;&#24182;&#24212;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model tr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#34701;&#21512;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#29992;&#20110;&#32534;&#30721;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#26368;&#20339;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11252</link><description>&lt;p&gt;
&#33258;&#21160;&#34701;&#21512;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20197;&#25552;&#21319;&#21307;&#30103;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Automated Fusion of Multimodal Electronic Health Records for Better Medical Predictions. (arXiv:2401.11252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#34701;&#21512;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#29992;&#20110;&#32534;&#30721;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#26368;&#20339;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#26426;&#26500;&#24191;&#27867;&#37319;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#20135;&#29983;&#20102;&#22823;&#37327;&#21307;&#30103;&#25968;&#25454;&#65292;&#20026;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;EHR&#25968;&#25454;&#20013;&#22797;&#26434;&#22810;&#26679;&#30340;&#27169;&#24577;&#21644;&#29305;&#24449;&#32467;&#26500;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#22522;&#20110;&#30452;&#35273;&#21644;&#32463;&#39564;&#30340;&#25163;&#24037;&#27169;&#22411;&#26550;&#26500;&#65292;&#23548;&#33268;&#23376;&#20248;&#27169;&#22411;&#26550;&#26500;&#21644;&#26377;&#38480;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#33258;&#21160;&#21270;EHR&#25968;&#25454;&#25366;&#25496;&#27169;&#22411;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoFM&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#25628;&#32034;&#29992;&#20110;&#32534;&#30721;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#26368;&#20339;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#22810;&#27169;&#24577;EHR&#25968;&#25454;&#21644;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Electronic Health Record (EHR) systems in healthcare institutes has generated vast amounts of medical data, offering significant opportunities for improving healthcare services through deep learning techniques. However, the complex and diverse modalities and feature structures in real-world EHR data pose great challenges for deep learning model design. To address the multi-modality challenge in EHR data, current approaches primarily rely on hand-crafted model architectures based on intuition and empirical experiences, leading to sub-optimal model architectures and limited performance. Therefore, to automate the process of model design for mining EHR data, we propose a novel neural architecture search (NAS) framework named AutoFM, which can automatically search for the optimal model architectures for encoding diverse input modalities and fusion strategies. We conduct thorough experiments on real-world multi-modal EHR data and prediction tasks, and the results 
&lt;/p&gt;</description></item><item><title>AFS-BM&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.11250</link><description>&lt;p&gt;
AFS-BM:&#36890;&#36807;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#20108;&#20540;&#23631;&#34109;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking. (arXiv:2401.11250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11250
&lt;/p&gt;
&lt;p&gt;
AFS-BM&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#26368;&#20851;&#38190;&#30340;&#20027;&#39064;&#20043;&#19968;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#22788;&#29702;&#30456;&#20851;&#29305;&#24449;&#12289;&#36866;&#24212;&#21487;&#21464;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#20108;&#20540;&#23631;&#34109;&#8221;(AFS-BM)&#12290;AFS-BM&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26469;&#21516;&#26102;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21644;&#20108;&#20540;&#23631;&#34109;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25345;&#32493;&#35843;&#25972;&#29305;&#24449;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;AFS-BM&#19982;&#24050;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods usin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#19981;&#21516;&#32463;&#39564;&#29255;&#27573;&#32452;&#21512;&#36215;&#26469;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#23646;&#24615;&#19982;&#32452;&#21512;&#27867;&#21270;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.11237</link><description>&lt;p&gt;
&#32553;&#23567;TD&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;--&#20174;&#19968;&#33324;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View. (arXiv:2401.11237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11237
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#19981;&#21516;&#32463;&#39564;&#29255;&#27573;&#32452;&#21512;&#36215;&#26469;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#23646;&#24615;&#19982;&#32452;&#21512;&#27867;&#21270;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#23558;&#32463;&#39564;&#29255;&#27573;&#32452;&#21512;&#36215;&#26469;&#35299;&#20915;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#32463;&#24120;&#34987;&#36861;&#27714;&#30340;&#29305;&#24615;&#26159;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19982;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#20960;&#31181;&#21306;&#21035;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#22522;&#20110;&#29616;&#25104;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#32452;&#21512;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#21462;&#24471;&#20986;&#33394;&#30340;&#32467;&#26524;&#65307;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#25918;&#24323;&#20102;&#36825;&#31181;&#37325;&#35201;&#30340;&#32452;&#21512;&#29305;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21363;&#23558;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#21644;&#36798;&#21040;&#30446;&#26631;&#22238;&#25253;&#20540;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23637;&#31034;&#20102;&#32452;&#21512;&#29305;&#24615;&#23545;&#24212;&#20102;&#19968;&#31181;&#32452;&#21512;&#27867;&#21270;&#65306;&#22312;(state, goal)&#23545;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;&#24076;&#26395;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#21516;&#26102;&#20986;&#29616;&#30340;(state, goal)&#23545;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#32452;&#21512;&#27867;&#21270;&#19982;i.i.d.&#27867;&#21270;&#26159;&#19981;&#21516;&#30340;&#12290;&#36825;&#31181;&#36830;&#25509;&#23558;&#32452;&#21512;&#29305;&#24615;&#19982;&#27867;&#21270;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;(TreeMIL)&#65292;&#29992;&#20110;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#25972;&#20010;&#24207;&#21015;&#20998;&#35299;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#24182;&#25552;&#21462;&#23376;&#24207;&#21015;&#29305;&#24449;&#65292;&#26088;&#22312;&#35299;&#20915;&#38598;&#20307;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.11235</link><description>&lt;p&gt;
TreeMIL&#65306;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly Detection with Inexact Supervision. (arXiv:2401.11235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;(TreeMIL)&#65292;&#29992;&#20110;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#25972;&#20010;&#24207;&#21015;&#20998;&#35299;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#24182;&#25552;&#21462;&#23376;&#24207;&#21015;&#29305;&#24449;&#65292;&#26088;&#22312;&#35299;&#20915;&#38598;&#20307;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22312;&#21307;&#30103;&#12289;&#32593;&#32476;&#21644;&#24037;&#19994;&#31561;&#21508;&#20010;&#39046;&#22495;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#32771;&#34385;&#21040;&#26631;&#31614;&#23545;&#20110;&#26816;&#27979;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#20294;&#24456;&#38590;&#33719;&#24471;&#65292;&#25105;&#20204;&#36716;&#32780;&#30740;&#31350;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#35757;&#32451;&#38454;&#27573;&#21482;&#25552;&#20379;&#24207;&#21015;&#32423;&#21035;&#30340;&#26631;&#31614;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#39044;&#27979;&#20986;&#28857;&#32423;&#21035;&#30340;&#24322;&#24120;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#20256;&#32479;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;(MIL)&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#40723;&#21169;&#22312;&#21508;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#24471;&#21040;&#39640;&#30340;&#24322;&#24120;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#19981;&#20165;&#38480;&#20110;&#20010;&#21035;&#28857;&#24322;&#24120;&#65292;&#23427;&#20204;&#36824;&#21487;&#20197;&#26159;&#38598;&#20307;&#24322;&#24120;&#65292;&#36890;&#24120;&#22312;&#23376;&#24207;&#21015;&#20013;&#23637;&#31034;&#20986;&#24322;&#24120;&#27169;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;MIL&#26694;&#26550;(TreeMIL)&#12290;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#19968;&#20010;N&#21449;&#26641;&#32467;&#26500;&#23558;&#25972;&#20010;&#24207;&#21015;&#20998;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#19981;&#21516;&#23618;&#32423;&#30340;&#33410;&#28857;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#23376;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#23376;&#24207;&#21015;&#29305;&#24449;&#34987;&#25552;&#21462;&#26469;&#39044;&#27979;&#33410;&#28857;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection (TSAD) plays a vital role in various domains such as healthcare, networks, and industry. Considering labels are crucial for detection but difficult to obtain, we turn to TSAD with inexact supervision: only series-level labels are provided during the training phase, while point-level anomalies are predicted during the testing phase. Previous works follow a traditional multi-instance learning (MIL) approach, which focuses on encouraging high anomaly scores at individual time steps. However, time series anomalies are not only limited to individual point anomalies, they can also be collective anomalies, typically exhibiting abnormal patterns over subsequences. To address the challenge of collective anomalies, in this paper, we propose a tree-based MIL framework (TreeMIL). We first adopt an N-ary tree structure to divide the entire series into multiple nodes, where nodes at different levels represent subsequences with different lengths. Then, the subsequence fe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;&#24223;&#27700;&#22788;&#29702;&#21378;&#28342;&#35299;&#27687;&#27987;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#21478;&#19968;&#20010;&#24037;&#19994;&#21378;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#35757;&#32451;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#22791;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.11217</link><description>&lt;p&gt;
&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#28151;&#21512;&#26041;&#27861;&#65306;&#25552;&#39640;&#24037;&#19994;&#24223;&#27700;&#22788;&#29702;&#21378;&#28342;&#35299;&#27687;&#27987;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Approach of Transfer Learning and Physics-Informed Modeling: Improving Dissolved Oxygen Concentration Prediction in an Industrial Wastewater Treatment Plant. (arXiv:2401.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;&#24223;&#27700;&#22788;&#29702;&#21378;&#28342;&#35299;&#27687;&#27987;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#21478;&#19968;&#20010;&#24037;&#19994;&#21378;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#35757;&#32451;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#22791;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#38750;&#32447;&#24615;&#21644;&#22797;&#26434;&#31995;&#32479;&#65288;&#22914;&#24223;&#27700;&#22788;&#29702;&#21333;&#20803;&#65289;&#30340;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#30528;&#32570;&#22833;&#12289;&#20302;&#36136;&#37327;&#25110;&#22122;&#22768;&#25968;&#25454;&#31561;&#38382;&#39064;&#12290;&#20256;&#36755;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#23558;&#26469;&#33258;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20256;&#36755;&#65288;i&#65289;&#25429;&#33719;&#36807;&#31243;&#24213;&#23618;&#29289;&#29702;&#29305;&#24615;&#30340;&#24320;&#28304;&#20223;&#30495;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#19982;&#30446;&#26631;&#24037;&#21378;&#26377;&#25152;&#19981;&#21516;&#65292;&#65288;ii&#65289;&#21478;&#19968;&#20010;&#29305;&#28857;&#26159;&#22122;&#22768;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#21516;&#19968;&#28860;&#27833;&#21378;&#30340;&#24037;&#19994;&#21378;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#26469;&#33258;&#65288;ii&#65289;&#27169;&#22411;&#65292;&#24182;&#23558;&#35757;&#32451;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#24314;&#27169;&#20026;&#29289;&#29702;&#20449;&#24687;&#65292;&#20174;&#24320;&#28304;&#27169;&#22411;&#30340;&#23548;&#20986;&#29289;&#29702;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing first principles models is a challenging task for nonlinear and complex systems such as a wastewater treatment unit. In recent years, data-driven models are widely used to overcome the complexity. However, they often suffer from issues such as missing, low quality or noisy data. Transfer learning is a solution for this issue where knowledge from another task is transferred to target one to increase the prediction performance. In this work, the objective is increasing the prediction performance of an industrial wastewater treatment plant by transferring the knowledge of (i) an open-source simulation model that captures the underlying physics of the process, albeit with dissimilarities to the target plant, (ii) another industrial plant characterized by noisy and limited data but located in the same refinery, and (iii) the model in (ii) and making the objective function of the training problem physics informed where the physics information derived from the open-source model i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24211;&#23884;&#20837;&#20013;&#30340;&#34892;&#36208;&#26041;&#26696;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20851;&#27880;&#20960;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#34892;&#36208;&#26041;&#26696;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#33719;&#24471;&#20803;&#32452;&#23884;&#20837;&#65292;&#24182;&#20445;&#25345;&#20854;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.11215</link><description>&lt;p&gt;
&#36873;&#25321;&#29992;&#20110;&#25968;&#25454;&#24211;&#23884;&#20837;&#30340;&#34892;&#36208;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Selecting Walk Schemes for Database Embedding. (arXiv:2401.11215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24211;&#23884;&#20837;&#20013;&#30340;&#34892;&#36208;&#26041;&#26696;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20851;&#27880;&#20960;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#34892;&#36208;&#26041;&#26696;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#33719;&#24471;&#20803;&#32452;&#23884;&#20837;&#65292;&#24182;&#20445;&#25345;&#20854;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20998;&#26512;&#30340;&#26426;&#26800;&#35774;&#22791;&#36890;&#24120;&#38656;&#35201;&#23545;&#36755;&#20837;&#36827;&#34892;&#25968;&#20540;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32452;&#25104;&#37096;&#20998;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#20803;&#32452;&#30340;&#23884;&#20837;&#65292;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#22522;&#20110;&#23545;&#25968;&#25454;&#24211;&#20013;&#30340;&#19968;&#32452;&#38543;&#26426;&#34892;&#36208;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#36866;&#29992;&#20110;&#21160;&#24577;&#25968;&#25454;&#24211;&#30340;FoRWaRD&#31639;&#27861;&#65292;&#20854;&#20013;&#34892;&#36208;&#26159;&#36890;&#36807;&#36981;&#24490;&#20803;&#32452;&#38388;&#30340;&#22806;&#38190;&#26469;&#37319;&#26679;&#30340;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#19981;&#21516;&#30340;&#34892;&#36208;&#26377;&#19981;&#21516;&#30340;&#27169;&#24335;&#65292;&#25110;&#32773;&#8220;&#34892;&#36208;&#26041;&#26696;&#8221;&#65292;&#36890;&#36807;&#21015;&#20986;&#34892;&#36208;&#20013;&#30340;&#20851;&#31995;&#21644;&#23646;&#24615;&#26469;&#27966;&#29983;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#19981;&#21516;&#30340;&#34892;&#36208;&#26041;&#26696;&#25551;&#36848;&#20102;&#25968;&#25454;&#24211;&#20013;&#19981;&#21516;&#24615;&#36136;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38598;&#20013;&#20851;&#27880;&#20960;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#34892;&#36208;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#33719;&#24471;&#20803;&#32452;&#23884;&#20837;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36136;&#37327;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#29992;&#20110;&#20803;&#32452;&#23884;&#20837;&#30340;&#26041;&#26696;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#20960;&#31181;&#23454;&#39564;&#31574;&#30053;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machinery for data analysis often requires a numeric representation of the input. Towards that, a common practice is to embed components of structured data into a high-dimensional vector space. We study the embedding of the tuples of a relational database, where existing techniques are often based on optimization tasks over a collection of random walks from the database. The focus of this paper is on the recent FoRWaRD algorithm that is designed for dynamic databases, where walks are sampled by following foreign keys between tuples. Importantly, different walks have different schemas, or "walk schemes", that are derived by listing the relations and attributes along the walk. Also importantly, different walk schemes describe relationships of different natures in the database. We show that by focusing on a few informative walk schemes, we can obtain tuple embedding significantly faster, while retaining the quality. We define the problem of scheme selection for tuple embedding, devise sev
&lt;/p&gt;</description></item><item><title>PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.11202</link><description>&lt;p&gt;
PartIR: &#20026;&#26426;&#22120;&#23398;&#20064;&#32452;&#21512;SPMD&#20998;&#21306;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11202
&lt;/p&gt;
&lt;p&gt;
PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#32467;&#21512;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20248;&#21270;&#22120;&#20998;&#29255;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#24403;&#31574;&#30053;&#21464;&#24471;&#22797;&#26434;&#26102;&#65292;&#20998;&#21306;&#24037;&#20855;&#38656;&#35201;&#20855;&#22791;&#20197;&#19979;&#29305;&#28857;&#65306;1&#65289;&#34920;&#36798;&#21147;&#24378;&#65292;&#20801;&#35768;&#32452;&#21512;&#31616;&#21333;&#31574;&#30053;&#65307;2&#65289;&#21487;&#39044;&#27979;&#24615;&#24378;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PartIR&#65292;&#19968;&#31181;&#29992;&#20110;NN&#20998;&#21306;&#30340;&#35774;&#35745;&#12290;PartIR&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#19982;&#30828;&#20214;&#21644;&#36816;&#34892;&#26102;&#26080;&#20851;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;API&#29992;&#20110;&#32452;&#21512;&#20998;&#29255;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#25972;&#20010;&#36807;&#31243;&#30001;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#26082;&#21487;&#20197;&#25163;&#21160;&#20063;&#21487;&#20197;&#33258;&#21160;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#19982;&#27169;&#22411;&#20195;&#30721;&#20998;&#24320;&#25351;&#23450;&#65292;&#26131;&#20110;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#31181;&#19981;&#21516;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#23637;&#31034;PartIR&#30340;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#37492;&#21035;&#23545;&#40784;&#30340;&#25237;&#24433;&#20449;&#24565;&#32593;&#32476;&#22312;&#22768;&#23398;&#20107;&#20214;&#20998;&#31867;&#20013;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;CNN&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11199</link><description>&lt;p&gt;
&#22522;&#20110;&#37492;&#21035;&#23545;&#40784;&#30340;&#25237;&#24433;&#20449;&#24565;&#32593;&#32476;&#22312;&#22768;&#23398;&#20107;&#20214;&#20998;&#31867;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;CNN&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Projected Belief Networks With Discriminative Alignment for Acoustic Event Classification: Rivaling State of the Art CNNs. (arXiv:2401.11199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11199
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37492;&#21035;&#23545;&#40784;&#30340;&#25237;&#24433;&#20449;&#24565;&#32593;&#32476;&#22312;&#22768;&#23398;&#20107;&#20214;&#20998;&#31867;&#20013;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;CNN&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#24433;&#20449;&#24565;&#32593;&#32476;&#65288;PBN&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#21487;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#30340;&#29983;&#25104;&#24615;&#38543;&#26426;&#32593;&#32476;&#65292;&#23427;&#22522;&#20110;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476;&#65288;FFNN&#65289;&#12290;&#29983;&#25104;&#20989;&#25968;&#36890;&#36807;&#22312;FFNN&#20013;&#36827;&#34892;&#8220;&#22791;&#20221;&#8221;&#25805;&#20316;&#26469;&#36816;&#20316;&#12290; PBN&#26159;&#20004;&#20010;&#32593;&#32476;&#65292;&#19968;&#20010;&#22312;&#27491;&#21521;&#26041;&#21521;&#19978;&#36816;&#20316;&#30340;FFNN&#21644;&#19968;&#20010;&#22312;&#21453;&#21521;&#26041;&#21521;&#19978;&#36816;&#20316;&#30340;&#29983;&#25104;&#32593;&#32476;&#12290;&#20004;&#20010;&#32593;&#32476;&#20849;&#21516;&#23384;&#22312;&#20110;&#30456;&#21516;&#30340;&#21442;&#25968;&#38598;&#19978;&#65292;&#20855;&#26377;&#33258;&#24049;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#21487;&#20197;&#20998;&#21035;&#25110;&#32852;&#21512;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;PBN&#26377;&#21487;&#33021;&#25317;&#26377;&#37492;&#21035;&#24615;&#21644;&#29983;&#25104;&#24615;&#20998;&#31867;&#22120;&#30340;&#26368;&#20339;&#29305;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#28508;&#21147;&#65292;&#23545;&#27599;&#20010;&#31867;&#21035;&#20998;&#21035;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;PBN&#65292;&#23545;&#32473;&#23450;&#31867;&#21035;&#26368;&#22823;&#21270;&#29983;&#25104;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;FFNN&#30456;&#23545;&#20110;&#8220;&#25152;&#26377;&#20854;&#20182;&#31867;&#21035;&#8221;&#30340;&#37492;&#21035;&#25104;&#26412;&#12290;&#36825;&#20010;&#25216;&#26415;&#34987;&#31216;&#20026;&#37492;&#21035;&#23545;&#40784;&#65288;PBN-DA&#65289;&#65292;&#23427;&#23558;&#20284;&#28982;&#20989;&#25968;&#30340;&#36718;&#24275;&#19982;&#20915;&#31574;&#36793;&#30028;&#23545;&#40784;&#65292;&#24182;&#23454;&#29616;&#20102;&#22823;&#22823;&#25913;&#21892;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The projected belief network (PBN) is a generative stochastic network with tractable likelihood function based on a feed-forward neural network (FFNN). The generative function operates by "backing up" through the FFNN. The PBN is two networks in one, a FFNN that operates in the forward direction, and a generative network that operates in the backward direction. Both networks co-exist based on the same parameter set, have their own cost functions, and can be separately or jointly trained. The PBN therefore has the potential to possess the best qualities of both discriminative and generative classifiers. To realize this potential, a separate PBN is trained on each class, maximizing the generative likelihood function for the given class, while minimizing the discriminative cost for the FFNN against "all other classes". This technique, called discriminative alignment (PBN-DA), aligns the contours of the likelihood function to the decision boundaries and attains vastly improved classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26446;&#32676;&#19978;&#28436;&#21270;&#30340;&#31995;&#32479;&#30340;&#29366;&#24577;&#35266;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#38480;&#21046;&#22312;&#26446;&#32676;&#19978;&#30340;&#35266;&#27979;&#22120;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#35757;&#32451;&#30340;&#31639;&#27861;&#26469;&#39044;&#27979;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;&#20174;&#26446;&#20195;&#25968;&#21040;&#32676;&#30340;&#26144;&#23556;&#20197;&#21450;&#32676;&#20316;&#29992;&#21644;&#24403;&#21069;&#29366;&#24577;&#26469;&#20272;&#35745;&#26102;&#21051;t&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.11196</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#29366;&#24577;&#35266;&#27979;&#22120;&#22312;&#26446;&#32676;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine learning based state observer for discrete time systems evolving on Lie groups. (arXiv:2401.11196v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26446;&#32676;&#19978;&#28436;&#21270;&#30340;&#31995;&#32479;&#30340;&#29366;&#24577;&#35266;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#38480;&#21046;&#22312;&#26446;&#32676;&#19978;&#30340;&#35266;&#27979;&#22120;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#35757;&#32451;&#30340;&#31639;&#27861;&#26469;&#39044;&#27979;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;&#20174;&#26446;&#20195;&#25968;&#21040;&#32676;&#30340;&#26144;&#23556;&#20197;&#21450;&#32676;&#20316;&#29992;&#21644;&#24403;&#21069;&#29366;&#24577;&#26469;&#20272;&#35745;&#26102;&#21051;t&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35266;&#27979;&#22120;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#20197;&#20351;&#35266;&#27979;&#22120;&#30340;&#29366;&#24577;&#38480;&#21046;&#22312;&#31995;&#32479;&#28436;&#21270;&#30340;&#26446;&#32676;&#19978;&#12290;&#20256;&#32479;&#30340;&#22312;&#26446;&#32676;&#19978;&#28436;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#35266;&#27979;&#22120;&#25216;&#26415;&#28041;&#21450;&#35774;&#35745;&#26446;&#32676;&#30340;&#22270;&#34920;&#65292;&#20026;&#27599;&#20010;&#22270;&#34920;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35266;&#27979;&#22120;&#65292;&#24182;&#26681;&#25454;&#31995;&#32479;&#29366;&#24577;&#20999;&#25442;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#20165;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#30340;0&#27979;&#24230;&#23376;&#38598;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#22270;&#34920;&#12290;&#21033;&#29992;&#36825;&#20010;&#32593;&#32476;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35266;&#27979;&#22120;&#65292;&#30830;&#20445;&#35266;&#27979;&#22120;&#30340;&#29366;&#24577;&#38480;&#21046;&#22312;&#26446;&#32676;&#19978;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#19968;&#20010;&#35757;&#32451;&#30340;&#31639;&#27861;&#26469;&#39044;&#27979;&#29366;&#24577;&#12290;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#22312;&#26446;&#32676;&#30340;&#26446;&#20195;&#25968;&#19978;&#39044;&#27979;&#20102;&#19968;&#20010;&#8220;&#35823;&#24046;&#39033;&#8221;&#65292;&#21033;&#29992;&#20102;&#20174;&#26446;&#20195;&#25968;&#21040;&#32676;&#30340;&#26144;&#23556;&#65292;&#24182;&#20351;&#29992;&#32676;&#20316;&#29992;&#21644;&#24403;&#21069;&#29366;&#24577;&#26469;&#20272;&#35745;&#26102;&#21051;t&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a machine learning based observer for systems evolving on manifolds is designed such that the state of the observer is restricted to the Lie group on which the system evolves. Conventional techniques involving machine learning based observers on systems evolving on Lie groups involve designing charts for the Lie group, training a machine learning based observer for each chart, and switching between the trained models based on the state of the system. We propose a novel deep learning based technique whose predictions are restricted to a measure 0 subset of Euclidean space without using charts. Using this network, we design an observer ensuring that the state of the observer is restricted to the Lie group, and predicting the state using only one trained algorithm. The deep learning network predicts an ``error term'' on the Lie algebra of the Lie group, uses the map from the Lie algebra to the group, and uses the group action and the present state to estimate the state at t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#26522;&#20030;DN&#20998;&#21306;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#24120;&#29992;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21457;&#29616;&#22914;&#26524;&#21482;&#23545;&#20855;&#26377;&#8220;&#22823;&#8221;&#20307;&#31215;&#30340;&#21306;&#22495;&#24863;&#20852;&#36259;&#65292;&#37027;&#20040;&#23545;&#31354;&#38388;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#38750;&#24120;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.11188</link><description>&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#26522;&#20030;&#28145;&#24230;&#32593;&#32476;&#20998;&#21306;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Fast and Exact Enumeration of Deep Networks Partitions Regions. (arXiv:2401.11188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#26522;&#20030;DN&#20998;&#21306;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#24120;&#29992;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21457;&#29616;&#22914;&#26524;&#21482;&#23545;&#20855;&#26377;&#8220;&#22823;&#8221;&#20307;&#31215;&#30340;&#21306;&#22495;&#24863;&#20852;&#36259;&#65292;&#37027;&#20040;&#23545;&#31354;&#38388;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#38750;&#24120;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;(DNs)&#30340;&#19968;&#20010;&#23500;&#26377;&#25104;&#26524;&#30340;&#34920;&#36848;&#26041;&#24335;&#26159;&#36890;&#36807;&#20998;&#27573;&#20223;&#23556;&#26679;&#26465;&#26469;&#23454;&#29616;&#65292;&#23427;&#26082;&#33021;&#22815;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#21448;&#33021;&#20026;&#20174;&#19994;&#32773;&#25552;&#20379;&#23454;&#36341;&#25351;&#23548;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;DN&#30340;&#36755;&#20837;&#26144;&#23556;&#34987;&#34920;&#36798;&#20026;&#25353;&#21306;&#22495;&#30340;&#20223;&#23556;&#26144;&#23556;&#65292;&#20854;&#20013;&#36825;&#20123;&#21306;&#22495;&#30001;&#27169;&#22411;&#30340;&#26550;&#26500;&#38544;&#24335;&#30830;&#23450;&#65292;&#24182;&#24418;&#25104;&#23427;&#20204;&#30340;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#20010;&#20998;&#21306;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#20998;&#21306;&#8212;&#8212;&#36825;&#20010;&#30740;&#31350;&#32447;&#20013;&#20135;&#29983;&#30340;&#25152;&#26377;&#32467;&#26524;&#37117;&#28041;&#21450;&#21040;&#23427;&#8212;&#8212;&#21482;&#22312;DN&#30340;&#36755;&#20837;&#31354;&#38388;&#30340;$2/3$&#32500;&#20999;&#29255;&#19978;&#35745;&#31639;&#36807;&#65292;&#25110;&#32773;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#26522;&#20030;DN&#20998;&#21306;&#21306;&#22495;&#30340;&#24182;&#34892;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26368;&#32456;&#33021;&#22815;&#23545;&#24120;&#29992;&#30340;&#36817;&#20284;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#20363;&#22914;&#22522;&#20110;DN&#36755;&#20837;&#31354;&#38388;&#30340;&#38543;&#26426;&#25277;&#26679;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#37325;&#35201;&#21457;&#29616;&#26159;&#65292;&#22914;&#26524;&#21482;&#23545;&#20855;&#26377;&#8220;&#22823;&#8221;&#20307;&#31215;&#30340;&#21306;&#22495;&#24863;&#20852;&#36259;&#65292;&#37027;&#20040;&#23545;&#31354;&#38388;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#38750;&#24120;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN's input-mapping is expressed as per-region affine mapping where those regions are implicitly determined by the model's architecture and form a partition of their input space. That partition -- which is involved in all the results spanned from this line of research -- has so far only been computed on $2/3$-dimensional slices of the DN's input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN's partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with ``large'' volume, then uniform sampling of the space is highly efficient
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.11176</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#30446;&#26631;&#23450;&#20301;: &#20351;&#29992;Cram&#233;r-Rao&#30028;&#38480;&#23545;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\'er-Rao Bound. (arXiv:2401.11176v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#38647;&#36798;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#36827;&#34892;&#31934;&#30830;&#30340;&#30446;&#26631;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36798;&#21040;Cram&#233;r-Rao&#30028;&#38480;&#65288;CRB&#65289;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#35823;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20248;&#20110;&#36825;&#20123;&#20256;&#32479;&#25216;&#26415;&#65292;&#22312;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#20195;&#34920;&#24615;&#30340;&#27169;&#25311;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22987;&#32456;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#20559;&#35265;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#20943;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern radar systems, precise target localization using azimuth and velocity estimation is paramount. Traditional unbiased estimation methods have leveraged gradient descent algorithms to reach the theoretical limits of the Cram\'er Rao Bound (CRB) for the error of the parameter estimates. In this study, we present a data-driven neural network approach that outperforms these traditional techniques, demonstrating improved accuracies in target azimuth and velocity estimation. Using a representative simulated scenario, we show that our proposed neural network model consistently achieves improved parameter estimates due to its inherently biased nature, yielding a diminished mean squared error (MSE). Our findings underscore the potential of employing deep learning methods in radar systems, paving the way for more accurate localization in cluttered and dynamic environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.11174</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#21035;&#35782;&#21035;&#29992;&#20110;&#25972;&#20307;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Prostatectomies&#30340;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#65288;GraSP&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#36827;&#34892;&#20102;&#23618;&#27425;&#21270;&#24314;&#27169;&#65292;&#21253;&#25324;&#19981;&#21516;&#31890;&#24230;&#30340;&#20114;&#34917;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#22806;&#31185;&#27963;&#21160;&#30340;&#22810;&#23618;&#27425;&#29702;&#35299;&#65292;&#21253;&#25324;&#22806;&#31185;&#38454;&#27573;&#21644;&#27493;&#39588;&#30340;&#35782;&#21035;&#20197;&#21450;&#21253;&#25324;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#21644;&#21407;&#23376;&#21487;&#35270;&#21160;&#20316;&#26816;&#27979;&#22312;&#20869;&#30340;&#30701;&#26399;&#20219;&#21153;&#12290;&#20026;&#20102;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#24418;&#22120;&#65288;Transformers&#65289;&#30340;&#34892;&#21160;&#12289;&#38454;&#27573;&#12289;&#27493;&#39588;&#21644;&#22120;&#26800;&#20998;&#21106;&#65288;TAPIS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#26469;&#33258;&#22120;&#26800;&#20998;&#21106;&#27169;&#22411;&#30340;&#23616;&#37096;&#21306;&#22495;&#24314;&#35758;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30701;&#26399;&#35782;&#21035;&#20219;&#21153;&#20013;&#21253;&#25324;&#20998;&#21106;&#27880;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#26174;&#20102;&#19981;&#21516;&#30340;&#31890;&#24230;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity require
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#27491;-&#36127;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#25991;&#26723;&#38598;&#25193;&#23637;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11145</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#27491;-&#36127;&#23398;&#20064;&#26041;&#27861;&#30340;&#25991;&#26723;&#38598;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Document Set Expansion with Positive-Unlabeled Learning: A Density Estimation-based Approach. (arXiv:2401.11145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#27491;-&#36127;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#25991;&#26723;&#38598;&#25193;&#23637;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#38598;&#25193;&#23637;&#26088;&#22312;&#22522;&#20110;&#19968;&#32452;&#31934;&#32454;&#20027;&#39064;&#30340;&#23567;&#22411;&#25991;&#26723;&#38598;&#21512;&#65292;&#20174;&#22823;&#22411;&#38598;&#21512;&#20013;&#35782;&#21035;&#30456;&#20851;&#25991;&#26723;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27491;-&#36127;&#23398;&#20064;&#26159;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20005;&#37325;&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#65292;&#20363;&#22914;&#27491;-&#36127;&#23398;&#20064;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20856;&#22411;&#25361;&#25112;&#65292;&#22914;&#26410;&#30693;&#31867;&#21035;&#20808;&#39564;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#20197;&#21450;&#38656;&#35201;&#36716;&#23548;&#22411;&#23454;&#39564;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#20840;&#26032;&#27491;-&#36127;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;puDE&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#19978;&#36848;&#38382;&#39064;&#12290;puDE&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#26082;&#19981;&#21463;&#38480;&#20110;SCAR&#20551;&#35774;&#65292;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#31867;&#21035;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;DSE&#20219;&#21153;&#30340;&#19968;&#31181;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document set expansion aims to identify relevant documents from a large collection based on a small set of documents that are on a fine-grained topic. Previous work shows that PU learning is a promising method for this task. However, some serious issues remain unresolved, i.e. typical challenges that PU methods suffer such as unknown class prior and imbalanced data, and the need for transductive experimental settings. In this paper, we propose a novel PU learning framework based on density estimation, called puDE, that can handle the above issues. The advantage of puDE is that it neither constrained to the SCAR assumption and nor require any class prior knowledge. We demonstrate the effectiveness of the proposed method using a series of real-world datasets and conclude that our method is a better alternative for the DSE task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20855;&#21464;&#37327;&#27861;&#35782;&#21035;&#21644;&#20272;&#35745;&#36830;&#32493;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19977;&#31867;&#30456;&#24212;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#32479;&#35745;&#24615;&#36136;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.11130</link><description>&lt;p&gt;
&#36890;&#36807;&#24037;&#20855;&#21464;&#37327;&#27861;&#35782;&#21035;&#21644;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#20559;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable. (arXiv:2401.11130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20855;&#21464;&#37327;&#27861;&#35782;&#21035;&#21644;&#20272;&#35745;&#36830;&#32493;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19977;&#31867;&#30456;&#24212;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#32479;&#35745;&#24615;&#36136;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20272;&#35745;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26465;&#20214;&#24179;&#22343;&#20559;&#22240;&#26524;&#25928;&#24212;&#65288;CAPCE&#65289;&#65292;&#20197;&#25581;&#31034;&#36830;&#32493;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#24037;&#20855;&#21464;&#37327;&#35774;&#32622;&#19979;&#35782;&#21035;CAPCE&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31867;CAPCE&#20272;&#35745;&#22120;&#65306;&#31579;&#36873;&#12289;&#21442;&#25968;&#21270;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;-&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23545;&#25552;&#20986;&#30340;CAPCE&#20272;&#35745;&#22120;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CARE&#30340;&#36890;&#29992;&#32593;&#32476;&#23433;&#20840;&#23545;&#25239;&#40065;&#26834;&#24615;&#35780;&#20272;&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#35299;&#20915;&#38598;&#25104;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2401.11126</link><description>&lt;p&gt;
CARE: &#38024;&#23545;&#23433;&#20840;&#24212;&#29992;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#32773;&#30340;&#38598;&#25104;&#23545;&#25239;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive Attackers for Security Applications. (arXiv:2401.11126v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CARE&#30340;&#36890;&#29992;&#32593;&#32476;&#23433;&#20840;&#23545;&#25239;&#40065;&#26834;&#24615;&#35780;&#20272;&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#35299;&#20915;&#38598;&#25104;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#38450;&#24481;&#22312;&#21508;&#31181;&#23433;&#20840;&#30456;&#20851;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#20063;&#24341;&#21457;&#20102;&#35768;&#22810;&#38382;&#39064;&#65306;&#19968;&#33324;&#30340;&#38598;&#25104;&#38450;&#24481;&#26159;&#21542;&#30830;&#20445;&#27604;&#20010;&#20307;&#26356;&#20855;&#40065;&#26834;&#24615;&#65311;&#24403;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#25915;&#20987;&#19982;&#38450;&#24481;&#19981;&#26029;&#21319;&#32423;&#26102;&#65292;&#26356;&#24378;&#22823;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#26159;&#21542;&#20250;&#20987;&#36133;&#29616;&#26377;&#30340;&#38598;&#25104;&#38450;&#24481;&#31574;&#30053;&#65311;&#38598;&#25104;&#38450;&#24481;&#26159;&#21542;&#33021;&#22815;&#21516;&#26102;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#36798;&#21040;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#24182;&#25269;&#24481;&#19981;&#26029;&#35843;&#25972;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#65311;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#27809;&#26377;&#20840;&#38754;&#35780;&#20272;&#38598;&#25104;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24179;&#21488;&#65292;&#36825;&#20123;&#37325;&#35201;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CARE&#30340;&#36890;&#29992;&#32593;&#32476;&#23433;&#20840;&#23545;&#25239;&#40065;&#26834;&#24615;&#35780;&#20272;&#24179;&#21488;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble defenses, are widely employed in various security-related applications to enhance model performance and robustness. The widespread adoption of these techniques also raises many questions: Are general ensembles defenses guaranteed to be more robust than individuals? Will stronger adaptive attacks defeat existing ensemble defense strategies as the cybersecurity arms race progresses? Can ensemble defenses achieve adversarial robustness to different types of attacks simultaneously and resist the continually adjusted adaptive attacks? Unfortunately, these critical questions remain unresolved as there are no platforms for comprehensive evaluation of ensemble adversarial attacks and defenses in the cybersecurity domain. In this paper, we propose a general Cybersecurity Adversarial Robustness Evaluation (CARE) platform aiming to bridge this gap.
&lt;/p&gt;</description></item><item><title>EMA-Net &#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#27169;&#22359;(CTAL)&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.11124</link><description>&lt;p&gt;
EMA-Net: &#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#29992;&#20110;&#31264;&#23494;&#22330;&#26223;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions. (arXiv:2401.11124v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11124
&lt;/p&gt;
&lt;p&gt;
EMA-Net &#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#27169;&#22359;(CTAL)&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22240;&#20854;&#33021;&#22815;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#20219;&#21153;&#65292;&#22312;&#20351;&#29992;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26356;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#27599;&#20010;&#20219;&#21153;&#24615;&#33021;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#20197;&#35299;&#30721;&#22120;&#20026;&#37325;&#28857;&#30340;&#26550;&#26500;&#36890;&#36807;&#20351;&#29992;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#20197;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#20197;&#21450;&#36328;&#20219;&#21153;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65288;EMA-Net&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#20219;&#21153;&#25913;&#36827;&#33021;&#21147;&#12290;EMA-Net&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#65288;CTAL&#65289;&#27169;&#22359;&#24039;&#22937;&#22320;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;CTAL&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#33021;&#22815;&#20197;&#26368;&#36866;&#21512;&#20219;&#21153;&#20146;&#21644;&#30697;&#38453;&#30340;&#26041;&#24335;&#25805;&#32437;&#20219;&#21153;&#20146;&#21644;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) has gained prominence for its ability to jointly predict multiple tasks, achieving better per-task performance while using fewer per-task model parameters than single-task learning. More recently, decoder-focused architectures have considerably improved multitask performance by refining task predictions using the features of other related tasks. However, most of these refinement methods fail to simultaneously capture local and global task-specific representations, as well as cross-task patterns in a parameter-efficient manner. In this paper, we introduce the Efficient Multitask Affinity Learning Network (EMA-Net), which is a lightweight framework that enhances the task refinement capabilities of multitask networks. EMA-Net adeptly captures local, global, and cross-task interactions using our novel Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in its ability to manipulate task affinity matrices in a manner that is optimally suited t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25112;&#30053;&#29289;&#32852;&#32593;&#37096;&#32626;&#35206;&#30422;&#28798;&#38590;&#21709;&#24212;&#26080;&#20154;&#26426;&#32676;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#25552;&#20379;&#28385;&#36275;&#26102;&#38388;&#21644;&#21151;&#29575;&#32422;&#26463;&#30340;&#26368;&#20339;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2401.11118</link><description>&lt;p&gt;
&#25112;&#30053;&#29289;&#32852;&#32593;&#37096;&#32626;&#35206;&#30422;&#28798;&#38590;&#21709;&#24212;&#26080;&#20154;&#26426;&#32676;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;(arXiv:2401.11118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Meta Reinforcement Learning for Strategic IoT Deployments Coverage in Disaster-Response UAV Swarms. (arXiv:2401.11118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25112;&#30053;&#29289;&#32852;&#32593;&#37096;&#32626;&#35206;&#30422;&#28798;&#38590;&#21709;&#24212;&#26080;&#20154;&#26426;&#32676;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#25552;&#20379;&#28385;&#36275;&#26102;&#38388;&#21644;&#21151;&#29575;&#32422;&#26463;&#30340;&#26368;&#20339;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26080;&#20154;&#26426;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#25552;&#20379;&#26080;&#32447;&#26381;&#21153;&#21644;&#25910;&#38598;&#28798;&#23475;&#21306;&#22495;&#25968;&#25454;&#31561;&#20851;&#38190;&#32039;&#24613;&#24212;&#29992;&#20013;&#20855;&#26377;&#26426;&#21160;&#24615;&#21644;&#36816;&#21160;&#28789;&#27963;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#26080;&#20154;&#26426;&#30340;&#36164;&#28304;&#26377;&#38480;&#12289;&#33021;&#28304;&#39044;&#31639;&#26377;&#38480;&#20197;&#21450;&#20005;&#26684;&#30340;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#23545;&#20110;&#36825;&#20123;&#24212;&#29992;&#30340;&#37319;&#29992;&#36896;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#32771;&#34385;&#19968;&#20010;&#26080;&#20154;&#26426;&#32676;&#20307;&#22312;&#19968;&#20010;&#21306;&#22495;&#20013;&#25910;&#38598;&#22320;&#38754;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20854;&#37325;&#28857;&#26159;&#20026;&#25112;&#30053;&#20301;&#32622;&#25552;&#20379;&#26356;&#22909;&#30340;&#26381;&#21153;&#65292;&#24182;&#20801;&#35768;&#26080;&#20154;&#26426;&#20197;&#21160;&#24577;&#26041;&#24335;&#21152;&#20837;&#21644;&#31163;&#24320;&#32676;&#20307;&#65288;&#20363;&#22914;&#20805;&#30005;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#27169;&#22411;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24635;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#22312;&#26368;&#30701;&#23436;&#25104;&#26102;&#38388;&#21644;&#26368;&#20302;&#20256;&#36755;&#21151;&#29575;&#32422;&#26463;&#19979;&#25552;&#20379;&#26080;&#20154;&#26426;&#30340;&#26368;&#20339;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the attention of researchers in academia and industry for their potential use in critical emergency applications, such as providing wireless services to ground users and collecting data from areas affected by disasters, due to their advantages in terms of maneuverability and movement flexibility. The UAVs' limited resources, energy budget, and strict mission completion time have posed challenges in adopting UAVs for these applications. Our system model considers a UAV swarm that navigates an area collecting data from ground IoT devices focusing on providing better service for strategic locations and allowing UAVs to join and leave the swarm (e.g., for recharging) in a dynamic way. In this work, we introduce an optimization model with the aim of minimizing the total energy consumption and provide the optimal path planning of UAVs under the constraints of minimum completion time and transmit power. The formulated optimizati
&lt;/p&gt;</description></item><item><title>SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.11113</link><description>&lt;p&gt;
SPAND: &#20351;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11113
&lt;/p&gt;
&lt;p&gt;
SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#34892;&#20026;&#23545;&#20581;&#24247;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#23545;&#36523;&#24515;&#20581;&#24247;&#30340;&#25351;&#31034;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#65292;&#21487;&#20197;&#24110;&#21161;&#31649;&#29702;&#30561;&#30496;&#24182;&#36861;&#36394;&#30456;&#20851;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#30561;&#30496;&#34892;&#20026;&#21462;&#20915;&#20110;&#20010;&#20307;&#30340;&#29983;&#29702;&#29366;&#20917;&#65292;&#20294;&#20063;&#21463;&#21040;&#25968;&#23383;&#23186;&#20307;&#20351;&#29992;&#12289;&#31038;&#20132;&#32593;&#32476;&#20256;&#26579;&#20197;&#21450;&#21608;&#22260;&#22825;&#27668;&#31561;&#22806;&#37096;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPAND&#65288;Sleep Prediction Architecture using Network Dynamics&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22270;&#32593;&#32476;&#20013;&#30340;&#31038;&#20132;&#20256;&#26579;&#26469;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#30340;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#26222;&#36941;&#23384;&#22312;&#30340;&#31227;&#21160;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#25552;&#21462;&#30340;&#29983;&#29702;&#21644;&#25163;&#26426;&#25968;&#25454;&#38598;&#25104;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#21253;&#21547;&#19982;&#30561;&#30496;&#34892;&#20026;&#26080;&#20851;&#30340;&#36830;&#25509;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;&#23616;&#38480;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#31361;&#26174;&#20986;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SPAND (Sleep Prediction Architecture using Network Dynamics), a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights th
&lt;/p&gt;</description></item><item><title>VONet&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#35270;&#39057;&#23545;&#35937;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;U-Net&#27880;&#24847;&#21147;&#21644;&#36880;&#20010;&#23545;&#35937;&#30340;&#24207;&#21015;VAE&#23454;&#29616;&#65292;&#20855;&#26377;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#25512;&#29702;&#36807;&#31243;&#21644;&#25552;&#21319;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#29305;&#28857;&#65292;&#25104;&#20026;&#20116;&#20010;MOVI&#25968;&#25454;&#38598;&#19978;&#39046;&#20808;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11110</link><description>&lt;p&gt;
VONet: &#26080;&#30417;&#30563;&#35270;&#39057;&#23545;&#35937;&#23398;&#20064;&#36890;&#36807;&#24182;&#34892;U-Net&#27880;&#24847;&#21147;&#21644;&#36880;&#20010;&#23545;&#35937;&#30340;&#24207;&#21015;VAE
&lt;/p&gt;
&lt;p&gt;
VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE. (arXiv:2401.11110v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11110
&lt;/p&gt;
&lt;p&gt;
VONet&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#35270;&#39057;&#23545;&#35937;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;U-Net&#27880;&#24847;&#21147;&#21644;&#36880;&#20010;&#23545;&#35937;&#30340;&#24207;&#21015;VAE&#23454;&#29616;&#65292;&#20855;&#26377;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#25512;&#29702;&#36807;&#31243;&#21644;&#25552;&#21319;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#29305;&#28857;&#65292;&#25104;&#20026;&#20116;&#20010;MOVI&#25968;&#25454;&#38598;&#19978;&#39046;&#20808;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#35270;&#39057;&#23545;&#35937;&#23398;&#20064;&#26088;&#22312;&#23558;&#35270;&#39057;&#22330;&#26223;&#20998;&#35299;&#20026;&#32467;&#26500;&#21270;&#30340;&#23545;&#35937;&#34920;&#31034;&#65292;&#26080;&#38656;&#28145;&#24230;&#12289;&#20809;&#27969;&#25110;&#20998;&#21106;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VONet&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;MONet&#21551;&#21457;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;VONet&#21033;&#29992;U-Net&#26550;&#26500;&#65292;&#37319;&#29992;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#24182;&#34892;&#27880;&#24847;&#21147;&#25512;&#29702;&#36807;&#31243;&#65292;&#21516;&#26102;&#20026;&#25152;&#26377;&#27133;&#29983;&#25104;&#27880;&#24847;&#21147;&#25513;&#30721;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#27599;&#20010;&#25513;&#30721;&#22312;&#36830;&#32493;&#35270;&#39057;&#24103;&#20013;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;VONet&#24320;&#21457;&#20102;&#19968;&#31181;&#36880;&#20010;&#23545;&#35937;&#30340;&#24207;&#21015;VAE&#26694;&#26550;&#12290;&#36825;&#20123;&#21019;&#26032;&#30340;&#32534;&#30721;&#22120;&#31471;&#25216;&#26415;&#19982;&#34920;&#36798;&#24615;&#24378;&#30340;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#30340;&#25972;&#21512;&#65292;&#20351;VONet&#25104;&#20026;&#28085;&#30422;&#20102;&#21508;&#31181;&#22797;&#26434;&#24615;&#35270;&#39057;&#30340;&#20116;&#20010;MOVI&#25968;&#25454;&#38598;&#19978;&#39046;&#20808;&#30340;&#26080;&#30417;&#30563;&#23545;&#35937;&#23398;&#20064;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/hnyu/vonet&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#28431;&#27934;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22686;&#21152;&#28431;&#27934;&#25968;&#37327;&#24182;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#30340;&#20989;&#25968;&#65292;&#23613;&#31649;&#23384;&#22312;&#22122;&#22768;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#28431;&#27934;&#39044;&#27979;&#27169;&#22411;&#20173;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.11105</link><description>&lt;p&gt;
&#28508;&#22312;&#28431;&#27934;&#26159;&#36719;&#20214;&#28431;&#27934;&#39044;&#27979;&#20013;&#30340;&#38544;&#34255;&#23453;&#30707;&#21527;&#65311;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Latent Vulnerabilities Hidden Gems for Software Vulnerability Prediction? An Empirical Study. (arXiv:2401.11105v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#28431;&#27934;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22686;&#21152;&#28431;&#27934;&#25968;&#37327;&#24182;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#30340;&#20989;&#25968;&#65292;&#23613;&#31649;&#23384;&#22312;&#22122;&#22768;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#28431;&#27934;&#39044;&#27979;&#27169;&#22411;&#20173;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#30456;&#20851;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#23545;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#36719;&#20214;&#28431;&#27934;&#39044;&#27979;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#22823;&#37096;&#20998;&#36719;&#20214;&#28431;&#27934;&#25968;&#25454;&#38598;&#37117;&#20381;&#36182;&#20110;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#26469;&#25552;&#21462;&#28431;&#27934;&#20989;&#25968;&#21644;&#20195;&#30721;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#27809;&#26377;&#32771;&#34385;&#21040;&#25910;&#38598;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#30340;&#28508;&#22312;&#28431;&#27934;&#12290;&#20851;&#20110;&#36825;&#20123;&#28508;&#22312;&#28431;&#27934;&#22312;&#28431;&#27934;&#39044;&#27979;&#20013;&#30340;&#23454;&#29992;&#24615;&#20063;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#24120;&#29992;&#30340;&#36719;&#20214;&#28431;&#27934;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#28431;&#27934;&#20989;&#25968;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#24182;&#23545;&#20854;&#22312;&#20989;&#25968;&#32423;&#21035;&#21644;&#20195;&#30721;&#34892;&#32423;&#21035;&#19978;&#30340;&#28431;&#27934;&#39044;&#27979;&#36827;&#34892;&#20102;&#21033;&#29992;&#12290;&#36890;&#36807;&#26368;&#20808;&#36827;&#30340;SZZ&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20102;&#36229;&#36807;10&#19975;&#20010;&#28508;&#22312;&#28431;&#27934;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#28508;&#22312;&#20989;&#25968;&#24179;&#22343;&#33021;&#22686;&#21152;4&#20493;&#30340;&#28431;&#27934;&#25968;&#37327;&#65292;&#24182;&#32416;&#27491;&#22810;&#36798;5,000&#20010;&#38169;&#35823;&#26631;&#35760;&#30340;&#20989;&#25968;&#65292;&#20294;&#22122;&#22768;&#27700;&#24179;&#32422;&#20026;6%&#12290;&#23613;&#31649;&#26377;&#22122;&#22768;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#28431;&#27934;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#20174;&#36825;&#20123;&#28508;&#22312;&#28431;&#27934;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting relevant and high-quality data is integral to the development of effective Software Vulnerability (SV) prediction models. Most of the current SV datasets rely on SV-fixing commits to extract vulnerable functions and lines. However, none of these datasets have considered latent SVs existing between the introduction and fix of the collected SVs. There is also little known about the usefulness of these latent SVs for SV prediction. To bridge these gaps, we conduct a large-scale study on the latent vulnerable functions in two commonly used SV datasets and their utilization for function-level and line-level SV predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more than 100k latent vulnerable functions in the studied datasets. We find that these latent functions can increase the number of SVs by 4x on average and correct up to 5k mislabeled functions, yet they have a noise level of around 6%. Despite the noise, we show that the state-of-the-art SV prediction 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21152;&#26435;K&#26368;&#36817;&#37051;&#31639;&#27861;&#39640;&#25928;&#35745;&#31639;Data Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25968;&#25454;&#36136;&#37327;&#21028;&#21035;&#26041;&#38754;&#20248;&#20110;&#26410;&#21152;&#26435;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.11103</link><description>&lt;p&gt;
&#21152;&#26435;&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#39640;&#25928;&#25968;&#25454;Shapley&#20540;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Shapley for Weighted Nearest Neighbor Algorithms. (arXiv:2401.11103v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21152;&#26435;K&#26368;&#36817;&#37051;&#31639;&#27861;&#39640;&#25928;&#35745;&#31639;Data Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25968;&#25454;&#36136;&#37327;&#21028;&#21035;&#26041;&#38754;&#20248;&#20110;&#26410;&#21152;&#26435;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#35780;&#20272;&#25991;&#29486;&#20013;&#20851;&#20110;&#21152;&#26435;K&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#25968;&#25454;Shapley&#20540;&#65288;WKNN-Shapley&#65289;&#39640;&#25928;&#35745;&#31639;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30828;&#26631;&#31614;KNN&#30340;&#20934;&#30830;&#24230;&#19982;&#31163;&#25955;&#26435;&#37325;&#35270;&#20026;&#25928;&#29992;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;WKNN-Shapley&#20540;&#30340;&#35745;&#31639;&#36716;&#21270;&#20026;&#19968;&#20010;&#35745;&#25968;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#29616;&#26377;&#25991;&#29486;&#20013;O(N^K)&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30830;&#23450;&#24615;&#36817;&#20284;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;Shapley&#20540;&#30340;&#20851;&#38190;&#20844;&#24179;&#24615;&#36136;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;WKNN-Shapley&#20540;&#30340;&#35745;&#31639;&#25928;&#29575;&#20197;&#21450;&#19982;&#26410;&#21152;&#26435;&#29256;&#26412;&#30456;&#27604;&#22312;&#21028;&#26029;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to address an open problem in data valuation literature concerning the efficient computation of Data Shapley for weighted $K$ nearest neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label KNN with discretized weights as the utility function, we reframe the computation of WKNN-Shapley into a counting problem and introduce a quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the best result from existing literature. We develop a deterministic approximation algorithm that further improves computational efficiency while maintaining the key fairness properties of the Shapley value. Through extensive experiments, we demonstrate WKNN-Shapley's computational efficiency and its superior performance in discerning data quality compared to its unweighted counterpart.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#35774;&#35745;&#38382;&#39064;&#29305;&#23450;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#21644;&#28145;&#24230;&#31070;&#32463;&#39044;&#27979;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#38480;&#21046;&#37327;&#23376;&#27604;&#29305;&#30340;&#36817;&#26399;&#37327;&#23376;&#26426;&#22120;&#19978;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11098</link><description>&lt;p&gt;
&#25552;&#21319;&#37327;&#23376;&#26680;&#20989;&#25968;&#24615;&#33021;&#30340;&#31070;&#32463;&#33258;&#21160;&#35774;&#35745;&#32773;
&lt;/p&gt;
&lt;p&gt;
Neural auto-designer for enhanced quantum kernels. (arXiv:2401.11098v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#35774;&#35745;&#38382;&#39064;&#29305;&#23450;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#21644;&#28145;&#24230;&#31070;&#32463;&#39044;&#27979;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#38480;&#21046;&#37327;&#23376;&#27604;&#29305;&#30340;&#36817;&#26399;&#37327;&#23376;&#26426;&#22120;&#19978;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#20989;&#25968;&#22312;&#25552;&#20379;&#35745;&#31639;&#20248;&#21183;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#36825;&#20123;&#26680;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#19982;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#35774;&#35745;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#22312;&#32570;&#20047;&#20805;&#20998;&#20808;&#39564;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#35774;&#35745;&#26377;&#25928;&#30340;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#33258;&#21160;&#21270;&#22320;&#35774;&#35745;&#38382;&#39064;&#29305;&#23450;&#30340;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#26469;&#22788;&#29702;&#38480;&#21046;&#37327;&#23376;&#27604;&#29305;&#30340;&#36817;&#26399;&#37327;&#23376;&#26426;&#22120;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#31070;&#32463;&#39044;&#27979;&#22120;&#26469;&#39640;&#25928;&#35780;&#20272;&#21508;&#31181;&#20505;&#36873;&#37327;&#23376;&#26680;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#28040;&#38500;&#26680;&#23494;&#24230;&#38382;&#39064;&#21644;&#30830;&#23450;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#29305;&#24449;&#26144;&#23556;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum kernels hold great promise for offering computational advantages over classical learners, with the effectiveness of these kernels closely tied to the design of the quantum feature map. However, the challenge of designing effective quantum feature maps for real-world datasets, particularly in the absence of sufficient prior information, remains a significant obstacle. In this study, we present a data-driven approach that automates the design of problem-specific quantum feature maps. Our approach leverages feature-selection techniques to handle high-dimensional data on near-term quantum machines with limited qubits, and incorporates a deep neural predictor to efficiently evaluate the performance of various candidate quantum kernels. Through extensive numerical simulations on different datasets, we demonstrate the superiority of our proposal over prior methods, especially for the capability of eliminating the kernel concentration issue and identifying the feature map with predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#30340;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65306;&#21253;&#32423;&#21035;&#25439;&#22833;&#21644;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#65292;&#24182;&#21457;&#29616;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#21487;&#20197;&#34987;&#35270;&#20026;&#21253;&#32423;&#21035;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11081</link><description>&lt;p&gt;
&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#65306;&#23454;&#20363;&#32423;&#21035;&#19982;&#21253;&#32423;&#21035;&#30340;&#25439;&#22833;&#20989;&#25968;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions. (arXiv:2401.11081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#30340;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65306;&#21253;&#32423;&#21035;&#25439;&#22833;&#21644;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#65292;&#24182;&#21457;&#29616;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#21487;&#20197;&#34987;&#35270;&#20026;&#21253;&#32423;&#21035;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#30340;&#22686;&#21152;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#22312;&#19982;&#23398;&#20064;&#32773;&#20849;&#20139;&#20043;&#21069;&#20250;&#34987;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#25935;&#24863;&#21709;&#24212;&#30340;&#38544;&#31169;&#12290;&#22312;&#32858;&#21512;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25968;&#25454;&#38598;&#34987;&#20998;&#32452;&#25104;&#26679;&#26412;&#30340;&#21253;&#65292;&#27599;&#20010;&#21253;&#21482;&#25552;&#20379;&#19968;&#20010;&#32858;&#21512;&#21709;&#24212;&#65292;&#25552;&#20379;&#20102;&#35813;&#21253;&#20013;&#20010;&#20307;&#21709;&#24212;&#30340;&#25688;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#30340;&#20004;&#31181;&#33258;&#28982;&#25439;&#22833;&#20989;&#25968;&#65306;&#21253;&#32423;&#21035;&#25439;&#22833;&#21644;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#12290;&#22312;&#21069;&#32773;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32858;&#21512;&#21709;&#24212;&#19982;&#32858;&#21512;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#25439;&#22833;&#26469;&#23398;&#20064;&#65292;&#32780;&#22312;&#21518;&#32773;&#20013;&#65292;&#27169;&#22411;&#26088;&#22312;&#23558;&#20010;&#20307;&#39044;&#27979;&#19982;&#32858;&#21512;&#21709;&#24212;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#21487;&#20197;&#34987;&#35270;&#20026;&#21253;&#32423;&#21035;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;&#36825;&#20010;&#35266;&#23519;&#35753;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#20851;&#20110;&#25152;&#24471;&#20272;&#35745;&#20540;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDE-GNN&#30340;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24494;&#20998;&#26041;&#31243;&#28608;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#38454;&#25110;&#20108;&#38454;&#26102;&#22495;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#23398;&#20064;&#26102;&#22495;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26356;&#24191;&#27867;&#30340;&#26102;&#22495;&#21160;&#24577;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20960;&#20010;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.11074</link><description>&lt;p&gt;
&#20851;&#20110;&#24494;&#20998;&#26041;&#31243;&#28608;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#22495;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Temporal Domain of Differential Equation Inspired Graph Neural Networks. (arXiv:2401.11074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDE-GNN&#30340;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24494;&#20998;&#26041;&#31243;&#28608;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#38454;&#25110;&#20108;&#38454;&#26102;&#22495;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#23398;&#20064;&#26102;&#22495;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26356;&#24191;&#27867;&#30340;&#26102;&#22495;&#21160;&#24577;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20960;&#20010;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#24314;&#27169;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#19968;&#20010;&#21019;&#26032;&#26159;&#24494;&#20998;&#26041;&#31243;&#28608;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DE-GNNs&#65289;&#65292;&#23427;&#21033;&#29992;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#21407;&#29702;&#26469;&#27169;&#25311;&#20855;&#26377;&#20869;&#32622;&#23646;&#24615;&#65288;&#22914;&#29305;&#24449;&#24179;&#28369;&#25110;&#20445;&#30041;&#65289;&#30340;&#22270;&#19978;&#30340;&#20449;&#24687;&#27969;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DE-GNNs&#20381;&#36182;&#20110;&#19968;&#38454;&#25110;&#20108;&#38454;&#26102;&#22495;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#25193;&#23637;&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#23450;&#20041;&#30340;&#26102;&#22495;&#20381;&#36182;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;TDE-GNN&#33021;&#22815;&#25429;&#25417;&#21040;&#36229;&#36807;&#19968;&#38454;&#25110;&#20108;&#38454;&#26041;&#27861;&#30340;&#21508;&#31181;&#26102;&#22495;&#21160;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#26102;&#22495;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20960;&#20010;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#23398;&#20064;&#26102;&#22495;&#20381;&#36182;&#24615;&#30340;&#30410;&#22788;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#20351;&#29992;&#39044;&#23450;&#20041;&#26102;&#22495;&#21160;&#24577;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling complex relationships in graph-structured data. A recent innovation in this field is the family of Differential Equation-Inspired Graph Neural Networks (DE-GNNs), which leverage principles from continuous dynamical systems to model information flow on graphs with built-in properties such as feature smoothing or preservation. However, existing DE-GNNs rely on first or second-order temporal dependencies. In this paper, we propose a neural extension to those pre-defined temporal dependencies. We show that our model, called TDE-GNN, can capture a wide range of temporal dynamics that go beyond typical first or second-order methods, and provide use cases where existing temporal models are challenged. We demonstrate the benefit of learning the temporal dependencies using our method rather than using pre-defined temporal dynamics on several graph benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11044</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#22312;&#20851;&#38190;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making. (arXiv:2401.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#22312;&#20998;&#31867;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#35299;&#37322;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#34892;&#20026;&#31185;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#65292;&#20854;&#20013;&#36131;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;ROC&#26354;&#32447;&#30340;&#26041;&#27861;&#24320;&#21457;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#20197;&#22686;&#24378;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#25191;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21015;&#38388;&#25968;&#25454;&#36716;&#25442;&#65292;&#21363;&#25277;&#35937;&#65292;&#36825;&#23545;SaNDA&#30340;&#20998;&#31867;&#36807;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#24182;&#25506;&#35752;&#20102;&#26367;&#20195;&#30340;&#25277;&#35937;&#21327;&#35758;&#65292;&#22914;&#24120;&#37327;&#20998;&#31665;&#21644;&#20998;&#20301;&#25968;&#12290;&#23558;&#26368;&#20339;&#30340;&#26041;&#27861;&#19982;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#22522;&#20934;&#27169;&#22411;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#25968;&#25454;&#19981;&#23436;&#25972;&#65292;SaNDA&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#24182;&#19988;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The applicability of widely adopted machine learning (ML) methods to classification is circumscribed by the imperatives of explicability and uncertainty, particularly evident in domains such as healthcare, behavioural sciences, and finances, wherein accountability assumes priority. Recently, Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the ability to perform classification in such domains, by developing a data abstraction protocol using a ROC curve-based method. This paper focuses on column-wise data transformations called abstractions, which are crucial for SaNDA's classification process and explores alternative abstractions protocols, such as constant binning and quantiles. The best-performing methods have been compared against Random Forest as a baseline for explainable methods. The results suggests that SaNDA can be a viable substitute for Random Forest when data is incomplete, even with minimal missing values. It consistently maintains high accuracy e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65288;EGNO&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#36712;&#36857;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#27493;&#39044;&#27979;&#65292;&#20197;&#20934;&#30830;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#21033;&#29992;&#31561;&#21464;&#26102;&#38388;&#21367;&#31215;&#26469;&#20445;&#25345;&#20854;&#20869;&#22312;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11037</link><description>&lt;p&gt;
&#24314;&#27169;&#19977;&#32500;&#21160;&#21147;&#23398;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;
&lt;/p&gt;
&lt;p&gt;
Equivariant Graph Neural Operator for Modeling 3D Dynamics. (arXiv:2401.11037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65288;EGNO&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#36712;&#36857;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#27493;&#39044;&#27979;&#65292;&#20197;&#20934;&#30830;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#21033;&#29992;&#31561;&#21464;&#26102;&#38388;&#21367;&#31215;&#26469;&#20445;&#25345;&#20854;&#20869;&#22312;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#65292;&#24314;&#27169;&#22797;&#26434;&#30340;&#19977;&#32500;&#20851;&#31995;&#31995;&#32479;&#21160;&#21147;&#23398;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#28041;&#21450;&#39046;&#22495;&#20174;&#20998;&#23376;&#27169;&#25311;&#21040;&#31890;&#23376;&#21147;&#23398;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#24050;&#21462;&#24471;&#33391;&#22909;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#36827;&#34892;&#19979;&#19968;&#27493;&#39044;&#27979;&#65292;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21363;&#31561;&#21464;&#22270;&#31070;&#32463;&#25805;&#20316;&#22120;&#65288;EGNO&#65289;&#65292;&#30452;&#25509;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#36712;&#36857;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#27493;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;EGNO&#26174;&#24335;&#22320;&#23398;&#20064;&#19977;&#32500;&#21160;&#21147;&#23398;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36890;&#36807;&#26102;&#38388;&#20989;&#25968;&#26469;&#24314;&#27169;&#21160;&#21147;&#23398;&#24182;&#23398;&#20064;&#31070;&#32463;&#25805;&#20316;&#22120;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#25429;&#25417;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#22312;&#30340;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#22312;&#20613;&#37324;&#21494;&#31354;&#38388;&#20013;&#21442;&#25968;&#21270;&#31561;&#21464;&#26102;&#38388;&#21367;&#31215;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21367;&#31215;&#23618;&#26500;&#24314;EGNO&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#27773;&#36710;CAN&#20013;&#20351;&#29992;&#39640;&#24230;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20837;&#20405;&#26816;&#27979;&#30340;&#26696;&#20363;&#65292;&#20197;&#24212;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#36830;&#25509;&#24615;&#21487;&#33021;&#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37327;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;CQMLP&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#22810;&#20010;&#25915;&#20987;&#21521;&#37327;&#65292;&#38477;&#20302;&#20102;&#36164;&#28304;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2401.11030</link><description>&lt;p&gt;
&#22312;&#27773;&#36710;CAN&#20013;&#25506;&#32034;&#39640;&#24230;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Highly Quantised Neural Networks for Intrusion Detection in Automotive CAN. (arXiv:2401.11030v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#27773;&#36710;CAN&#20013;&#20351;&#29992;&#39640;&#24230;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20837;&#20405;&#26816;&#27979;&#30340;&#26696;&#20363;&#65292;&#20197;&#24212;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#36830;&#25509;&#24615;&#21487;&#33021;&#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37327;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;CQMLP&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#22810;&#20010;&#25915;&#20987;&#21521;&#37327;&#65292;&#38477;&#20302;&#20102;&#36164;&#28304;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#36710;&#36742;&#21253;&#25324;&#26234;&#33021;&#31995;&#32479;&#65292;&#22914;&#36830;&#25509;&#30340;&#33258;&#20027;&#39550;&#39542;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#65292;&#20197;&#22686;&#24378;&#39550;&#39542;&#20307;&#39564;&#65292;&#36825;&#26159;&#36890;&#36807;&#22686;&#21152;&#19982;&#22522;&#30784;&#35774;&#26045;&#30340;&#36830;&#25509;&#24615;&#21644;&#26469;&#33258;&#19981;&#21516;&#24863;&#30693;&#27169;&#24335;&#30340;&#20449;&#24687;&#34701;&#21512;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#22686;&#38271;&#30340;&#36830;&#25509;&#24615;&#19982;&#36710;&#36742;&#20869;&#30340;&#20256;&#32479;&#32593;&#32476;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#21457;&#21160;&#23545;&#20851;&#38190;&#36710;&#36742;&#31995;&#32479;&#30340;&#20027;&#21160;&#21644;&#34987;&#21160;&#25915;&#20987;&#65292;&#30452;&#25509;&#24433;&#21709;&#20056;&#23458;&#30340;&#23433;&#20840;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#24050;&#22312;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#25104;&#21151;&#26816;&#27979;&#21040;&#22810;&#20010;&#30446;&#26631;&#25915;&#20987;&#21521;&#37327;&#65292;&#20854;&#37096;&#32626;&#36890;&#36807;&#38024;&#23545;&#20302;&#21151;&#32791;&#24179;&#21488;&#30340;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21487;&#33021;&#30340;&#12290;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#27169;&#22411;&#21516;&#26102;&#26816;&#27979;&#22810;&#20010;&#25915;&#20987;&#21521;&#37327;&#65292;&#22686;&#21152;&#20102;&#38754;&#31215;&#65288;&#36164;&#28304;&#65289;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#23450;&#20041;&#37327;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;CQMLP&#65289;&#20316;&#20026;&#22810;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicles today comprise intelligent systems like connected autonomous driving and advanced driving assistance systems (ADAS) to enhance the driving experience, which is enabled through increased connectivity to infrastructure and fusion of information from different sensing modes. However, the rising connectivity coupled with the legacy network architecture within vehicles can be exploited for launching active and passive attacks on critical vehicle systems and directly affecting the safety of passengers. Machine learning-based intrusion detection models have been shown to successfully detect multiple targeted attack vectors in recent literature, whose deployments are enabled through quantised neural networks targeting low-power platforms. Multiple models are often required to simultaneously detect multiple attack vectors, increasing the area, (resource) cost, and energy consumption. In this paper, we present a case for utilising custom-quantised MLP's (CQMLP) as a multi-class classifi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#36890;&#36807;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65292;&#28040;&#38500;&#29305;&#23450;&#23458;&#25143;&#31471;&#25110;&#25968;&#25454;&#28857;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#30830;&#20999;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;TV&#31283;&#23450;FL&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30830;&#20999;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.11018</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#12289;&#21487;&#35777;&#26126;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication Efficient and Provable Federated Unlearning. (arXiv:2401.11018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11018
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#36890;&#36807;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65292;&#28040;&#38500;&#29305;&#23450;&#23458;&#25143;&#31471;&#25110;&#25968;&#25454;&#28857;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#30830;&#20999;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;TV&#31283;&#23450;FL&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30830;&#20999;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#28040;&#38500;&#29305;&#23450;&#23458;&#25143;&#31471;&#25110;&#25968;&#25454;&#28857;&#23545;&#20840;&#23616;&#27169;&#22411;&#24433;&#21709;&#30340;&#26032;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#28304;&#20110;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#21644;FL&#20013;&#30340;&#38544;&#31169;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#20999;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65292;&#28385;&#36275;&#20004;&#20010;&#22522;&#26412;&#26631;&#20934;&#65306;&#36890;&#20449;&#25928;&#29575;&#21644;&#30830;&#20999;&#21462;&#28040;&#23398;&#20064;&#30340;&#21487;&#35777;&#26126;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19968;&#33268;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;"&#30830;&#20999;"&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#30830;&#20445;&#21462;&#28040;&#23398;&#20064;&#21518;&#30340;&#27169;&#22411;&#22312;&#32479;&#35745;&#19978;&#19981;&#21487;&#21306;&#20998;&#20110;&#22312;&#27809;&#26377;&#34987;&#21024;&#38500;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#24555;&#36895;&#30830;&#20999;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#30340;&#20851;&#38190;&#23646;&#24615;&#65306;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#31283;&#23450;&#24615;&#65292;&#35813;&#23646;&#24615;&#34913;&#37327;&#20102;&#27169;&#22411;&#21442;&#25968;&#23545;&#25968;&#25454;&#38598;&#32454;&#24494;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FATS&#30340;TV&#31283;&#23450;FL&#31639;&#27861;&#65292;&#23545;&#21407;&#26377;&#30340;FL&#31639;&#27861;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study federated unlearning, a novel problem to eliminate the impact of specific clients or data points on the global model learned via federated learning (FL). This problem is driven by the right to be forgotten and the privacy challenges in FL. We introduce a new framework for exact federated unlearning that meets two essential criteria: \textit{communication efficiency} and \textit{exact unlearning provability}. To our knowledge, this is the first work to tackle both aspects coherently. We start by giving a rigorous definition of \textit{exact} federated unlearning, which guarantees that the unlearned model is statistically indistinguishable from the one trained without the deleted data. We then pinpoint the key property that enables fast exact federated unlearning: total variation (TV) stability, which measures the sensitivity of the model parameters to slight changes in the dataset. Leveraging this insight, we develop a TV-stable FL algorithm called \texttt{FATS}, which modifies
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#25581;&#31034;&#35828;&#35805;&#32773;&#23884;&#20837;&#20013;&#30340;&#24773;&#24863;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#12290;&#30740;&#31350;&#21457;&#29616;&#24773;&#24863;&#20449;&#24687;&#21487;&#20197;&#30452;&#25509;&#20174;&#35828;&#35805;&#32773;&#23884;&#20837;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24773;&#24863;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23545;&#25239;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11017</link><description>&lt;p&gt;
&#25581;&#31034;&#35828;&#35805;&#32773;&#23884;&#20837;&#20013;&#30340;&#24773;&#24863;&#32858;&#31867;&#65306;&#19968;&#31181;&#23545;&#35805;&#32773;&#34920;&#36798;&#24335;&#35782;&#21035;&#30340;&#23545;&#25239;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Revealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition. (arXiv:2401.11017v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#25581;&#31034;&#35828;&#35805;&#32773;&#23884;&#20837;&#20013;&#30340;&#24773;&#24863;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#12290;&#30740;&#31350;&#21457;&#29616;&#24773;&#24863;&#20449;&#24687;&#21487;&#20197;&#30452;&#25509;&#20174;&#35828;&#35805;&#32773;&#23884;&#20837;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24773;&#24863;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23545;&#25239;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#32773;&#23884;&#20837;&#25658;&#24102;&#26377;&#26377;&#20215;&#20540;&#30340;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#20351;&#20854;&#25104;&#20026;&#22686;&#24378;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#26377;&#21069;&#26223;&#30340;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#35748;&#20026;&#24773;&#24863;&#20449;&#24687;&#26159;&#38388;&#25509;&#23884;&#20837;&#22312;&#35828;&#35805;&#32773;&#23884;&#20837;&#20013;&#30340;&#65292;&#23548;&#33268;&#23427;&#20204;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#24773;&#24863;&#19982;&#26368;&#20808;&#36827;&#30340;&#35828;&#35805;&#32773;&#23884;&#20837;&#20043;&#38388;&#30340;&#30452;&#25509;&#32780;&#26377;&#29992;&#30340;&#32852;&#31995;&#65292;&#21363;&#20869;&#37096;&#35828;&#35805;&#32773;&#32858;&#31867;&#30340;&#24418;&#24335;&#12290;&#36890;&#36807;&#36827;&#34892;&#35814;&#23613;&#30340;&#32858;&#31867;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#24773;&#24863;&#20449;&#24687;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#20174;&#35828;&#35805;&#32773;&#23884;&#20837;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24773;&#24863;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#26681;&#25454;&#35828;&#35805;&#32773;&#23884;&#20837;&#30340;&#20869;&#37096;&#32858;&#31867;&#26469;&#37319;&#26679;&#27491;&#20363;&#21644;&#36127;&#20363;&#12290;&#35813;&#31574;&#30053;&#21033;&#29992;&#22823;&#37327;&#30340;&#24773;&#24863;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#26126;&#26174;&#30340;&#25928;&#26524;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker embeddings carry valuable emotion-related information, which makes them a promising resource for enhancing speech emotion recognition (SER), especially with limited labeled data. Traditionally, it has been assumed that emotion information is indirectly embedded within speaker embeddings, leading to their under-utilization. Our study reveals a direct and useful link between emotion and state-of-the-art speaker embeddings in the form of intra-speaker clusters. By conducting a thorough clustering analysis, we demonstrate that emotion information can be readily extracted from speaker embeddings. In order to leverage this information, we introduce a novel contrastive pretraining approach applied to emotion-unlabeled data for speech emotion recognition. The proposed approach involves the sampling of positive and the negative examples based on the intra-speaker clusters of speaker embeddings. The proposed strategy, which leverages extensive emotion-unlabeled data, leads to a significa
&lt;/p&gt;</description></item><item><title>&#22312;&#32771;&#34385;-&#28982;&#21518;-&#36873;&#25321;&#30340;&#25490;&#21517;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30830;&#23450;&#32771;&#34385;&#27010;&#29575;&#30340;&#30028;&#38480;&#12290;&#23613;&#31649;&#19981;&#33021;&#20934;&#30830;&#30830;&#23450;&#32771;&#34385;&#27010;&#29575;&#65292;&#20294;&#22312;&#24050;&#30693;&#22791;&#36873;&#26041;&#26696;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#22791;&#36873;&#27010;&#29575;&#30340;&#30456;&#23545;&#22823;&#23567;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.11016</link><description>&lt;p&gt;
&#22312;&#32771;&#34385;-&#28982;&#21518;-&#36873;&#25321;&#25490;&#21517;&#27169;&#22411;&#20013;&#30830;&#23450;&#32771;&#34385;&#27010;&#29575;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models. (arXiv:2401.11016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11016
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;-&#28982;&#21518;-&#36873;&#25321;&#30340;&#25490;&#21517;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30830;&#23450;&#32771;&#34385;&#27010;&#29575;&#30340;&#30028;&#38480;&#12290;&#23613;&#31649;&#19981;&#33021;&#20934;&#30830;&#30830;&#23450;&#32771;&#34385;&#27010;&#29575;&#65292;&#20294;&#22312;&#24050;&#30693;&#22791;&#36873;&#26041;&#26696;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#22791;&#36873;&#27010;&#29575;&#30340;&#30456;&#23545;&#22823;&#23567;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#29702;&#35770;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#35266;&#28857;&#35748;&#20026;&#65292;&#20010;&#20307;&#22312;&#20570;&#20986;&#36873;&#25321;&#20043;&#21069;&#65292;&#20250;&#20808;&#36827;&#34892;&#20004;&#27493;&#30340;&#36807;&#31243;&#65292;&#39318;&#20808;&#36873;&#25321;&#19968;&#20123;&#22791;&#36873;&#26041;&#26696;&#36827;&#34892;&#32771;&#34385;&#65292;&#28982;&#21518;&#20174;&#25152;&#24471;&#30340;&#32771;&#34385;&#38598;&#21512;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#8220;&#32771;&#34385;&#28982;&#21518;&#36873;&#25321;&#8221;&#30340;&#24773;&#26223;&#19979;&#25512;&#26029;&#26410;&#35266;&#23519;&#21040;&#30340;&#32771;&#34385;&#38598;&#21512;&#65288;&#25110;&#32773;&#22791;&#36873;&#26041;&#26696;&#30340;&#32771;&#34385;&#27010;&#29575;&#65289;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#23545;&#20110;&#20855;&#26377;&#24378;&#29420;&#31435;&#24615;&#20551;&#35774;&#30340;&#31616;&#21333;&#32771;&#34385;&#27169;&#22411;&#65292;&#22312;&#24050;&#30693;&#22791;&#36873;&#26041;&#26696;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#20063;&#26080;&#27861;&#30830;&#23450;&#36523;&#20221;&#12290;&#25105;&#20204;&#32771;&#34385;&#23558;&#32771;&#34385;-&#28982;&#21518;-&#36873;&#25321;&#27169;&#22411;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;top-k&#25490;&#21517;&#30340;&#24773;&#26223;&#65292;&#20551;&#35774;&#25490;&#21517;&#26159;&#26681;&#25454;Plackett-Luce&#27169;&#22411;&#22312;&#37319;&#26679;&#20102;&#32771;&#34385;&#38598;&#21512;&#21518;&#26500;&#24314;&#30340;&#12290;&#23613;&#31649;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#22791;&#36873;&#26041;&#26696;&#30340;&#32771;&#34385;&#27010;&#29575;&#20173;&#26087;&#19981;&#33021;&#30830;&#23450;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#33719;&#24471;&#22791;&#36873;&#26041;&#26696;&#25928;&#29992;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#22791;&#36873;&#27010;&#29575;&#30456;&#23545;&#22823;&#23567;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#26399;&#26395;&#32771;&#34385;&#38598;&#21512;&#22823;&#23567;&#30340;&#26465;&#20214;&#36827;&#34892;&#25512;&#23548;&#65292;&#25105;&#20204;&#24471;&#21040;&#32477;&#23545;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common theory of choice posits that individuals make choices in a two-step process, first selecting some subset of the alternatives to consider before making a selection from the resulting consideration set. However, inferring unobserved consideration sets (or item consideration probabilities) in this "consider then choose" setting poses significant challenges, because even simple models of consideration with strong independence assumptions are not identifiable, even if item utilities are known. We consider a natural extension of consider-then-choose models to a top-$k$ ranking setting, where we assume rankings are constructed according to a Plackett-Luce model after sampling a consideration set. While item consideration probabilities remain non-identified in this setting, we prove that knowledge of item utilities allows us to infer bounds on the relative sizes of consideration probabilities. Additionally, given a condition on the expected consideration set size, we derive absolute u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#21487;&#20197;&#22312;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.10989</link><description>&lt;p&gt;
&#20855;&#26377;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#30340;&#21487;&#35777;&#20280;&#32553;&#24615;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#21487;&#20197;&#22312;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#20855;&#26377;&#28385;&#31209;&#21327;&#26041;&#24046;&#36924;&#36817;&#30340;&#21464;&#20998;&#26063;&#22312;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#26080;&#35770;&#26159;&#20174;&#23454;&#35777;&#19978;&#36824;&#26159;&#29702;&#35770;&#19978;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#30456;&#27604;&#65292;&#28385;&#31209;&#21464;&#20998;&#26063;&#22312;&#38382;&#39064;&#30340;&#32500;&#24230;&#19978;&#25193;&#23637;&#24471;&#24456;&#24046;&#12290;&#36825;&#23545;&#20855;&#26377;&#26412;&#22320;&#21464;&#37327;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#23588;&#20026;&#20851;&#38190;&#65292;&#23427;&#20204;&#30340;&#32500;&#24230;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#32780;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#22797;&#26434;&#24615;&#23545;&#25968;&#25454;&#38598;&#22823;&#23567;N&#23384;&#22312;&#26126;&#30830;&#30340;O(N^2)&#20381;&#36182;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#26576;&#20123;&#23610;&#24230;&#30697;&#38453;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24615;O(N)&#65292;&#20174;&#32780;&#19982;N&#30340;&#32553;&#25918;&#26356;&#22909;&#22320;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on l
&lt;/p&gt;</description></item><item><title>T2MAC&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#21644;&#21487;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#36873;&#25321;&#24615;&#21442;&#19982;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#25972;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10973</link><description>&lt;p&gt;
T2MAC: &#36890;&#36807;&#36873;&#25321;&#24615;&#21442;&#19982;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#25972;&#21512;&#23454;&#29616;&#30446;&#26631;&#21644;&#21487;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
T2MAC: Targeted and Trusted Multi-Agent Communication through Selective Engagement and Evidence-Driven Integration. (arXiv:2401.10973v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10973
&lt;/p&gt;
&lt;p&gt;
T2MAC&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#21644;&#21487;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#36873;&#25321;&#24615;&#21442;&#19982;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#25972;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#21327;&#35843;&#22810;&#20010;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#19968;&#31181;&#26377;&#25928;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24191;&#25773;&#36890;&#20449;&#19978;&#65292;&#36825;&#19981;&#20165;&#32570;&#20047;&#23454;&#29992;&#24615;&#65292;&#32780;&#19988;&#36824;&#23548;&#33268;&#20449;&#24687;&#20887;&#20313;&#12290;&#36825;&#31181;&#19968;&#20992;&#20999;&#30340;&#20449;&#24687;&#21487;&#33021;&#23545;&#36890;&#20449;&#25928;&#29575;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24120;&#24120;&#37319;&#29992;&#22522;&#30784;&#26426;&#21046;&#26469;&#25972;&#21512;&#35266;&#23519;&#21040;&#30340;&#21644;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;T2MAC&#30340;&#30446;&#26631;&#21644;&#21487;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#36873;&#25321;&#24615;&#21442;&#19982;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#25972;&#21512;&#12290;&#36890;&#36807;T2MAC&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#21046;&#23450;&#20010;&#24615;&#21270;&#28040;&#24687;&#65292;&#30830;&#23450;&#29702;&#24819;&#30340;&#36890;&#20449;&#31383;&#21475;&#65292;&#24182;&#19982;&#21487;&#38752;&#30340;&#20249;&#20276;&#36827;&#34892;&#20114;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#22312;&#25509;&#25910;&#28040;&#24687;&#21518;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#20174;&#19981;&#21516;&#26469;&#28304;&#35266;&#23519;&#21644;&#25509;&#25910;&#30340;&#20449;&#24687;&#36827;&#34892;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication stands as a potent mechanism to harmonize the behaviors of multiple agents. However, existing works primarily concentrate on broadcast communication, which not only lacks practicality, but also leads to information redundancy. This surplus, one-fits-all information could adversely impact the communication efficiency. Furthermore, existing works often resort to basic mechanisms to integrate observed and received information, impairing the learning process. To tackle these difficulties, we propose Targeted and Trusted Multi-Agent Communication (T2MAC), a straightforward yet effective method that enables agents to learn selective engagement and evidence-driven integration. With T2MAC, agents have the capability to craft individualized messages, pinpoint ideal communication windows, and engage with reliable partners, thereby refining communication efficiency. Following the reception of messages, the agents integrate information observed and received from different sources at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#31867;&#20998;&#23376;&#33021;&#37327;&#26223;&#35266;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#35299;&#37322;&#21160;&#24577;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#21270;&#23398;&#31354;&#38388;&#25506;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.10972</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#23545;&#20998;&#23376;&#33021;&#37327;&#26223;&#35266;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering Molecular Energy Landscapes by Adaptive Network Embedding. (arXiv:2401.10972v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#31867;&#20998;&#23376;&#33021;&#37327;&#26223;&#35266;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#35299;&#37322;&#21160;&#24577;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#21270;&#23398;&#31354;&#38388;&#25506;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#25506;&#32034;&#25152;&#26377;&#21487;&#33021;&#30340;&#23567;&#20998;&#23376;&#30340;&#21270;&#23398;&#31354;&#38388;&#65292;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#21387;&#32553;&#31995;&#32479;&#30340;&#32500;&#24230;&#26469;&#20419;&#36827;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#32593;&#32476;&#23884;&#20837;&#25216;&#26415;&#65292;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#28508;&#22312;&#33021;&#37327;&#26223;&#35266;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#24471;&#21040;&#36890;&#36807;&#23884;&#20837;&#20989;&#25968;&#23450;&#20041;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#20026;&#20102;&#25193;&#23637;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#29109;&#25935;&#24863;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#22522;&#20110;&#20803;&#21160;&#21147;&#23398;&#21644;&#36716;&#21464;&#36335;&#24452;&#29702;&#35770;&#65292;&#23545;&#33021;&#37327;&#26223;&#35266;&#36827;&#34892;&#23618;&#27425;&#37319;&#26679;&#12290;&#36890;&#36807;&#32771;&#34385;&#31995;&#32479;&#33021;&#37327;&#26223;&#35266;&#25152;&#38544;&#21547;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#35299;&#37322;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;Lennard-Jones (LJ)&#31751;&#21644;&#20154;&#31867;DNA&#24207;&#21015;&#26469;&#28436;&#31034;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to efficiently explore the chemical space of all possible small molecules, a common approach is to compress the dimension of the system to facilitate downstream machine learning tasks. Towards this end, we present a data driven approach for clustering potential energy landscapes of molecular structures by applying recently developed Network Embedding techniques, to obtain latent variables defined through the embedding function. To scale up the method, we also incorporate an entropy sensitive adaptive scheme for hierarchical sampling of the energy landscape, based on Metadynamics and Transition Path Theory. By taking into account the kinetic information implied by a system's energy landscape, we are able to interpret dynamical node-node relationships in reduced dimensions. We demonstrate the framework through Lennard-Jones (LJ) clusters and a human DNA sequence.
&lt;/p&gt;</description></item><item><title>HOSC&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#38160;&#21033;&#29305;&#24449;&#21644;&#20302;&#39057;&#36807;&#28193;&#12290;&#23427;&#21487;&#20197;&#22312;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#25552;&#20379;&#25913;&#21892;&#36136;&#37327;&#30340;&#25554;&#20837;&#24335;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10967</link><description>&lt;p&gt;
HOSC:&#19968;&#31181;&#29992;&#20110;&#20445;&#30041;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#20013;&#38160;&#21033;&#29305;&#24449;&#30340;&#21608;&#26399;&#24615;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
HOSC: A Periodic Activation Function for Preserving Sharp Features in Implicit Neural Representations. (arXiv:2401.10967v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10967
&lt;/p&gt;
&lt;p&gt;
HOSC&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#38160;&#21033;&#29305;&#24449;&#21644;&#20302;&#39057;&#36807;&#28193;&#12290;&#23427;&#21487;&#20197;&#22312;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#25552;&#20379;&#25913;&#21892;&#36136;&#37327;&#30340;&#25554;&#20837;&#24335;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#29992;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#38544;&#24335;&#34920;&#31034;&#20449;&#21495;&#65288;&#20363;&#22914;&#22270;&#20687;&#65292;&#22330;&#26223;&#25110;&#20960;&#20309;&#20307;&#65289;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#21033;&#29992;&#36873;&#25321;&#28608;&#27963;&#20989;&#25968;&#65292;&#25110;&#32773;&#20165;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Hyperbolic Oscillation function (HOSC) &#30340;&#26032;&#22411;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#20855;&#26377;&#21487;&#25511;&#30340;&#38160;&#21033;&#24230;&#21442;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#28608;&#27963;&#20989;&#25968;&#19981;&#21516;&#65292;HOSC&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#26356;&#22909;&#22320;&#25429;&#25417;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#31361;&#21464;&#65292;&#20174;&#32780;&#25429;&#25417;&#24213;&#23618;&#25968;&#25454;&#30340;&#38160;&#21033;&#25110;&#24613;&#21095;&#29305;&#24449;&#65292;&#20197;&#21450;&#24179;&#28369;&#30340;&#20302;&#39057;&#36807;&#28193;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#27169;&#22359;&#21270;&#24615;&#65292;HOSC&#25552;&#20379;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#21151;&#33021;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#20854;&#32435;&#20837;&#20219;&#20309;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#38544;&#24335;&#34920;&#31034;&#20449;&#21495;&#30340;&#26041;&#27861;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36890;&#29992;&#20219;&#21153;&#20013;&#23558;HOSC&#19982;&#20854;&#20182;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23454;&#35777;&#26174;&#31034;&#20102;&#25152;&#24471;&#21040;&#34920;&#31034;&#30340;&#36136;&#37327;&#30340;&#25913;&#21892;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#23398;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed methods for implicitly representing signals such as images, scenes, or geometries using coordinate-based neural network architectures often do not leverage the choice of activation functions, or do so only to a limited extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC), a novel activation function with a controllable sharpness parameter. Unlike any previous activations, HOSC has been specifically designed to better capture sudden changes in the input signal, and hence sharp or acute features of the underlying data, as well as smooth low-frequency transitions. Due to its simplicity and modularity, HOSC offers a plug-and-play functionality that can be easily incorporated into any existing method employing a neural network as a way of implicitly representing a signal. We benchmark HOSC against other popular activations in an array of general tasks, empirically showing an improvement in the quality of obtained representations, provide the mathe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;OLOR&#65292;&#36890;&#36807;&#32467;&#21512;&#20248;&#21270;&#22120;&#21644;&#26435;&#37325;&#22238;&#28378;&#39033;&#65292;&#35299;&#20915;&#20102;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10962</link><description>&lt;p&gt;
&#19968;&#27493;&#23398;&#20064;&#65292;&#19968;&#27493;&#35780;&#23457;
&lt;/p&gt;
&lt;p&gt;
One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;OLOR&#65292;&#36890;&#36807;&#32467;&#21512;&#20248;&#21270;&#22120;&#21644;&#26435;&#37325;&#22238;&#28378;&#39033;&#65292;&#35299;&#20915;&#20102;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#35270;&#35273;&#24494;&#35843;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#27861;&#8212;&#8212;&#23436;&#20840;&#24494;&#35843;&#65292;&#23384;&#22312;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#21482;&#19987;&#27880;&#20110;&#25311;&#21512;&#19979;&#28216;&#35757;&#32451;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;OLOR&#65288;&#19968;&#27493;&#23398;&#20064;&#65292;&#19968;&#27493;&#35780;&#23457;&#65289;&#12290;OLOR&#23558;&#24494;&#35843;&#19982;&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#23558;&#26435;&#37325;&#22238;&#28378;&#39033;&#21152;&#20837;&#21040;&#27599;&#20010;&#27493;&#39588;&#30340;&#26435;&#37325;&#26356;&#26032;&#39033;&#20013;&#12290;&#36825;&#30830;&#20445;&#20102;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#30340;&#26435;&#37325;&#33539;&#22260;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#24809;&#32602;&#26041;&#27861;&#65292;&#36890;&#36807; penalty decay &#21644;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#26469;&#35843;&#25972;&#23618;&#30340;&#26435;&#37325;&#22238;&#28378;&#31243;&#24230;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual fine-tuning has garnered significant attention with the rise of pre-trained vision models. The current prevailing method, full fine-tuning, suffers from the issue of knowledge forgetting as it focuses solely on fitting the downstream training set. In this paper, we propose a novel weight rollback-based fine-tuning method called OLOR (One step Learning, One step Review). OLOR combines fine-tuning with optimizers, incorporating a weight rollback term into the weight update term at each step. This ensures consistency in the weight range of upstream and downstream models, effectively mitigating knowledge forgetting and enhancing fine-tuning performance. In addition, a layer-wise penalty is presented to employ penalty decay and the diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks. Through extensive experiments on various tasks such as image classification, object detection, semantic segmentation, and instance segmentation, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#22788;&#29702;&#20998;&#24067;&#21644;&#36816;&#36755;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#21327;&#35843;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25919;&#31574;&#23545;&#40784;&#12289;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#12289;&#24212;&#23545;&#38750;&#24179;&#31283;&#24615;&#12289;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#20116;&#20010;&#26041;&#38754;&#24212;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#65292;&#20026;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#23545;&#40784;&#26234;&#33021;&#20307;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10949</link><description>&lt;p&gt;
&#12298;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning. (arXiv:2401.10949v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#22788;&#29702;&#20998;&#24067;&#21644;&#36816;&#36755;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#21327;&#35843;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25919;&#31574;&#23545;&#40784;&#12289;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#12289;&#24212;&#23545;&#38750;&#24179;&#31283;&#24615;&#12289;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#20116;&#20010;&#26041;&#38754;&#24212;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#65292;&#20026;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#23545;&#40784;&#26234;&#33021;&#20307;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#25972;&#21512;&#12290;&#35813;&#25972;&#21512;&#20351;&#29992;OT&#22788;&#29702;&#20998;&#24067;&#21644;&#36816;&#36755;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;MARL&#30340;&#25928;&#29575;&#12289;&#21327;&#35843;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;OT&#22312;&#20197;&#19979;&#20116;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#20197;&#24433;&#21709;MARL&#65306;&#65288;1&#65289;&#25919;&#31574;&#23545;&#40784;&#65292;&#21033;&#29992;OT&#30340;Wasserstein&#24230;&#37327;&#26469;&#23558;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#40784;&#21040;&#32479;&#19968;&#30340;&#30446;&#26631;&#19978;&#65307;&#65288;2&#65289;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#65292;&#21033;&#29992;OT&#26469;&#20248;&#21270;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36164;&#28304;&#20998;&#37197;&#65307;&#65288;3&#65289;&#24212;&#23545;&#38750;&#24179;&#31283;&#24615;&#65292;&#21033;&#29992;OT&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#65307;&#65288;4&#65289;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#65292;&#21033;&#29992;OT&#23558;&#22823;&#35268;&#27169;&#23398;&#20064;&#30446;&#26631;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#20219;&#21153;&#65307;&#65288;5&#65289;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#65292;&#24212;&#29992;OT&#21407;&#21017;&#26469;&#24320;&#21457;&#21487;&#25345;&#32493;&#30340;MARL&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;OT&#19982;MARL&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#22914;&#20309;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#23545;&#40784;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the integration of optimal transport (OT) theory with multi-agent reinforcement learning (MARL). This integration uses OT to handle distributions and transportation problems to enhance the efficiency, coordination, and adaptability of MARL. There are five key areas where OT can impact MARL: (1) policy alignment, where OT's Wasserstein metric is used to align divergent agent strategies towards unified goals; (2) distributed resource management, employing OT to optimize resource allocation among agents; (3) addressing non-stationarity, using OT to adapt to dynamic environmental shifts; (4) scalable multi-agent learning, harnessing OT for decomposing large-scale learning objectives into manageable tasks; and (5) enhancing energy efficiency, applying OT principles to develop sustainable MARL systems. This paper articulates how the synergy between OT and MARL can address scalability issues, optimize resource distribution, align agent policies in cooperative environments,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#38477;&#32500;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#20013;&#30340;&#21508;&#20010;&#21464;&#37327;&#29420;&#31435;&#35745;&#31639;&#21644;&#26657;&#20934;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#26367;&#25442;&#20026;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26469;&#23454;&#29616;&#65292;&#28982;&#21518;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.10945</link><description>&lt;p&gt;
Twin-in-the-Loop Observers&#30340;&#33258;&#21160;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Automatic dimensionality reduction of Twin-in-the-Loop Observers. (arXiv:2401.10945v1 [cs.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#38477;&#32500;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#20013;&#30340;&#21508;&#20010;&#21464;&#37327;&#29420;&#31435;&#35745;&#31639;&#21644;&#26657;&#20934;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#26367;&#25442;&#20026;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26469;&#23454;&#29616;&#65292;&#28982;&#21518;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#32570;&#28857;&#65306;&#27599;&#20010;&#35201;&#20272;&#35745;&#30340;&#21464;&#37327;&#37117;&#26159;&#29992;&#29420;&#31435;&#30340;&#31616;&#21270;&#28388;&#27874;&#27169;&#22359;&#35745;&#31639;&#30340;&#12290;&#36825;&#20123;&#27169;&#22359;&#24182;&#34892;&#36816;&#34892;&#24182;&#38656;&#35201;&#21333;&#29420;&#26657;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;Twin-in-the-Loop(TiL)&#35266;&#27979;&#22120;&#26550;&#26500;&#65306;&#20272;&#35745;&#22120;&#20013;&#30340;&#32463;&#20856;&#31616;&#21270;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#34987;&#19968;&#20010;&#23436;&#25972;&#30340;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26367;&#20195;&#12290;DT&#30340;&#29366;&#24577;&#36890;&#36807;&#32447;&#24615;&#26102;&#19981;&#21464;&#30340;&#36755;&#20986;&#35823;&#24046;&#23450;&#24459;&#23454;&#26102;&#26657;&#27491;&#12290;&#30001;&#20110;&#27169;&#25311;&#22120;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#20998;&#26512;&#20844;&#24335;&#21487;&#29992;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;&#32463;&#20856;&#30340;&#28388;&#27874;&#22120;&#35843;&#33410;&#25216;&#26415;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#23558;&#29992;&#20110;&#35299;&#20915;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;&#30001;&#20110;DT&#30340;&#22797;&#26434;&#24615;&#65292;&#20248;&#21270;&#38382;&#39064;&#26159;&#39640;&#32500;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#35843;&#33410;&#39640;&#22797;&#26434;&#24230;&#35266;&#27979;&#22120;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art vehicle dynamics estimation techniques usually share one common drawback: each variable to estimate is computed with an independent, simplified filtering module. These modules run in parallel and need to be calibrated separately. To solve this issue, a unified Twin-in-the-Loop (TiL) Observer architecture has recently been proposed: the classical simplified control-oriented vehicle model in the estimators is replaced by a full-fledged vehicle simulator, or digital twin (DT). The states of the DT are corrected in real time with a linear time invariant output error law. Since the simulator is a black-box, no explicit analytical formulation is available, hence classical filter tuning techniques cannot be used. Due to this reason, Bayesian Optimization will be used to solve a data-driven optimization problem to tune the filter. Due to the complexity of the DT, the optimization problem is high-dimensional. This paper aims to find a procedure to tune the high-complexity obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#21453;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#36866;&#24212;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;MUL&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#30340;&#21464;&#21270;&#21644;&#20262;&#29702;&#32771;&#34385;&#21160;&#24577;&#35843;&#25972;&#31995;&#32479;&#30693;&#35782;&#12290;&#36890;&#36807;&#25209;&#21028;&#24615;&#26816;&#39564;&#21644;&#25991;&#29486;&#26803;&#29702;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;MUL&#22914;&#20309;&#25913;&#21464;&#25512;&#33616;&#12289;&#29992;&#25143;&#20449;&#20219;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#36335;&#24452;&#30340;&#35265;&#35299;&#12290;&#24378;&#35843;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#65292;&#24182;&#40723;&#21169;&#20197;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#25512;&#21160;MUL&#22312;&#23433;&#20840;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10942</link><description>&lt;p&gt;
&#26426;&#22120;&#21453;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning for Recommendation Systems: An Insight. (arXiv:2401.10942v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#21453;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#36866;&#24212;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;MUL&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#30340;&#21464;&#21270;&#21644;&#20262;&#29702;&#32771;&#34385;&#21160;&#24577;&#35843;&#25972;&#31995;&#32479;&#30693;&#35782;&#12290;&#36890;&#36807;&#25209;&#21028;&#24615;&#26816;&#39564;&#21644;&#25991;&#29486;&#26803;&#29702;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;MUL&#22914;&#20309;&#25913;&#21464;&#25512;&#33616;&#12289;&#29992;&#25143;&#20449;&#20219;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#36335;&#24452;&#30340;&#35265;&#35299;&#12290;&#24378;&#35843;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#65292;&#24182;&#40723;&#21169;&#20197;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#25512;&#21160;MUL&#22312;&#23433;&#20840;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MUL&#65289;&#65292;&#35299;&#20915;&#20102;&#36866;&#24212;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;MUL&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#30340;&#21464;&#21270;&#21644;&#20262;&#29702;&#32771;&#34385;&#21160;&#24577;&#35843;&#25972;&#31995;&#32479;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;MUL&#30340;&#22522;&#26412;&#21407;&#29702;&#12289;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21644;&#31639;&#27861;&#36879;&#26126;&#24615;&#31561;&#25361;&#25112;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#26816;&#39564;&#12290;&#23427;&#26803;&#29702;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#25552;&#20379;&#20102;MUL&#22914;&#20309;&#25913;&#21464;&#25512;&#33616;&#30340;&#35265;&#35299;&#65292;&#25506;&#35752;&#20102;&#29992;&#25143;&#20449;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#22312;&#36127;&#36131;&#20219;&#21644;&#29992;&#25143;&#20851;&#27880;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36335;&#24452;&#12290;&#26412;&#25991;&#24341;&#23548;&#30740;&#31350;&#20154;&#21592;&#38754;&#23545;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#65292;&#40723;&#21169;&#20197;&#28385;&#36275;&#26377;&#38024;&#23545;&#24615;&#30340;&#25968;&#25454;&#21024;&#38500;&#23454;&#38469;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#36129;&#29486;&#12290;&#24378;&#35843;MUL&#22312;&#23433;&#20840;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#25512;&#21160;&#20854;&#36793;&#30028;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review explores machine unlearning (MUL) in recommendation systems, addressing adaptability, personalization, privacy, and bias challenges. Unlike traditional models, MUL dynamically adjusts system knowledge based on shifts in user preferences and ethical considerations. The paper critically examines MUL's basics, real-world applications, and challenges like algorithmic transparency. It sifts through literature, offering insights into how MUL could transform recommendations, discussing user trust, and suggesting paths for future research in responsible and user-focused artificial intelligence (AI). The document guides researchers through challenges involving the trade-off between personalization and privacy, encouraging contributions to meet practical demands for targeted data removal. Emphasizing MUL's role in secure and adaptive machine learning, the paper proposes ways to push its boundaries. The novelty of this paper lies in its exploration of the limitations of the methods, w
&lt;/p&gt;</description></item><item><title>Crowd-PrefRL&#26159;&#19968;&#31181;&#22522;&#20110;&#20247;&#21253;&#30340;&#20559;&#22909;&#21453;&#39304;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26469;&#33258;&#32676;&#20307;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#24378;&#22823;&#22320;&#32858;&#21512;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#24182;&#20272;&#35745;&#29992;&#25143;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10941</link><description>&lt;p&gt;
Crowd-PrefRL: &#22522;&#20110;&#20247;&#21253;&#30340;&#20559;&#22909;&#21453;&#39304;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Crowd-PrefRL: Preference-Based Reward Learning from Crowds. (arXiv:2401.10941v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10941
&lt;/p&gt;
&lt;p&gt;
Crowd-PrefRL&#26159;&#19968;&#31181;&#22522;&#20110;&#20247;&#21253;&#30340;&#20559;&#22909;&#21453;&#39304;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26469;&#33258;&#32676;&#20307;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#24378;&#22823;&#22320;&#32858;&#21512;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#24182;&#20272;&#35745;&#29992;&#25143;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#34892;&#20026;&#23545;&#30340;&#20559;&#22909;&#36827;&#34892;&#20154;&#31867;&#21453;&#39304;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#38590;&#20197;&#25351;&#23450;&#25968;&#20540;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#23613;&#31649;&#36825;&#20010;&#33539;&#24335;&#21033;&#29992;&#20102;&#20154;&#31867;&#30340;&#21453;&#39304;&#65292;&#20294;&#30446;&#21069;&#23558;&#21453;&#39304;&#35270;&#20026;&#21333;&#20010;&#20154;&#31867;&#29992;&#25143;&#25152;&#32473;&#20986;&#30340;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20197;&#24378;&#22823;&#30340;&#26041;&#24335;&#21512;&#24182;&#26469;&#33258;&#32676;&#20307;&#65288;&#21363;&#29992;&#25143;&#38598;&#21512;&#65289;&#30340;&#20559;&#22909;&#21453;&#39304;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#32780;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#30340;&#21453;&#39304;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#20173;&#28982;&#34987;&#30740;&#31350;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Crowd-PrefRL&#65292;&#19968;&#20010;&#21033;&#29992;&#26469;&#33258;&#32676;&#20307;&#30340;&#21453;&#39304;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#21033;&#29992;&#26410;&#30693;&#19987;&#19994;&#27700;&#24179;&#21644;&#21487;&#38752;&#24615;&#30340;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;Crowd-PrefRL&#19981;&#20165;&#33021;&#22815;&#24378;&#22823;&#22320;&#32858;&#21512;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#65292;&#36824;&#33021;&#22815;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (RL) provides a framework to train agents using human feedback through pairwise preferences over pairs of behaviors, enabling agents to learn desired behaviors when it is difficult to specify a numerical reward function. While this paradigm leverages human feedback, it currently treats the feedback as given by a single human user. Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of users) in a robust manner remains a challenge, and the problem of training RL agents using feedback from multiple human users remains understudied. In this work, we introduce Crowd-PrefRL, a framework for performing preference-based RL leveraging feedback from crowds. This work demonstrates the viability of learning reward functions from preference feedback provided by crowds of unknown expertise and reliability. Crowd-PrefRL not only robustly aggregates the crowd preference feedback, but also estimates the reliability of each user within the cr
&lt;/p&gt;</description></item><item><title>RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10940</link><description>&lt;p&gt;
RELIANCE: &#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10940
&lt;/p&gt;
&lt;p&gt;
RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#27867;&#28389;&#30340;&#26102;&#20195;&#65292;&#36776;&#21035;&#26032;&#38395;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RELIANCE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#40065;&#26834;&#20449;&#24687;&#21644;&#34394;&#20551;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20808;&#36827;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#12290;RELIANCE&#30001;&#20116;&#20010;&#19981;&#21516;&#30340;&#22522;&#26412;&#27169;&#22411;&#32452;&#25104;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BiLSTMs&#65289;&#12290;RELIANCE&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#38598;&#25104;&#30340;&#26234;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;RELIANCE&#22312;&#21306;&#20998;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#34920;&#26126;&#20854;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#36229;&#36807;&#20102;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#25104;&#20026;&#35780;&#20272;&#20449;&#24687;&#28304;&#21487;&#38752;&#24615;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10938</link><description>&lt;p&gt;
&#21363;&#20351;&#35299;&#37322;&#65306;&#24418;&#24335;&#22522;&#30784;&#65292;&#20248;&#20808;&#32423;&#21644;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#23376;&#36816;&#34892;&#65292;&#32570;&#20047;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#65292;&#32780;&#21448;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#12290;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#35797;&#22270;&#22238;&#31572;&#20026;&#20160;&#20040;&#32473;&#23450;&#27169;&#22411;&#22914;&#20309;&#23545;&#20010;&#20307;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#20851;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#24050;&#32463;&#36827;&#34892;&#20102;&#37325;&#35201;&#24037;&#20316;&#65292;&#20294;&#23545;&#21322;&#20107;&#23454;&#35299;&#37322;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21322;&#20107;&#23454;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#20197;&#21450;&#19981;&#21516;&#27169;&#22411;&#31867;&#21035;&#20013;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#34920;&#26126;&#32447;&#24615;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#27604;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#20110;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#30340;&#20559;&#22909;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#26080;&#35770;&#26159;&#22312;&#21322;&#20107;&#23454;&#36824;&#26159;&#21453;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#35299;&#37322;&#33021;&#21147;&#21644;&#29992;&#25143;&#20013;&#24515;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable AI has received significant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classified in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual `even-if' thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#36136;&#25276;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#24179;&#22343;&#27861;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;ETH&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#19981;&#21516;&#21152;&#23494;&#36135;&#24065;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#22312;XTZ&#21644;ATOM&#30340;&#30701;&#26399;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#32467;&#26524;&#31361;&#26174;&#20102;&#22823;&#22810;&#25968;&#36164;&#20135;&#36136;&#25276;&#22870;&#21169;&#30340;&#31283;&#23450;&#21487;&#39044;&#27979;&#24615;&#65292;MATIC&#26159;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.10931</link><description>&lt;p&gt;
&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#36136;&#25276;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Forecasting Cryptocurrency Staking Rewards. (arXiv:2401.10931v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#36136;&#25276;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#24179;&#22343;&#27861;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;ETH&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#19981;&#21516;&#21152;&#23494;&#36135;&#24065;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#22312;XTZ&#21644;ATOM&#30340;&#30701;&#26399;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#32467;&#26524;&#31361;&#26174;&#20102;&#22823;&#22810;&#25968;&#36164;&#20135;&#36136;&#25276;&#22870;&#21169;&#30340;&#31283;&#23450;&#21487;&#39044;&#27979;&#24615;&#65292;MATIC&#26159;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#21363;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#36136;&#25276;&#22870;&#21169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#25237;&#36164;&#32773;&#25552;&#20379;&#28508;&#22312;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#39044;&#27979;&#26041;&#27861;&#65306;a&#65289;&#31616;&#21333;&#30340;&#28369;&#21160;&#31383;&#21475;&#24179;&#22343;&#27861;&#65292;&#21644;b&#65289;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;7&#22825;&#28369;&#21160;&#31383;&#21475;&#24179;&#22343;&#27861;&#65292;ETH&#36136;&#25276;&#22870;&#21169;&#21487;&#20197;&#22312;1&#22825;&#21644;7&#22825;&#30340;&#39044;&#27979;&#20013;&#65292;&#39044;&#27979;&#35823;&#24046;&#22343;&#22312;&#22343;&#20540;&#30340;0.7%&#21644;1.1%&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19981;&#21516;&#21152;&#23494;&#36135;&#24065;&#65288;&#21253;&#25324;SOL&#12289;XTZ&#12289;ATOM&#21644;MATIC&#65289;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#23545;&#20110;XTZ&#21644;ATOM&#65292;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#22312;&#30701;&#26399;&#39044;&#27979;&#20013;&#34987;&#35748;&#20026;&#20248;&#20110;&#28369;&#21160;&#31383;&#21475;&#24179;&#22343;&#27861;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;&#22823;&#22810;&#25968;&#36164;&#20135;&#30340;&#36136;&#25276;&#22870;&#21169;&#36890;&#24120;&#26159;&#31283;&#23450;&#21487;&#39044;&#27979;&#30340;&#65292;&#32780;MATIC&#21017;&#26159;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research explores a relatively unexplored area of predicting cryptocurrency staking rewards, offering potential insights to researchers and investors. We investigate two predictive methodologies: a) a straightforward sliding-window average, and b) linear regression models predicated on historical data. The findings reveal that ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day look-aheads respectively, using a 7-day sliding-window average approach. Additionally, we discern diverse prediction accuracies across various cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is identified as superior to the moving-window average for perdicting in the short term for XTZ and ATOM. The results underscore the generally stable and predictable nature of staking rewards for most assets, with MATIC presenting a noteworthy exception.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21322;&#27491;&#23450;&#35268;&#21010;&#36827;&#34892;&#20154;&#32676;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#21407;&#22987;&#31181;&#32676;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#65292;&#36866;&#29992;&#20110;&#31181;&#32676;&#20043;&#38388;&#24046;&#24322;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.10927</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#27491;&#23450;&#35268;&#21010;&#30340;&#21435;&#20559;&#21644;&#23616;&#37096;&#20998;&#26512;&#36827;&#34892;&#20154;&#32676;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing and a local analysis for population clustering using semidefinite programming. (arXiv:2401.10927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21322;&#27491;&#23450;&#35268;&#21010;&#36827;&#34892;&#20154;&#32676;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#21407;&#22987;&#31181;&#32676;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#65292;&#36866;&#29992;&#20110;&#31181;&#32676;&#20043;&#38388;&#24046;&#24322;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#28151;&#21512;&#30340;2&#20010;&#27425;&#39640;&#26031;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#23567;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#21306;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21516;&#19968;&#20316;&#32773;&#25552;&#20986;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23558;&#25968;&#25454;&#26681;&#25454;&#20854;&#21407;&#22987;&#31181;&#32676;&#22823;&#33268;&#20998;&#20026;&#20004;&#32452;&#65292;&#32473;&#23450;&#19968;&#20010;&#23567;&#26679;&#26412;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#23558;&#20010;&#20307;&#26681;&#25454;&#20854;&#21407;&#22987;&#31181;&#32676;&#20351;&#29992;p&#20010;&#26631;&#35760;&#36827;&#34892;&#32858;&#31867;&#65292;&#24403;&#20219;&#24847;&#20004;&#20010;&#31181;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#24456;&#23567;&#26102;&#12290;&#25105;&#20204;&#22522;&#20110;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#24418;&#24335;&#26500;&#24314;&#65292;&#35813;&#35268;&#21010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#22312;&#19968;&#20010;&#22270;&#19978;&#25214;&#21040;&#26368;&#22823;&#21106;&#65292;&#20854;&#20013;&#21106;&#20013;&#30340;&#36793;&#26435;&#37325;&#34920;&#31034;&#22522;&#20110;&#23427;&#20204;&#30340;p&#20010;&#29305;&#24449;&#30340;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#19981;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#25105;&#20204;&#29992;&#916;^2:=p&#947;&#26469;&#34920;&#31034;&#20004;&#20010;&#20013;&#24515;&#65288;&#22343;&#20540;&#21521;&#37327;&#65289;&#20043;&#38388;&#30340;&#8467;_2^2&#36317;&#31163;&#65292;&#21363;&#956;^(1), &#956;^(2)&#8712;&#8477;^p&#12290;&#30446;&#26631;&#26159;&#22312;&#20132;&#25442;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20840;&#38754;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $2$ sub-gaussian distributions. In particular, we analyze computational efficient algorithms proposed by the same author, to partition data into two groups approximately according to their population of origin given a small sample. This work is motivated by the application of clustering individuals according to their population of origin using $p$ markers, when the divergence between any two of the populations is small. We build upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features. Here we use $\Delta^2 :=p \gamma$ to denote the $\ell_2^2$ distance between two centers (mean vectors), namely, $\mu^{(1)}$, $\mu^{(2)}$ $\in$ $\mathbb{R}^p$. The goal is to allow a full range of tradeoffs between
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#22522;&#20110;&#25512;&#21644;&#25289;&#30340;&#36890;&#20449;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#26368;&#20248;&#31574;&#30053;&#19982;&#20449;&#24687;&#20215;&#20540;&#26368;&#22823;&#21270;&#30456;&#19968;&#33268;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#25289;&#30340;&#36890;&#20449;&#21487;&#33021;&#27604;&#22522;&#20110;&#25512;&#30340;&#36890;&#20449;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.10921</link><description>&lt;p&gt;
&#22522;&#20110;&#25512;&#21644;&#25289;&#30340;&#26377;&#25928;&#36890;&#20449;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Push- and Pull-based Effective Communication in Cyber-Physical Systems. (arXiv:2401.10921v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#22522;&#20110;&#25512;&#21644;&#25289;&#30340;&#36890;&#20449;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#26368;&#20248;&#31574;&#30053;&#19982;&#20449;&#24687;&#20215;&#20540;&#26368;&#22823;&#21270;&#30456;&#19968;&#33268;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#25289;&#30340;&#36890;&#20449;&#21487;&#33021;&#27604;&#22522;&#20110;&#25512;&#30340;&#36890;&#20449;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#20013;&#65292;&#20004;&#32452;&#21442;&#19982;&#32773;&#36890;&#36807;&#20114;&#21160;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#24615;&#33021;&#65306;&#35266;&#27979;&#21644;&#20256;&#25773;&#31995;&#32479;&#29366;&#24577;&#30340;&#20256;&#24863;&#22120;&#21644;&#22522;&#20110;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36827;&#34892;&#29289;&#29702;&#20915;&#31574;&#30340;&#25191;&#34892;&#22120;&#12290;&#34429;&#28982;&#36890;&#24120;&#20551;&#35774;&#20256;&#24863;&#22120;&#23450;&#26399;&#20256;&#36755;&#26356;&#26032;&#65292;&#20165;&#22312;&#24517;&#35201;&#26102;&#36820;&#22238;&#21453;&#39304;&#20449;&#21495;&#65292;&#24182;&#22240;&#27492;&#26681;&#25454;&#36890;&#20449;&#31574;&#30053;&#35843;&#25972;&#29289;&#29702;&#20915;&#31574;&#65292;&#20294;&#23454;&#38469;&#19978;&#36890;&#36807;&#36873;&#25321;&#22522;&#20110;&#25512;&#30340;&#36890;&#20449;&#65288;&#20256;&#24863;&#22120;&#33258;&#20027;&#21551;&#21160;&#26356;&#26032;&#65289;&#21644;&#22522;&#20110;&#25289;&#30340;&#36890;&#20449;&#65288;&#25191;&#34892;&#22120;&#35831;&#27714;&#26356;&#26032;&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;CPS&#20013;&#22522;&#20110;&#25512;&#21644;&#25289;&#30340;&#36890;&#20449;&#30340;&#20998;&#26512;&#27169;&#22411;&#65292;&#35266;&#23519;&#21040;&#31574;&#30053;&#30340;&#26368;&#20248;&#24615;&#19982;&#20449;&#24687;&#20215;&#20540;&#65288;VoI&#65289;&#26368;&#22823;&#21270;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23613;&#31649;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#25289;&#30340;&#36890;&#20449;&#21487;&#33021;&#20250;&#27604;&#22522;&#20110;&#25512;&#30340;&#36890;&#20449;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Cyber Physical Systems (CPSs), two groups of actors interact toward the maximization of system performance: the sensors, observing and disseminating the system state, and the actuators, performing physical decisions based on the received information. While it is generally assumed that sensors periodically transmit updates, returning the feedback signal only when necessary, and consequently adapting the physical decisions to the communication policy, can significantly improve the efficiency of the system. In particular, the choice between push-based communication, in which updates are initiated autonomously by the sensors, and pull-based communication, in which they are requested by the actuators, is a key design step. In this work, we propose an analytical model for optimizing push- and pull-based communication in CPSs, observing that the policy optimality coincides with Value of Information (VoI) maximization. Our results also highlight that, despite providing a better optimal solu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#32929;&#31080;&#24066;&#22330;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#36842;&#22763;&#23612;&#32929;&#31080;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#36873;&#25321;&#65292;&#21457;&#29616;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2401.10903</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32929;&#31080;&#24066;&#22330;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#36842;&#22763;&#23612;&#32929;&#31080;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock. (arXiv:2401.10903v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#32929;&#31080;&#24066;&#22330;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#36842;&#22763;&#23612;&#32929;&#31080;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#36873;&#25321;&#65292;&#21457;&#29616;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#19968;&#20221;&#21253;&#21547;750&#20010;&#23454;&#20363;&#21644;16&#20010;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#32929;&#31080;&#24066;&#22330;&#20998;&#26512;&#12290;&#20998;&#26512;&#21253;&#25324;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512; (EDA) &#37096;&#20998;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#25968;&#25454;&#20934;&#22791;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#20998;&#26512;&#30340;&#35265;&#35299;&#12290;&#22312;&#20998;&#26512;&#20013;&#36824;&#36816;&#29992;&#20102;Fama French 3&#22240;&#23376;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#32447;&#24615;&#22238;&#24402;&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document presents a stock market analysis conducted on a dataset consisting of 750 instances and 16 attributes donated in 2014-10-23. The analysis includes an exploratory data analysis (EDA) section, feature engineering, data preparation, model selection, and insights from the analysis. The Fama French 3-factor model is also utilized in the analysis. The results of the analysis are presented, with linear regression being the best-performing model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10895</link><description>&lt;p&gt;
&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;(SCRA)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#28436;&#21464;&#65292;&#38761;&#26032;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;&#36825;&#31181;&#28436;&#21464;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#22312;&#29616;&#20195;&#20379;&#24212;&#38142;&#20013;&#30830;&#20445;&#36816;&#33829;&#30340;&#38887;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#38656;&#35201;&#31283;&#20581;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#12290;&#20197;&#24448;&#30340;&#32508;&#36848;&#24050;&#32463;&#27010;&#36848;&#20102;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#20294;&#24573;&#35270;&#20102;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#29702;&#35299;&#20854;&#22312;SCRA&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;1717&#31687;&#35770;&#25991;&#65292;&#24182;&#20174;2014&#24180;&#33267;2023&#24180;&#20043;&#38388;&#21457;&#34920;&#30340;48&#31687;&#25991;&#31456;&#20013;&#33719;&#24471;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#35813;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#25506;&#31350;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12289;&#26041;&#27861;&#35770;&#12289;&#30740;&#31350;&#32467;&#26524;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
&lt;/p&gt;</description></item><item><title>Starlit&#26159;&#19968;&#20010;&#26032;&#30340;&#21487;&#25193;&#23637;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#27491;&#24335;&#30340;&#23433;&#20840;&#23450;&#20041;&#21644;&#35777;&#26126;&#12289;&#20551;&#23450;&#20923;&#32467;&#36134;&#25143;&#12289;&#35268;&#27169;&#25193;&#22823;&#12289;&#36523;&#20221;&#23545;&#40784;&#38454;&#27573;&#21644;&#38590;&#20197;&#25269;&#25239;&#23458;&#25143;&#31471;&#36864;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.10765</link><description>&lt;p&gt;
Starlit: &#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#20197;&#22686;&#24378;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection. (arXiv:2401.10765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10765
&lt;/p&gt;
&lt;p&gt;
Starlit&#26159;&#19968;&#20010;&#26032;&#30340;&#21487;&#25193;&#23637;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#27491;&#24335;&#30340;&#23433;&#20840;&#23450;&#20041;&#21644;&#35777;&#26126;&#12289;&#20551;&#23450;&#20923;&#32467;&#36134;&#25143;&#12289;&#35268;&#27169;&#25193;&#22823;&#12289;&#36523;&#20221;&#23545;&#40784;&#38454;&#27573;&#21644;&#38590;&#20197;&#25269;&#25239;&#23458;&#25143;&#31471;&#36864;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#25968;&#25454;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#26412;&#22320;&#25968;&#25454;&#30340;&#21508;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#65292;&#36991;&#20813;&#30452;&#25509;&#25968;&#25454;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35782;&#21035;&#27450;&#35784;&#37329;&#34701;&#20132;&#26131;&#30340;&#26368;&#26032;FL&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20197;&#19979;&#19968;&#20123;&#38480;&#21046;&#65306;&#32570;&#20047;&#27491;&#24335;&#30340;&#23433;&#20840;&#23450;&#20041;&#21644;&#35777;&#26126;&#65292;&#20551;&#23450;&#37329;&#34701;&#26426;&#26500;&#20107;&#20808;&#20923;&#32467;&#21487;&#30097;&#23458;&#25143;&#30340;&#36134;&#25143;&#65288;&#38480;&#21046;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#37319;&#29992;&#65289;&#65292;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#28041;&#21450;$O(n^2)$&#30340;&#35745;&#31639;&#26114;&#36149;&#30340;&#27169;&#22359;&#25351;&#25968;&#36816;&#31639;&#65288;&#20854;&#20013;$n$&#26159;&#37329;&#34701;&#26426;&#26500;&#30340;&#24635;&#25968;&#65289;&#25110;&#32773;&#39640;&#25928;&#29575;&#20302;&#30340;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#65292;&#20551;&#35774;&#21508;&#26041;&#24050;&#32463;&#23436;&#25104;&#20102;&#36523;&#20221;&#23545;&#40784;&#38454;&#27573;&#65292;&#22240;&#27492;&#23558;&#20854;&#25490;&#38500;&#22312;&#23454;&#26045;&#12289;&#24615;&#33021;&#35780;&#20272;&#21644;&#23433;&#20840;&#20998;&#26512;&#20043;&#22806;&#65292;&#24182;&#19988;&#38590;&#20197;&#25269;&#25239;&#23458;&#25143;&#31471;&#30340;&#36864;&#20986;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25193;&#23637;&#38544;&#31169;&#20445;&#25252;FL&#26426;&#21046;&#8212;&#8212;Starlit&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a data-minimization approach enabling collaborative model training across diverse clients with local data, avoiding direct data exchange. However, state-of-the-art FL solutions to identify fraudulent financial transactions exhibit a subset of the following limitations. They (1) lack a formal security definition and proof, (2) assume prior freezing of suspicious customers' accounts by financial institutions (limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$ computationally expensive modular exponentiation (where $n$ is the total number of financial institutions) or highly inefficient fully homomorphic encryption, (4) assume the parties have already completed the identity alignment phase, hence excluding it from the implementation, performance evaluation, and security analysis, and (5) struggle to resist clients' dropouts. This work introduces Starlit, a novel scalable privacy-preserving FL mechanism that overcomes these limitations
&lt;/p&gt;</description></item><item><title>PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.10586</link><description>&lt;p&gt;
PuriDefense&#65306;&#29992;&#20110;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38543;&#26426;&#23616;&#37096;&#38544;&#24335;&#23545;&#25239;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10586
&lt;/p&gt;
&lt;p&gt;
PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;&#23545;&#25239;&#35757;&#32451;&#12289;&#26799;&#24230;&#25513;&#30422;&#21644;&#36755;&#20837;&#36716;&#25442;&#65292;&#35201;&#20040;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#35201;&#20040;&#25439;&#23475;&#38750;&#23545;&#25239;&#36755;&#20837;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;PuriDefense&#65292;&#22312;&#20302;&#25512;&#29702;&#25104;&#26412;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#30340;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#23616;&#37096;&#38544;&#24335;&#20989;&#25968;&#24182;&#37325;&#24314;&#33258;&#28982;&#22270;&#20687;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#32435;&#20837;&#20928;&#21270;&#36807;&#31243;&#26469;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23545;CIFAR-10&#21644;ImageNet&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20928;&#21270;&#22120;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2401.10451</link><description>&lt;p&gt;
&#23398;&#20064;&#36741;&#21161;&#30340;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#35268;&#21010;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach. (arXiv:2401.10451v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#23545;&#20110;&#21306;&#22495;&#33021;&#28304;&#31995;&#32479;&#30340;&#25104;&#26412;&#25928;&#30410;&#20302;&#30899;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#30830;&#20445;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#39044;&#26399;&#32467;&#26524;&#65292;&#24314;&#27169;&#32771;&#34385;&#21040;&#22825;&#27668;&#30456;&#20851;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#20379;&#24212;&#21644;&#33021;&#28304;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38543;&#26426;&#20248;&#21270;&#27169;&#22411;&#36890;&#24120;&#27604;&#30830;&#23450;&#24615;&#27169;&#22411;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36817;&#20284;&#35299;&#27861;&#26469;&#21487;&#34892;&#22320;&#35299;&#20915;&#20004;&#38454;&#27573;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#25628;&#32034;&#26102;&#38388;&#24207;&#21015;&#32858;&#21512;&#36229;&#21442;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#35745;&#31639;&#22312;&#20379;&#38656;&#39044;&#27979;&#30340;&#39564;&#35777;&#38598;&#19978;&#26368;&#23567;&#21270;&#25104;&#26412;&#30340;&#36817;&#20284;&#35299;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#32452;&#20445;&#30041;&#30340;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving large-scale capacity expansion problems (CEPs) is central to cost-effective decarbonization of regional-scale energy systems. To ensure the intended outcomes of CEPs, modeling uncertainty due to weather-dependent variable renewable energy (VRE) supply and energy demand becomes crucially important. However, the resulting stochastic optimization models are often less computationally tractable than their deterministic counterparts. Here, we propose a learning-assisted approximate solution method to tractably solve two-stage stochastic CEPs. Our method identifies low-cost planning decisions by constructing and solving a sequence of tractable temporally aggregated surrogate problems. We adopt a Bayesian optimization approach to searching the space of time series aggregation hyperparameters and compute approximate solutions that minimize costs on a validation set of supply-demand projections. Importantly, we evaluate solved planning outcomes on a held-out set of test projections. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.10393</link><description>&lt;p&gt;
&#33258;&#28982;&#30340;&#21151;&#29575;&#27861;&#21017;&#23398;&#20064;&#29615;&#22659;&#20013;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65288;CI&#65289;&#65306;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#19982;&#20154;&#31867;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#21487;&#20197;&#36830;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#20250;&#26126;&#26174;&#24536;&#35760;&#20808;&#21069;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#20943;&#36731;CI&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#27491;&#21017;&#21270;&#12289;&#22238;&#24518;&#12289;&#29983;&#25104;&#24615;&#22238;&#25918;&#21644;&#27987;&#32553;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#25351;&#23548;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36935;&#21040;&#20219;&#21153;&#30340;&#27010;&#29575;&#19982;&#26368;&#21518;&#19968;&#27425;&#25191;&#34892;&#20219;&#21153;&#30340;&#26102;&#38388;&#25104;&#21151;&#29575;&#27861;&#21017;&#36882;&#20943;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27169;&#25311;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20943;&#36731;CI&#25216;&#26415;&#30340;&#30495;&#23454;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#38754;&#20020;&#30340;&#21151;&#29575;&#27861;&#21017;&#29615;&#22659;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#22238;&#24518;&#26041;&#27861;&#26102;&#65292;CI&#30340;&#20943;&#36731;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#36825;&#31181;&#22522;&#20110;&#22238;&#24518;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
&lt;/p&gt;</description></item><item><title>Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10371</link><description>&lt;p&gt;
Langevin&#36951;&#24536;&#65306;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#26426;&#22120;&#36951;&#24536;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10371
&lt;/p&gt;
&lt;p&gt;
Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37319;&#29992;&#30830;&#20445;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27861;&#24459;&#65292;&#26426;&#22120;&#36951;&#24536;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#36817;&#20284;&#36951;&#24536;&#23450;&#20041;&#65292;&#31867;&#20284;&#20110;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#23450;&#20041;&#65292;&#20854;&#20013;&#38544;&#31169;&#34987;&#23450;&#20041;&#20026;&#23545;&#37325;&#26032;&#35757;&#32451;&#30340;&#32479;&#35745;&#19981;&#21487;&#21306;&#20998;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Langevin&#36951;&#24536;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#30340;&#38544;&#31169;&#20445;&#35777;&#30340;&#36951;&#24536;&#26694;&#26550;&#12290;Langevin&#36951;&#24536;&#22312;&#31639;&#27861;&#19978;&#32479;&#19968;&#20102;DP&#23398;&#20064;&#36807;&#31243;&#21644;&#38544;&#31169;&#35748;&#35777;&#30340;&#36951;&#24536;&#36807;&#31243;&#12290;&#20854;&#20013;&#21253;&#25324;&#38750;&#20984;&#38382;&#39064;&#30340;&#36817;&#20284;&#35748;&#35777;&#36951;&#24536;&#65292;&#30456;&#23545;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#29992;&#20110;&#22810;&#20010;&#36951;&#24536;&#35831;&#27714;&#30340;&#39034;&#24207;&#21644;&#25209;&#37327;&#36951;&#24536;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Langevin&#36951;&#24536;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#27963;&#21160;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#20102;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#30740;&#31350;&#25104;&#26524;&#20026;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26080;&#38382;&#21367;&#35843;&#26597;&#30340;&#26041;&#24335;&#23545;&#20010;&#24615;&#30456;&#20851;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25512;&#21160;&#20010;&#24615;&#30740;&#31350;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.10305</link><description>&lt;p&gt;
&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25512;&#26029;&#20010;&#24615;&#29305;&#24449;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning Approach. (arXiv:2401.10305v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#27963;&#21160;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#20102;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#30740;&#31350;&#25104;&#26524;&#20026;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26080;&#38382;&#21367;&#35843;&#26597;&#30340;&#26041;&#24335;&#23545;&#20010;&#24615;&#30456;&#20851;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25512;&#21160;&#20010;&#24615;&#30740;&#31350;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#25163;&#26426;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#27963;&#21160;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#20010;&#24615;&#29305;&#24449;&#12290;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#35760;&#24405;&#21644;&#36816;&#21160;&#27169;&#24335;&#35745;&#31639;&#24471;&#20986;&#19968;&#32452;&#30693;&#24773;&#25351;&#26631;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20004;&#31867;&#38382;&#39064;&#19978;&#39044;&#27979;&#29992;&#25143;&#30340;&#20010;&#24615;&#29305;&#24449;&#65292;&#36798;&#21040;&#20102;0.78&#30340;F1&#20998;&#25968;&#12290;&#37492;&#20110;&#25163;&#26426;&#25910;&#38598;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#20010;&#24615;&#29305;&#24449;&#25351;&#26631;&#20026;&#31038;&#20250;&#31185;&#23398;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;&#34892;&#20026;&#27169;&#24335;&#65292;&#23427;&#20204;&#34987;&#35777;&#26126;&#26377;&#24046;&#24322;&#24615;&#22320;&#39044;&#27979;&#20102;&#20116;&#22823;&#20154;&#26684;&#29305;&#24449;&#12290;&#23427;&#20204;&#26377;&#28508;&#21147;&#20197;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26080;&#38382;&#21367;&#35843;&#26597;&#30340;&#26041;&#24335;&#65292;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#19978;&#30740;&#31350;&#19982;&#20010;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#20016;&#23500;&#34892;&#20026;&#25968;&#25454;&#30340;&#32452;&#21512;&#22914;&#20309;&#24110;&#21161;&#25512;&#21160;&#20010;&#24615;&#30740;&#31350;&#65292;&#24182;&#19988;&#21487;&#20197;&#21521;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study provides evidence that personality can be reliably predicted from activity data collected through mobile phone sensors. Employing a set of well informed indicators calculable from accelerometer records and movement patterns, we were able to predict users' personality up to a 0.78 F1 score on a two class problem. Given the fast growing number of data collected from mobile phones, our novel personality indicators open the door to exciting avenues for future research in social sciences. Our results reveal distinct behavioral patterns that proved to be differentially predictive of big five personality traits. They potentially enable cost effective, questionnaire free investigation of personality related questions at an unprecedented scale. Overall, this paper shows how a combination of rich behavioral data obtained with smartphone sensing and the use of machine learning techniques can help to advance personality research and can inform both practitioners and researchers about th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10155</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A novel hybrid time-varying graph neural network for traffic flow forecasting. (arXiv:2401.10155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20934;&#30830;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#30830;&#20445;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39640;&#25928;&#36816;&#34892;&#30340;&#22522;&#30784;&#12290;&#22312;&#29616;&#26377;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#22270;&#26469;&#25551;&#36848;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#19981;&#21516;&#20132;&#36890;&#33410;&#28857;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#39044;&#23450;&#20041;&#22270;&#25551;&#36848;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#23613;&#31649;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30340;&#26102;&#21464;&#22270;&#33021;&#37096;&#20998;&#20811;&#26381;&#39044;&#23450;&#20041;&#22270;&#30340;&#32570;&#28857;&#65292;&#20294;&#29616;&#26377;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#26377;&#38480;&#12290;&#20363;&#22914;&#65292;&#26102;&#21464;&#22270;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time and accurate traffic flow prediction is the foundation for ensuring the efficient operation of intelligent transportation systems.In existing traffic flow prediction methods based on graph neural networks (GNNs), pre-defined graphs were usually used to describe the spatial correlations of different traffic nodes in urban road networks. However, the ability of pre-defined graphs used to describe spatial correlation was limited by prior knowledge and graph generation methods. Although time-varying graphs based on data-driven learning can partially overcome the drawbacks of pre-defined graphs, the learning ability of existing adaptive graphs was limited. For example, time-varying graphs cannot adequately capture the inherent spatial correlations in traffic flow data.In order to solve these problems, we have proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.10107</link><description>&lt;p&gt;
&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#19982;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30446;&#30340;&#65306;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#30446;&#21069;&#34987;&#29992;&#20316;&#35780;&#20272;&#30561;&#30496;&#38556;&#30861;&#30340;&#22522;&#20934;&#12290;&#20854;&#19981;&#33298;&#36866;&#12289;&#19981;&#36866;&#21512;&#23478;&#24237;&#20351;&#29992;&#20197;&#21450;&#22312;&#30561;&#30496;&#36136;&#37327;&#35780;&#20272;&#20013;&#24341;&#20837;&#20559;&#24046;&#30340;&#38382;&#39064;&#38656;&#35201;&#25506;&#32034;&#26356;&#23569;&#20405;&#20837;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#26159;&#32819;&#20869;&#33041;&#30005;&#20256;&#24863;&#22120;&#65292;&#23427;&#22312;&#33298;&#36866;&#24615;&#12289;&#22266;&#23450;&#30005;&#26497;&#20301;&#32622;&#12289;&#25239;&#30005;&#30913;&#24178;&#25200;&#24615;&#21644;&#26131;&#20110;&#20351;&#29992;&#24615;&#26041;&#38754;&#22343;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#35780;&#20272;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#19982;&#26631;&#20934;PSG&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35780;&#20272;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#25512;&#23548;&#30340;&#30561;&#30496;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20174;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;30&#31186;&#26102;&#22495;&#21644;&#39057;&#22495;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#21482;&#32771;&#34385;&#22312;PSG&#35780;&#20998;&#21592;&#21644;&#32819;&#20869;&#33041;&#30005;&#35780;&#20998;&#21592;&#36798;&#25104;&#19968;&#33268;&#26102;&#30340;&#26102;&#27573;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;PSG&#25512;&#23548;&#21644;&#21333;&#36890;&#36947;&#32819;&#20869;&#33041;&#30005;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.  Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#35889;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20013;&#30340;&#22270;&#23646;&#24615;&#21644;&#32467;&#26500;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28789;&#27963;&#22320;&#20445;&#30041;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#21516;&#26102;&#22686;&#21152;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09953</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#26865;&#38236;: &#20809;&#35889;&#35270;&#35282;&#19979;&#30340;&#22270;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification. (arXiv:2401.09953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09953
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#35889;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20013;&#30340;&#22270;&#23646;&#24615;&#21644;&#32467;&#26500;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28789;&#27963;&#22320;&#20445;&#30041;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#21516;&#26102;&#22686;&#21152;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#39318;&#36873;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22270;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25552;&#39640;&#21152;&#24378;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#22686;&#24378;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#22270;&#23646;&#24615;&#25197;&#26354;&#21644;&#21463;&#38480;&#32467;&#26500;&#21464;&#21270;&#31561;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#24320;&#21457;&#26356;&#21152;&#20445;&#30041;&#23646;&#24615;&#24182;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65311;&#36890;&#36807;&#20809;&#35889;&#38236;&#22836;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#23646;&#24615;&#12289;&#23427;&#20204;&#30340;&#22686;&#24378;&#21644;&#23427;&#20204;&#30340;&#20809;&#35889;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#25345;&#29983;&#25104;&#30340;&#22686;&#24378;&#22270;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#21551;&#21457;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;DP-Noise&#21644;DP-Mask&#65292;&#23427;&#20204;&#28789;&#27963;&#22320;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#24182;&#20016;&#23500;&#20102;&#22686;&#24378;&#22270;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#30452;&#25509;&#26041;&#27861;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direct
&lt;/p&gt;</description></item><item><title>PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09793</link><description>&lt;p&gt;
PatchAD: &#22522;&#20110;&#22359;&#30340;MLP-Mixer&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09793
&lt;/p&gt;
&lt;p&gt;
PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#20107;&#20214;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#22312;&#32570;&#20047;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#27491;&#24120;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#22815;&#36731;&#37327;&#32423;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PatchAD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#34920;&#24449;&#25552;&#21462;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PatchAD&#30001;&#22235;&#20010;&#29420;&#29305;&#30340;MLP Mixer&#32452;&#25104;&#65292;&#19987;&#38376;&#21033;&#29992;MLP&#26550;&#26500;&#23454;&#29616;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21019;&#26032;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#32531;&#35299;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.09671</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65306;&#19968;&#31181;&#22810;&#26679;&#21270;&#20998;&#24067;&#21305;&#37197;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65288;UDT&#65289;&#26088;&#22312;&#25214;&#21040;&#23558;&#19968;&#20010;&#39046;&#22495;&#30340;&#26679;&#26412;&#65288;&#20363;&#22914;&#32032;&#25551;&#65289;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#29031;&#29255;&#65289;&#30340;&#20989;&#25968;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#39640;&#23618;&#35821;&#20041;&#24847;&#20041;&#65288;&#20063;&#31216;&#20026;&#8220;&#20869;&#23481;&#8221;&#65289;&#12290;&#36825;&#20123;&#36716;&#25442;&#20989;&#25968;&#36890;&#24120;&#36890;&#36807;&#36716;&#25442;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23547;&#25214;&#12290;CycleGAN&#21487;&#20197;&#35828;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#25351;&#20986;CycleGAN&#21450;&#20854;&#21464;&#20307;&#21487;&#33021;&#26080;&#27861;&#35782;&#21035;&#25152;&#38656;&#30340;&#36716;&#25442;&#20989;&#25968;&#65292;&#24182;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#23616;&#38480;&#24615;&#28304;&#20110;&#23398;&#20064;&#20934;&#21017;&#35299;&#31354;&#38388;&#20013;&#23384;&#22312;&#22810;&#20010;&#36716;&#25442;&#20989;&#25968;&#65292;&#31216;&#20026;&#8220;&#20445;&#24230;&#33258;&#21516;&#26500;&#65288;MPA&#65289;&#8221;&#12290;&#23613;&#31649;&#24847;&#35782;&#21040;&#20102;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#31350;&#20102;&#26680;&#24515;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;MPA&#28040;&#38500;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2401.09451</link><description>&lt;p&gt;
&#25193;&#25955;&#39537;&#21160;&#30340;&#20998;&#23376;&#26500;&#35937;&#39044;&#27979;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20108;&#32500;&#22270;&#24418;&#34920;&#31034;&#20013;&#25512;&#26029;&#20986;&#19977;&#32500;&#20998;&#23376;&#26500;&#22411;&#30340;&#20219;&#21153;&#22312;&#35745;&#31639;&#21270;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#23545;&#25105;&#20204;&#29702;&#35299;&#20998;&#23376;&#26426;&#21046;&#21644;&#30456;&#20114;&#20316;&#29992;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25512;&#21160;&#20102;&#36825;&#31181;&#39044;&#27979;&#24314;&#27169;&#31934;&#24230;&#30340;&#31361;&#30772;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20998;&#21449;&#31574;&#30053;&#65306;&#39318;&#20808;&#20272;&#35745;&#21407;&#23376;&#38388;&#36317;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#20915;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#26469;&#22609;&#36896;&#20998;&#23376;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39034;&#24207;&#26041;&#27861;&#26377;&#26102;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#21040;&#23616;&#37096;&#21407;&#23376;&#25490;&#21015;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25439;&#23475;&#32467;&#26524;&#32467;&#26500;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#19981;&#36275;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21069;&#21355;&#30340;&#29983;&#25104;&#26694;&#26550;&#65306;\method{}&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36880;&#27493;&#21487;&#25512;&#24191;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#36890;&#36807;&#23545;&#31181;&#26063;&#20027;&#20041;&#30340;&#27010;&#24565;&#21270;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;XLM-R&#21644;XLM-R-Racismo&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#31181;&#26063;&#20027;&#20041;&#20998;&#31867;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.09333</link><description>&lt;p&gt;
&#26426;&#22120;&#33021;&#22815;&#30475;&#21040;&#39068;&#33394;&#65306;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#30340;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora. (arXiv:2401.09333v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36880;&#27493;&#21487;&#25512;&#24191;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#36890;&#36807;&#23545;&#31181;&#26063;&#20027;&#20041;&#30340;&#27010;&#24565;&#21270;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;XLM-R&#21644;XLM-R-Racismo&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#31181;&#26063;&#20027;&#20041;&#20998;&#31867;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35782;&#21035;&#21644;&#20998;&#31867;&#25991;&#26412;&#20013;&#30340;&#31181;&#26063;&#20027;&#20041;&#35821;&#35328;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23567;&#35268;&#27169;&#30340;&#36136;&#24615;&#26041;&#27861;&#25110;&#22823;&#35268;&#27169;&#30340;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#26126;&#26174;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36880;&#27493;&#21487;&#25512;&#24191;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#21516;&#24418;&#24335;&#30340;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#31181;&#26063;&#20027;&#20041;&#21450;&#20854;&#19981;&#21516;&#34920;&#29616;&#24418;&#24335;&#36827;&#34892;&#27010;&#24565;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31181;&#26063;&#20027;&#20041;&#34920;&#29616;&#24418;&#24335;&#32622;&#20110;&#24863;&#20852;&#36259;&#30340;&#26102;&#38388;&#21644;&#22320;&#28857;&#32972;&#26223;&#19979;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#35782;&#21035;&#23427;&#20204;&#30340;&#35805;&#35821;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;XLM-RoBERTa&#65288;XLM-R&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#30340;&#36328;&#35821;&#35328;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;XLM-R&#21644;XLM-R-Racismo&#65288;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#23545;&#31181;&#26063;&#20027;&#20041;&#36827;&#34892;&#20998;&#31867;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#28041;&#21450;2018&#24180;&#33267;2021&#24180;&#21380;&#29916;&#22810;&#23572;&#26412;&#22303;&#32676;&#20307;&#30340;&#25512;&#25991;&#35821;&#26009;&#24211;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods to identify and classify racist language in text rely on small-n qualitative approaches or large-n approaches focusing exclusively on overt forms of racist discourse. This article provides a step-by-step generalizable guideline to identify and classify different forms of racist discourse in large corpora. In our approach, we start by conceptualizing racism and its different manifestations. We then contextualize these racist manifestations to the time and place of interest, which allows researchers to identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a cross-lingual model for supervised text classification with a cutting-edge contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our pretrained model, outperform other state-of-the-art approaches in classifying racism in large corpora. We illustrate our approach using a corpus of tweets relating to the Ecuadorian ind\'igena community between 2018 and 2021.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#65292;&#20854;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#65292;&#23548;&#33268;&#22312;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09031</link><description>&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65306;&#26102;&#38388;&#27493;&#24341;&#36215;&#30340;&#23545;&#24433;&#21709;&#20272;&#35745;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation. (arXiv:2401.09031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#65292;&#20854;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#65292;&#23548;&#33268;&#22312;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#21487;&#20197;&#23558;&#27169;&#22411;&#34892;&#20026;&#36861;&#28335;&#21040;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20026;&#29702;&#35299;&#8220;&#40657;&#31665;&#8221;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#20043;&#38388;&#30340;&#21487;&#37327;&#21270;&#32852;&#31995;&#65292;&#20294;&#22312;&#19982;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#25193;&#25955;&#27169;&#22411;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#38754;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#31995;&#21015;&#26102;&#38388;&#27493;&#39588;&#32780;&#19981;&#26159;&#20043;&#21069;&#30340;&#30636;&#26102;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#25805;&#20316;&#65292;&#23545;&#30452;&#25509;&#23558;&#29616;&#26377;&#26694;&#26550;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diffusion-TracIn&#65292;&#23427;&#21253;&#21547;&#20102;&#36825;&#31181;&#26102;&#38388;&#21160;&#21147;&#23398;&#65292;&#24182;&#35266;&#23519;&#21040;&#26679;&#26412;&#30340;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#12290;&#36825;&#31181;&#36235;&#21183;&#23548;&#33268;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#23588;&#20026;&#26126;&#26174;&#65292;&#23548;&#33268;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24433;&#21709;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data attribution methods trace model behavior back to its training dataset, offering an effective approach to better understand ``black-box'' neural networks. While prior research has established quantifiable links between model output and training data in diverse settings, interpreting diffusion model outputs in relation to training samples remains underexplored. In particular, diffusion models operate over a sequence of timesteps instead of instantaneous input-output relationships in previous contexts, posing a significant challenge to extend existing frameworks to diffusion models directly. Notably, we present Diffusion-TracIn that incorporates this temporal dynamics and observe that samples' loss gradient norms are highly dependent on timestep. This trend leads to a prominent bias in influence estimation, and is particularly noticeable for samples trained on large-norm-inducing timesteps, causing them to be generally influential. To mitigate this effect, we introduce Diffusion-ReTr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AntiPhishStack&#30340;LSTM-based&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#20248;&#21270;&#32593;&#32476;&#38035;&#40060;URL&#30340;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;URL&#21644;&#23383;&#31526;&#32423;TF-IDF&#29305;&#24449;&#36827;&#34892;&#23545;&#31216;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#23545;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#23041;&#32961;&#30340;&#24212;&#23545;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#22686;&#21152;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#21018;&#24615;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.08947</link><description>&lt;p&gt;
AntiPhishStack&#65306;&#22522;&#20110;LSTM&#30340;&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#29992;&#20110;&#20248;&#21270;&#32593;&#32476;&#38035;&#40060;URL&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized Phishing URLs Detection. (arXiv:2401.08947v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AntiPhishStack&#30340;LSTM-based&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#20248;&#21270;&#32593;&#32476;&#38035;&#40060;URL&#30340;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;URL&#21644;&#23383;&#31526;&#32423;TF-IDF&#29305;&#24449;&#36827;&#34892;&#23545;&#31216;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#23545;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#23041;&#32961;&#30340;&#24212;&#23545;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#22686;&#21152;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#21018;&#24615;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38761;&#21629;&#24615;&#30340;&#22312;&#32447;&#32593;&#32476;&#26381;&#21153;&#30340;&#19981;&#26029;&#20381;&#36182;&#24341;&#20837;&#20102;&#26356;&#39640;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#23613;&#31649;&#26377;&#24191;&#27867;&#30340;&#23433;&#20840;&#25514;&#26045;&#65292;&#32593;&#32476;&#38035;&#40060;&#20173;&#28982;&#24102;&#26469;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#25163;&#21160;&#29305;&#24449;&#30340;&#32593;&#32476;&#38035;&#40060;&#31995;&#32479;&#22312;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#31574;&#30053;&#19978;&#24456;&#22256;&#38590;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#35299;&#20915;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#21644;&#24694;&#24847;URL&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AntiPhishStack&#30340;&#20004;&#38454;&#27573;&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#65292;&#26088;&#22312;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#35813;&#27169;&#22411;&#23545;URL&#21644;&#23383;&#31526;&#32423;TF-IDF&#29305;&#24449;&#36827;&#34892;&#23545;&#31216;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#23545;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#23041;&#32961;&#30340;&#24212;&#23545;&#33021;&#21147;&#12290;&#31532;&#19968;&#38454;&#27573;&#22312;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19978;&#35757;&#32451;&#29305;&#24449;&#65292;&#37319;&#29992;K&#25240;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#22343;&#20540;&#39044;&#27979;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#31532;&#20108;&#38454;&#27573;&#37319;&#29992;&#20004;&#23618;&#22534;&#21472;LSTM&#32593;&#32476;&#65292;&#37197;&#21512;&#20116;&#20010;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#36827;&#34892;&#21160;&#24577;&#32534;&#35793;&#65292;&#30830;&#20445;&#22312;&#36825;&#20123;&#29305;&#24449;&#19978;&#33719;&#24471;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#21152;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#21018;&#24615;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating reliance on revolutionary online web services has introduced heightened security risks, with persistent challenges posed by phishing despite extensive security measures. Traditional phishing systems, reliant on machine learning and manual features, struggle with evolving tactics. Recent advances in deep learning offer promising avenues for tackling novel phishing challenges and malicious URLs. This paper introduces a two-phase stack generalized model named AntiPhishStack, designed to detect phishing sites. The model leverages the learning of URLs and character-level TF-IDF features symmetrically, enhancing its ability to combat emerging phishing threats. In Phase I, features are trained on a base machine learning classifier, employing K-fold cross-validation for robust mean prediction. Phase II employs a two-layered stacked-based LSTM network with five adaptive optimizers for dynamic compilation, ensuring premier prediction on these features. Additionally, the symmetrica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35757;&#32451;&#38598;&#32500;&#24230;&#26377;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35748;&#20026;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;&#21487;&#33021;&#26159;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#30340;&#37096;&#20998;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2401.08865</link><description>&lt;p&gt;
Intrinsic Dataset Properties&#23545;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65306;&#25581;&#31034;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#23398;&#20064;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35757;&#32451;&#38598;&#32500;&#24230;&#26377;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35748;&#20026;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;&#21487;&#33021;&#26159;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#30340;&#37096;&#20998;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#36825;&#22312;&#20174;&#33258;&#28982;&#22270;&#20687;&#21040;&#20854;&#20182;&#19987;&#38376;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#22270;&#20687;&#65289;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26102;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35757;&#32451;&#38598;&#30340;&#22266;&#26377;&#32500;&#24230;($d_{data}$)&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#38169;&#35823;&#19968;&#33324;&#20250;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#65288;&#25918;&#23556;&#23398;&#65289;&#21644;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#20043;&#38388;&#30340;&#36825;&#31181;&#20851;&#31995;&#30340;&#38497;&#23789;&#31243;&#24230;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#19988;&#26080;&#29616;&#26377;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#24182;&#32463;&#39564;&#35777;&#19968;&#20010;&#19982;$d_{data}$&#30456;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#26469;&#35299;&#20915;&#36825;&#20010;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#32771;&#34385;&#21040;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;($K_F$)&#36825;&#19968;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#37096;&#20998;&#35299;&#37322;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#26174;&#33879;&#32553;&#25918;&#24046;&#24322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#27979;&#37327;&#36825;&#19968;&#25351;&#26631;&#21487;&#20197;&#25552;&#20379;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602;&#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#21457;&#29616;IFI6&#21644;IFI27&#31561;&#22522;&#22240;&#20316;&#20026;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#33021;&#26377;&#25928;&#20998;&#31867;&#19981;&#21516;&#38454;&#27573;&#30340;&#20234;&#27874;&#25289;&#24863;&#26579;&#12290;</title><link>http://arxiv.org/abs/2401.08738</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289;&#20013;&#20234;&#27874;&#25289;&#30149;&#27602;&#23545;&#22522;&#22240;&#34920;&#36798;&#24433;&#21709;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates. (arXiv:2401.08738v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602;&#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#21457;&#29616;IFI6&#21644;IFI27&#31561;&#22522;&#22240;&#20316;&#20026;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#33021;&#26377;&#25928;&#20998;&#31867;&#19981;&#21516;&#38454;&#27573;&#30340;&#20234;&#27874;&#25289;&#24863;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#30417;&#30563;&#22411;&#24133;&#24230;-&#39640;&#24230;&#35780;&#20998; (SMAS) &#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602; (EBOV) &#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289; (NHPs) &#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#26469;&#33258;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602;&#30340; NHPs &#30340;NanoString&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;SMAS&#31995;&#32479;&#36827;&#34892;&#24494;&#22937;&#30340;&#23487;&#20027;-&#30149;&#21407;&#20307;&#30456;&#20114;&#20316;&#29992;&#20998;&#26512;&#12290;SMAS&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;&#21644;&#34920;&#36798;&#21464;&#21270;&#30340;&#22522;&#22240;&#36873;&#25321;&#65292;&#37319;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#22914;&#36923;&#36753;&#22238;&#24402;&#20197;&#20934;&#30830;&#21306;&#20998;RT-qPCR&#38451;&#24615;&#21644;&#38452;&#24615;NHP&#26679;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#39033;&#37325;&#35201;&#21457;&#29616;&#26159;&#37492;&#23450;&#20102;IFI6&#21644;IFI27&#20316;&#20026;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#34920;&#29616;&#20986;100%&#30340;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215; (AUC) &#25351;&#26631;&#22312;&#20998;&#31867;&#19981;&#21516;&#38454;&#27573;&#30340;&#20234;&#27874;&#25289;&#24863;&#26579;&#20013;&#12290;&#38500;IFI6&#21644;IFI27&#22806;&#65292;&#22522;&#22240;&#22914;MX1&#65292;OAS1&#21644;ISG15&#26174;&#33879;&#19978;&#35843;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This study introduces the Supervised Magnitude-Altitude Scoring (SMAS) methodology, a machine learning-based approach, for analyzing gene expression data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV). We utilize a comprehensive dataset of NanoString gene expression profiles from Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen interaction analysis. SMAS effectively combines gene selection based on statistical significance and expression changes, employing linear classifiers such as logistic regression to accurately differentiate between RT-qPCR positive and negative NHP samples. A key finding of our research is the identification of IFI6 and IFI27 as critical biomarkers, demonstrating exceptional predictive performance with 100% accuracy and Area Under the Curve (AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6 and IFI27, genes, including MX1, OAS1, and ISG15, were significantly upregulated, highlighting the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;WAVES&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#25972;&#21512;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#22810;&#26679;&#21270;&#21387;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#27700;&#21360;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08573</link><description>&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Robustness of Image Watermarks. (arXiv:2401.08573v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;WAVES&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#25972;&#21512;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#22810;&#26679;&#21270;&#21387;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#27700;&#21360;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#24369;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;WAVES&#65288;&#36890;&#36807;&#22686;&#24378;&#30340;&#21387;&#21147;&#27979;&#35797;&#36827;&#34892;&#27700;&#21360;&#20998;&#26512;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;WAVES&#25972;&#21512;&#20102;&#26816;&#27979;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#30001;&#22810;&#26679;&#21270;&#21387;&#21147;&#27979;&#35797;&#32452;&#25104;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#12290;WAVES&#20013;&#30340;&#25915;&#20987;&#33539;&#22260;&#20174;&#20256;&#32479;&#30340;&#22270;&#20687;&#22833;&#30495;&#21040;&#25193;&#25955;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#39640;&#32423;&#21644;&#26032;&#39062;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;&#20004;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#22270;&#20687;&#36136;&#37327;&#38477;&#20302;&#31243;&#24230;&#21644;&#25915;&#20987;&#21518;&#27700;&#21360;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#24615;&#33021;&#19982;&#36136;&#37327;2D&#22270;&#65292;&#21464;&#21270;&#22522;&#20110;&#20960;&#31181;&#31361;&#20986;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#28982;&#21518;&#29992;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#26032;&#39062;&#26041;&#27861;&#23558;&#23427;&#20204;&#32858;&#21512;&#65292;&#20174;&#32780;&#20026;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#21644;&#25915;&#20987;&#33021;&#21147;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#30011;&#38754;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#35780;&#20272;&#25581;&#31034;&#20102;&#20197;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the weaknesses of image watermarking techniques. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel benchmark for assessing watermark robustness, overcoming the limitations of current evaluation methods.WAVES integrates detection and identification tasks, and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced and novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. We develop a series of Performance vs. Quality 2D plots, varying over several prominent image similarity metrics, which are then aggregated in a heuristically novel manner to paint an overall picture of watermark robustness and attack potency. Our comprehensive evaluation reveals previously undetected vulnerabilities of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21463;&#38480;&#23545;&#25239;&#22810;&#38754;&#20307;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07991</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21463;&#38480;&#23545;&#25239;&#22810;&#38754;&#20307;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes. (arXiv:2401.07991v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21463;&#38480;&#23545;&#25239;&#22810;&#38754;&#20307;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#21487;&#33021;&#20250;&#34987;&#29983;&#25104;&#30340;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#27450;&#39575;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;DNNs&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#28155;&#21152;&#21040;&#24178;&#20928;&#26679;&#26412;&#30340;&#33539;&#25968;&#26377;&#30028;&#25200;&#21160;&#30340;&#36755;&#20986;&#38598;&#26469;&#35757;&#32451;&#40065;&#26834;&#30340;DNNs&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38598;&#21512;&#31216;&#20026;&#23545;&#25239;&#22810;&#38754;&#20307;&#65292;&#27599;&#20010;&#24178;&#20928;&#26679;&#26412;&#37117;&#26377;&#30456;&#24212;&#30340;&#23545;&#25239;&#22810;&#38754;&#20307;&#12290;&#23454;&#38469;&#19978;&#65292;&#22914;&#26524;&#25152;&#26377;&#26679;&#26412;&#30340;&#30456;&#24212;&#22810;&#38754;&#20307;&#37117;&#26159;&#32039;&#20945;&#30340;&#65292;&#21363;&#23427;&#20204;&#19981;&#19982;DNN&#30340;&#20915;&#31574;&#36793;&#30028;&#30456;&#20132;&#65292;&#37027;&#20040;DNN&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#20869;&#37096;&#24037;&#20316;&#26159;&#22522;&#20110;&#23398;&#20064;&#21463;&#38480;&#30340;&#23545;&#25239;&#22810;&#38754;&#20307;(CAP)&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CAP&#22312;&#25913;&#21892;&#27169;&#22411;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;(&#21253;&#25324;AutoAttack)&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) could be deceived by generating human-imperceptible perturbations of clean samples. Therefore, enhancing the robustness of DNNs against adversarial attacks is a crucial task. In this paper, we aim to train robust DNNs by limiting the set of outputs reachable via a norm-bounded perturbation added to a clean sample. We refer to this set as adversarial polytope, and each clean sample has a respective adversarial polytope. Indeed, if the respective polytopes for all the samples are compact such that they do not intersect the decision boundaries of the DNN, then the DNN is robust against adversarial samples. Hence, the inner-working of our algorithm is based on learning \textbf{c}onfined \textbf{a}dversarial \textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we demonstrate the effectiveness of CAP over existing adversarial robustness methods in improving the robustness of models against state-of-the-art attacks including AutoAttack.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.07656</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24615;&#33021;&#26356;&#22909;&#30340;POMDP&#31574;&#30053;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Explainable and Better Performing Representations of POMDP Strategies. (arXiv:2401.07656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#35760;&#24518;&#12290;&#19968;&#31181;&#34920;&#31034;&#36825;&#31181;&#35760;&#24518;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;L*&#31639;&#27861;&#23398;&#20064;&#31574;&#30053;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#31574;&#30053;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#20307;&#31215;&#26174;&#33879;&#26356;&#23567;&#65292;&#22240;&#27492;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#25913;&#21892;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#19982;&#30452;&#25509;&#20174;POMDP&#21512;&#25104;&#33258;&#21160;&#26426;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19981;&#21487;&#27604;&#25311;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using a modification of the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;ICON&#65289;&#35299;&#20915;PDE&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;ICON&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;PDEs&#12290;</title><link>http://arxiv.org/abs/2401.07364</link><description>&lt;p&gt;
PDE&#24191;&#20041;&#21270;&#30340;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#32593;&#32476;&#65306;&#23545;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar Nonlinear Conservation Laws. (arXiv:2401.07364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;ICON&#65289;&#35299;&#20915;PDE&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;ICON&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;PDEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;&#19968;&#20010;&#38024;&#23545;&#21508;&#31181;PDE&#30456;&#20851;&#31185;&#23398;&#23398;&#20064;&#20219;&#21153;&#30340;&#21333;&#19968;&#22823;&#27169;&#22411;&#65311;&#36825;&#20010;&#27169;&#22411;&#33021;&#21542;&#22312;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#21040;&#26032;&#30340;PDE&#65292;&#29978;&#33267;&#26159;&#26032;&#24418;&#24335;&#30340;PDE&#65311;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#23398;&#20064;&#21450;&#20854;&#23545;&#24212;&#27169;&#22411;In-Context Operator Networks&#65288;ICON&#65289;&#20195;&#34920;&#20102;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#20043;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;ICON&#23545;&#31532;&#19968;&#20010;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;ICON&#35299;&#20915;PDE&#38382;&#39064;&#30340;&#35814;&#32454;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;ICON&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#25968;&#25454;&#25552;&#31034;&#26469;&#36827;&#34892;&#19981;&#21516;&#26041;&#31243;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#31532;&#20108;&#20010;&#38382;&#39064;&#30340;&#31215;&#26497;&#35777;&#25454;&#65292;&#21363;ICON&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#19968;&#20123;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;PDE&#12290;&#36825;&#36890;&#36807;&#23545;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#30340;&#30740;&#31350;&#21152;&#20197;&#35828;&#26126;&#65292;&#36825;&#26159;&#19968;&#31867;&#20855;&#26377;&#26102;&#38388;&#28436;&#21270;&#30340;PDE&#26063;&#32676;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#25193;&#23637;ICON&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we build a single large model for a wide range of PDE-related scientific learning tasks? Can this model generalize to new PDEs, even of new forms, without any fine-tuning? In-context operator learning and the corresponding model In-Context Operator Networks (ICON) represent an initial exploration of these questions. The capability of ICON regarding the first question has been demonstrated previously. In this paper, we present a detailed methodology for solving PDE problems with ICON, and show how a single ICON model can make forward and reverse predictions for different equations with different strides, provided with appropriately designed data prompts. We show the positive evidence to the second question, i.e., ICON can generalize well to some PDEs with new forms without any fine-tuning. This is exemplified through a study on 1D scalar nonlinear conservation laws, a family of PDEs with temporal evolution. We also show how to broaden the range of problems that an ICON model can add
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.06344</link><description>&lt;p&gt;
&#36229;&#32423;-STTN&#65306;&#31038;&#20132;&#32676;&#20307;&#24863;&#30693;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#29992;&#20110;&#20154;&#20307;&#36712;&#36857;&#39044;&#27979;&#19982;&#36229;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#26381;&#21153;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#39044;&#27979;&#25317;&#25380;&#30340;&#24847;&#22270;&#21644;&#36712;&#36857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29702;&#35299;&#29615;&#22659;&#21160;&#24577;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#19981;&#20165;&#22240;&#20026;&#23545;&#24314;&#27169;&#25104;&#23545;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#30721;&#25317;&#25380;&#22330;&#26223;&#20013;&#20840;&#38754;&#30340;&#25104;&#23545;&#21644;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hyper-STTN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#22312;Hyper-STTN&#20013;&#65292;&#36890;&#36807;&#19968;&#32452;&#22810;&#23610;&#24230;&#36229;&#22270;&#26500;&#24314;&#20102;&#25317;&#25380;&#30340;&#32676;&#20307;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#36229;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#32676;&#20307;&#22823;&#23567;&#65292;&#36890;&#36807;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#27010;&#29575;&#30340;&#36229;&#22270;&#35889;&#21367;&#31215;&#36827;&#34892;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#22312;&#31354;&#38388;-&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#23545;&#29031;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer net
&lt;/p&gt;</description></item><item><title>DFU&#26159;&#19968;&#31181;&#23610;&#24230;&#40065;&#26834;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06144</link><description>&lt;p&gt;
DFU: &#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#30340;&#23610;&#24230;&#40065;&#26834;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DFU: scale-robust diffusion model for zero-shot super-resolution image generation. (arXiv:2401.06144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06144
&lt;/p&gt;
&lt;p&gt;
DFU&#26159;&#19968;&#31181;&#23610;&#24230;&#40065;&#26834;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#22266;&#23450;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#27809;&#26377;&#30456;&#24212;&#20998;&#36776;&#29575;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#24456;&#38590;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#12290;&#20511;&#37492;&#25805;&#20316;&#31526;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;Dual-FNO UNet (DFU)&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#21516;&#26102;&#32452;&#21512;&#31354;&#38388;&#21644;&#20809;&#35889;&#20449;&#24687;&#26469;&#36817;&#20284;&#35780;&#20998;&#25805;&#20316;&#31526;&#12290;&#23558;DFU&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#21487;&#25193;&#23637;&#24615;&#65306;1&#65289;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#21516;&#26102;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;FID&#65292;&#32780;&#21333;&#19968;&#22266;&#23450;&#20998;&#36776;&#29575;&#30340;&#35757;&#32451;&#21017;&#19981;&#33021;&#23454;&#29616;&#65307;2&#65289;DFU&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#35757;&#32451;&#20998;&#36776;&#29575;&#20043;&#22806;&#65292;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#21327;&#35843;&#12289;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21363;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#65307;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#35843;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;FID&#20026;11.3&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion generative models have achieved remarkable success in generating images with a fixed resolution. However, existing models have limited ability to generalize to different resolutions when training data at those resolutions are not available. Leveraging techniques from operator learning, we present a novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the score operator by combining both spatial and spectral information at multiple resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1) simultaneously training on multiple resolutions improves FID over training at any single fixed resolution; 2) DFU generalizes beyond its training resolutions, allowing for coherent, high-fidelity generation at higher-resolutions with the same model, i.e. zero-shot super-resolution image-generation; 3) we propose a fine-tuning strategy to further enhance the zero-shot super-resolution image-generation capability of our model, leading to a FID of 11.3 at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03506</link><description>&lt;p&gt;
DiarizationLM: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#30446;&#26631;&#65292;&#22914;&#25913;&#21892;&#20998;&#31163;&#23545;&#35805;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#65292;&#25110;&#20943;&#23569;&#35789;&#32423;&#20998;&#31163;&#38169;&#35823;&#29575;&#65288;WDER&#65289;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#34987;&#34920;&#31034;&#20026;&#19968;&#31181;&#32039;&#20945;&#30340;&#25991;&#26412;&#26684;&#24335;&#65292;&#20854;&#21253;&#21547;&#22312;&#19968;&#20010;&#21487;&#36873;&#25321;&#35843;&#25972;&#30340;LLM&#30340;&#25552;&#31034;&#20013;&#12290;LLM&#30340;&#36755;&#20986;&#21487;&#20197;&#20316;&#20026;&#25152;&#38656;&#25913;&#36827;&#30340;&#31934;&#32454;&#21270;&#20998;&#31163;&#32467;&#26524;&#12290;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;ASR&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#22312;Fisher&#30005;&#35805;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23558;WDER&#38477;&#20302;55.5%&#65292;&#22312;Callhome&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#38477;&#20302;44.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
&lt;/p&gt;</description></item><item><title>&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.03197</link><description>&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#19982;&#31574;&#30053;&#22686;&#24378;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03197
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#30528;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#12290;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#21644;&#22312;&#32447;&#25628;&#32034;&#65288;&#22914;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65289;&#12290;&#21069;&#32773;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65288;&#36890;&#24120;&#22312;&#25191;&#34892;&#20043;&#21069;&#23436;&#25104;&#65289;&#65292;&#32780;&#21518;&#32773;&#22312;&#20915;&#31574;&#26102;&#20351;&#29992;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#26377;&#21069;&#26223;&#30340;&#34892;&#21160;&#36712;&#36857;&#12290;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20195;&#29702;&#25805;&#20316;&#30340;&#29615;&#22659;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20004;&#31181;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#37117;&#23384;&#22312;&#32570;&#38519;--&#19968;&#26041;&#38754;&#65292;&#25191;&#34892;&#20043;&#21069;&#23398;&#20064;&#30340;&#31574;&#30053;&#22312;&#29615;&#22659;&#25913;&#21464;&#26102;&#21464;&#24471;&#38472;&#26087;&#65292;&#37325;&#26032;&#23398;&#20064;&#38656;&#35201;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#32447;&#25628;&#32034;&#22312;&#20801;&#35768;&#30340;&#36816;&#34892;&#26102;&#38388;&#26377;&#38480;&#26102;&#21487;&#33021;&#20250;&#36820;&#22238;&#27425;&#20248;&#34892;&#21160;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConPreDiff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#26469;&#25913;&#21892;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#35299;&#30721;&#22120;&#40723;&#21169;&#27599;&#20010;&#28857;&#39044;&#27979;&#20854;&#37051;&#22495;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21435;&#38500;&#35299;&#30721;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#37325;&#24314;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.02015</link><description>&lt;p&gt;
&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#19982;&#19978;&#19979;&#25991;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConPreDiff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#26469;&#25913;&#21892;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#35299;&#30721;&#22120;&#40723;&#21169;&#27599;&#20010;&#28857;&#39044;&#27979;&#20854;&#37051;&#22495;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21435;&#38500;&#35299;&#30721;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#26497;&#22823;&#25552;&#21319;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#20687;&#32032;&#25110;&#29305;&#24449;&#32422;&#26463;&#22312;&#31354;&#38388;&#36724;&#19978;&#23545;&#25439;&#22351;&#22270;&#20687;&#36827;&#34892;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28857;&#23545;&#28857;&#30340;&#37325;&#24314;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#20445;&#30041;&#27599;&#20010;&#39044;&#27979;&#20687;&#32032;/&#29305;&#24449;&#30340;&#37051;&#22495;&#19978;&#19979;&#25991;&#65292;&#24433;&#21709;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;ConPreDiff&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#25193;&#25955;&#21435;&#22122;&#22359;&#30340;&#26411;&#31471;&#22686;&#21152;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#35299;&#30721;&#22120;&#65292;&#26126;&#30830;&#22320;&#40723;&#21169;&#27599;&#20010;&#28857;&#39044;&#27979;&#20854;&#37051;&#22495;&#19978;&#19979;&#25991;&#65288;&#21363;&#22810;&#27493;&#38271;&#29305;&#24449;/&#20196;&#29260;/&#20687;&#32032;&#65289;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21435;&#38500;&#35299;&#30721;&#22120;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27599;&#20010;&#28857;&#21487;&#20197;&#26356;&#22909;&#22320;&#37325;&#24314;&#33258;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01841</link><description>&lt;p&gt;
&#25353;&#29031;&#20320;&#30340;&#23398;&#20064;&#34892;&#21160;&#65306;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#22788;&#29702;&#38750;&#31283;&#24577;&#29615;&#22659;&#26159;&#19968;&#20010;&#22522;&#26412;&#65288;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#26410;&#35299;&#20915;&#30340;&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#22806;&#37096;&#29615;&#22659;&#26465;&#20214;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#31867;&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NSMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NSMDP&#20915;&#31574;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#35774;&#24403;&#21069;&#26102;&#21051;&#26356;&#26032;&#30340;&#29615;&#22659;&#21160;&#24577;&#26159;&#24050;&#30693;&#30340;&#65288;&#23613;&#31649;&#26410;&#26469;&#21160;&#24577;&#21487;&#33021;&#20250;&#25913;&#21464;&#65289;&#65307;&#20854;&#27425;&#65292;&#35268;&#21010;&#36807;&#31243;&#20027;&#35201;&#26159;&#24754;&#35266;&#30340;&#65292;&#21363;&#20195;&#29702;&#20154;&#20250;&#8220;&#23433;&#20840;&#34892;&#21160;&#8221;&#20197;&#32771;&#34385;&#29615;&#22659;&#30340;&#38750;&#31283;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#26080;&#25928;&#30340;-&#26356;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#24456;&#23569;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#24403;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#26102;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#26356;&#26032;&#30340;&#21160;&#24577;&#24182;&#36991;&#20813;&#24754;&#35266;&#65292;&#33267;&#23569;&#22312;&#20854;&#23545;&#21160;&#24577;&#26377;&#20449;&#24515;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21464;&#20307;NPG-HM&#65292;&#37319;&#29992;Hessian&#36741;&#21161;&#21160;&#37327;&#25216;&#26415;&#36827;&#34892;&#26041;&#24046;&#20943;&#23567;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35299;&#20915;&#23376;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;NPG-HM&#22312;&#36890;&#29992;Fisher&#38750;&#36864;&#21270;&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#21487;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#21518;&#36845;&#20195;&#30340;$\epsilon$-&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#22312;Mujoco&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01084</link><description>&lt;p&gt;
&#20855;&#26377;Hessian&#36741;&#21161;&#21160;&#37327;&#26041;&#24046;&#20943;&#23567;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20840;&#23616;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction. (arXiv:2401.01084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21464;&#20307;NPG-HM&#65292;&#37319;&#29992;Hessian&#36741;&#21161;&#21160;&#37327;&#25216;&#26415;&#36827;&#34892;&#26041;&#24046;&#20943;&#23567;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35299;&#20915;&#23376;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;NPG-HM&#22312;&#36890;&#29992;Fisher&#38750;&#36864;&#21270;&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#21487;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#21518;&#36845;&#20195;&#30340;$\epsilon$-&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#22312;Mujoco&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;NPG&#21464;&#20307;&#65292;&#21629;&#21517;&#20026;NPG-HM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Hessian&#36741;&#21161;&#21160;&#37327;&#25216;&#26415;&#36827;&#34892;&#26041;&#24046;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#23376;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NPG-HM&#21487;&#20197;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#26368;&#21518;&#36845;&#20195;&#30340;$\epsilon$-&#26368;&#20248;&#24615;&#65292;&#36825;&#26159;&#22312;&#36890;&#29992;&#30340;Fisher&#38750;&#36864;&#21270;&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24050;&#30693;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#25910;&#25947;&#20998;&#26512;&#24314;&#31435;&#22312;&#38024;&#23545;NPG&#30340;&#26494;&#24347;&#24369;&#26799;&#24230;&#20248;&#21183;&#24615;&#36136;&#20197;&#21450;&#22788;&#29702;&#23376;&#38382;&#39064;&#26102;&#30340;&#38169;&#35823;&#20998;&#35299;&#30340;&#20860;&#23481;&#20989;&#25968;&#36924;&#36817;&#26694;&#26550;&#19979;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;Mujoco&#29615;&#22659;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;NPG-HM&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31574;&#30053;&#26041;&#27861;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural policy gradient (NPG) and its variants are widely-used policy search methods in reinforcement learning. Inspired by prior work, a new NPG variant coined NPG-HM is developed in this paper, which utilizes the Hessian-aided momentum technique for variance reduction, while the sub-problem is solved via the stochastic gradient descent method. It is shown that NPG-HM can achieve the global last iterate $\epsilon$-optimality with a sample complexity of $\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policy gradient type methods under the generic Fisher non-degenerate policy parameterizations. The convergence analysis is built upon a relaxed weak gradient dominance property tailored for NPG under the compatible function approximation framework, as well as a neat way to decompose the error when handling the sub-problem. Moreover, numerical experiments on Mujoco-based environments demonstrate the superior performance of NPG-HM over other state-of-the-art policy g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#39640;&#26893;&#29289;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#33719;&#24471;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21516;&#26102;&#36890;&#36807;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#40065;&#26834;&#24615;&#21487;&#33021;&#20197;&#20998;&#31867;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#32780;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#24615;&#33021;&#25439;&#22833;&#33976;&#39311;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.00334</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#21487;&#35299;&#37322;&#24615;&#23548;&#21521;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#39640;&#26893;&#29289;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#33719;&#24471;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21516;&#26102;&#36890;&#36807;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#40065;&#26834;&#24615;&#21487;&#33021;&#20197;&#20998;&#31867;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#32780;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#24615;&#33021;&#25439;&#22833;&#33976;&#39311;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#26893;&#29289;&#21494;&#29255;&#30142;&#30149;&#20998;&#31867;&#65292;&#24182;&#25506;&#32034;&#20102;&#23545;&#25239;&#35757;&#32451;&#12289;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#27169;&#22411;&#21387;&#32553;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22686;&#24378;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#23041;&#32961;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30830;&#20445;&#20934;&#30830;&#20998;&#31867;&#12290;&#20511;&#21161;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#40065;&#26834;&#24615;&#21487;&#33021;&#20197;&#20998;&#31867;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#23545;&#24120;&#35268;&#27979;&#35797;&#30340;&#24615;&#33021;&#25439;&#22833;&#20026;3%-20%&#65292;&#23545;&#23545;&#25239;&#25915;&#20987;&#27979;&#35797;&#30340;&#24615;&#33021;&#25552;&#39640;&#20026;50%-70%&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#22312;&#24615;&#33021;&#31245;&#26377;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#27604;&#22797;&#26434;&#27169;&#22411;&#39640;15-25&#20493;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33976;&#39311;&#20102;&#26356;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on plant leaf disease classification and explores three crucial aspects: adversarial training, model explainability, and model compression. The models' robustness against adversarial attacks is enhanced through adversarial training, ensuring accurate classification even in the presence of threats. Leveraging explainability techniques, we gain insights into the model's decision-making process, improving trust and transparency. Additionally, we explore model compression techniques to optimize computational efficiency while maintaining classification performance. Through our experiments, we determine that on a benchmark dataset, the robustness can be the price of the classification accuracy with performance reductions of 3%-20% for regular tests and gains of 50%-70% for adversarial attack tests. We also demonstrate that a student model can be 15-25 times more computationally efficient for a slight performance reduction, distilling the knowledge of more complex models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36793;&#24066;&#22330;&#20013;&#21305;&#37197;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#36138;&#24515;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#21442;&#19982;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00313</link><description>&lt;p&gt;
&#22312;&#21452;&#36793;&#24066;&#22330;&#20013;&#21305;&#37197;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Matching of Users and Creators in Two-Sided Markets with Departures. (arXiv:2401.00313v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36793;&#24066;&#22330;&#20013;&#21305;&#37197;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#36138;&#24515;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#21442;&#19982;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#35768;&#22810;&#22312;&#32447;&#24179;&#21488;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#32593;&#31449;&#65292;&#37117;&#26159;&#26725;&#25509;&#20869;&#23481;&#21019;&#20316;&#32773;&#19982;&#29992;&#25143;&#30340;&#21452;&#36793;&#24066;&#22330;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#30340;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#29992;&#25143;&#20559;&#22909;&#21644;&#20915;&#31574;&#19978;&#65292;&#24182;&#27809;&#26377;&#21516;&#26102;&#32771;&#34385;&#21040;&#21019;&#20316;&#32773;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20869;&#23481;&#25512;&#33616;&#27169;&#22411;&#65292;&#26126;&#30830;&#20851;&#27880;&#29992;&#25143;-&#20869;&#23481;&#21305;&#37197;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#20854;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#22914;&#26524;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#27809;&#26377;&#36275;&#22815;&#30340;&#21442;&#19982;&#24863;&#65292;&#20182;&#20204;&#37117;&#21487;&#33021;&#27704;&#20037;&#31163;&#24320;&#24179;&#21488;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#26681;&#25454;&#24403;&#21069;&#21305;&#37197;&#30340;&#23454;&#29992;&#24615;&#20915;&#23450;&#26159;&#21542;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#21442;&#19982;&#65306;&#29992;&#25143;&#22522;&#20110;&#25512;&#33616;&#20869;&#23481;&#19982;&#20854;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#21019;&#20316;&#32773;&#21017;&#22522;&#20110;&#20854;&#21463;&#20247;&#35268;&#27169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#36138;&#24515;&#31639;&#27861;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#21019;&#20316;&#32773;&#30340;&#31163;&#24320;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#21442;&#19982;&#24230;&#20219;&#24847;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#32771;&#34385;&#21040;&#21452;&#26041;&#21033;&#30410;&#26368;&#22823;&#21270;&#25972;&#20307;&#21442;&#19982;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many online platforms of today, including social media sites, are two-sided markets bridging content creators and users. Most of the existing literature on platform recommendation algorithms largely focuses on user preferences and decisions, and does not simultaneously address creator incentives. We propose a model of content recommendation that explicitly focuses on the dynamics of user-content matching, with the novel property that both users and creators may leave the platform permanently if they do not experience sufficient engagement. In our model, each player decides to participate at each time step based on utilities derived from the current match: users based on alignment of the recommended content with their preferences, and creators based on their audience size. We show that a user-centric greedy algorithm that does not consider creator departures can result in arbitrarily poor total engagement, relative to an algorithm that maximizes total engagement while accounting for two
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;OOD&#24191;&#20041;&#21270;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#26679;&#21270;&#26041;&#27861;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#19988;&#20165;&#20165;&#36827;&#34892;&#22810;&#26679;&#21270;&#26159;&#19981;&#36275;&#20197;&#23454;&#29616;OOD&#24191;&#20041;&#21270;&#30340;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2312.16313</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#35299;&#26512;OOD&#24191;&#20041;&#21270;&#30340;&#20851;&#38190;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Key Components of OOD Generalization via Diversification. (arXiv:2312.16313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;OOD&#24191;&#20041;&#21270;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#26679;&#21270;&#26041;&#27861;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#19988;&#20165;&#20165;&#36827;&#34892;&#22810;&#26679;&#21270;&#26159;&#19981;&#36275;&#20197;&#23454;&#29616;OOD&#24191;&#20041;&#21270;&#30340;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#38598;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#35299;&#37322;&#35757;&#32451;&#38598;&#21516;&#26679;&#33391;&#22909;&#30340;&#32447;&#32034;&#65292;&#21363;&#23398;&#20064;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#37117;&#20250;&#23548;&#33268;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#27491;&#30830;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#35768;&#22810;&#32447;&#32034;&#21487;&#33021;&#26159;&#34394;&#20551;&#30340;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#22833;&#21435;&#20102;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#22240;&#27492;&#26080;&#27861;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#8220;&#22810;&#26679;&#21270;&#8221;&#26041;&#27861;&#36890;&#36807;&#25214;&#21040;&#20381;&#36182;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#20010;&#19981;&#21516;&#30340;&#20551;&#35774;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36825;&#31867;&#26041;&#27861;&#24182;&#30830;&#23450;&#23545;&#20854;OOD&#24191;&#20041;&#21270;&#33021;&#21147;&#30340;&#36129;&#29486;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;(1) &#22810;&#26679;&#21270;&#26041;&#27861;&#23545;&#20110;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#24403;&#36828;&#31163;&#26041;&#27861;&#29305;&#23450;&#30340;&#26368;&#20339;&#28857;&#26102;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;(2) &#20165;&#20165;&#36827;&#34892;&#22810;&#26679;&#21270;&#26159;&#19981;&#36275;&#20197;&#23454;&#29616;OOD&#24191;&#20041;&#21270;&#30340;&#12290;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#65292;&#20363;&#22914;
&lt;/p&gt;
&lt;p&gt;
Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and consequently fail to generalize to out-of-distribution (OOD) data. Recently developed "diversification" methods (Lee et al., 2023; Pagliardini et al., 2023) approach this problem by finding multiple diverse hypotheses that rely on different features. This paper aims to study this class of methods and identify the key components contributing to their OOD generalization abilities.  We show that (1) diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. (2) Diversification alone is insufficient for OOD generalization. The choice of the used learning algorithm, e.g., the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65288;TDCFD&#65289;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#29305;&#24449;&#20540;&#36716;&#21270;&#20026;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#26469;&#23454;&#29616;&#21487;&#20449;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2312.16113</link><description>&lt;p&gt;
&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#65306;&#26397;&#30528;&#21487;&#20449;&#30340;&#39118;&#38505;&#39044;&#27979;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction. (arXiv:2312.16113v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16113
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65288;TDCFD&#65289;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#29305;&#24449;&#20540;&#36716;&#21270;&#20026;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#26469;&#23454;&#29616;&#21487;&#20449;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#23545;&#20854;&#22312;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#32570;&#20047;&#22240;&#26524;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#24212;&#23545;&#65292;&#23548;&#33268;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65288;TDCFD&#65289;&#65292;&#23558;&#21407;&#22987;&#29305;&#24449;&#20540;&#36716;&#21270;&#20026;&#29305;&#23450;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#30340;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#12290;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#26377;&#21161;&#20110;&#25551;&#36848;&#35813;&#29305;&#24449;&#30340;&#20540;&#23545;&#39118;&#38505;&#39044;&#27979;&#32467;&#26524;&#30340;&#36129;&#29486;&#31243;&#24230;&#12290;&#22312;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#20043;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20855;&#26377;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#31934;&#30830;&#24230;/&#21484;&#22238;&#29575;&#30340;&#21487;&#20449;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;TDCFD&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since artificial intelligence has seen tremendous recent successes in many areas, it has sparked great interest in its potential for trustworthy and interpretable risk prediction. However, most models lack causal reasoning and struggle with class imbalance, leading to poor precision and recall. To address this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to transform original feature values into causal feature attributions for the specific risk prediction task. The causal feature attribution helps describe how much contribution the value of this feature can make to the risk prediction result. After the causal feature distillation, a deep neural network is applied to produce trustworthy prediction results with causal interpretability and high precision/recall. We evaluate the performance of our TDCFD method on several synthetic and real datasets, and the results demonstrate its superiority over the state-of-the-art methods regarding precision, recall, interpretabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#27599;&#20010;&#21464;&#28857;&#23545;&#24212;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;GAN&#21028;&#21035;&#22120;&#30340;&#36755;&#20986;&#26816;&#27979;&#21464;&#28857;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.13152</link><description>&lt;p&gt;
&#24102;&#26377;&#21464;&#28857;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65306;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach. (arXiv:2312.13152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#27599;&#20010;&#21464;&#28857;&#23545;&#24212;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;GAN&#21028;&#21035;&#22120;&#30340;&#36755;&#20986;&#26816;&#27979;&#21464;&#28857;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#30340;&#38543;&#26426;&#29616;&#35937;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26102;&#38388;&#24207;&#21015;&#30001;&#21333;&#20010;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#30340;&#24773;&#20917;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#21487;&#33021;&#26159;&#23616;&#38480;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20026;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32852;&#21512;&#23398;&#20064;&#26410;&#30693;&#30340;&#21464;&#28857;&#21644;&#19982;&#27599;&#20010;&#21464;&#28857;&#23545;&#24212;&#30340;&#19981;&#21516;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26694;&#26550;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#21464;&#28857;&#26159;&#26681;&#25454;GAN&#21028;&#21035;&#22120;&#30340;&#36755;&#20986;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#34987;&#26816;&#27979;&#21040;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#27599;&#19968;&#27493;&#20013;&#65292;&#21464;&#28857;&#21644;SDE&#27169;&#22411;&#21442;&#25968;&#20197;&#20132;&#26367;&#30340;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#32467;&#26524;&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic differential equations (SDEs) have been widely used to model real world random phenomena. Existing works mainly focus on the case where the time series is modeled by a single SDE, which might be restrictive for modeling time series with distributional shift. In this work, we propose a change point detection algorithm for time series modeled as neural SDEs. Given a time series dataset, the proposed method jointly learns the unknown change points and the parameters of distinct neural SDE models corresponding to each change point. Specifically, the SDEs are learned under the framework of generative adversarial networks (GANs) and the change points are detected based on the output of the GAN discriminator in a forward pass. At each step of the proposed algorithm, the change points and the SDE model parameters are updated in an alternating fashion. Numerical results on both synthetic and real datasets are provided to validate the performance of our algorithm in comparison to clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UMAP Mixup&#30340;Mixup&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#19982;&#25237;&#24433;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#8220;&#22312;&#27969;&#24418;&#19978;&#8221;&#30340;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.13141</link><description>&lt;p&gt;
&#22686;&#21152;&#27969;&#24418;&#65306;&#23558;Mixup&#27491;&#21017;&#21270;&#19982;UMAP&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Augment on Manifold: Mixup Regularization with UMAP. (arXiv:2312.13141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UMAP Mixup&#30340;Mixup&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#19982;&#25237;&#24433;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#8220;&#22312;&#27969;&#24418;&#19978;&#8221;&#30340;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#24050;&#34987;&#35777;&#26126;&#65292;&#20294;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UMAP Mixup&#30340;Mixup&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#26088;&#22312;&#20026;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#8220;&#22312;&#27969;&#24418;&#19978;&#8221;&#30340;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#19982;&#25237;&#24433;&#30340;&#38477;&#32500;&#25216;&#26415;&#65292;&#30830;&#20445;Mixup&#25805;&#20316;&#20135;&#29983;&#30340;&#21512;&#25104;&#26679;&#26412;&#20301;&#20110;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#25968;&#25454;&#27969;&#24418;&#19978;&#12290;&#22312;&#21508;&#31181;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;UMAP Mixup&#19982;&#20854;&#20182;Mixup&#21464;&#20307;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#31034;&#20986;&#20854;&#20316;&#20026;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26377;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation techniques play an important role in enhancing the performance of deep learning models. Despite their proven benefits in computer vision tasks, their application in the other domains remains limited. This paper proposes a Mixup regularization scheme, referred to as UMAP Mixup, designed for ``on-manifold" automated data augmentation for deep learning predictive models. The proposed approach ensures that the Mixup operations result in synthesized samples that lie on the data manifold of the features and labels by utilizing a dimensionality reduction technique known as uniform manifold approximation and projection. Evaluations across diverse regression tasks show that UMAP Mixup is competitive with or outperforms other Mixup variants, show promise for its potential as an effective tool for enhancing the generalization performance of deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LRS&#30340;&#36716;&#31227;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#26367;&#20195;&#27169;&#22411;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#40657;&#30418;&#25915;&#20987;&#36816;&#34892;&#26356;&#22909;&#65292;&#29983;&#25104;&#26356;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.13118</link><description>&lt;p&gt;
LRS: &#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#26367;&#20195;&#27169;&#22411;&#22686;&#24378;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
LRS: Enhancing Adversarial Transferability through Lipschitz Regularized Surrogate. (arXiv:2312.13118v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LRS&#30340;&#36716;&#31227;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#26367;&#20195;&#27169;&#22411;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#40657;&#30418;&#25915;&#20987;&#36816;&#34892;&#26356;&#22909;&#65292;&#29983;&#25104;&#26356;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#23545;&#20110;&#22522;&#20110;&#36716;&#31227;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#29983;&#25104;&#21487;&#36716;&#31227;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#25915;&#20987;&#32473;&#23450;&#30340;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#19978;&#65292;&#32780;&#26367;&#20195;&#27169;&#22411;&#21644;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#34987;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;"Lipschitz&#27491;&#21017;&#21270;&#26367;&#20195;&#27169;&#22411;"&#65288;LRS&#65289;&#30340;&#36716;&#31227;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26367;&#20195;&#27169;&#22411;&#36716;&#21270;&#20026;&#26377;&#21033;&#20110;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#24418;&#24335;&#65292;&#20351;&#29992;&#36825;&#26679;&#36716;&#21270;&#21518;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20219;&#20309;&#29616;&#26377;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#40657;&#30418;&#25915;&#20987;&#37117;&#33021;&#22815;&#27491;&#24120;&#36816;&#34892;&#65292;&#19988;&#24615;&#33021;&#26356;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#26367;&#20195;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;Lipschitz&#27491;&#21017;&#21270;&#65292;&#20197;&#23454;&#29616;&#26356;&#24179;&#28369;&#21644;&#21487;&#25511;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#29983;&#25104;&#26356;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#26367;&#20195;&#27169;&#22411;&#20869;&#37096;&#23646;&#24615;&#19982;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transferability of adversarial examples is of central importance to transfer-based black-box adversarial attacks. Previous works for generating transferable adversarial examples focus on attacking \emph{given} pretrained surrogate models while the connections between surrogate models and adversarial trasferability have been overlooked. In this paper, we propose {\em Lipschitz Regularized Surrogate} (LRS) for transfer-based black-box attacks, a novel approach that transforms surrogate models towards favorable adversarial transferability. Using such transformed surrogate models, any existing transfer-based black-box attack can run without any change, yet achieving much better performance. Specifically, we impose Lipschitz regularization on the loss landscape of surrogate models to enable a smoother and more controlled optimization process for generating more transferable adversarial examples. In addition, this paper also sheds light on the connection between the inner properties of s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26032;&#24120;&#24577;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#20272;&#35745;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#31574;&#30053;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22686;&#21152;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11976</link><description>&lt;p&gt;
&#24403;&#27169;&#22411;&#36935;&#35265;&#26032;&#24120;&#24577;&#65306;&#38024;&#23545;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
When Model Meets New Normals: Test-time Adaptation for Unsupervised Time-series Anomaly Detection. (arXiv:2312.11976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26032;&#24120;&#24577;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#20272;&#35745;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#31574;&#30053;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22686;&#21152;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22788;&#29702;&#36890;&#36807;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#27491;&#24120;&#24615;&#26469;&#26816;&#27979;&#24322;&#24120;&#26102;&#38388;&#27493;&#39588;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#27491;&#24120;&#24615;&#30340;&#27010;&#24565;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#23548;&#33268;&#20986;&#29616;&#20102;&#8220;&#26032;&#24120;&#24577;&#38382;&#39064;&#8221;&#65292;&#21363;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#27491;&#24120;&#24615;&#30340;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#25913;&#21464;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30740;&#31350;&#20013;&#26032;&#24120;&#24577;&#38382;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#36235;&#21183;&#20272;&#35745;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#27491;&#24120;&#24615;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#25152;&#25552;&#31574;&#30053;&#32435;&#20837;&#24322;&#24120;&#26816;&#27979;&#22120;&#20013;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series anomaly detection deals with the problem of detecting anomalous timesteps by learning normality from the sequence of observations. However, the concept of normality evolves over time, leading to a "new normal problem", where the distribution of normality can be changed due to the distribution shifts between training and test data. This paper highlights the prevalence of the new normal problem in unsupervised time-series anomaly detection studies. To tackle this issue, we propose a simple yet effective test-time adaptation strategy based on trend estimation and a self-supervised approach to learning new normalities during inference. Extensive experiments on real-world benchmarks demonstrate that incorporating the proposed strategy into the anomaly detector consistently improves the model's performance compared to the baselines, leading to robustness to the distribution shifts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#19977;&#23618;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#23618;&#27425;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;TLO&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#956;&#21078;&#20998;&#26500;&#24314;&#36229;&#22810;&#38754;&#20307;&#36817;&#20284;&#24182;&#20197;&#24322;&#27493;&#26041;&#24335;&#27714;&#35299;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;TLO&#24037;&#20316;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#21644;&#25910;&#25947;&#36895;&#24230;&#32570;&#20047;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.11835</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#32852;&#37030;&#19977;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Federated Trilevel Learning. (arXiv:2312.11835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#19977;&#23618;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#23618;&#27425;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;TLO&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#956;&#21078;&#20998;&#26500;&#24314;&#36229;&#22810;&#38754;&#20307;&#36817;&#20284;&#24182;&#20197;&#24322;&#27493;&#26041;&#24335;&#27714;&#35299;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;TLO&#24037;&#20316;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#21644;&#25910;&#25947;&#36895;&#24230;&#32570;&#20047;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#23618;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;&#19977;&#23618;&#20248;&#21270;&#65288;TLO&#65289;&#65292;&#34987;&#35748;&#20026;&#26159;&#23618;&#27425;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#22823;&#24314;&#27169;&#24037;&#20855;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#22914;&#40065;&#26834;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#35299;&#20915;TLO&#38382;&#39064;&#22312;&#20110;&#20854;&#23884;&#22871;&#30340;&#20915;&#31574;&#32467;&#26500;&#32780;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;TLO&#24037;&#20316;&#38754;&#20020;&#20197;&#19979;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#23427;&#20204;&#37117;&#19987;&#27880;&#20110;&#38750;&#20998;&#24067;&#24335;&#35774;&#32622;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#65307;2&#65289;&#23427;&#20204;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#38750;&#28176;&#36827;&#25910;&#25947;&#20998;&#26512;&#65292;&#21363;&#21051;&#30011;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#19977;&#23618;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;TLO&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#956;&#21078;&#20998;&#26500;&#24314;TLO&#38382;&#39064;&#30340;&#36229;&#22810;&#38754;&#20307;&#36817;&#20284;&#65292;&#24182;&#20197;&#24322;&#27493;&#26041;&#24335;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#956;&#21078;&#20998;&#26159;&#23545;TLO&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#25928;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trilevel learning, also called trilevel optimization (TLO), has been recognized as a powerful modelling tool for hierarchical decision process and widely applied in many machine learning applications, such as robust neural architecture search, hyperparameter optimization, and domain adaptation. Tackling TLO problems has presented a great challenge due to their nested decision-making structure. In addition, existing works on TLO face the following key challenges: 1) they all focus on the non-distributed setting, which may lead to privacy breach; 2) they do not offer any non-asymptotic convergence analysis which characterizes how fast an algorithm converges. To address the aforementioned challenges, this paper proposes an asynchronous federated trilevel optimization method to solve TLO problems. The proposed method utilizes $\mu$-cuts to construct a hyper-polyhedral approximation for the TLO problem and solve it in an asynchronous manner. We demonstrate that the proposed $\mu$-cuts are a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21517;&#20026;TVQ-VAE&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2312.11532</link><description>&lt;p&gt;
Topic-VQ-VAE: &#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21517;&#20026;TVQ-VAE&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Vector-Quantized Variational Auto-Encoder&#65288;VQ-VAE&#65289;&#20013;&#30340;&#38544;&#21464;&#37327;&#30721;&#26412;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#31163;&#25955;&#22320;&#23553;&#35013;&#20102;&#39044;&#35757;&#32451;&#23884;&#20837;&#65288;&#20363;&#22914;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#26681;&#25454;&#23545;&#38544;&#21464;&#37327;&#30721;&#26412;&#21644;&#23884;&#20837;&#30340;&#26032;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#31216;&#20026;Topic-VQ-VAE&#65288;TVQ-VAE&#65289;&#65292;&#23427;&#21487;&#20197;&#21453;&#21521;&#29983;&#25104;&#19982;&#30456;&#24212;&#38544;&#21464;&#37327;&#30721;&#26412;&#30456;&#20851;&#30340;&#21407;&#22987;&#25991;&#26723;&#12290;TVQ-VAE&#21487;&#20197;&#36890;&#36807;&#21253;&#25324;&#20256;&#32479;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#20998;&#24067;&#21644;&#33258;&#22238;&#24402;&#22270;&#20687;&#29983;&#25104;&#22312;&#20869;&#30340;&#21508;&#31181;&#29983;&#25104;&#20998;&#24067;&#26469;&#21487;&#35270;&#21270;&#20027;&#39064;&#12290;&#25105;&#20204;&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#22270;&#20687;&#29983;&#25104;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TVQ-VAE&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#25581;&#31034;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;TVQ-VAE&#30340;&#23448;&#26041;&#23454;&#29616;&#21487;&#22312;https://github.com/clo&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeRDaVa&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21024;&#38500;&#40065;&#26834;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#39044;&#27979;&#21024;&#38500;&#21518;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#23545;&#27599;&#20010;&#25968;&#25454;&#28304;&#30340;&#36129;&#29486;&#36827;&#34892;&#20272;&#20540;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35745;&#31639;&#12290;&#25512;&#24191;&#21040;Risk-DeRDaVa&#20197;&#28385;&#36275;&#23545;&#26368;&#22351;/&#26368;&#22909;&#24773;&#20917;&#24863;&#21040;&#39118;&#38505;&#21388;&#24694;/&#23547;&#27714;&#30340;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2312.11413</link><description>&lt;p&gt;
DeRDaVa: &#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21024;&#38500;&#40065;&#26834;&#25968;&#25454;&#20272;&#20540;
&lt;/p&gt;
&lt;p&gt;
DeRDaVa: Deletion-Robust Data Valuation for Machine Learning. (arXiv:2312.11413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11413
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeRDaVa&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21024;&#38500;&#40065;&#26834;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#39044;&#27979;&#21024;&#38500;&#21518;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#23545;&#27599;&#20010;&#25968;&#25454;&#28304;&#30340;&#36129;&#29486;&#36827;&#34892;&#20272;&#20540;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35745;&#31639;&#12290;&#25512;&#24191;&#21040;Risk-DeRDaVa&#20197;&#28385;&#36275;&#23545;&#26368;&#22351;/&#26368;&#22909;&#24773;&#20917;&#24863;&#21040;&#39118;&#38505;&#21388;&#24694;/&#23547;&#27714;&#30340;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#28041;&#21450;&#30830;&#23450;&#22312;&#39044;&#27979;&#20013;&#23545;&#25968;&#25454;&#28304;&#36827;&#34892;&#20844;&#24179;&#20272;&#20215;&#20197;&#34917;&#20607;&#23427;&#20204;&#65292;&#25110;&#30830;&#23450;&#26368;&#26377;&#29992;&#25110;&#26368;&#26080;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#38543;&#30528;&#20010;&#20154;&#25968;&#25454;&#25152;&#26377;&#26435;&#21644;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#21487;&#33021;&#38656;&#35201;&#28385;&#36275;&#26356;&#22810;&#30340;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#12290;&#36825;&#24341;&#21457;&#20102;&#29616;&#26377;&#24037;&#20316;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#21024;&#38500;&#21518;&#25968;&#25454;&#20272;&#20540;&#20998;&#25968;&#26159;&#21542;&#20173;&#28982;&#20844;&#24179;&#65311;&#26159;&#21542;&#24517;&#39035;&#26114;&#36149;&#22320;&#37325;&#26032;&#35745;&#31639;&#20998;&#25968;&#65311;&#31572;&#26696;&#26159;&#21542;&#23450;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#37325;&#26032;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#39044;&#27979;&#21024;&#38500;&#21518;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;DeRDaVa&#26469;&#23545;&#27599;&#20010;&#25968;&#25454;&#28304;&#30340;&#36129;&#29486;&#36827;&#34892;&#20272;&#20540;&#12290;DeRDaVa&#21487;&#20197;&#39640;&#25928;&#22320;&#36817;&#20284;&#65292;&#24182;&#32473;&#26356;&#26377;&#29992;&#25110;&#26356;&#19981;&#23481;&#26131;&#34987;&#21024;&#38500;&#30340;&#25968;&#25454;&#20998;&#37197;&#26356;&#39640;&#30340;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;DeRDaVa&#21040;Risk-DeRDaVa&#65292;&#20197;&#28385;&#36275;&#23545;&#26368;&#22351;/&#26368;&#22909;&#24773;&#20917;&#24863;&#21040;&#39118;&#38505;&#21388;&#24694;/&#23547;&#27714;&#30340;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is concerned with determining a fair valuation of data from data sources to compensate them or to identify training examples that are the most or least useful for predictions. With the rising interest in personal data ownership and data protection regulations, model owners will likely have to fulfil more data deletion requests. This raises issues that have not been addressed by existing works: Are the data valuation scores still fair with deletions? Must the scores be expensively recomputed? The answer is no. To avoid recomputations, we propose using our data valuation framework DeRDaVa upfront for valuing each data source's contribution to preserving robust model performance after anticipated data deletions. DeRDaVa can be efficiently approximated and will assign higher values to data that are more useful or less likely to be deleted. We further generalize DeRDaVa to Risk-DeRDaVa to cater to risk-averse/seeking model owners who are concerned with the worst/best-cases mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2312.10305</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#40065;&#26834;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20449;&#21495;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20840;&#23616;&#22768;&#23398;&#29305;&#24449;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#21442;&#32771;&#35821;&#38899;&#20013;&#19982;&#35828;&#35805;&#20154;&#36523;&#20221;&#26080;&#20851;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#22312;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#20013;&#20986;&#29616;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21442;&#32771;&#35821;&#38899;&#32534;&#30721;&#32593;&#32476;&#21644;&#20840;&#23616;&#20449;&#24687;&#20998;&#35299;&#32593;&#32476;&#36880;&#28176;&#20998;&#35299;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#19981;&#30456;&#20851;&#22240;&#32032;&#12290;&#25105;&#20204;&#19987;&#38376;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#36866;&#24212;&#35843;&#21046;Transformer&#26469;&#30830;&#20445;&#28151;&#21512;&#20449;&#21495;&#30340;&#22768;&#23398;&#34920;&#31034;&#19981;&#21463;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech signals are inherently complex as they encompass both global acoustic characteristics and local semantic information. However, in the task of target speech extraction, certain elements of global and local semantic information in the reference speech, which are irrelevant to speaker identity, can lead to speaker confusion within the speech extraction network. To overcome this challenge, we propose a self-supervised disentangled representation learning method. Our approach tackles this issue through a two-phase process, utilizing a reference speech encoding network and a global information disentanglement network to gradually disentangle the speaker identity information from other irrelevant factors. We exclusively employ the disentangled speaker identity information to guide the speech extraction network. Moreover, we introduce the adaptive modulation Transformer to ensure that the acoustic representation of the mixed signal remains undisturbed by the speaker embeddings. This com
&lt;/p&gt;</description></item><item><title>&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.07930</link><description>&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07930
&lt;/p&gt;
&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#27867;&#21270;&#20102;&#25152;&#26377;&#20043;&#21069;&#32479;&#35745;&#27700;&#21360;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23454;&#36341;&#20013;&#30340;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#23454;&#29616;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#30340;&#20551;&#35774;&#26816;&#39564;&#29615;&#22659;&#19979;&#34920;&#24449;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#20197;&#21450;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#29615;&#22659;&#20013;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#12290;&#22312;&#36755;&#20986;&#26159;$n$&#20010;&#20196;&#29260;&#30340;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#38656;&#35201;&#20445;&#35777;&#23567;&#30340;&#31532;&#19968;&#31867;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#24314;&#31435;&#20102;&#36817;&#20046;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;$ h ^ {-2} $&#36895;&#29575;&#30456;&#27604;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#27599;&#20010;&#20196;&#29260;&#30340;&#24179;&#22343;&#29109;$h$&#30340;&#36895;&#29575;&#20026;$ \Theta(h ^ {-1} \log (1/h)) $&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#39044;&#26399;&#22238;&#25253;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#31574;&#30053;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#65292;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#20998;&#24067;&#30340;&#25193;&#25955;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#27604;&#36739;&#31574;&#30053;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.07178</link><description>&lt;p&gt;
&#36229;&#36234;&#39044;&#26399;&#22238;&#25253;&#65306;&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#32771;&#34385;&#25919;&#31574;&#21487;&#22797;&#21046;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#39044;&#26399;&#22238;&#25253;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#31574;&#30053;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#65292;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#20998;&#24067;&#30340;&#25193;&#25955;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#27604;&#36739;&#31574;&#30053;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#24212;&#29992;&#36890;&#24120;&#22312;&#29615;&#22659;&#20013;&#23384;&#22312;&#22122;&#22768;&#25110;&#38543;&#26426;&#24615;&#12290;&#38500;&#20102;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#20043;&#22806;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30456;&#21516;&#30340;&#31574;&#30053;&#22312;&#19981;&#21516;&#30340;&#35797;&#39564;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#21363;&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#24378;&#21270;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#35780;&#20272;&#31243;&#24207;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#26469;&#24635;&#32467;&#32467;&#26524;&#20998;&#24067;&#65292;&#32780;&#19981;&#32771;&#34385;&#20998;&#24067;&#30340;&#25193;&#25955;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#31181;&#25193;&#25955;&#23450;&#20041;&#20026;&#25919;&#31574;&#30340;&#21487;&#22797;&#21046;&#24615;&#65306;&#24403;&#22810;&#27425;&#35797;&#39564;&#26102;&#65292;&#25919;&#31574;&#33719;&#24471;&#31867;&#20284;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#20013;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#29616;&#26377;&#30340;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#30340;&#31243;&#24207;&#22312;&#20004;&#20010;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65306;&#31532;&#19968;&#65292;&#23384;&#22312;&#26080;&#25968;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24615;&#33021;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#26435;&#34913;&#65292;&#20294;&#26159;&#23427;&#20204;&#21487;&#20197;&#26377;&#30456;&#21516;&#30340;&#39044;&#26399;&#22238;&#25253;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#27604;&#36739;&#31574;&#30053;&#26102;&#30340;&#26377;&#25928;&#24615;&#65307;&#31532;&#20108;&#65292;&#39044;&#26399;&#22238;&#25253;&#24230;&#37327;&#27809;&#26377;&#30041;&#19979;&#36275;&#22815;&#30340;&#31354;&#38388;&#65292;&#20197;&#25551;&#36848;&#20998;&#24067;&#30340;&#20854;&#20182;&#20851;&#38190;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications in Reinforcement Learning (RL) usually have noise or stochasticity present in the environment. Beyond their impact on learning, these uncertainties lead the exact same policy to perform differently, i.e. yield different return, from one roll-out to another. Common evaluation procedures in RL summarise the consequent return distributions using solely the expected return, which does not account for the spread of the distribution. Our work defines this spread as the policy reproducibility: the ability of a policy to obtain similar performance when rolled out many times, a crucial property in some real-world applications. We highlight that existing procedures that only use the expected return are limited on two fronts: first an infinite number of return distributions with a wide range of performance-reproducibility trade-offs can have the same expected return, limiting its effectiveness when used for comparing policies; second, the expected return metric does not leave an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;</title><link>http://arxiv.org/abs/2312.05134</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;k&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#65292;&#24050;&#25104;&#20026;&#36866;&#24212;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#22810;&#32452;&#21512;&#20316;&#31561;&#38656;&#27714;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;MDL&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20063;&#31216;&#20026;&#25353;&#38656;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#38024;&#23545;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#25509;&#36817;&#20110;&#65288;d+k&#65289;/&#949;^2&#65288;&#22312;&#26576;&#20123;&#23545;&#25968;&#22240;&#23376;&#20013;&#65289;&#65292;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24605;&#24819;&#21644;&#29702;&#35770;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26680;&#26426;&#22120;&#39044;&#22788;&#29702;&#20013;&#20351;&#29992;Nystrom&#36924;&#36817;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#33021;&#22815;&#35753;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2312.03311</link><description>&lt;p&gt;
&#23545;&#20110;&#26680;&#26426;&#22120;&#22312;&#39044;&#22788;&#29702;&#20013;&#30340;Nystrom&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26680;&#26426;&#22120;&#39044;&#22788;&#29702;&#20013;&#20351;&#29992;Nystrom&#36924;&#36817;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#33021;&#22815;&#35753;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31867;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;&#23398;&#20064;&#26680;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#38656;&#35201;&#20855;&#26377;&#36845;&#20195;&#24615;&#36136;&#65292;&#20294;&#30001;&#20110;&#31967;&#31957;&#30340;&#26465;&#20214;&#65292;&#25910;&#25947;&#21487;&#33021;&#24456;&#24930;&#12290;&#35889;&#39044;&#22788;&#29702;&#26159;&#21152;&#24555;&#35757;&#32451;&#26680;&#27169;&#22411;&#36845;&#20195;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#21644;&#23384;&#20648;&#35889;&#39044;&#22788;&#29702;&#22120;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#26680;&#26041;&#27861;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;Nystrom&#36924;&#36817;&#30340;&#35889;&#39044;&#22788;&#29702;&#22120;&#36890;&#24120;&#26356;&#20415;&#23452;&#21644;&#26356;&#23481;&#26131;&#35745;&#31639;&#21644;&#23384;&#20648;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#36825;&#31181;&#36924;&#36817;&#39044;&#22788;&#29702;&#22120;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#30456;&#20851;&#30340;&#23545;&#25968;&#26679;&#26412;&#25968;&#37327;&#33021;&#22815;&#35753;&#22522;&#20110;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25317;&#22622;&#24863;&#30693;&#20998;&#24067;&#24335;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#36890;&#36807;&#25913;&#36827;&#20998;&#24067;&#24335;&#36138;&#24515;&#26694;&#26550;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#26080;&#32447;&#22810;&#36339;&#32593;&#32476;&#20013;&#30340;&#25317;&#22622;&#24182;&#25552;&#21319;&#20102;&#25191;&#34892;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2312.02471</link><description>&lt;p&gt;
&#26080;&#32447;&#22810;&#36339;&#32593;&#32476;&#20013;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25317;&#22622;&#24863;&#30693;&#20998;&#24067;&#24335;&#20219;&#21153;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Congestion-aware Distributed Task Offloading in Wireless Multi-hop Networks Using Graph Neural Networks. (arXiv:2312.02471v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25317;&#22622;&#24863;&#30693;&#20998;&#24067;&#24335;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#36890;&#36807;&#25913;&#36827;&#20998;&#24067;&#24335;&#36138;&#24515;&#26694;&#26550;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#26080;&#32447;&#22810;&#36339;&#32593;&#32476;&#20013;&#30340;&#25317;&#22622;&#24182;&#25552;&#21319;&#20102;&#25191;&#34892;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21368;&#36733;&#24050;&#25104;&#20026;&#31227;&#21160;&#21644;&#26234;&#33021;&#35774;&#22791;&#36793;&#32536;&#26234;&#33021;&#30340;&#22522;&#30784;&#32452;&#20214;&#12290;&#29616;&#26377;&#30340;&#21368;&#36733;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#19978;&#65292;&#24573;&#35270;&#20102;&#22810;&#20010;&#31227;&#21160;&#35774;&#22791;&#24341;&#36215;&#30340;&#28508;&#22312;&#32593;&#32476;&#25317;&#22622;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#32447;&#22810;&#36339;&#32593;&#32476;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#24320;&#38144;&#30340;&#12289;&#25317;&#22622;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#20998;&#24067;&#24335;&#36138;&#24515;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#22312;&#27169;&#25311;&#30340;&#26080;&#32447;&#22810;&#36339;&#32593;&#32476;&#20013;&#65292;&#33410;&#28857;&#25968;&#20026;20-110&#20010;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#26368;&#30701;&#36335;&#24452;&#36335;&#30001;&#21644;&#22522;&#20110;&#31454;&#20105;&#30340;&#38142;&#36335;&#35843;&#24230;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#22312;&#20943;&#23569;&#25317;&#22622;&#25110;&#19981;&#31283;&#23450;&#38431;&#21015;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#26412;&#22320;&#35745;&#31639;&#30340;&#25191;&#34892;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational offloading has become an enabling component for edge intelligence in mobile and smart devices. Existing offloading schemes mainly focus on mobile devices and servers, while ignoring the potential network congestion caused by tasks from multiple mobile devices, especially in wireless multi-hop networks. To fill this gap, we propose a low-overhead, congestion-aware distributed task offloading scheme by augmenting a distributed greedy framework with graph-based machine learning. In simulated wireless multi-hop networks with 20-110 nodes and a resource allocation scheme based on shortest path routing and contention-based link scheduling, our approach is demonstrated to be effective in reducing congestion or unstable queues under the context-agnostic baseline, while improving the execution latency over local computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2312.02277</link><description>&lt;p&gt;
ALEXR:&#19968;&#31181;&#29992;&#20110;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#30340;&#26368;&#20248;&#21333;&#24490;&#29615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#31867;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#30340;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#65288;cFCCO&#65289;&#38382;&#39064;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#24490;&#29615;&#21407;&#22987;-&#23545;&#20598;&#22359;&#22352;&#26631;&#36817;&#31471;&#31639;&#27861;&#65292;&#31216;&#20026;ALEXR&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#22359;&#22352;&#26631;&#38543;&#26426;&#38236;&#20687;&#19978;&#21319;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#21644;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#12290;&#25105;&#20204;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20989;&#25968;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;ALEXR&#22312;&#20984;&#21644;&#24378;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#19981;&#20165;&#25913;&#36827;&#20102;&#20197;&#21069;&#22312;&#24179;&#28369;cFCCO&#38382;&#39064;&#19978;&#30340;&#26368;&#20339;&#36895;&#24230;&#65292;&#36824;&#25193;&#23637;&#20102;cFCCO&#30340;&#33539;&#22260;&#65292;&#29992;&#20110;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38750;&#24179;&#28369;&#38382;&#39064;&#65292;&#22914;GDRO&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#19979;&#30028;&#65292;&#20197;&#35777;&#26126;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits a class of convex Finite-Sum Coupled Compositional Stochastic Optimization (cFCCO) problems with many applications, including group distributionally robust optimization (GDRO), learning with imbalanced data, reinforcement learning, and learning to rank. To better solve these problems, we introduce an efficient single-loop primal-dual block-coordinate proximal algorithm, dubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror ascent updates for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we present lower complexity bounds to demonstrate that the con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPU&#30456;&#20301;&#25240;&#21472;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;GPFC&#65292;&#29992;&#20110;&#25506;&#27979;&#31995;&#22806;&#34892;&#26143;&#20940;&#21464;&#12290;GPFC&#21033;&#29992;&#24182;&#34892;&#21270;&#30340;&#24555;&#36895;&#25240;&#21472;&#31639;&#27861;&#25918;&#22823;&#20302;&#20449;&#22122;&#27604;&#30340;&#20940;&#26085;&#20449;&#21495;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#36895;&#25628;&#32034;&#12290;&#19982;&#20027;&#35201;&#30340;BLS&#26041;&#27861;&#30456;&#27604;&#65292;GPFC&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;&#20934;&#30830;&#29575;&#20026;97%&#12290;</title><link>http://arxiv.org/abs/2312.02063</link><description>&lt;p&gt;
GPU&#30456;&#20301;&#25240;&#21472;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25506;&#27979;&#31995;&#22806;&#34892;&#26143;&#20940;&#21464;
&lt;/p&gt;
&lt;p&gt;
The GPU Phase Folding and Deep Learning Method for Detecting Exoplanet Transits. (arXiv:2312.02063v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPU&#30456;&#20301;&#25240;&#21472;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;GPFC&#65292;&#29992;&#20110;&#25506;&#27979;&#31995;&#22806;&#34892;&#26143;&#20940;&#21464;&#12290;GPFC&#21033;&#29992;&#24182;&#34892;&#21270;&#30340;&#24555;&#36895;&#25240;&#21472;&#31639;&#27861;&#25918;&#22823;&#20302;&#20449;&#22122;&#27604;&#30340;&#20940;&#26085;&#20449;&#21495;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#36895;&#25628;&#32034;&#12290;&#19982;&#20027;&#35201;&#30340;BLS&#26041;&#27861;&#30456;&#27604;&#65292;GPFC&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;&#20934;&#30830;&#29575;&#20026;97%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;GPU&#30456;&#20301;&#25240;&#21472;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#31995;&#32479;GPFC&#65292;&#29992;&#20110;&#21033;&#29992;&#20940;&#26085;&#26041;&#27861;&#25506;&#27979;&#31995;&#22806;&#34892;&#26143;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;GPU&#19978;&#24182;&#34892;&#21270;&#30340;&#24555;&#36895;&#25240;&#21472;&#31639;&#27861;&#65292;&#29992;&#20110;&#25918;&#22823;&#20302;&#20449;&#22122;&#27604;&#30340;&#20940;&#26085;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#36895;&#25628;&#32034;&#12290;&#19968;&#20010;&#22312;&#20004;&#30334;&#19975;&#20010;&#21512;&#25104;&#20809;&#21464;&#26354;&#32447;&#19978;&#35757;&#32451;&#30340;CNN&#20250;&#22312;&#27599;&#20010;&#21608;&#26399;&#19978;&#25253;&#21578;&#19968;&#20010;&#34920;&#26126;&#34892;&#26143;&#20449;&#21495;&#21487;&#33021;&#24615;&#30340;&#20998;&#25968;&#12290;&#34429;&#28982;GPFC&#26041;&#27861;&#22312;&#21608;&#26399;&#33539;&#22260;&#19978;&#36866;&#29992;&#24191;&#27867;&#65292;&#20294;&#27492;&#30740;&#31350;&#19987;&#27880;&#20110;&#25506;&#27979;&#36712;&#36947;&#21608;&#26399;&#23567;&#20110;&#19968;&#22825;&#30340;&#36229;&#30701;&#21608;&#26399;&#34892;&#26143;&#12290;GPFC&#22312;&#36895;&#24230;&#19978;&#27604;&#20027;&#35201;&#30340;&#30418;&#24335;&#26368;&#23567;&#20108;&#20056;&#65288;BLS&#65289;&#26041;&#27861;&#25552;&#39640;&#20102;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;BLS&#30456;&#27604;&#65292;GPFC&#22312;&#35757;&#32451;&#20934;&#30830;&#29575;&#20026;97%&#12289;&#22312;&#30456;&#21516;&#20551;&#38451;&#24615;&#29575;&#19979;&#30495;&#27491;&#38451;&#24615;&#29575;&#26356;&#39640;&#12289;&#22312;&#30456;&#21516;&#21484;&#22238;&#29575;&#19979;&#31934;&#30830;&#24230;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents GPFC, a novel Graphics Processing Unit (GPU) Phase Folding and Convolutional Neural Network (CNN) system to detect exoplanets using the transit method. We devise a fast folding algorithm parallelized on a GPU to amplify low signal-to-noise ratio transit signals, allowing a search at high precision and speed. A CNN trained on two million synthetic light curves reports a score indicating the likelihood of a planetary signal at each period. While the GPFC method has broad applicability across period ranges, this research specifically focuses on detecting ultra-short-period planets with orbital periods less than one day. GPFC improves on speed by three orders of magnitude over the predominant Box-fitting Least Squares (BLS) method. Our simulation results show GPFC achieves $97%$ training accuracy, higher true positive rate at the same false positive rate of detection, and higher precision at the same recall rate when compared to BLS. GPFC recovers $100\%$ of known ultra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36890;&#29992;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#21487;&#20197;&#25511;&#21046;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#20219;&#20309;&#28304;&#31867;&#21035;&#21040;&#20219;&#20309;&#30446;&#26631;&#31867;&#21035;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#32780;&#21482;&#38656;&#22686;&#21152;&#23569;&#37327;&#30340;&#27745;&#26579;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.00157</link><description>&lt;p&gt;
&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Backdoor Attacks. (arXiv:2312.00157v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00157
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#21487;&#20197;&#25511;&#21046;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#20219;&#20309;&#28304;&#31867;&#21035;&#21040;&#20219;&#20309;&#30446;&#26631;&#31867;&#21035;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#32780;&#21482;&#38656;&#22686;&#21152;&#23569;&#37327;&#30340;&#27745;&#26579;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27745;&#26579;&#65292;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#29992;&#20110;&#31713;&#25913;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#30001;&#20110;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#27169;&#22411;&#21482;&#38656;&#35201;&#35757;&#32451;&#19968;&#27425;&#65292;&#28982;&#21518;&#22810;&#27425;&#37325;&#22797;&#20351;&#29992;&#12290;&#19982;&#23545;&#25239;&#24615;&#26679;&#26412;&#19981;&#21516;&#65292;&#21518;&#38376;&#25915;&#20987;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#31867;&#21035;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#20219;&#20309;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#24182;&#19981;&#19968;&#23450;&#26159;&#30495;&#23454;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#23384;&#22312;&#26356;&#26377;&#25928;&#30340;&#36890;&#29992;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#20801;&#35768;&#36890;&#36807;&#22686;&#21152;&#23569;&#37327;&#30340;&#27745;&#26579;&#26679;&#26412;&#26469;&#25511;&#21046;&#20174;&#20219;&#20309;&#28304;&#31867;&#21035;&#21040;&#20219;&#20309;&#30446;&#26631;&#31867;&#21035;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#29983;&#25104;&#20855;&#26377;&#26174;&#33879;&#29305;&#24449;&#30340;&#35302;&#21457;&#22120;&#65292;&#20351;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#12290;&#25105;&#20204;&#21046;&#20316;&#30340;&#35302;&#21457;&#22120;&#21033;&#29992;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#36328;&#31867;&#21035;&#27745;&#26579;&#30340;&#29616;&#35937;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#30340;&#35302;&#21457;&#22120;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#23398;&#20064;&#20854;&#20182;&#31867;&#21035;&#30340;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and re-used many times. Unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. One might expect that targeting many classes through a naive composition of attacks vastly increases the number of poison samples. We show this is not necessarily true and more efficient, universal data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a small increase in poison samples. Our idea is to generate triggers with salient characteristics that the model can learn. The triggers we craft exploit a phenomenon we call inter-class poison transferability, where learning a trigger from one class makes the model more vulnerable to learning triggers for other classes. We demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16141</link><description>&lt;p&gt;
SNNs&#20013;&#22522;&#20110;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;&#20462;&#21098;&#26041;&#27861;&#65292;&#21463;&#21040;&#20851;&#38190;&#24615;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33410;&#33021;&#21644;&#26080;&#20056;&#27861;&#29305;&#24615;&#65292;SNNs&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28145;&#24230;SNNs&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#32473;&#27169;&#22411;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#32593;&#32476;&#20462;&#21098;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#35268;&#27169;&#26469;&#20943;&#23569;&#27169;&#22411;&#37096;&#32626;&#30340;&#30828;&#20214;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SNN&#20462;&#21098;&#26041;&#27861;&#30001;&#20110;&#20462;&#21098;&#36845;&#20195;&#22686;&#21152;&#20102;SNNs&#30340;&#35757;&#32451;&#38590;&#24230;&#65292;&#23548;&#33268;&#20462;&#21098;&#25104;&#26412;&#39640;&#26114;&#19988;&#24615;&#33021;&#25439;&#22833;&#20005;&#37325;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#29992;&#20110;SNN&#20462;&#21098;&#30340;&#20877;&#29983;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#24182;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#20013;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20302;&#25104;&#26412;&#24230;&#37327;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#20462;&#21098;&#21518;&#23545;&#25152;&#20462;&#21098;&#32467;&#26500;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#20877;&#29983;&#37027;&#20123;&#20855;&#26377;&#36739;&#39640;&#20851;&#38190;&#24615;&#30340;&#32467;&#26500;&#65292;&#20197;&#33719;&#21462;&#20851;&#38190;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2311.14212</link><description>&lt;p&gt;
&#27880;&#37322;&#25935;&#24863;&#24615;&#65306;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#25910;&#38598;&#26102;&#65292;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#12289;&#32473;&#20104;&#27880;&#37322;&#32773;&#30340;&#25351;&#31034;&#12289;&#27880;&#37322;&#32773;&#30340;&#29305;&#24449;&#20197;&#21450;&#20182;&#20204;&#20043;&#38388;&#30340;&#20114;&#21160;&#37117;&#21487;&#33021;&#23545;&#35757;&#32451;&#25968;&#25454;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21019;&#24314;&#27880;&#37322;&#24037;&#20855;&#26102;&#30340;&#35774;&#35745;&#36873;&#25321;&#20063;&#20250;&#24433;&#21709;&#22522;&#20110;&#24471;&#21040;&#30340;&#27880;&#37322;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#27880;&#37322;&#25935;&#24863;&#24615;"&#36825;&#20010;&#26415;&#35821;&#65292;&#29992;&#26469;&#25351;&#20195;&#27880;&#37322;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#20197;&#21450;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#21644;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20116;&#31181;&#23454;&#39564;&#26465;&#20214;&#19979;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#65292;&#38543;&#26426;&#23558;&#27880;&#37322;&#32773;&#20998;&#37197;&#21040;&#19981;&#21516;&#26465;&#20214;&#19979;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#24471;&#21040;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#27599;&#20010;&#26465;&#20214;&#30340;&#20445;&#30041;&#37096;&#20998;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20197;&#19979;&#26041;&#38754;&#26465;&#20214;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65306;1&#65289;&#20167;&#24680;&#35328;&#35770;/&#20882;&#29359;&#24615;&#35821;&#35328;&#27880;&#37322;&#30340;&#27604;&#20363;&#65292;2&#65289;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#21407;&#23376;&#38388;&#21183;&#65292;&#29992;&#20110;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#27169;&#25311;Wurtzite&#27694;&#21270;&#38109;&#30340;&#22768;&#23376;&#20256;&#36755;&#24615;&#36136;&#12290;&#37319;&#29992;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#65288;ACE&#65289;&#26694;&#26550;&#65292;&#35813;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#22810;&#20010;&#24615;&#36136;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#36890;&#36807;&#19982;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#35745;&#31639;&#21644;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#25551;&#36848;&#38750;&#35856;&#25391;&#22768;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36824;&#23558;&#35813;&#21183;&#24212;&#29992;&#20110;&#26230;&#26684;&#21160;&#21147;&#23398;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27491;&#20132;&#24212;&#21464;&#23545;Wurtzite&#27694;&#21270;&#38109;&#30340;&#28909;&#23548;&#29575;&#21644;&#22768;&#23376;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.11990</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#21183;&#29992;&#20110;&#24555;&#36895;&#21644;&#37327;&#23376;&#31934;&#30830;&#30340;Wurtzite AlN&#28909;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN. (arXiv:2311.11990v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#21407;&#23376;&#38388;&#21183;&#65292;&#29992;&#20110;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#27169;&#25311;Wurtzite&#27694;&#21270;&#38109;&#30340;&#22768;&#23376;&#20256;&#36755;&#24615;&#36136;&#12290;&#37319;&#29992;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#65288;ACE&#65289;&#26694;&#26550;&#65292;&#35813;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#22810;&#20010;&#24615;&#36136;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#36890;&#36807;&#19982;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#35745;&#31639;&#21644;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#25551;&#36848;&#38750;&#35856;&#25391;&#22768;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36824;&#23558;&#35813;&#21183;&#24212;&#29992;&#20110;&#26230;&#26684;&#21160;&#21147;&#23398;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27491;&#20132;&#24212;&#21464;&#23545;Wurtzite&#27694;&#21270;&#38109;&#30340;&#28909;&#23548;&#29575;&#21644;&#22768;&#23376;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#65288;ACE&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#21407;&#23376;&#38388;&#21183;&#65292;&#21487;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#27169;&#25311;Wurtzite&#27694;&#21270;&#38109;&#30340;&#22768;&#23376;&#20256;&#36755;&#24615;&#36136;&#12290; ACE&#21183;&#23545;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;w-AlN&#30340;&#19968;&#31995;&#21015;&#24615;&#36136;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#22522;&#24577;&#26230;&#26684;&#21442;&#25968;&#65292;&#27604;&#28909;&#23481;&#65292;&#28909;&#33192;&#32960;&#31995;&#25968;&#65292;&#20307;&#27169;&#37327;&#21644;&#35856;&#25391;&#22768;&#23376;&#20998;&#25955;&#12290;&#36890;&#36807;&#23558;ACE&#39044;&#27979;&#20540;&#19982;DFT&#35745;&#31639;&#21644;&#23454;&#39564;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#26230;&#26684;&#28909;&#23548;&#29575;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;ACE&#21183;&#22312;&#25551;&#36848;&#38750;&#35856;&#25391;&#22768;&#23376;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#25972;&#20307;&#33021;&#21147;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#21183;&#36827;&#34892;&#26230;&#26684;&#21160;&#21147;&#23398;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#27491;&#20132;&#24212;&#21464;&#23545;w-AlN&#30340;&#28909;&#23548;&#29575;&#21644;&#22768;&#23376;&#24615;&#36136;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20854;&#20026;&#36817;&#32467;&#33410;&#21306;&#30340;&#37325;&#35201;&#35843;&#33410;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using the atomic cluster expansion (ACE) framework, we develop a machine learning interatomic potential for fast and accurately modelling the phonon transport properties of wurtzite aluminum nitride. The predictive power of the ACE potential against density functional theory (DFT) is demonstrated across a broad range of properties of w-AlN, including ground-state lattice parameters, specific heat capacity, coefficients of thermal expansion, bulk modulus, and harmonic phonon dispersions. Validation of lattice thermal conductivity is further carried out by comparing the ACE-predicted values to the DFT calculations and experiments, exhibiting the overall capability of our ACE potential in sufficiently describing anharmonic phonon interactions. As a practical application, we perform a lattice dynamics analysis using the potential to unravel the effects of biaxial strains on thermal conductivity and phonon properties of w-AlN, which is identified as a significant tuning factor for near-junc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23545;&#25239;&#25915;&#20987;&#21644;&#31995;&#32479;&#25552;&#31034;&#28431;&#27934;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;GPT-4V&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASP&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#20197;&#25628;&#32034;&#28508;&#22312;&#30340;&#30772;&#35299;&#25552;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#20154;&#24037;&#20462;&#25913;&#65292;&#25104;&#21151;&#29575;&#25552;&#39640;&#21040;98.7%&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20462;&#25913;&#31995;&#32479;&#25552;&#31034;&#23545;&#35299;&#38145;GPT-4V&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.09127</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#23545;&#25239;&#25915;&#20987;&#21644;&#31995;&#32479;&#25552;&#31034;&#30340;&#30772;&#35299;GPT-4V
&lt;/p&gt;
&lt;p&gt;
Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts. (arXiv:2311.09127v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09127
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23545;&#25239;&#25915;&#20987;&#21644;&#31995;&#32479;&#25552;&#31034;&#28431;&#27934;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;GPT-4V&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASP&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#20197;&#25628;&#32034;&#28508;&#22312;&#30340;&#30772;&#35299;&#25552;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#20154;&#24037;&#20462;&#25913;&#65292;&#25104;&#21151;&#29575;&#25552;&#39640;&#21040;98.7%&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20462;&#25913;&#31995;&#32479;&#25552;&#31034;&#23545;&#35299;&#38145;GPT-4V&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#20851;&#20110;&#30772;&#35299;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#65292;&#36739;&#23569;&#20851;&#27880;&#27169;&#22411;API&#30340;&#28431;&#27934;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20197;&#19979;&#24037;&#20316;&#65306;1&#65289;&#25105;&#20204;&#21457;&#29616;&#20102;GPT-4V&#20013;&#30340;&#31995;&#32479;&#25552;&#31034;&#27844;&#28431;&#28431;&#27934;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#21462;&#20102;GPT-4V&#30340;&#20869;&#37096;&#31995;&#32479;&#25552;&#31034;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;MLLMs&#23384;&#22312;&#28508;&#22312;&#30340;&#21487;&#21033;&#29992;&#30340;&#23433;&#20840;&#39118;&#38505;&#65307;2&#65289;&#22522;&#20110;&#33719;&#21462;&#30340;&#31995;&#32479;&#25552;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SASP&#65288;&#36890;&#36807;&#31995;&#32479;&#25552;&#31034;&#30340;&#33258;&#23545;&#25239;&#25915;&#20987;&#65289;&#30340;&#26032;&#22411;MLLM&#30772;&#35299;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;GPT-4&#20316;&#20026;&#32418;&#38431;&#24037;&#20855;&#26469;&#38024;&#23545;&#33258;&#36523;&#36827;&#34892;&#25915;&#20987;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#31363;&#21462;&#30340;&#31995;&#32479;&#25552;&#31034;&#25628;&#32034;&#28508;&#22312;&#30340;&#30772;&#35299;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;GPT-4&#30340;&#20998;&#26512;&#28155;&#21152;&#20102;&#20154;&#24037;&#20462;&#25913;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#36827;&#19968;&#27493;&#25552;&#39640;&#21040;98.7&#65285;&#65307;3&#65289;&#25105;&#20204;&#35780;&#20272;&#20102;&#20462;&#25913;&#31995;&#32479;&#25552;&#31034;&#23545;&#35299;&#38145;GPT-4V&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities, especially in model API. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully extract the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2) Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20026;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#20915;&#31574;&#32773;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36873;&#25321;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20102;&#21508;&#31181;&#24314;&#27169;&#23646;&#24615;&#21644;&#23545;&#25163;&#24341;&#36215;&#30340;&#36716;&#21464;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.09018</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
On the Foundation of Distributionally Robust Reinforcement Learning. (arXiv:2311.09018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20026;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#20915;&#31574;&#32773;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36873;&#25321;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20102;&#21508;&#31181;&#24314;&#27169;&#23646;&#24615;&#21644;&#23545;&#25163;&#24341;&#36215;&#30340;&#36716;&#21464;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#20110;&#23545;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#20043;&#38388;&#29615;&#22659;&#21464;&#21270;&#26102;&#40065;&#26834;&#31574;&#30053;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20026;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#36890;&#36807;&#19968;&#20010;&#20197;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DRMDPs&#65289;&#20026;&#20013;&#24515;&#30340;&#32508;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#20915;&#31574;&#32773;&#22312;&#19968;&#20010;&#30001;&#23545;&#25163;&#25805;&#32437;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#36716;&#21464;&#19979;&#36873;&#25321;&#26368;&#20248;&#31574;&#30053;&#12290;&#36890;&#36807;&#32479;&#19968;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#34920;&#36848;&#65292;&#25105;&#20204;&#20005;&#26684;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#20915;&#31574;&#32773;&#21644;&#23545;&#25163;&#30340;&#21508;&#31181;&#24314;&#27169;&#23646;&#24615;&#30340;DRMDPs&#65292;&#21253;&#25324;&#36866;&#24212;&#24615;&#31890;&#24230;&#12289;&#25506;&#32034;&#21382;&#21490;&#20381;&#36182;&#24615;&#12289;&#39532;&#23572;&#31185;&#22827;&#21644;&#39532;&#23572;&#31185;&#22827;&#26102;&#38388;&#40784;&#27425;&#30340;&#20915;&#31574;&#32773;&#21644;&#23545;&#25163;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23545;&#25163;&#24341;&#36215;&#30340;&#36716;&#21464;&#30340;&#28789;&#27963;&#24615;&#65292;&#30740;&#31350;&#20102;SA&#21644;S-&#30697;&#24418;&#24615;&#12290;&#22312;&#36825;&#20010;DRMDP&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#29616;&#40065;&#26834;&#24615;&#25152;&#38656;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener&#28388;&#27874;&#22120;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;(&#19981;)&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20840;&#23616;&#30456;&#20851;&#30340;&#21367;&#31215;&#26041;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#21387;&#32553;&#12289;&#21307;&#23398;&#22270;&#20687;&#22635;&#20805;&#12289;&#32763;&#35793;&#20998;&#31867;&#21644;&#38750;&#21442;&#25968;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.06558</link><description>&lt;p&gt;
Convolve and Conquer: &#20351;&#29992;Wiener&#28388;&#27874;&#22120;&#36827;&#34892;&#25968;&#25454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Convolve and Conquer: Data Comparison with Wiener Filters. (arXiv:2311.06558v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.06558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener&#28388;&#27874;&#22120;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;(&#19981;)&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20840;&#23616;&#30456;&#20851;&#30340;&#21367;&#31215;&#26041;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#21387;&#32553;&#12289;&#21307;&#23398;&#22270;&#20687;&#22635;&#20805;&#12289;&#32763;&#35793;&#20998;&#31867;&#21644;&#38750;&#21442;&#25968;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#35780;&#20272;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;/&#25110;&#30456;&#20284;&#24615;&#23450;&#20041;&#21644;&#22609;&#36896;&#20102;&#19982;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#27604;&#36739;&#25968;&#25454;&#26041;&#27861;&#24120;&#24120;&#22312;&#25429;&#25417;&#36825;&#31181;&#20998;&#24067;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#25110;&#32773;&#32570;&#20047;&#20248;&#21270;&#25152;&#38656;&#30340;&#33391;&#22909;&#25968;&#23398;&#23646;&#24615;&#65288;&#22914;&#24179;&#28369;&#24615;&#12289;&#21487;&#24494;&#24615;&#25110;&#20984;&#24615;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;Wiener&#28388;&#27874;&#22120;&#29702;&#35770;&#21551;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#25104;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;(&#19981;)&#30456;&#20284;&#24615;&#12290;Wiener&#28388;&#27874;&#22120;&#30340;&#21367;&#31215;&#24615;&#36136;&#20351;&#25105;&#20204;&#33021;&#22815;&#20840;&#38754;&#22320;&#20197;&#20840;&#23616;&#30456;&#20851;&#30340;&#26041;&#24335;&#27604;&#36739;&#25968;&#25454;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#25968;&#25454;&#21387;&#32553;&#12289;&#21307;&#23398;&#22270;&#20687;&#22635;&#20805;&#12289;&#32763;&#35793;&#20998;&#31867;&#21644;&#38750;&#21442;&#25968;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22343;&#26041;&#35823;&#24046;&#30456;&#27604;&#65292;&#22312;&#37325;&#26500;&#22270;&#20687;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#24863;&#30693;&#36136;&#37327;&#21644;&#25968;&#25454;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#23545;&#24179;&#31227;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitative evaluations of differences and/or similarities between data samples define and shape optimisation problems associated with learning data distributions. Current methods to compare data often suffer from limitations in capturing such distributions or lack desirable mathematical properties for optimisation (e.g. smoothness, differentiability, or convexity). In this paper, we introduce a new method to measure (dis)similarities between paired samples inspired by Wiener-filter theory. The convolutional nature of Wiener filters allows us to comprehensively compare data samples in a globally correlated way. We validate our approach in four machine learning applications: data compression, medical imaging imputation, translated classification, and non-parametric generative modelling. Our results demonstrate increased resolution in reconstructed images with better perceptual quality and higher data fidelity, as well as robustness against translations, compared to conventional mean-sq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22343;&#34913;&#30340;&#36870;&#38382;&#39064;&#65292;&#22522;&#20110;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23548;&#39057;&#31526;&#21495;&#21644;&#26410;&#30693;&#30340;&#34928;&#33853;&#20449;&#36947;&#20197;&#21450;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#27700;&#24179;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.06101</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;MIMO&#22343;&#34913;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models. (arXiv:2311.06101v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22343;&#34913;&#30340;&#36870;&#38382;&#39064;&#65292;&#22522;&#20110;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23548;&#39057;&#31526;&#21495;&#21644;&#26410;&#30693;&#30340;&#34928;&#33853;&#20449;&#36947;&#20197;&#21450;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#27700;&#24179;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65289;&#20855;&#26377;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#33021;&#21147;&#12290;&#22312;ICL&#20013;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#21644;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#20960;&#20010;&#31034;&#20363;&#30452;&#25509;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#37327;&#65292;&#23545;&#26032;&#36755;&#20837;&#36827;&#34892;&#20915;&#31574;&#12290;&#26080;&#38656;&#26174;&#24335;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#35843;&#25972;&#20915;&#31574;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#24418;&#24335;&#65292;&#21487;&#20197;&#35266;&#23519;&#20960;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;ICL&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;ICL&#26469;&#35299;&#20915;&#22522;&#20110;&#23548;&#39057;&#31526;&#21495;&#19978;&#19979;&#25991;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22343;&#34913;&#30340;&#36870;&#38382;&#39064;&#12290;&#19968;&#20010;&#20219;&#21153;&#30001;&#26410;&#30693;&#30340;&#34928;&#33853;&#20449;&#36947;&#21644;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#27700;&#24179;&#23450;&#20041;&#65292;&#21487;&#33021;&#26159;&#24050;&#30693;&#30340;&#12290;&#20026;&#20102;&#31361;&#26174;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#28508;&#21147;&#65292;&#25105;&#20204;&#20801;&#35768;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#23384;&#22312;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained sequence models, such as transformer-based architectures, have been recently shown to have the capacity to carry out in-context learning (ICL). In ICL, a decision on a new input is made via a direct mapping of the input and of a few examples from the given task, serving as the task's context, to the output variable. No explicit updates of the model parameters are needed to tailor the decision to a new task. Pre-training, which amounts to a form of meta-learning, is based on the observation of examples from several related tasks. Prior work has shown ICL capabilities for linear regression. In this study, we leverage ICL to address the inverse problem of multiple-input and multiple-output (MIMO) equalization based on a context given by pilot symbols. A task is defined by the unknown fading channel and by the signal-to-noise ratio (SNR) level, which may be known. To highlight the practical potential of the approach, we allow the presence of quantization of the received s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#23186;&#20307;&#32452;&#21512;&#24314;&#27169;&#20013;&#21033;&#29992;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#26041;&#31243;&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#25552;&#20986;&#20102;&#23558;&#39532;&#20811;&#24605;-&#29627;&#23572;&#20857;&#26364;&#26041;&#31243;&#21644;&#31859;&#27663;&#26041;&#31243;&#32435;&#20837;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#24191;&#21578;&#32972;&#26223;&#19979;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.05587</link><description>&lt;p&gt;
&#22522;&#20110;&#24418;&#29366;&#21644;&#28431;&#26007;&#25928;&#24212;&#30340;&#23186;&#20307;&#32452;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Methods for Media Mix Modelling with shape and funnel effects. (arXiv:2311.05587v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#23186;&#20307;&#32452;&#21512;&#24314;&#27169;&#20013;&#21033;&#29992;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#26041;&#31243;&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#25552;&#20986;&#20102;&#23558;&#39532;&#20811;&#24605;-&#29627;&#23572;&#20857;&#26364;&#26041;&#31243;&#21644;&#31859;&#27663;&#26041;&#31243;&#32435;&#20837;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#24191;&#21578;&#32972;&#26223;&#19979;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#22823;&#36827;&#23637;&#31361;&#26174;&#20102;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#27169;&#22411;&#22312;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#25193;&#25955;&#26041;&#31243;&#30340;&#27169;&#22411;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22270;&#20687;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#39532;&#20811;&#24605;-&#29627;&#23572;&#20857;&#26364;&#26041;&#31243;&#21644;&#31859;&#27663;&#26041;&#31243;&#22312;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#65288;MMM&#65289;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20123;&#26041;&#31243;&#32435;&#20837;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#20197;&#20998;&#26512;&#24191;&#21578;&#32972;&#26223;&#19979;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#12290;&#36825;&#20123;&#26041;&#31243;&#32452;&#22312;&#20934;&#30830;&#25551;&#36848;&#31038;&#20132;&#20114;&#21160;&#21644;&#28040;&#36153;&#32773;&#24191;&#21578;&#20114;&#21160;&#31561;&#22797;&#26434;&#31995;&#32479;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant progress in generative AI has highlighted the important role of physics-inspired models that utilize advanced mathematical concepts based on fundamental physics principles to enhance artificial intelligence capabilities. Among these models, those based on diffusion equations have greatly improved image quality. This study aims to explore the potential uses of Maxwell-Boltzmann equation, which forms the basis of the kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix Modelling (MMM) applications. We propose incorporating these equations into Hierarchical Bayesian models to analyse consumer behaviour in the context of advertising. These equation sets excel in accurately describing the random dynamics in complex systems like social interactions and consumer-advertising interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;Langevin Monte Carlo&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#34920;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.03242</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36817;&#20284;Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures. (arXiv:2311.03242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;Langevin Monte Carlo&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#65288;&#22914;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#65289;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#65292;&#20174;&#32780;&#20174;&#32473;&#23450;&#30340;&#30446;&#26631;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21463;Langevin Monte Carlo (LMC)&#31639;&#27861;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#22522;&#20110;LMC&#25200;&#21160;&#32467;&#26524;&#65292;&#22312;Wasserstein-2&#36317;&#31163;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#23545;&#20110;&#24179;&#28369;&#30340;&#23545;&#25968;&#20985;&#30446;&#26631;&#20998;&#24067;&#30340;&#36924;&#36817;&#36895;&#24230;&#12290;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#20110;&#25200;&#21160;LMC&#36807;&#31243;&#30340;&#20013;&#38388;&#24230;&#37327;&#30340;&#20122;&#39640;&#26031;&#24615;&#27010;&#24565;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#25200;&#21160;&#20551;&#35774;&#25512;&#23548;&#20986;&#20102;&#20013;&#38388;&#26041;&#24046;&#20195;&#29702;&#30340;&#22686;&#38271;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#36817;&#20284;&#26679;&#26412;&#19982;&#30446;&#26631;&#20998;&#24067;&#26144;&#23556;&#30340;&#34920;&#36798;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#32763;&#35793;&#12289;&#23545;&#40784;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#38754;&#30340;&#25361;&#25112;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#25968;&#25454;&#20559;&#24046;&#21644;&#8220;&#22823;&#25968;&#25454;&#8221;&#31232;&#32570;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.02332</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#20020;&#24202;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#65306;&#35843;&#26597;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02332
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#32763;&#35793;&#12289;&#23545;&#40784;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#38754;&#30340;&#25361;&#25112;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#25968;&#25454;&#20559;&#24046;&#21644;&#8220;&#22823;&#25968;&#25454;&#8221;&#31232;&#32570;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24050;&#32463;&#20174;&#20256;&#32479;&#21644;&#32479;&#35745;&#26041;&#27861;&#36716;&#21521;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#24403;&#21069;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#32763;&#35793;&#12289;&#23545;&#40784;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#21019;&#26032;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#20020;&#24202;&#39044;&#27979;&#30340;&#36716;&#21464;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#30097;&#38382;&#65292;&#20851;&#27880;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#19982;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#20559;&#24046;&#21644;&#8220;&#22823;&#25968;&#25454;&#8221;&#30340;&#31232;&#32570;&#24615;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#26377;&#25928;&#21019;&#26032;&#21644;&#21512;&#20316;&#21162;&#21147;&#20197;&#36827;&#19968;&#27493;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models. This survey navigates the current landscape of multimodal ML, focusing on its profound impact on medical image analysis and clinical decision support systems. Emphasizing challenges and innovations in addressing multimodal representation, fusion, translation, alignment, and co-learning, the paper explores the transformative potential of multimodal models for clinical predictions. It also questions practical implementation of such models, bringing attention to the dynamics between decision support systems and healthcare providers. Despite advancements, challenges such as data biases and the scarcity of "big data" in many biomedical domains persist. We conclude with a discussion on effective innovation and collaborative efforts to further the miss
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32570;&#38519;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#26368;&#30456;&#20284;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00931</link><description>&lt;p&gt;
&#20174;&#19981;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#32570;&#38519;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Defect Prediction from Unrealistic Data. (arXiv:2311.00931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00931
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32570;&#38519;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#26368;&#30456;&#20284;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#27169;&#22411;&#65292;&#22914;CodeBERT&#21644;CodeT5&#65292;&#25104;&#20026;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#24222;&#22823;&#19988;&#38656;&#35201;&#30456;&#24212;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24456;&#23569;&#25552;&#20379;&#12290;&#30456;&#21453;&#65292;&#20351;&#29992;&#36828;&#27604;&#30495;&#23454;&#25968;&#25454;&#38598;&#26356;&#22823;&#20294;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;&#20154;&#20026;&#27880;&#20837;&#32570;&#38519;&#30340;&#20989;&#25968;&#65289;&#26469;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27492;&#31867;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#21482;&#22312;&#31867;&#20284;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#31243;&#24207;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20551;&#35774;&#36825;&#31181;&#24046;&#24322;&#26159;&#30001;&#20110;&#23384;&#22312;&#24178;&#25200;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#20351;&#27169;&#22411;&#20559;&#31163;&#20102;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#36825;&#20123;&#22823;&#32780;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#20013;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#30340;&#31034;&#20363;&#26368;&#30456;&#20284;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#20102;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#31243;&#24207;&#30340;&#39640;&#32500;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial progr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20174;&#20855;&#26377;&#32473;&#23450;&#21021;&#22987;&#29366;&#24577;&#30340;&#35299;&#36807;&#31243;&#30340;&#20998;&#24067;&#20013;&#35782;&#21035;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#21457;&#29983;&#22120;&#30340;&#26465;&#20214;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#23545;&#20110;&#20855;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#30340;SDE&#30340;&#35782;&#21035;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.19491</link><description>&lt;p&gt;
&#20855;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#30340;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21457;&#29983;&#22120;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Generator Identification for Linear SDEs with Additive and Multiplicative Noise. (arXiv:2310.19491v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20174;&#20855;&#26377;&#32473;&#23450;&#21021;&#22987;&#29366;&#24577;&#30340;&#35299;&#36807;&#31243;&#30340;&#20998;&#24067;&#20013;&#35782;&#21035;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#21457;&#29983;&#22120;&#30340;&#26465;&#20214;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#23545;&#20110;&#20855;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#30340;SDE&#30340;&#35782;&#21035;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20855;&#26377;&#32473;&#23450;&#22266;&#23450;&#21021;&#22987;&#29366;&#24577;&#30340;&#35299;&#36807;&#31243;&#30340;&#20998;&#24067;&#20013;&#35782;&#21035;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#21457;&#29983;&#22120;&#30340;&#26465;&#20214;&#12290;&#36825;&#20123;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#22312;&#20351;&#29992;&#32447;&#24615;SDE&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#24471;&#21487;&#20197;&#20174;&#20854;&#35266;&#27979;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#24178;&#39044;&#21518;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20855;&#20307;&#25512;&#23548;&#20986;&#20102;&#35782;&#21035;&#20855;&#26377;&#21152;&#24615;&#22122;&#22768;&#30340;&#32447;&#24615;SDE&#30340;&#21457;&#29983;&#22120;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#20197;&#21450;&#35782;&#21035;&#20855;&#26377;&#20056;&#24615;&#22122;&#22768;&#30340;&#32447;&#24615;SDE&#30340;&#21457;&#29983;&#22120;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;SDE&#65292;&#24471;&#21040;&#30340;&#26465;&#20214;&#26159;&#19968;&#33324;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24471;&#21040;&#30340;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#20197;&#22686;&#24378;&#23545;&#20854;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#25903;&#25345;&#24182;&#35777;&#23454;&#20102;&#25105;&#20204;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present conditions for identifying the generator of a linear stochastic differential equation (SDE) from the distribution of its solution process with a given fixed initial state. These identifiability conditions are crucial in causal inference using linear SDEs as they enable the identification of the post-intervention distributions from its observational distribution. Specifically, we derive a sufficient and necessary condition for identifying the generator of linear SDEs with additive noise, as well as a sufficient condition for identifying the generator of linear SDEs with multiplicative noise. We show that the conditions derived for both types of SDEs are generic. Moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. To validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#38750;&#24179;&#31283;&#21644;&#29305;&#24449;&#25968;&#30446;&#24222;&#22823;&#19988;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17544</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29305;&#24449;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting. (arXiv:2310.17544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17544
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#38750;&#24179;&#31283;&#21644;&#29305;&#24449;&#25968;&#30446;&#24222;&#22823;&#19988;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#21644;&#26679;&#26412;&#26377;&#38480;&#30340;&#22823;&#37327;&#29305;&#24449;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#36873;&#25321;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#26469;&#21033;&#29992;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#29305;&#24449;&#23376;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#21478;&#19968;&#31181;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#25439;&#22833;&#12290;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#20801;&#35768;&#28789;&#27963;&#30340;&#28145;&#24230;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#23618;&#27425;&#22320;&#21033;&#29992;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a novel ensemble approach for feature selection based on hierarchical stacking in cases of non-stationarity and limited number of samples with large number of features. Our approach exploits the co-dependency between features using a hierarchical structure. Initially, a machine learning model is trained using a subset of features, and then the model's output is updated using another algorithm with the remaining features to minimize the target loss. This hierarchical structure allows for flexible depth and feature selection. By exploiting feature co-dependency hierarchically, our proposed approach overcomes the limitations of traditional feature selection methods and feature importance scores. The effectiveness of the approach is demonstrated on synthetic and real-life datasets, indicating improved performance with scalability and stability compared to the traditional methods and state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#24211;&#23384;&#21040;&#36135;&#21160;&#24577;&#19979;&#30340;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#20462;&#25913;&#35746;&#36141;&#25968;&#37327;&#20197;&#28385;&#36275;&#20379;&#24212;&#21830;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#21608;&#26399;&#24615;&#23457;&#26680;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#23450;&#20041;&#20026;&#22806;&#37096;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.17168</link><description>&lt;p&gt;
&#23398;&#20064;&#22788;&#29702;&#20855;&#26377;&#19968;&#33324;&#24211;&#23384;&#21040;&#36135;&#21160;&#24577;&#30340;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#24211;&#23384;&#21040;&#36135;&#21160;&#24577;&#19979;&#30340;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#20462;&#25913;&#35746;&#36141;&#25968;&#37327;&#20197;&#28385;&#36275;&#20379;&#24212;&#21830;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#21608;&#26399;&#24615;&#23457;&#26680;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#23450;&#20041;&#20026;&#22806;&#37096;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38754;&#23545;&#19968;&#33324;&#21040;&#36135;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#23398;&#20064;&#21644;&#22238;&#27979;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#25968;&#37327;&#38543;&#26102;&#38388;&#21040;&#36135;&#27169;&#22411;&#65288;QOT&#65289;&#12290;&#22312;&#23454;&#38469;&#20379;&#24212;&#38142;&#20013;&#65292;&#25105;&#20204;&#36824;&#20801;&#35768;&#20462;&#25913;&#35746;&#36141;&#25968;&#37327;&#20197;&#28385;&#36275;&#20379;&#24212;&#21830;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#35746;&#36141;&#26368;&#20302;&#25968;&#37327;&#21644;&#25209;&#27425;&#22823;&#23567;&#32422;&#26463;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22788;&#29702;&#20219;&#24847;&#21040;&#36135;&#21160;&#24577;&#25110;&#20219;&#24847;&#21518;&#32493;&#22788;&#29702;&#30340;&#35746;&#36141;&#25968;&#37327;&#30340;&#30740;&#31350;&#12290;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;Madeka&#31561;&#65292;2022&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21516;&#26679;&#23558;&#21608;&#26399;&#24615;&#23457;&#26680;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#23450;&#20041;&#20026;&#22806;&#37096;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#29366;&#24577;&#19981;&#21463;&#20195;&#29702;&#30340;&#25511;&#21046;&#12290;Madeka&#31561;&#20154;&#65288;2022&#65289;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#22238;&#25918;&#21382;&#21490;&#25968;&#25454;&#20197;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#20363;&#23376;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#32435;&#20837;&#21040;&#36135;&#36807;&#31243;&#30340;&#21382;&#21490;&#22238;&#25918;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we address the problem of learning and backtesting inventory control policies in the presence of general arrival dynamics -- which we term as a quantity-over-time arrivals model (QOT). We also allow for order quantities to be modified as a post-processing step to meet vendor constraints such as order minimum and batch size constraints -- a common practice in real supply chains. To the best of our knowledge this is the first work to handle either arbitrary arrival dynamics or an arbitrary downstream post-processing of order quantities. Building upon recent work (Madeka et al., 2022) we similarly formulate the periodic review inventory control problem as an exogenous decision process, where most of the state is outside the control of the agent. Madeka et al. (2022) show how to construct a simulator that replays historic data to solve this class of problem. In our case, we incorporate a deep generative model for the arrivals process as part of the history replay. By formulat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.12817</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;&#28857;&#20113;&#20998;&#21106;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#38169;Transformer&#27169;&#22411;&#65288;MIT&#65289;&#65292;&#29992;&#20110;&#32771;&#34385;2D&#21644;3D&#25968;&#25454;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#22312;&#28857;&#20113;&#20998;&#21106;&#20013;&#20114;&#34917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;2D&#27880;&#37322;&#26469;&#23454;&#29616;2D-3D&#20449;&#24687;&#34701;&#21512;&#12290;&#37492;&#20110;&#28857;&#20113;&#30340;&#39640;&#27880;&#37322;&#25104;&#26412;&#65292;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26377;&#25928;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#38656;&#27714;&#38750;&#24120;&#36843;&#20999;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#22330;&#26223;&#32423;&#31867;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#35745;&#31639;3D&#28857;&#20113;&#21644;2D&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#12290;&#35299;&#30721;&#22120;&#23454;&#29616;&#20132;&#38169;&#30340;2D-3D&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#24182;&#36827;&#34892;&#38544;&#24335;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#35299;&#30721;&#22120;&#23618;&#20013;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#26159;&#20114;&#34917;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#29992;&#20110;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#21644;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#12289;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#31561;&#39640;&#25928;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.09126</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#29992;&#20110;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising. (arXiv:2310.09126v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#29992;&#20110;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#21644;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#12289;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#31561;&#39640;&#25928;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#22312;&#31227;&#21160;&#25668;&#24433;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#20027;&#27969;&#26041;&#27861;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#26367;&#20195;&#23545;&#24212;&#30495;&#23454;&#25968;&#25454;&#30340;&#39640;&#25928;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21463;&#22122;&#22768;&#27169;&#22411;&#31934;&#24230;&#30340;&#38480;&#21046;&#65292;&#38477;&#20302;&#20102;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#26694;&#26550;&#65292;&#23398;&#20064;&#19968;&#20010;&#20174;&#26263;&#22330;&#20013;&#33719;&#24471;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#12290;PNNP&#38598;&#25104;&#20102;&#19977;&#31181;&#39640;&#25928;&#25216;&#26415;&#65306;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#65288;PND&#65289;&#65292;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#65288;PPM&#65289;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#65288;DDL&#65289;&#12290;PND&#23558;&#26263;&#22330;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#32452;&#20998;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#22788;&#29702;&#19981;&#21516;&#27700;&#24179;&#30340;&#22122;&#22768;&#65292;&#38477;&#20302;&#20102;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#30340;&#22797;&#26434;&#24230;&#12290;PPM&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20808;&#39564;&#26377;&#25928;&#22320;&#32422;&#26463;&#29983;&#25104;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light raw image denoising plays a crucial role in mobile photography, and learning-based methods have become the mainstream approach. Training the learning-based methods with synthetic data emerges as an efficient and practical alternative to paired real data. However, the quality of synthetic data is inherently limited by the low accuracy of the noise model, which decreases the performance of low-light raw image denoising. In this paper, we develop a novel framework for accurate noise modeling that learns a physics-guided noise neural proxy (PNNP) from dark frames. PNNP integrates three efficient techniques: physics-guided noise decoupling (PND), physics-guided proxy model (PPM), and differentiable distribution-oriented loss (DDL). The PND decouples the dark frame into different components and handles different levels of noise in a flexible manner, which reduces the complexity of the noise neural proxy. The PPM incorporates physical priors to effectively constrain the generated no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23398;&#20064;&#24050;&#30693;&#39592;&#26550;&#30340;&#26377;&#30028;&#24230;&#22810;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#36825;&#23545;&#20110;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.06333</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#24050;&#30693;&#39592;&#26550;&#30340;&#26377;&#30028;&#24230;&#22810;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning bounded-degree polytrees with known skeleton. (arXiv:2310.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23398;&#20064;&#24050;&#30693;&#39592;&#26550;&#30340;&#26377;&#30028;&#24230;&#22810;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#36825;&#23545;&#20110;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#39640;&#32500;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#31867;&#20016;&#23500;&#30340;&#22810;&#26641;&#65288;polytrees&#65289;&#8212;&#8212;&#26377;&#30028;&#24230;&#22810;&#26641;&#65292;&#24314;&#31435;&#20102;&#39640;&#25928;&#36866;&#24403;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#26377;&#30028;&#24230;&#22810;&#26641;&#26159;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#23376;&#31867;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#22270;&#27169;&#22411;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;Bhattacharyya&#31561;&#20154;&#65288;2021&#65289;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#22312;&#24050;&#30693;&#26080;&#21521;&#22270;&#65288;&#39592;&#26550;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;1-&#22810;&#26641;&#24674;&#22797;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#20182;&#20204;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#23398;&#20064;&#20219;&#20309;&#26377;&#30028;&#24230;&#30340;$d$-&#22810;&#26641;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#19982;&#20449;&#24687;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#32467;&#21512;&#36215;&#26469;&#65292;&#34920;&#26126;&#23545;&#32500;&#24230;&#21644;&#30446;&#26631;&#31934;&#24230;&#21442;&#25968;&#30340;&#20381;&#36182;&#20960;&#20046;&#26159;&#32039;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03298</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#28508;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#23618;&#27425;&#21270;&#22810;&#20445;&#30495;&#24230;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;&#26041;&#27861;&#22312;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#21644;&#35774;&#35745;&#20248;&#21270;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#20302;&#20445;&#30495;&#24230;&#65288;LF&#65289;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MF&#26041;&#27861;&#20551;&#23450;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#26159;&#21160;&#24577;&#20998;&#37197;&#36164;&#28304;&#22312;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#20043;&#38388;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MF&#26041;&#27861;&#20381;&#36182;&#20110;&#20445;&#30495;&#24230;&#32423;&#21035;&#30340;&#23618;&#27425;&#20551;&#35774;&#65292;&#25110;&#32773;&#26080;&#27861;&#25429;&#25417;&#22810;&#20010;&#20445;&#30495;&#24230;&#32423;&#21035;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#24182;&#21033;&#29992;&#20854;&#26469;&#37327;&#21270;&#26410;&#26469;&#26679;&#26412;&#30340;&#20215;&#20540;&#21644;&#23548;&#33322;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#21516;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#23884;&#20837;&#21644;&#30456;&#20851;&#30340;&#20808;&#39564;-&#21518;&#39564;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#20197;&#26174;&#24335;&#22320;&#21033;&#29992;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;&#22635;&#20805;&#37319;&#26679;&#36845;&#20195;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#25105;&#20204;&#30830;&#23450;&#20855;&#26377;&#26368;&#22823;&#28508;&#21147;&#24433;&#21709;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential imp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;</title><link>http://arxiv.org/abs/2310.02255</link><description>&lt;p&gt;
MathVista: &#29992;GPT-4V&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#25968;&#23398;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#25361;&#25112;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;6141&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#26377;28&#20010;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;3&#20010;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65288;&#21363;IQTest&#12289;FunctionQA&#21644;PaperQA&#65289;&#12290;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#31934;&#32454;&#30340;&#12289;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#32452;&#21512;&#25512;&#29702;&#65292;&#36825;&#20123;&#37117;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;MathVista&#65292;&#25105;&#20204;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#20026;49.9%&#65292;&#26126;&#26174;&#20248;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#65292;&#30456;&#24046;15.1%&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
&lt;/p&gt;</description></item><item><title>GenSim&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20016;&#23500;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#19987;&#23478;&#31034;&#33539;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#27169;&#25311;&#25968;&#25454;&#20013;&#32570;&#20047;&#20219;&#21153;&#32423;&#21035;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#20219;&#21153;&#32423;&#21035;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01361</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#27169;&#25311;&#20219;&#21153;&#30340;GenSim
&lt;/p&gt;
&lt;p&gt;
GenSim: Generating Robotic Simulation Tasks via Large Language Models. (arXiv:2310.01361v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01361
&lt;/p&gt;
&lt;p&gt;
GenSim&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20016;&#23500;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#19987;&#23478;&#31034;&#33539;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#27169;&#25311;&#25968;&#25454;&#20013;&#32570;&#20047;&#20219;&#21153;&#32423;&#21035;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#20219;&#21153;&#32423;&#21035;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25910;&#38598;&#22823;&#37327;&#30495;&#23454;&#20132;&#20114;&#25968;&#25454;&#26469;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#24448;&#24448;&#20195;&#20215;&#39640;&#26114;&#65292;&#22240;&#27492;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#36890;&#24120;&#37319;&#29992;&#27169;&#25311;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#22330;&#26223;&#32423;&#21035;&#30340;&#22810;&#26679;&#24615;&#65288;&#20363;&#22914;&#23545;&#35937;&#23454;&#20363;&#21644;&#23039;&#21183;&#65289;&#65292;&#32780;&#24573;&#35270;&#20102;&#20219;&#21153;&#32423;&#21035;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#26159;&#22240;&#20026;&#38656;&#35201;&#20154;&#24037;&#21162;&#21147;&#25552;&#20986;&#21644;&#39564;&#35777;&#26032;&#30340;&#20219;&#21153;&#12290;&#36825;&#23548;&#33268;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31574;&#30053;&#24456;&#38590;&#23637;&#31034;&#26174;&#33879;&#30340;&#20219;&#21153;&#32423;&#21035;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22522;&#20110;&#22330;&#26223;&#21644;&#32534;&#30721;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20016;&#23500;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#19987;&#23478;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;GenSim&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#20004;&#31181;&#27169;&#24335;&#65306;&#30446;&#26631;&#23548;&#21521;&#29983;&#25104;&#65292;&#20854;&#20013;&#23558;&#30446;&#26631;&#20219;&#21153;&#25552;&#20379;&#32473;LLM&#65292;LLM&#25552;&#20986;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#30340;&#20219;&#21153;&#35838;&#31243;&#65307;&#25506;&#32034;&#24615;&#29983;&#25104;&#65292;&#20854;&#20013;LLM&#20174;&#20808;&#21069;&#30340;&#20219;&#21153;&#20013;&#21551;&#21160;&#65292;&#24182;&#36845;&#20195;&#22320;&#25552;&#20986;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26367;&#20195;&#23494;&#38053;&#26469;&#22797;&#21046;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16952</link><description>&lt;p&gt;
&#21033;&#29992;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging Optimization for Adaptive Attacks on Image Watermarks. (arXiv:2309.16952v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26367;&#20195;&#23494;&#38053;&#26469;&#22797;&#21046;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#38752;&#30340;&#29992;&#25143;&#21487;&#20197;&#28389;&#29992;&#22270;&#20687;&#29983;&#25104;&#22120;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#21697;&#24182;&#21442;&#19982;&#22312;&#32447;&#30340;&#22403;&#22334;&#20449;&#24687;&#25110;&#34394;&#20551;&#23459;&#20256;&#27963;&#21160;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#26631;&#35760;&#38544;&#34255;&#20449;&#24687;&#26469;&#38450;&#27490;&#28389;&#29992;&#65292;&#24182;&#20351;&#29992;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#36827;&#34892;&#26816;&#27979;&#12290;&#27700;&#21360;&#25216;&#26415;&#30340;&#26680;&#24515;&#23433;&#20840;&#23646;&#24615;&#26159;&#40065;&#26834;&#24615;&#65292;&#21363;&#25915;&#20987;&#32773;&#21482;&#33021;&#36890;&#36807;&#22823;&#24133;&#38477;&#20302;&#22270;&#20687;&#36136;&#37327;&#26469;&#36867;&#36991;&#26816;&#27979;&#12290;&#35780;&#20272;&#40065;&#26834;&#24615;&#38656;&#35201;&#20026;&#29305;&#23450;&#30340;&#27700;&#21360;&#31639;&#27861;&#35774;&#35745;&#33258;&#36866;&#24212;&#25915;&#20987;&#12290;&#35780;&#20272;&#27700;&#21360;&#31639;&#27861;&#21450;&#20854;&#65288;&#33258;&#36866;&#24212;&#65289;&#25915;&#20987;&#26102;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#30830;&#23450;&#33258;&#36866;&#24212;&#25915;&#20987;&#26159;&#21542;&#26159;&#26368;&#20248;&#30340;&#65292;&#21363;&#23427;&#26159;&#26368;&#20339;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30446;&#26631;&#20989;&#25968;&#24182;&#23558;&#33258;&#36866;&#24212;&#25915;&#20987;&#35270;&#20026;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#21019;&#24314;&#21487;&#24494;&#20998;&#30340;&#26367;&#20195;&#23494;&#38053;&#26469;&#26412;&#22320;&#22797;&#21046;&#31192;&#23494;&#27700;&#21360;&#23494;&#38053;&#65292;&#20197;&#20248;&#21270;&#25915;&#20987;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in online spam or disinformation campaigns. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. A challenge when evaluating watermarking algorithms and their (adaptive) attacks is to determine whether an adaptive attack is optimal, i.e., it is the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack's 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#20998;&#26512;&#24314;&#27169;&#65292;&#20998;&#26512;&#20102;&#32435;&#31859;&#35774;&#22791;&#30340;&#36890;&#20449;&#21644;&#33021;&#28304;&#32422;&#26463;&#23545;&#23450;&#20301;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.16034</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#20998;&#26512;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization. (arXiv:2309.16034v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#20998;&#26512;&#24314;&#27169;&#65292;&#20998;&#26512;&#20102;&#32435;&#31859;&#35774;&#22791;&#30340;&#36890;&#20449;&#21644;&#33021;&#28304;&#32422;&#26463;&#23545;&#23450;&#20301;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#25216;&#26415;&#21644;&#26448;&#26009;&#31185;&#23398;&#30340;&#36827;&#23637;&#20026;&#32435;&#31859;&#23610;&#24230;&#35774;&#22791;&#30340;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#36825;&#20123;&#35774;&#22791;&#32467;&#21512;&#20102;&#20256;&#24863;&#12289;&#35745;&#31639;&#12289;&#25968;&#25454;&#21644;&#33021;&#28304;&#20648;&#23384;&#20197;&#21450;&#26080;&#32447;&#36890;&#20449;&#12290;&#22312;&#31934;&#23494;&#21307;&#23398;&#20013;&#65292;&#36825;&#20123;&#32435;&#31859;&#35774;&#22791;&#23545;&#20110;&#30142;&#30149;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#30417;&#27979;&#21576;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#32780;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#27969;&#24341;&#23548;&#23450;&#20301;&#65292;&#21363;&#23558;&#25152;&#24863;&#30693;&#30340;&#29983;&#29289;&#20107;&#20214;&#19982;&#20107;&#20214;&#26412;&#36523;&#30340;&#20301;&#32622;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#31934;&#23494;&#21307;&#23398;&#30340;&#35282;&#24230;&#30475;&#23558;&#20855;&#26377;&#26497;&#22823;&#30340;&#30410;&#22788;&#12290;&#32435;&#31859;&#35774;&#22791;&#30340;&#32435;&#31859;&#23610;&#24230;&#29305;&#24615;&#20197;&#21450;&#34880;&#28082;&#27969;&#21160;&#29615;&#22659;&#30340;&#25361;&#25112;&#24615;&#23548;&#33268;&#30446;&#21069;&#30340;&#27969;&#24341;&#23548;&#23450;&#20301;&#26041;&#27861;&#22312;&#36890;&#20449;&#21644;&#33021;&#28304;&#30456;&#20851;&#33021;&#21147;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#32435;&#31859;&#35774;&#22791;&#30340;&#36890;&#20449;&#21644;&#33021;&#28304;&#32422;&#26463;&#23548;&#33268;&#27969;&#24341;&#23548;&#23450;&#20301;&#30340;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24314;&#27169;&#30740;&#31350;&#20102;&#36825;&#20123;&#19981;&#23436;&#32654;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in nanotechnology and material science are paving the way toward nanoscale devices that combine sensing, computing, data and energy storage, and wireless communication. In precision medicine, these nanodevices show promise for disease diagnostics, treatment, and monitoring from within the patients' bloodstreams. Assigning the location of a sensed biological event with the event itself, which is the main proposition of flow-guided in-body nanoscale localization, would be immensely beneficial from the perspective of precision medicine. The nanoscale nature of the nanodevices and the challenging environment that the bloodstream represents, result in current flow-guided localization approaches being constrained in their communication and energy-related capabilities. The communication and energy constraints of the nanodevices result in different features of raw data for flow-guided localization, in turn affecting its performance. An analytical modeling of the effects of imperfe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#26550;&#26500;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33151;&#24335;&#36816;&#21160;&#30340;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#23545;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15462</link><description>&lt;p&gt;
DTC: &#28145;&#24230;&#36319;&#36394;&#25511;&#21046;--&#19968;&#31181;&#32479;&#19968;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#21151;&#33021;&#21644;&#40065;&#26834;&#30340;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
DTC: Deep Tracking Control -- A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion. (arXiv:2309.15462v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#26550;&#26500;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33151;&#24335;&#36816;&#21160;&#30340;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#23545;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33151;&#24335;&#36816;&#21160;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#38656;&#35201;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26469;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#65292;&#33151;&#24335;&#31995;&#32479;&#20351;&#29992;&#36870;&#21160;&#21147;&#23398;&#30340;&#36712;&#36857;&#20248;&#21270;&#25511;&#21046;&#12290;&#36825;&#31181;&#23618;&#27425;&#21270;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22240;&#20854;&#30452;&#35266;&#30340;&#25104;&#26412;&#20989;&#25968;&#35843;&#25972;&#12289;&#20934;&#30830;&#30340;&#35268;&#21010;&#20197;&#21450;&#36229;&#36807;&#21313;&#24180;&#30340;&#24191;&#27867;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#28145;&#21051;&#29702;&#35299;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#19981;&#21305;&#37197;&#21644;&#20551;&#35774;&#30340;&#36829;&#21453;&#26159;&#25925;&#38556;&#25805;&#20316;&#30340;&#24120;&#35265;&#21407;&#22240;&#65292;&#24182;&#21487;&#33021;&#22952;&#30861;&#25104;&#21151;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#36716;&#25442;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#20223;&#30495;&#30340;&#24378;&#21270;&#23398;&#20064;&#20135;&#29983;&#20102;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#40065;&#26834;&#24615;&#21644;&#24674;&#22797;&#33021;&#21147;&#30340;&#36816;&#21160;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#23398;&#20064;&#31639;&#27861;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#37117;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#36825;&#31181;&#29615;&#22659;&#20013;&#21512;&#27861;&#30340;&#33853;&#33050;&#28857;&#24456;&#23569;&#65292;&#27604;&#22914;&#32570;&#21475;&#25110;&#36339;&#30707;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged locomotion is a complex control problem that requires both accuracy and robustness to cope with real-world challenges. Legged systems have traditionally been controlled using trajectory optimization with inverse dynamics. Such hierarchical model-based methods are appealing due to intuitive cost function tuning, accurate planning, and most importantly, the insightful understanding gained from more than one decade of extensive research. However, model mismatch and violation of assumptions are common sources of faulty operation and may hinder successful sim-to-real transfer. Simulation-based reinforcement learning, on the other hand, results in locomotion policies with unprecedented robustness and recovery skills. Yet, all learning algorithms struggle with sparse rewards emerging from environments where valid footholds are rare, such as gaps or stepping stones. In this work, we propose a hybrid control architecture that combines the advantages of both worlds to simultaneously achie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2309.13365</link><description>&lt;p&gt;
&#20915;&#31574;&#26641;&#31574;&#30053;&#22312;IBMDP&#20013;&#30340;Actor-Critic&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65288;arXiv:2309.13365v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs. (arXiv:2309.13365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#23433;&#20840;&#26816;&#26597;&#26469;&#24314;&#31435;&#23545;&#36825;&#20123;AI&#30340;&#20449;&#20219;&#12290;&#29305;&#21035;&#26159;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#25552;&#20379;&#20102;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#20307;&#35270;&#35282;&#65292;&#24182;&#36879;&#26126;&#22320;&#25581;&#31034;&#20102;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#23545;&#20110;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20915;&#31574;&#26641;&#36807;&#22823;&#65292;&#21487;&#35299;&#37322;&#24615;&#23601;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;RL&#25506;&#32034;DT&#30340;&#31354;&#38388;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22686;&#21152;&#21160;&#20316;&#26469;&#25910;&#38598;&#20851;&#20110;&#38544;&#34255;&#36755;&#20837;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#36866;&#24403;&#22320;&#23545;&#36825;&#20123;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#20195;&#29702;&#23398;&#20064;&#22914;&#20309;&#22312;&#26641;&#30340;&#22823;&#23567;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#26435;&#34913;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#38656;&#35201;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#36825;&#19968;&#31867;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of AI models allows for user safety checks to build trust in such AIs. In particular, Decision Trees (DTs) provide a global look at the learned model and transparently reveal which features of the input are critical for making a decision. However, interpretability is hindered if the DT is too large. To learn compact trees, a recent Reinforcement Learning (RL) framework has been proposed to explore the space of DTs using deep RL. This framework augments a decision problem (e.g. a supervised classification task) with additional actions that gather information about the features of an otherwise hidden input. By appropriately penalizing these actions, the agent learns to optimally trade-off size and performance of DTs. In practice, a reactive policy for a partially observable Markov decision process (MDP) needs to be learned, which is still an open problem. We show in this paper that deep RL can fail even on simple toy tasks of this class. However, when the underlying deci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;</title><link>http://arxiv.org/abs/2309.12701</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21457;&#29616;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming. (arXiv:2309.12701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20915;&#31574;&#26641;&#30001;&#20110;&#21487;&#20197;&#34987;&#20154;&#31867;&#26816;&#26597;&#21644;&#35299;&#37322;&#32780;&#20855;&#26377;&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30828;&#20214;&#30340;&#36827;&#27493;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#30340;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#27604;&#36890;&#24120;&#30340;&#36138;&#23146;&#26041;&#27861;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#26641;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20248;&#31639;&#27861;&#36820;&#22238;&#30340;&#26159;&#19968;&#20010;&#20248;&#21270;&#25163;&#21160;&#23450;&#20041;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#21333;&#20010;&#26641;&#65292;&#36890;&#36807;&#25351;&#23450;&#26368;&#22823;&#20915;&#31574;&#33410;&#28857;&#25968;&#37327;&#26469;&#33719;&#24471;&#65292;&#23545;&#20110;&#36825;&#20010;&#26435;&#34913;&#30340;&#36136;&#37327;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65288;MDP&#65289;&#24418;&#24335;&#26469;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21333;&#19968;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#35745;&#31639;&#20986;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35753;&#29992;&#25143;&#20107;&#21518;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#26641;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and run
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11680</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#22270;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20445;&#30041;&#23545;&#25968;&#25454;&#30340;&#29420;&#21344;&#25511;&#21046;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#19987;&#26377;&#25968;&#25454;&#21019;&#24314;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#36817;&#26399;&#25552;&#20986;&#30340;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#23398;&#20064;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#20204;&#23398;&#20250;&#25429;&#25417;&#24213;&#23618;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#25512;&#29702;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;FL&#26694;&#26550;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#20840;&#23616;&#30340;NGM&#27169;&#22411;&#65292;&#20174;&#26412;&#22320;NGM&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#24179;&#22343;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;FedNGMs&#36991;&#20813;&#20102;&#31070;&#32463;&#20803;&#21305;&#37197;&#26694;&#26550;&#65288;&#22914;&#32852;&#37030;&#21305;&#37197;&#24179;&#22343;&#65289;&#20013;&#27169;&#22411;&#21442;&#25968;&#29190;&#28856;&#30340;&#32570;&#28857;&#21644;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#20840;&#23616;&#27169;&#22411;&#22823;&#23567;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;</title><link>http://arxiv.org/abs/2309.10688</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#21516;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10688
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20851;&#38190;&#21442;&#25968;&#26159;&#27599;&#20010;&#27493;&#39588;&#32771;&#34385;&#30340;&#25968;&#25454;&#37327;&#25110;&#25209;&#37327;&#22823;&#23567;B&#20197;&#21450;&#27493;&#38271;&#25110;&#23398;&#20064;&#29575;&#951;&#12290;&#23545;&#20110;&#23567;&#30340;B&#21644;&#22823;&#30340;&#951;&#65292;SGD&#23545;&#24212;&#20110;&#21442;&#25968;&#30340;&#38543;&#26426;&#28436;&#21270;&#65292;&#20854;&#22122;&#22768;&#24133;&#24230;&#30001;&#8220;&#28201;&#24230;&#8221;T=&#951;/B&#25511;&#21046;&#12290;&#28982;&#32780;&#24403;&#25209;&#37327;&#22823;&#23567;B&#8805;B^*&#36275;&#22815;&#22823;&#26102;&#65292;&#36825;&#31181;&#25551;&#36848;&#34987;&#35266;&#23519;&#21040;&#22833;&#25928;&#65292;&#25110;&#32773;&#22312;&#28201;&#24230;&#36275;&#22815;&#23567;&#26102;&#31616;&#21270;&#20026;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#21449;&#21457;&#29983;&#30340;&#20301;&#32622;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#24863;&#30693;&#22120;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20851;&#38190;&#39044;&#27979;&#20173;&#36866;&#29992;&#20110;&#28145;&#24230;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;B-&#951;&#24179;&#38754;&#19978;&#33719;&#24471;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#65292;&#23558;&#19977;&#20010;&#21160;&#24577;&#38454;&#27573;&#20998;&#24320;&#65306;&#65288;i&#65289;&#21463;&#28201;&#24230;&#25511;&#21046;&#30340;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#65292;&#65288;ii&#65289;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#21644;
&lt;/p&gt;
&lt;p&gt;
Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;Q&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.03202</link><description>&lt;p&gt;
&#23545;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;Q&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#20215;&#20540;&#36845;&#20195;&#65288;VI&#65289;&#21644;&#29366;&#24577;-&#21160;&#20316;-&#22870;&#21169;-&#29366;&#24577;-&#21160;&#20316;&#65288;SARSA&#65289;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#22522;&#20110;&#31574;&#30053;&#22806;&#30340;Q&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#21547;2000-2023&#24180;&#22810;&#24180;&#32929;&#24066;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#20998;&#26512;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#27573;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#65306;&#19968;&#20010;&#21253;&#25324;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#24180;&#20221;&#65292;&#19968;&#20010;&#19981;&#21253;&#25324;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#27604;&#22522;&#20934;&#31574;&#30053;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#20248;&#20110;Q&#23398;&#20064;&#65292;&#20984;&#26174;&#20102;&#31616;&#21333;&#31574;&#30053;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;Q&#23398;&#20064;&#30340;&#24615;&#33021;&#21487;&#33021;&#22240;&#26465;&#20214;&#30340;&#21464;&#21270;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work seeks to answer key research questions regarding the viability of reinforcement learning over the S&amp;P 500 index. The on-policy techniques of Value Iteration (VI) and State-action-reward-state-action (SARSA) are implemented along with the off-policy technique of Q-Learning. The models are trained and tested on a dataset comprising multiple years of stock market data from 2000-2023. The analysis presents the results and findings from training and testing the models using two different time periods: one including the COVID-19 pandemic years and one excluding them. The results indicate that including market data from the COVID-19 period in the training dataset leads to superior performance compared to the baseline strategies. During testing, the on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the influence of bias-variance tradeoff and the generalization capabilities of simpler policies. However, it is noted that the performance of Q-learning may vary depe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#65292;&#38024;&#23545;&#22235;&#24029;&#30465;&#30340;&#30899;&#25490;&#25918;&#24773;&#20917;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.01115</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;&#65306;&#22235;&#24029;&#30465;&#30899;&#25490;&#25918;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province. (arXiv:2309.01115v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#65292;&#38024;&#23545;&#22235;&#24029;&#30465;&#30340;&#30899;&#25490;&#25918;&#24773;&#20917;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30697;&#38453;&#24402;&#19968;&#21270;&#23545;&#22235;&#24029;&#30465;46&#20010;&#20851;&#38190;&#20135;&#19994;2000-2019&#24180;&#30340;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;DBSCAN&#32858;&#31867;&#35782;&#21035;&#20102;16&#20010;&#29305;&#24449;&#31867;&#21035;&#20197;&#23458;&#35266;&#22320;&#20998;&#32452;&#34892;&#19994;&#12290;&#25509;&#19979;&#26469;&#65292;&#37319;&#29992;&#32602;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#36807;&#25311;&#21512;&#25511;&#21046;&#12289;&#39640;&#32500;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#22797;&#26434;&#33021;&#28304;&#25968;&#25454;&#22788;&#29702;&#30340;&#20248;&#21183;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29028;&#28845;&#21608;&#22260;&#30340;&#31532;&#20108;&#20010;&#32858;&#31867;&#22240;&#29983;&#20135;&#38656;&#27714;&#32780;&#20135;&#29983;&#30340;&#25490;&#25918;&#37327;&#26368;&#39640;&#12290;&#20197;&#27773;&#27833;&#21644;&#28966;&#28845;&#20026;&#20013;&#24515;&#30340;&#32858;&#31867;&#30340;&#25490;&#25918;&#37327;&#20063;&#24456;&#26174;&#33879;&#12290;&#22522;&#20110;&#27492;&#65292;&#20943;&#25490;&#24314;&#35758;&#21253;&#25324;&#28165;&#27905;&#29028;&#25216;&#26415;&#12289;&#20132;&#36890;&#31649;&#29702;&#12289;&#38050;&#38081;&#20013;&#30340;&#29028;&#30005;&#26367;&#20195;&#21644;&#34892;&#19994;&#26631;&#20934;&#21270;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23458;&#35266;&#36873;&#25321;&#22240;&#32032;&#65292;&#24182;&#26088;&#22312;&#25506;&#32034;&#26032;&#30340;&#20943;&#25490;&#36884;&#24452;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan industries using matrix normalization. DBSCAN clustering identified 16 feature classes to objectively group industries. Penalized regression models were then applied for their advantages in overfitting control, high-dimensional data processing, and feature selection - well-suited for the complex energy data. Results showed the second cluster around coal had highest emissions due to production needs. Emissions from gasoline-focused and coke-focused clusters were also significant. Based on this, emission reduction suggestions included clean coal technologies, transportation management, coal-electricity replacement in steel, and industry standardization. The research introduced unsupervised learning to objectively select factors and aimed to explore new emission reduction avenues. In summary, the study identified industry groupings, assessed emissions drivers, and proposed scientific reduction strategies to bette
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TExplain&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#24615;&#21477;&#23376;&#26469;&#29702;&#35299;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00733</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#35270;&#35273;&#29305;&#24449;&#21040;&#25991;&#26412;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Learned Visual Features to Textual Explanations. (arXiv:2309.00733v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TExplain&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#24615;&#21477;&#23376;&#26469;&#29702;&#35299;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;TExplain&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;LLMs&#20043;&#38388;&#24314;&#31435;&#36830;&#25509;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#30340;&#21477;&#23376;&#26469;&#35299;&#37322;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#22270;&#20687;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#21477;&#23376;&#28982;&#21518;&#29992;&#20110;&#25552;&#21462;&#26368;&#39057;&#32321;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#20840;&#38754;&#29702;&#35299;&#20998;&#31867;&#22120;&#20013;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#21644;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#19982;&#35270;&#35273;&#34920;&#31034;&#23545;&#24212;&#30340;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#26469;&#25581;&#31034;&#29420;&#31435;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the learned features of vision models has posed a longstanding challenge in the field of machine learning. To address this issue, we propose a novel method that leverages the capabilities of large language models (LLMs) to interpret the learned features of pre-trained image classifiers. Our method, called TExplain, tackles this task by training a neural network to establish a connection between the feature space of image classifiers and LLMs. Then, during inference, our approach generates a vast number of sentences to explain the features learned by the classifier for a given image. These sentences are then used to extract the most frequent words, providing a comprehensive understanding of the learned features and patterns within the classifier. Our method, for the first time, utilizes these frequent words corresponding to a visual representation to provide insights into the decision-making process of the independently trained classifier, enabling the detection of spurious
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13894</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices. (arXiv:2308.13894v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#31227;&#21160;&#26234;&#33021;&#30340;&#26684;&#23616;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#20110;&#23545;&#19979;&#28216;&#31227;&#21160;&#20219;&#21153;&#36827;&#34892;LLM&#30340;&#24494;&#35843;&#65292;&#36825;&#34987;&#31216;&#20026;FedLLM&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#30001;&#24222;&#22823;&#27169;&#22411;&#22823;&#23567;&#24341;&#36215;&#30340;&#32593;&#32476;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#19982;&#31227;&#21160;&#35774;&#22791;&#30340;&#25972;&#21512;&#26041;&#38754;&#24182;&#27809;&#26377;&#23454;&#38469;&#32531;&#35299;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#32531;&#24930;&#30340;&#27169;&#22411;&#25910;&#25947;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;FedLLM&#30340;&#25928;&#29575;&#12290;FwdLLM&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#35774;&#22791;&#25191;&#34892;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#12290;&#22240;&#27492;&#65292;FwdLLM&#20855;&#26377;&#26356;&#22909;&#30340;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#65288;&#36890;&#36807;&#31227;&#21160;NPUs&#21644;&#25193;&#22823;&#30340;&#21442;&#19982;&#35774;&#22791;&#25968;&#32452;&#65289;&#12290;FwdLLM&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35774;&#35745;&#23637;&#24320;&#65306;&#65288;1&#65289;&#23558;&#26080;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#19982;p
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence.  In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.13507</link><description>&lt;p&gt;
&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#26159;&#21542;&#22686;&#21152;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27807;&#36890;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13507
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25104;&#20026;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#36890;&#24120;&#20250;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#20943;&#23569;&#38656;&#27714;&#21644;&#32534;&#30721;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;LLMs&#20063;&#24212;&#35813;&#37319;&#29992;&#21516;&#26679;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#26368;&#32456;&#20195;&#30721;&#20043;&#21069;&#25552;&#20986;&#28145;&#20837;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20943;&#36731;&#20351;&#29992;LLMs&#36827;&#34892;&#32534;&#31243;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#24847;&#22270;&#35268;&#33539;&#19981;&#26126;&#30830;&#12289;&#35745;&#31639;&#24605;&#32500;&#19981;&#36275;&#21644;&#20195;&#30721;&#36136;&#37327;&#19981;&#29702;&#24819;&#12290;&#36825;&#21453;&#36807;&#26469;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#33258;&#20449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#26356;&#22909;&#30340;&#27807;&#36890;&#25216;&#24039;&#26469;&#22686;&#21152;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27807;&#36890;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#30340;&#27807;&#36890;&#22120;&#26469;&#35782;&#21035;&#39640;&#24230;&#19981;&#30830;&#23450;&#25110;&#20449;&#24515;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf
&lt;/p&gt;</description></item><item><title>Animal3D&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21754;&#20083;&#21160;&#29289;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25163;&#24037;&#26631;&#27880;&#21644;&#26816;&#26597;&#30830;&#20445;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;Animal3D&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#24418;&#29366;&#21644;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20351;&#29992;&#21482;&#26377;Animal3D&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#12289;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#21644;&#24494;&#35843;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11737</link><description>&lt;p&gt;
Animal3D:&#19968;&#20221;&#20840;&#38754;&#30340;3D&#21160;&#29289;&#23039;&#24577;&#21644;&#24418;&#29366;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape. (arXiv:2308.11737v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11737
&lt;/p&gt;
&lt;p&gt;
Animal3D&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21754;&#20083;&#21160;&#29289;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25163;&#24037;&#26631;&#27880;&#21644;&#26816;&#26597;&#30830;&#20445;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;Animal3D&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#24418;&#29366;&#21644;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20351;&#29992;&#21482;&#26377;Animal3D&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#12289;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#21644;&#24494;&#35843;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#26159;&#29702;&#35299;&#21160;&#29289;&#34892;&#20026;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#23545;&#20110;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#31561;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#32570;&#20047;&#21253;&#21547;&#39640;&#36136;&#37327;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#27880;&#37322;&#30340;&#20840;&#38754;&#22810;&#26679;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Animal3D&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21754;&#20083;&#21160;&#29289;&#21160;&#29289;3D&#23039;&#24577;&#21644;&#24418;&#29366;&#20272;&#35745;&#30340;&#39318;&#20010;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;Animal3D&#30001;&#26469;&#33258;40&#20010;&#21754;&#20083;&#21160;&#29289;&#29289;&#31181;&#30340;3379&#20010;&#22270;&#20687;&#32452;&#25104;&#65292;&#21253;&#21547;26&#20010;&#20851;&#38190;&#28857;&#30340;&#39640;&#36136;&#37327;&#27880;&#37322;&#65292;&#20197;&#21450;SMAL&#27169;&#22411;&#30340;&#23039;&#24577;&#21644;&#24418;&#29366;&#21442;&#25968;&#12290;&#25152;&#26377;&#27880;&#37322;&#37117;&#32463;&#36807;&#22810;&#38454;&#27573;&#25163;&#24037;&#26631;&#27880;&#21644;&#26816;&#26597;&#65292;&#20197;&#30830;&#20445;&#26368;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;Animal3D&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20195;&#34920;&#24615;&#30340;&#24418;&#29366;&#21644;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#20102;&#20197;&#19979;&#22522;&#20934;&#27979;&#35797;&#65306;&#65288;1&#65289;&#21482;&#20351;&#29992;Animal3D&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#65288;2&#65289;&#20174;&#21512;&#25104;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#65288;3&#65289;&#24494;&#35843;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MC-CP&#30340;&#26032;&#22411;&#28151;&#21512;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;dropout&#26041;&#27861;&#19982;&#21512;&#35268;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33410;&#30465;&#36164;&#28304;&#21644;&#20135;&#29983;&#40065;&#26834;&#39044;&#27979;&#38598;/&#21306;&#38388;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;MC-CP&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25552;&#21319;</title><link>http://arxiv.org/abs/2308.09647</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#35268;&#30340;&#33945;&#29305;&#21345;&#27931;&#39044;&#27979;&#23454;&#29616;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction. (arXiv:2308.09647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09647
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MC-CP&#30340;&#26032;&#22411;&#28151;&#21512;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;dropout&#26041;&#27861;&#19982;&#21512;&#35268;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33410;&#30465;&#36164;&#28304;&#21644;&#20135;&#29983;&#40065;&#26834;&#39044;&#27979;&#38598;/&#21306;&#38388;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;MC-CP&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#36816;&#34892;&#25552;&#20379;&#20445;&#35777;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#20272;&#35745;&#27599;&#20010;&#39044;&#27979;&#30340;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#32771;&#34385;&#38543;&#26426;&#24615;&#21644;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#24433;&#21709;&#26469;&#25351;&#23548;&#20915;&#31574;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;UQ&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#35201;&#20040;&#38750;&#24120;&#26114;&#36149;&#65292;&#35201;&#20040;&#20135;&#29983;&#20445;&#23432;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;UQ&#26041;&#27861;MC-CP&#65292;&#23427;&#23558;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#65288;MC&#65289;dropout&#26041;&#27861;&#19982;&#21512;&#35268;&#39044;&#27979;&#65288;CP&#65289;&#30456;&#32467;&#21512;&#12290;MC-CP&#22312;&#36816;&#34892;&#26102;&#33258;&#36866;&#24212;&#35843;&#33410;&#20256;&#32479;&#30340;MC dropout&#20197;&#33410;&#30465;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#24471;&#39044;&#27979;&#21487;&#20197;&#34987;CP&#20351;&#29992;&#65292;&#24471;&#21040;&#40065;&#26834;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MC-CP&#30456;&#27604;MC dropout&#12289;RAPS&#21644;CQR&#31561;&#20808;&#36827;&#30340;UQ&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classificati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#30452;&#35266;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.09543</link><description>&lt;p&gt;
&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Latent State Models of Training Dynamics. (arXiv:2308.09543v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09543
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#30452;&#35266;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24615;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#25968;&#25454;&#39034;&#24207;&#21644;&#21021;&#22987;&#21270;&#30340;&#24046;&#24322;&#22914;&#20309;&#23454;&#38469;&#20307;&#29616;&#22312;&#27169;&#22411;&#20013;&#65292;&#20197;&#33267;&#20110;&#19968;&#20123;&#35757;&#32451;&#36816;&#34892;&#34920;&#29616;&#20986;&#33394;&#25110;&#25910;&#25947;&#26356;&#24555;&#65311;&#27492;&#22806;&#65292;&#25105;&#20204;&#22914;&#20309;&#35299;&#37322;&#20135;&#29983;&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#34920;&#24449;&#19981;&#21516;&#36712;&#36857;&#30340;&#30456;&#21464;&#65311;&#20026;&#20102;&#29702;&#35299;&#38543;&#26426;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21160;&#24577;&#21644;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#22810;&#27425;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35745;&#31639;&#21508;&#31181;&#25351;&#26631;&#65292;&#22914;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;$L_2$&#33539;&#25968;&#12289;&#22343;&#20540;&#21644;&#26041;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#32467;&#26524;&#24207;&#21015;&#19978;&#25311;&#21512;&#20102;&#19968;&#20010;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(HMM)&#12290;HMM&#34920;&#31034;&#35757;&#32451;&#36807;&#31243;&#26159;&#19968;&#20010;&#22312;&#28508;&#22312;&#29366;&#24577;&#20043;&#38388;&#36716;&#25442;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#33879;&#21464;&#21270;&#30340;&#30452;&#35266;&#27010;&#36848;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;grokking&#20219;&#21153;&#12289;&#22270;&#20687;&#20998;&#31867;&#31561;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#20302;&#32500;&#12289;&#31163;&#25955;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image clas
&lt;/p&gt;</description></item><item><title>Bandit&#21453;&#39304;&#19979;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#65292;&#26080;&#35770;&#26631;&#31614;&#31354;&#38388;&#26159;&#21542;&#26080;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.04620</link><description>&lt;p&gt;
&#22810;&#31867;&#22312;&#32447;&#23398;&#20064;&#22312;Bandit&#21453;&#39304;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04620
&lt;/p&gt;
&lt;p&gt;
Bandit&#21453;&#39304;&#19979;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#65292;&#26080;&#35770;&#26631;&#31614;&#31354;&#38388;&#26159;&#21542;&#26080;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Bandit&#21453;&#39304;&#19979;&#30340;&#22810;&#31867;&#22312;&#32447;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;(daniely2013price)&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#23637;&#31034;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#26159;&#22810;&#31867;&#22312;&#32447;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#21363;&#20351;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#30028;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;(hanneke2023multiclass)&#30340;&#26368;&#36817;&#24037;&#20316;&#65292;&#20182;&#20204;&#22312;&#26631;&#31614;&#31354;&#38388;&#26080;&#30028;&#30340;&#20840;&#20449;&#24687;&#35774;&#32622;&#20013;&#65292;&#23637;&#31034;&#20102;Littlestone&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#26080;&#20154;&#26426;&#22312;&#19977;&#32500;&#31354;&#20013;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#23567;&#21306;&#20851;&#32852;&#20915;&#31574;&#21644;&#31227;&#21160;&#36895;&#24230;&#65292;&#20197;&#25552;&#21319;&#20132;&#36890;&#21644;&#36890;&#20449;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;18.32%&#12290;</title><link>http://arxiv.org/abs/2307.13158</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#26426;&#36895;&#24230;&#25511;&#21046;&#20860;&#39038;&#36991;&#38556;&#21644;&#20132;&#25509;&#24863;&#30693;&#30340;&#23567;&#21306;&#20851;&#32852;&#65306;&#24102;&#26377;&#21160;&#20316;&#20998;&#25903;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching. (arXiv:2307.13158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#26080;&#20154;&#26426;&#22312;&#19977;&#32500;&#31354;&#20013;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#23567;&#21306;&#20851;&#32852;&#20915;&#31574;&#21644;&#31227;&#21160;&#36895;&#24230;&#65292;&#20197;&#25552;&#21319;&#20132;&#36890;&#21644;&#36890;&#20449;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;18.32%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#19977;&#32500;&#31354;&#20013;&#39640;&#36895;&#20844;&#36335;&#19978;&#22810;&#26080;&#20154;&#26426;&#30340;&#23567;&#21306;&#20851;&#32852;&#20915;&#31574;&#21644;&#31227;&#21160;&#36895;&#24230;&#65292;&#20197;&#25552;&#21319;&#20132;&#36890;&#21644;&#36890;&#20449;&#24615;&#33021;&#65292;&#21253;&#25324;&#36991;&#38556;&#12289;&#36830;&#25509;&#24615;&#21644;&#20132;&#25509;&#25928;&#26524;&#12290;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#20013;&#26080;&#20154;&#26426;&#30340;&#29366;&#24577;&#30001;&#36895;&#24230;&#21644;&#36890;&#20449;&#25968;&#25454;&#36895;&#29575;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#20915;&#31574;&#27169;&#22359;&#21644;&#22810;&#20010;&#32593;&#32476;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#19987;&#38376;&#22788;&#29702;&#20108;&#32500;&#20132;&#36890;-&#36890;&#20449;&#31354;&#38388;&#20013;&#30340;&#29305;&#23450;&#21160;&#20316;&#32500;&#24230;&#12290;&#36825;&#31181;&#35774;&#35745;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#22810;&#32500;&#21160;&#20316;&#31354;&#38388;&#65292;&#20351;&#24471;&#21508;&#20010;&#21160;&#20316;&#32500;&#24230;&#21487;&#20197;&#29420;&#31435;&#20915;&#31574;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#20998;&#25903;&#22411;&#23545;&#25239;&#24615;Q&#32593;&#32476;&#65288;BDQ&#65289;&#21644;&#20998;&#25903;&#22411;&#23545;&#25239;&#24615;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;Dueling DDQN&#65289;&#65292;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;18.32%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;NeuralSAT&#65292;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.</title><link>http://arxiv.org/abs/2307.10266</link><description>&lt;p&gt;
&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;DPLL(T)&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10266
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;NeuralSAT&#65292;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#36719;&#20214;&#19968;&#26679;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;DNNs&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#24182;&#21463;&#21040;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#22312;&#24320;&#21457;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;DNN&#39564;&#35777;&#25216;&#26415;&#21644;&#24037;&#20855;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NeuralSAT&#65292;&#19968;&#31181;&#29992;&#20110;DNN&#39564;&#35777;&#30340;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;&#12290;NeuralSAT&#30340;&#35774;&#35745;&#36981;&#24490;&#20102;&#29992;&#20110;&#29616;&#20195;SMT&#27714;&#35299;&#30340;DPLL(T)&#31639;&#27861;&#65292;&#21253;&#25324;&#65288;&#20914;&#31361;&#65289;&#23376;&#21477;&#23398;&#20064;&#12289;&#25277;&#35937;&#21644;&#29702;&#35770;&#27714;&#35299;&#65292;&#22240;&#27492;NeuralSAT&#21487;&#20197;&#34987;&#35270;&#20026;DNNs&#30340;&#19968;&#20010;SMT&#26694;&#26550;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;NeuralSAT&#21407;&#22411;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#36866;&#24403;&#30340;&#20248;&#21270;&#21644;&#24037;&#31243;&#21270;&#65292;NeuralSAT&#33021;&#22815;&#23558;&#29616;&#20195;SAT/SMT&#27714;&#35299;&#22120;&#30340;&#33021;&#21147;&#21644;&#25104;&#21151;&#24102;&#21040;DNN&#39564;&#35777;&#20013;&#12290;NeuralSAT&#21487;&#20174;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;&#65306;https://github.com/dynaroars/neuralsat-solver
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
&lt;/p&gt;</description></item><item><title>EasyTPP&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#30340;&#20013;&#24515;&#36164;&#28304;&#24211;&#65292;&#25552;&#20379;&#32479;&#19968;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#30028;&#38754;&#21644;&#24191;&#27867;&#30340;&#35780;&#20272;&#31243;&#24207;&#65292;&#35299;&#20915;&#20102;&#35813;&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.08097</link><description>&lt;p&gt;
EasyTPP: &#36808;&#21521;&#24320;&#25918;&#22522;&#20934;&#27979;&#35797;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EasyTPP: Towards Open Benchmarking Temporal Point Processes. (arXiv:2307.08097v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08097
&lt;/p&gt;
&lt;p&gt;
EasyTPP&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#30340;&#20013;&#24515;&#36164;&#28304;&#24211;&#65292;&#25552;&#20379;&#32479;&#19968;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#30028;&#38754;&#21644;&#24191;&#27867;&#30340;&#35780;&#20272;&#31243;&#24207;&#65292;&#35299;&#20915;&#20102;&#35813;&#39046;&#22495;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#12289;&#22312;&#32447;&#36141;&#29289;&#12289;&#31038;&#20132;&#32593;&#32476;&#31561;&#29616;&#23454;&#19990;&#30028;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#36825;&#31867;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65288;TPPs&#65289;&#24050;&#25104;&#20026;&#26368;&#33258;&#28982;&#21644;&#26377;&#31454;&#20105;&#21147;&#30340;&#27169;&#22411;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#30028;&#37117;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#23581;&#35797;&#20043;&#38388;&#32570;&#20047;&#19968;&#20010;&#20013;&#24515;&#22522;&#20934;&#12290;&#36825;&#31181;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23545;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#21644;&#32467;&#26524;&#30340;&#20877;&#29616;&#65292;&#21487;&#33021;&#20250;&#20943;&#24930;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyTPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#30340;&#30740;&#31350;&#36164;&#20135;&#65288;&#20363;&#22914;&#25968;&#25454;&#12289;&#27169;&#22411;&#12289;&#35780;&#20272;&#31243;&#24207;&#12289;&#25991;&#26723;&#65289;&#30340;&#38598;&#20013;&#23384;&#20648;&#24211;&#12290;&#25105;&#20204;&#30340;EasyTPP&#23545;&#27492;&#39046;&#22495;&#26377;&#20960;&#20010;&#29420;&#29305;&#30340;&#36129;&#29486;&#65306;&#32479;&#19968;&#30340;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#28155;&#21152;&#26032;&#25968;&#25454;&#38598;&#30340;&#30028;&#38754;&#65307;&#24191;&#27867;&#30340;&#35780;&#20272;&#31243;&#24207;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn't been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, the first central repository of research assets (e.g., data, models, evaluation programs, documentations) in the area of event sequence modeling. Our EasyTPP makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06564</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#35302;&#21457;&#24178;&#39044;&#26469;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22686;&#21152;&#27491;&#38754;&#26696;&#20363;&#32467;&#26524;&#30340;&#27010;&#29575;&#12290;&#36825;&#20123;&#24178;&#39044;&#26159;&#26681;&#25454;&#24178;&#39044;&#31574;&#30053;&#35302;&#21457;&#30340;&#12290;&#24378;&#21270;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#36890;&#36807;&#35797;&#38169;&#23398;&#20064;&#24178;&#39044;&#31574;&#30053;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#20551;&#35774;&#21487;&#29992;&#20110;&#25191;&#34892;&#24178;&#39044;&#30340;&#36164;&#28304;&#25968;&#37327;&#26159;&#26080;&#38480;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#36164;&#28304;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#22256;&#22659;&#26159;&#22522;&#20110;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#30340;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#26469;&#35302;&#21457;&#24178;&#39044;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#23545;&#24178;&#39044;&#30340;&#24517;&#35201;&#24615;&#25110;&#25928;&#26524;&#23384;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23558;&#26377;&#38480;&#30340;&#36164;&#28304;&#29992;&#20110;&#24178;&#39044;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes. These interventions are triggered according to an intervention policy. Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error. Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice. This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization. Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04417</link><description>&lt;p&gt;
&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#20844;&#27491;&#24863;&#30693;&#32852;&#37030;&#26497;&#23567;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31649;&#29702;&#29992;&#25143;&#25968;&#25454;&#30340;&#33258;&#30001;&#24230;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#20559;&#21521;&#20110;&#25935;&#24863;&#22240;&#32032;&#35832;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#24102;&#26377;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#20844;&#24179;&#32852;&#37030;&#24179;&#22343;&#27861; (FFALM)&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;FL&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#35757;&#32451;&#30446;&#26631;&#26045;&#21152;&#20102;&#20844;&#24179;&#32422;&#26463;&#65292;&#24182;&#35299;&#20915;&#20102;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;FFALM&#30340;&#25910;&#25947;&#36895;&#29575;&#30340;&#29702;&#35770;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;CelebA&#21644;UTKFace&#25968;&#25454;&#38598;&#20013;&#20805;&#20998;&#32771;&#34385;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;FFALM &#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#65288;&#30475;&#12289;&#35760;&#20303;&#12289;&#25512;&#29702;&#65289;&#36880;&#27493;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17778</link><description>&lt;p&gt;
&#30475;&#30475;&#12289;&#35760;&#20303;&#21644;&#25512;&#29702;&#65306;&#22522;&#20110;&#26426;&#29702;&#30340;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Look, Remember and Reason: Visual Reasoning with Grounded Rationales. (arXiv:2306.17778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17778
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#65288;&#30475;&#12289;&#35760;&#20303;&#12289;&#25512;&#29702;&#65289;&#36880;&#27493;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36827;&#34892;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#35814;&#32454;&#30740;&#31350;&#12290;&#22312;&#35768;&#22810;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38656;&#35201;&#23558;&#35270;&#35273;&#20449;&#24687;&#32039;&#23494;&#34701;&#21512;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36825;&#20010;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#31181;&#20302;&#32423;&#35270;&#35273;&#33021;&#21147;&#12290;&#23427;&#36890;&#24120;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#8220;&#30475;&#65292;&#35760;&#20303;&#65292;&#25512;&#29702;&#8221;&#30340;&#19977;&#20010;&#27493;&#39588;&#36807;&#31243;&#65306;&#36890;&#36807;&#36880;&#27493;&#36827;&#34892;&#20302;&#32423;&#35270;&#35273;&#36807;&#31243;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#65292;&#30452;&#21040;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#25105;&#20204;&#36981;&#24490;&#30456;&#21516;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#26550;&#26500;&#26356;&#25913;&#65292;&#20351;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35270;&#35273;&#25512;&#29702;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#30340;&#21407;&#29702;&#65292;&#20801;&#35768;&#25105;&#20204;&#38598;&#25104;&#20302;&#32423;&#35270;&#35273;&#33021;&#21147;&#65292;&#22914;&#23545;&#35937;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have recently shown human level performance on a variety of reasoning tasks. However, the ability of these models to perform complex visual reasoning has not been studied in detail yet. A key challenge in many visual reasoning tasks is that the visual information needs to be tightly integrated in the reasoning process. We propose to address this challenge by drawing inspiration from human visual problem solving which depends on a variety of low-level visual capabilities. It can often be cast as the three step-process of ``Look, Remember, Reason'': visual information is incrementally extracted using low-level visual routines in a step-by-step fashion until a final answer is reached. We follow the same paradigm to enable existing large language models, with minimal changes to the architecture, to solve visual reasoning problems. To this end, we introduce rationales over the visual input that allow us to integrate low-level visual capabilities, such as object recogni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#24369;&#20984;&#24046;&#20998;&#37325;&#26500;&#19982;&#21452;&#23618;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#22810;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.16761</link><description>&lt;p&gt;
&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#24369;&#20984;&#24046;&#20998;&#37325;&#26500;&#19982;&#21452;&#23618;&#35268;&#21010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs. (arXiv:2306.16761v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#24369;&#20984;&#24046;&#20998;&#37325;&#26500;&#19982;&#21452;&#23618;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#22810;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ye&#31561;&#20154;&#35774;&#35745;&#20102;&#19968;&#20010;&#35299;&#20915;&#29305;&#23450;&#31867;&#21035;&#21452;&#23618;&#35268;&#21010;&#30340;&#31639;&#27861;&#65292;&#37325;&#28857;&#26159;&#19982;&#36229;&#21442;&#25968;&#36873;&#25321;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22522;&#20110;&#20540;&#20989;&#25968;&#26041;&#27861;&#25913;&#20889;&#30340;&#24046;&#20998;&#20984;&#31639;&#27861;&#12290;&#22312;&#19979;&#23618;&#38382;&#39064;&#23436;&#20840;&#20984;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#29305;&#21035;&#24378;&#22823;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#25110;&#26368;&#23567;&#32477;&#23545;&#25910;&#32553;&#21644;&#36873;&#25321;&#31639;&#23376;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36866;&#24212;&#26356;&#22810;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#19979;&#23618;&#23436;&#20840;&#20984;&#24615;&#30340;&#22522;&#26412;&#20551;&#35774;&#22823;&#22823;&#21066;&#24369;&#20026;&#24369;&#20984;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19979;&#23618;&#38382;&#39064;&#30340;Moreau&#21253;&#32476;&#36827;&#34892;&#37325;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#37325;&#26500;&#26159;&#19968;&#20010;&#24369;&#20984;&#24046;&#20998;&#35268;&#21010;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36880;&#27493;&#25910;&#25947;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24369;&#20984;&#24046;&#20998;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Ye et al. (Mathematical Programming 2023) designed an algorithm for solving a specific class of bilevel programs with an emphasis on applications related to hyperparameter selection, utilizing the difference of convex algorithm based on the value function approach reformulation. The proposed algorithm is particularly powerful when the lower level problem is fully convex , such as a support vector machine model or a least absolute shrinkage and selection operator model. In this paper, to suit more applications related to machine learning and statistics, we substantially weaken the underlying assumption from lower level full convexity to weak convexity. Accordingly, we propose a new reformulation using Moreau envelope of the lower level problem and demonstrate that this reformulation is a difference of weakly convex program. Subsequently, we develop a sequentially convergent algorithm for solving this difference of weakly convex program. To evaluate the effectiveness of our app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#30340;&#39318;&#20010;&#26377;&#38480;&#26102;&#38388;&#23545;&#25968;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#29992;&#20110;&#39640;&#26031;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#20808;&#39564;&#20215;&#20540;&#20197;&#21450;&#23545;$\tilde{O}(\sqrt{n})$&#30028;&#38480;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2306.09136</link><description>&lt;p&gt;
&#23545;&#25968;&#36125;&#21494;&#26031;&#36951;&#25022;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Logarithmic Bayes Regret Bounds. (arXiv:2306.09136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#30340;&#39318;&#20010;&#26377;&#38480;&#26102;&#38388;&#23545;&#25968;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#29992;&#20110;&#39640;&#26031;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#20808;&#39564;&#20215;&#20540;&#20197;&#21450;&#23545;$\tilde{O}(\sqrt{n})$&#30028;&#38480;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#23548;&#20986;&#20102;&#39318;&#20010;&#26377;&#38480;&#26102;&#38388;&#23545;&#25968;&#36951;&#25022;&#36793;&#30028;&#12290;&#23545;&#20110;&#39640;&#26031;&#36172;&#21338;&#26426;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$O(c_h \log^2 n)$&#30340;&#36793;&#30028;&#65292;&#20854;&#20013;$c_h$&#26159;&#19982;&#20808;&#39564;&#30456;&#20851;&#30340;&#24120;&#37327;&#12290;&#36825;&#19982;Lai&#65288;1987&#65289;&#30340;&#28176;&#36817;&#19979;&#38480;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#26377;&#25152;&#19981;&#21516;&#65292;&#19988;&#31616;&#21333;&#19988;&#26222;&#36941;&#12290;&#20026;&#20102;&#26174;&#31034;&#19968;&#33324;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#24212;&#29992;&#20110;&#32447;&#24615;&#36172;&#21338;&#26426;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#38416;&#26126;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#20808;&#39564;&#30340;&#20215;&#20540;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#20256;&#36882;&#32473;&#23398;&#20064;&#32773;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#23427;&#20204;&#26174;&#30528;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;$\tilde{O}(\sqrt{n})$&#30028;&#38480;&#65292;&#23613;&#31649;&#23384;&#22312;&#19979;&#38480;&#65292;&#20294;&#24050;&#25104;&#20026;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive the first finite-time logarithmic regret bounds for Bayesian bandits. For Gaussian bandits, we obtain a $O(c_h \log^2 n)$ bound, where $c_h$ is a prior-dependent constant. This matches the asymptotic lower bound of Lai (1987). Our proofs mark a technical departure from prior works, and are simple and general. To show generality, we apply our technique to linear bandits. Our bounds shed light on the value of the prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve the $\tilde{O}(\sqrt{n})$ bounds, that despite the existing lower bounds, have become standard in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;YOLOv5&#26131;&#21463;&#22810;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377; important implications for the safety and reliability of object detection algorithms used in traffic.</title><link>http://arxiv.org/abs/2306.06071</link><description>&lt;p&gt;
YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. (arXiv:2306.06071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;YOLOv5&#26131;&#21463;&#22810;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377; important implications for the safety and reliability of object detection algorithms used in traffic.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;YOLOv5&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#23454;&#29616;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#33030;&#24369;&#24615;&#12290;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#65288;&#21253;&#25324;L-BFGS&#12289;FGSM&#12289;C&amp;W&#12289;BIM&#12289;PGD&#12289;One Pixel Attack&#21644;UAP&#65289;&#23545;YOLOv5&#22312;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;YOLOv5&#26131;&#21463;&#36825;&#20123;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#38543;&#30528;&#25200;&#21160;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#20998;&#31867;&#38169;&#35823;&#29575;&#20063;&#20250;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#35299;&#37322;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;&#26412;&#25991;&#30340;&#21457;&#29616;&#23545;&#20110;&#20132;&#36890;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper implements and investigates popular adversarial attacks on the YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the YOLOv5 to adversarial attacks in the context of traffic and road sign detection. The paper investigates the impact of different types of attacks, including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&amp;W) attack, the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD) attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on the accuracy of YOLOv5 in detecting traffic and road signs. The results show that YOLOv5 is susceptible to these attacks, with misclassification rates increasing as the magnitude of the perturbations increases. We also explain the results using saliency maps. The findings of this paper have important implications for the safety and reliability of object detection algorithms used in traf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06064</link><description>&lt;p&gt;
&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;NP&#38590;/&#23436;&#20840;&#32452;&#21512;&#38382;&#39064;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36229;&#36234;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#20854;&#38271;&#26399;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#26356;&#20248;&#35299;&#26469;&#36229;&#36234;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#32780;&#26053;&#34892;&#21830;&#38382;&#39064;(TSP)&#26159;&#32463;&#24120;&#34987;&#36825;&#20123;&#26041;&#27861;&#30596;&#20934;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35299;&#20915;TSP&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38382;&#39064;&#22266;&#26377;&#30340;&#8220;&#31639;&#27861;&#8221;&#26412;&#36136;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#35774;&#35745;&#29992;&#20110;TSP&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#24120;&#24120;&#21033;&#29992;&#35832;&#22914;&#26597;&#25214;&#26368;&#23567;&#29983;&#25104;&#26641;&#20043;&#31867;&#30340;&#25104;&#29087;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;TSP&#38382;&#39064;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#23545;TSP&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#20043;&#21069;&#65292;&#22312;&#30456;&#20851;&#31639;&#27861;&#19978;&#23545;&#25105;&#20204;&#30340;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. The Travelling Salesman Problem (TSP) is a prominent combinatorial optimisation problem often targeted by such approaches. However, current neural-based methods for solving TSP often overlook the inherent "algorithmic" nature of the problem. In contrast, heuristics designed for TSP frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of TSP problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on TSP instances. Our results demonstrate that, using this learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#24230;&#30456;&#20284;&#30340;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#65292;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#20851;&#31995;&#19981;&#28165;&#26224;&#32780;&#24341;&#36215;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.05023</link><description>&lt;p&gt;
&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#20013;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#24230;&#30456;&#20284;&#30340;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#65292;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#20851;&#31995;&#19981;&#28165;&#26224;&#32780;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20013;&#65292;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#25351;&#30340;&#26159;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#30340;&#30456;&#20284;&#24230;&#36807;&#39640;&#65292;&#23548;&#33268;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#28508;&#22312;&#21464;&#37327;&#20445;&#23384;&#30340;&#36755;&#20837;&#25968;&#25454;&#20449;&#24687;&#36739;&#23569;&#65292;&#26080;&#27861;&#20026;&#35299;&#30721;&#22120;&#30340;&#25968;&#25454;&#37325;&#24314;&#36807;&#31243;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#35813;&#29616;&#35937;&#19968;&#30452;&#26159;VAEs&#24615;&#33021;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20294;&#26159;&#23545;&#20110;&#21518;&#39564;&#23849;&#28291;&#30340;&#29702;&#35770;&#21364;&#30456;&#23545;&#34180;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#26631;&#20934;&#30340;VAEs&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#31867;&#37325;&#35201;&#32780;&#24120;&#35265;&#21448;&#36739;&#23569;&#30740;&#31350;&#30340;VAEs&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21363;&#20855;&#26377;&#20004;&#20010;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#30340;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#65292;&#25552;&#21319;&#20102;&#23545;&#21518;&#39564;&#23849;&#28291;&#30340;&#29702;&#35770;&#35748;&#35782;&#65292;&#35777;&#26126;&#20102;&#20854;&#25104;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
The posterior collapse phenomenon in variational autoencoders (VAEs), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAEs preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAEs performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAEs. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically, via a non-trivial theoretical analysis of linear conditional VAEs and hierarchical VAEs with two levels of latent, we prove that the cause of posterior collapses i
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.02869</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36951;&#25022;&#24179;&#34913;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02869
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#36873;&#25321;&#65292;&#20854;&#20013;&#20803;&#23398;&#20064;&#22120;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#24182;&#26681;&#25454;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#25512;&#33616;&#30340;&#31574;&#30053;&#21160;&#24577;&#20915;&#31574;&#12290;&#25105;&#20204;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#26469;&#25191;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#19982;&#27492;&#30456;&#20851;&#30340;&#26368;&#36817;&#25991;&#29486;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#27809;&#26377;&#20551;&#35774;&#20219;&#20309;&#20851;&#20110;&#22522;&#26412;&#23398;&#20064;&#22120;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#20505;&#36873;&#36951;&#25022;&#20445;&#35777;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#25581;&#31034;&#36825;&#20123;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#20803;&#23398;&#20064;&#22120;&#33021;&#22815;&#21033;&#29992;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#23454;&#38469;&#36951;&#25022;&#65288;&#32780;&#19981;&#26159;&#26399;&#26395;&#36951;&#25022;&#65289;&#65292;&#24182;&#25361;&#36873;&#20986;&#26368;&#20339;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#25805;&#20316;&#26356;&#20026;&#38596;&#24515;&#21187;&#21187;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#24182;&#19988;&#38500;&#20102;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#35777;&#26126;&#27169;&#22411;&#36873;&#25321;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22788;&#29702;&#23454;&#38469;&#36951;&#25022;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets ins
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01589</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#21407;&#23376;&#27169;&#25311;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#32780;&#29983;&#25104;&#21442;&#32771;&#35745;&#31639;&#26159;&#35745;&#31639;&#19978;&#35201;&#27714;&#24456;&#39640;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25551;&#36848;&#21270;&#23398;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#26680;&#22343;&#20540;&#23884;&#20837;&#12290;&#25105;&#20204;&#20174;&#39044;&#20808;&#22312;OC20&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#30340;GNN&#20013;&#25552;&#21462;&#29305;&#24449;&#26144;&#23556;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20174;&#20652;&#21270;&#36807;&#31243;&#30340;&#31995;&#32479;&#29305;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#36890;&#36807;&#28789;&#27963;&#30340;&#26680;&#20989;&#25968;&#26469;&#22686;&#24378;&#65292;&#35813;&#26680;&#20989;&#25968;&#21253;&#25324;&#21270;&#23398;&#29289;&#31181;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36880;&#28176;&#22797;&#26434;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#65292;&#25913;&#36827;&#20102;&#20381;&#36182;GNNs&#25110;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18394</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#23398;&#20064;&#30340;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27491;&#21017;&#21270;&#24120;&#29992;&#20110;&#35299;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#20808;&#39564;&#20449;&#24687;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#21442;&#25968;&#21152;&#20197;&#26435;&#34913;&#65292;&#32780;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#20363;&#22914;&#24046;&#24322;&#21407;&#21017;&#21644;L-&#26354;&#32447;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#21512;&#36866;&#30340;&#21442;&#25968;&#20540;&#65292;&#20294;&#26159;&#36817;&#24180;&#26469;&#65292;&#19968;&#31181;&#21483;&#20570;&#21452;&#23618;&#23398;&#20064;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#29992;&#20110;&#30830;&#23450;&#26368;&#20248;&#21442;&#25968;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#31574;&#30053;&#26377;&#21508;&#31181;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21452;&#23618;&#23398;&#20064;&#30340;&#33391;&#22909;&#24615;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#26465;&#20214;&#26469;&#34920;&#24449;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#27491;&#20540;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.17028</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26356;&#22909;Batch&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#36807;&#20110;&#31616;&#21333;&#21270;&#38382;&#39064;&#65292;&#20551;&#35774;&#35823;&#24046;&#36807;&#31243;&#26159;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#35823;&#24046;&#36807;&#31243;&#20013;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#23545;&#20915;&#31574;&#24615;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#20943;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#35823;&#24046;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#22686;&#24378;&#27010;&#29575;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#36896;&#19968;&#20010;mini-batch&#65292;&#20316;&#20026;$D$&#20010;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#27573;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#26174;&#24335;&#22320;&#23398;&#20064;&#19968;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#35206;&#30422;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#35823;&#24046;&#30456;&#20851;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#21487;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
&lt;/p&gt;</description></item><item><title>DiffusionNAG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#31070;&#32463;&#32467;&#26500;&#30340;&#26377;&#21521;&#22270;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#22120;&#30340;&#25351;&#23548;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#20219;&#21153;&#26368;&#20248;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.16943</link><description>&lt;p&gt;
DiffusionNAG: &#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39044;&#27979;&#24341;&#23548;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models. (arXiv:2305.16943v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16943
&lt;/p&gt;
&lt;p&gt;
DiffusionNAG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#31070;&#32463;&#32467;&#26500;&#30340;&#26377;&#21521;&#22270;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#22120;&#30340;&#25351;&#23548;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#20219;&#21153;&#26368;&#20248;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#26041;&#27861;&#23384;&#22312;&#30528;&#23545;&#35768;&#22810;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#26550;&#26500;&#36827;&#34892;&#37325;&#22797;&#37319;&#26679;&#21644;&#35757;&#32451;&#25152;&#38656;&#30340;&#36807;&#38271;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;NAS&#36716;&#21521;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#26465;&#20214;&#31070;&#32463;&#32467;&#26500;&#29983;&#25104;(NAG)&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;DiffusionNAG&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#32467;&#26500;&#35270;&#20026;&#26377;&#21521;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32467;&#26500;&#30340;&#22270;&#25193;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#22120;&#30340;&#25351;&#23548;&#19979;&#65292;DiffusionNAG&#21487;&#20197;&#36890;&#36807;&#20174;&#26356;&#26377;&#21487;&#33021;&#28385;&#36275;&#25152;&#38656;&#29305;&#24615;&#30340;&#21306;&#22495;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#28789;&#27963;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#20219;&#21153;&#26368;&#20248;&#32467;&#26500;&#12290;&#19982;&#20351;&#29992;&#23646;&#24615;&#39044;&#27979;&#22120;&#23545;&#26550;&#26500;&#36827;&#34892;&#37319;&#26679;&#21644;&#36807;&#28388;&#30340;&#20808;&#21069;NAS&#26041;&#26696;&#30456;&#27604;&#65292;&#36825;&#31181;&#26465;&#20214;NAG&#26041;&#26696;&#26174;&#33879;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20110;&#39044;&#27979;&#22120;&#30340;NAS&#22330;&#26223;&#19979;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DiffusionNAG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35889;&#21464;&#25442;&#65288;ST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#35843;&#33410;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39057;&#35889;&#65292;&#24182;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#12290;&#20854;&#20013;&#65292;INTL&#26159;ST&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#33021;&#22815;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#35843;&#33410;&#21040;&#19968;&#20010;&#31561;&#29305;&#24449;&#20540;&#20998;&#24067;&#65292;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16789</link><description>&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#35843;&#33410;&#39057;&#35889;
&lt;/p&gt;
&lt;p&gt;
Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35889;&#21464;&#25442;&#65288;ST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#35843;&#33410;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39057;&#35889;&#65292;&#24182;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#12290;&#20854;&#20013;&#65292;INTL&#26159;ST&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#33021;&#22815;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#35843;&#33410;&#21040;&#19968;&#20010;&#31561;&#29305;&#24449;&#20540;&#20998;&#24067;&#65292;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30333;&#21270;&#25439;&#22833;&#20026;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#36991;&#20813;&#20102;&#29305;&#24449;&#23849;&#28291;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35889;&#21464;&#25442;&#65288;ST&#65289;&#26694;&#26550;&#65292;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#26144;&#23556;&#21040;&#25152;&#38656;&#30340;&#20998;&#24067;&#65292;&#24182;&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#36890;&#36807;&#38544;&#24335;&#26799;&#24230;&#26356;&#26032;&#26469;&#35843;&#21046;&#23884;&#20837;&#30340;&#39057;&#35889;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30333;&#21270;&#21464;&#25442;&#26159;ST&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#36824;&#26377;&#20854;&#20182;&#23454;&#20363;&#21487;&#20197;&#36991;&#20813;&#23849;&#28291;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;INTL&#65288;IterNorm with trace loss&#65289;&#30340;&#26032;&#23454;&#20363;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;INTL&#21487;&#20197;&#36991;&#20813;&#23849;&#28291;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#35843;&#33410;&#21040;&#19968;&#20010;&#31561;&#29305;&#24449;&#20540;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;INTL&#23454;&#29616;&#20102;76.6&#65285;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#27969;&#24418;&#25193;&#25955;&#22330;&#26159;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#29983;&#25104;&#36830;&#32493;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#29305;&#24449;&#20989;&#25968;&#23450;&#20041;&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;&#22352;&#26631;&#31995;&#65292;&#24182;&#19988;&#20351;&#29992;&#22810;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#34920;&#31034;&#20989;&#25968;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.15586</link><description>&lt;p&gt;
&#27969;&#24418;&#25193;&#25955;&#22330;
&lt;/p&gt;
&lt;p&gt;
Manifold Diffusion Fields. (arXiv:2305.15586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15586
&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#25193;&#25955;&#22330;&#26159;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#29983;&#25104;&#36830;&#32493;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#29305;&#24449;&#20989;&#25968;&#23450;&#20041;&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;&#22352;&#26631;&#31995;&#65292;&#24182;&#19988;&#20351;&#29992;&#22810;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#34920;&#31034;&#20989;&#25968;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#27969;&#24418;&#25193;&#25955;&#22330;&#65288;MDF&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#23450;&#20041;&#36830;&#32493;&#20989;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#35889;&#20960;&#20309;&#20998;&#26512;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#36890;&#36807;Laplace-Beltrami&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#23450;&#20041;&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;&#22352;&#26631;&#31995;&#12290;MDF&#20351;&#29992;&#22810;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#26500;&#25104;&#30340;&#26174;&#24335;&#21442;&#25968;&#21270;&#26469;&#34920;&#31034;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#27969;&#24418;&#19978;&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#19988;&#23545;&#27969;&#24418;&#30340;&#21018;&#24615;&#21644;&#31561;&#36317;&#21464;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27969;&#24418;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;MDF&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Manifold Diffusion Fields (MDF), an approach to learn generative models of continuous functions defined over Riemannian manifolds. Leveraging insights from spectral geometry analysis, we define an intrinsic coordinate system on the manifold via the eigen-functions of the Laplace-Beltrami Operator. MDF represents functions using an explicit parametrization formed by a set of multiple input-output pairs. Our approach allows to sample continuous functions on manifolds and is invariant with respect to rigid and isometric transformations of the manifold. Empirical results on several datasets and manifolds show that MDF can capture distributions of such functions with better diversity and fidelity than previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.14189</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#65306;&#22686;&#21152;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#31034;&#35789;&#35821;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#20013;&#65292;&#20351;&#29992;&#20849;&#20139;&#30340;&#35789;&#27719;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#38500;&#20102;&#31616;&#21333;&#30340;&#35774;&#35745;&#22806;&#65292;&#20849;&#20139;&#26631;&#35760;&#22312;&#31215;&#26497;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20551;&#35774;&#20849;&#20139;&#26631;&#35760;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#35789;&#27719;&#30340;&#37325;&#21472;&#36739;&#23567;&#26102;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#19981;&#21516;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#36716;&#31227;&#34987;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35789;&#31561;&#20215;&#31867;&#23450;&#20041;&#20102;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#65292;&#24182;&#20381;&#36182;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65306;1) &#20855;&#26377;&#30456;&#20284;&#21547;&#20041;&#30340;&#35789;&#30340;&#23884;&#20837;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26356;&#22909;&#22320;&#23545;&#40784;&#65292;2) &#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39640;&#21644;&#20302;&#36164;&#28304;MNMT&#26041;&#38754;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;BLEU&#25552;&#21319;&#36798;2.3&#20010;&#28857;&#65292;3) &#38656;&#35201;&#23569;&#20110;1.0%&#30340;&#39069;&#22806;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#30340;&#22686;&#21152;&#26377;&#38480;&#65292;&#32780;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12997</link><description>&lt;p&gt;
EXACT&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20840;&#38754;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EXACT: Extensive Attack for Split Learning. (arXiv:2305.12997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35757;&#32451;&#21644;&#37096;&#32626;&#21033;&#29992;&#31169;&#20154;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#20351;&#25105;&#20204;&#22312;&#25512;&#26029;&#26399;&#38388;&#23436;&#20840;&#36991;&#20813;&#19982;&#31532;&#19977;&#26041;&#26381;&#21153;&#22120;&#20849;&#20139;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19982;&#26381;&#21153;&#22120;&#31471;&#30456;&#27604;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#36890;&#24120;&#36739;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#19968;&#23567;&#32452;&#35774;&#22791;&#29305;&#24449;&#19988;&#38656;&#35201;&#36275;&#22815;&#23567;&#25165;&#33021;&#22312;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#12290;&#20998;&#24067;&#24335;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#23558;&#19968;&#20010;&#22823;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#65292;&#22823;&#37096;&#20998;&#20301;&#20110;&#26381;&#21153;&#22120;&#31471;&#65292;&#23567;&#37096;&#20998;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#26088;&#22312;&#25972;&#21512;&#31169;&#26377;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#38656;&#35201;&#22312;&#20998;&#30028;&#22788;&#20132;&#25442;&#26799;&#24230;&#65292;&#36825;&#21487;&#33021;&#32534;&#30721;&#31169;&#26377;&#29305;&#24449;&#25110;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EXACT&#65288;Extensive Attack for Split Learning&#65289;&#30340;&#26032;&#39062;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#30340;&#22122;&#22768;&#23454;&#29616;&#23433;&#20840;&#30340;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving machine learning (PPML) can help us train and deploy models that utilize private information. In particular, on-device Machine Learning allows us to completely avoid sharing information with a third-party server during inference. However, on-device models are typically less accurate when compared to the server counterparts due to the fact that (1) they typically only rely on a small set of on-device features and (2) they need to be small enough to run efficiently on end-user devices. Split Learning (SL) is a promising approach that can overcome these limitations. In SL, a large machine learning model is divided into two parts, with the bigger part residing on the server-side and a smaller part executing on-device, aiming to incorporate the private features. However, end-to-end training of such models requires exchanging gradients at the cut layer, which might encode private features or labels. In this paper, we provide insights into potential privacy risks associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#28145;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#35777;&#26126;&#20102;$\mathcal{O}(\log d)$&#30340;&#28145;&#24230;&#23601;&#36275;&#20197;&#23454;&#29616;&#26222;&#36866;&#24615;&#65292;&#29992;CNN&#23398;&#20064;&#31232;&#30095;&#20989;&#25968;&#21482;&#38656;&#35201;$\tilde{\mathcal{O}}(\log^2d)$&#20010;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23616;&#37096;&#36830;&#25509;&#32593;&#32476;&#65288;LCN&#65289;&#20998;&#26512;&#20102;&#26435;&#37325;&#20849;&#20139;&#21644;&#23616;&#37096;&#24615;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#21306;&#21035;&#65292;&#24471;&#20986;&#20102;&#23427;&#20204;&#22312;&#34920;&#31034;&#38656;&#35201;&#26377;&#38480;&#24179;&#31227;&#31561;&#21464;&#21644;&#39640;&#26041;&#21521;&#36873;&#25321;&#24615;&#30340;&#20989;&#25968;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08404</link><description>&lt;p&gt;
&#28145;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#24402;&#32435;&#20559;&#32622;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analysis of Inductive Biases in Deep Convolutional Networks. (arXiv:2305.08404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#28145;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#35777;&#26126;&#20102;$\mathcal{O}(\log d)$&#30340;&#28145;&#24230;&#23601;&#36275;&#20197;&#23454;&#29616;&#26222;&#36866;&#24615;&#65292;&#29992;CNN&#23398;&#20064;&#31232;&#30095;&#20989;&#25968;&#21482;&#38656;&#35201;$\tilde{\mathcal{O}}(\log^2d)$&#20010;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23616;&#37096;&#36830;&#25509;&#32593;&#32476;&#65288;LCN&#65289;&#20998;&#26512;&#20102;&#26435;&#37325;&#20849;&#20139;&#21644;&#23616;&#37096;&#24615;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#21306;&#21035;&#65292;&#24471;&#20986;&#20102;&#23427;&#20204;&#22312;&#34920;&#31034;&#38656;&#35201;&#26377;&#38480;&#24179;&#31227;&#31561;&#21464;&#21644;&#39640;&#26041;&#21521;&#36873;&#25321;&#24615;&#30340;&#20989;&#25968;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;CNN&#22312;&#35270;&#35273;&#20219;&#21153;&#19978;&#34920;&#29616;&#24322;&#24120;&#20986;&#33394;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;CNN&#30340;&#26222;&#36866;&#24615;&#65292;&#21363;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathcal{O}(\log d)$&#30340;&#28145;&#24230;&#23601;&#36275;&#20197;&#23454;&#29616;&#26222;&#36866;&#24615;&#65292;&#20854;&#20013;$d$&#26159;&#36755;&#20837;&#32500;&#24230;&#12290;&#36825;&#30456;&#27604;&#20110;&#29616;&#26377;&#32467;&#26524;&#38656;&#35201;$\Omega(d)$&#30340;&#28145;&#24230;&#26159;&#19968;&#39033;&#37325;&#22823;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#29992;CNN&#23398;&#20064;&#31232;&#30095;&#20989;&#25968;&#21482;&#38656;&#35201;$\tilde{\mathcal{O}}(\log^2d)$&#20010;&#26679;&#26412;&#65292;&#34920;&#26126;&#28145;&#24230;CNN&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#31243;&#31232;&#30095;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#20849;&#20139;&#26435;&#37325;&#21644;&#23616;&#37096;&#24615;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#36890;&#36807;&#23545;&#31216;&#24615;&#24471;&#20986;&#32467;&#35770;&#12290;&#20026;&#20102;&#21306;&#20998;&#36825;&#20004;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23616;&#37096;&#36830;&#25509;&#32593;&#32476;&#65288;LCN&#65289;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#34920;&#31034;&#38656;&#35201;&#26377;&#38480;&#24179;&#31227;&#31561;&#21464;&#21644;&#39640;&#26041;&#21521;&#36873;&#25321;&#24615;&#30340;&#20989;&#25968;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#28145;CNN&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#21147;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases in convolutional neural networks (CNNs), which are believed to be vital drivers behind CNNs' exceptional performance on vision-like tasks. We first analyze the universality of CNNs, i.e., the ability to approximate continuous functions. We prove that a depth of $\mathcal{O}(\log d)$ is sufficient for achieving universality, where $d$ is the input dimension. This is a significant improvement over existing results that required a depth of $\Omega(d)$. We also prove that learning sparse functions with CNNs needs only $\tilde{\mathcal{O}}(\log^2d)$ samples, indicating that deep CNNs can efficiently capture long-range sparse correlations. Note that all these are achieved through a novel combination of increased network depth and the utilization of multichanneling and downsampling.  Lastly, we study the inductive biases of weight sharing and locality through the lens of symmetry. To separate two biases, we introduce locally-connected networks (LCN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04073</link><description>&lt;p&gt;
&#29992;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Explaining RL Decisions with Trajectories. (arXiv:2305.04073v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26159;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#20013;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#36825;&#20123;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#29992;&#32534;&#30721;&#30340;&#26041;&#24335;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature, the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set. Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video gam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00418</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#27880;&#37322;&#12289;&#29616;&#26377;&#20195;&#30721;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#26469;&#29983;&#25104;&#20195;&#30721;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#19977;&#20010;&#29983;&#25104;&#27169;&#22411;&#65288;CodeGen&#12289;Codex&#21644;GPT-3.5&#65289;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#65288;HumanEval&#21644;Evosuite SF110&#65289;&#26469;&#35843;&#26597;&#29615;&#22659;&#29983;&#25104;&#23545;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26681;&#25454;&#32534;&#35793;&#29575;&#12289;&#27979;&#35797;&#27491;&#30830;&#24615;&#12289;&#35206;&#30422;&#29575;&#21644;&#27979;&#35797;&#21619;&#36947;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Codex&#27169;&#22411;&#22312;HumanEval&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;80%&#30340;&#35206;&#30422;&#29575;&#65292;&#20294;&#22312;EvoSuite SF110&#22522;&#20934;&#20013;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#36229;&#36807;2%&#30340;&#35206;&#30422;&#29575;&#12290;&#29983;&#25104;&#30340;&#27979;&#35797;&#36824;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#65292;&#27604;&#22914;&#37325;&#22797;&#30340;&#26029;&#35328;&#21644;&#31354;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#31995;&#21015;&#24402;&#19968;&#21270;&#30697;&#26469;&#20351;&#29992;&#23376;&#39640;&#26031;&#20869;&#22312;&#30697;&#33539;&#23454;&#29616;&#32039;&#20945;&#30340;&#38750;&#28176;&#36827;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#30340;Hoeffding&#23376;&#39640;&#26031;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23376;&#39640;&#26031;&#22270;&#26816;&#26597;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#22823;&#23567;&#30340;&#23376;&#39640;&#26031;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.07287</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#39640;&#26031;&#20869;&#22312;&#30697;&#33539;&#23454;&#29616;&#32039;&#20945;&#30340;&#38750;&#28176;&#36827;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm. (arXiv:2303.07287v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#31995;&#21015;&#24402;&#19968;&#21270;&#30697;&#26469;&#20351;&#29992;&#23376;&#39640;&#26031;&#20869;&#22312;&#30697;&#33539;&#23454;&#29616;&#32039;&#20945;&#30340;&#38750;&#28176;&#36827;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#30340;Hoeffding&#23376;&#39640;&#26031;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23376;&#39640;&#26031;&#22270;&#26816;&#26597;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#22823;&#23567;&#30340;&#23376;&#39640;&#26031;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of achieving tight non-asymptotic inference by using sub-Gaussian intrinsic moment norm through maximizing a series of normalized moments, which can lead to tighter Hoeffding's sub-Gaussian concentration inequalities and can be checked with sub-Gaussian plot for sub-Gaussian data with a finite sample size.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#28176;&#36827;&#32479;&#35745;&#25512;&#26029;&#20013;&#65292;&#23376;&#39640;&#26031;&#20998;&#24067;&#30340;&#26041;&#24046;&#31867;&#22411;&#21442;&#25968;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#32463;&#39564;&#30697;&#29983;&#25104;&#20989;&#25968;&#65288;MGF&#65289;&#30340;&#30452;&#25509;&#20272;&#35745;&#36825;&#20123;&#21442;&#25968;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#31995;&#21015;&#24402;&#19968;&#21270;&#30697;&#26469;&#20351;&#29992;&#23376;&#39640;&#26031;&#20869;&#22312;&#30697;&#33539;[Buldygin&#21644;Kozachenko&#65288;2000&#65289;&#65292;&#23450;&#29702;1.3]&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25512;&#33616;&#30340;&#33539;&#25968;&#19981;&#20165;&#21487;&#20197;&#24674;&#22797;&#30456;&#24212;MGF&#30340;&#25351;&#25968;&#30697;&#30028;&#38480;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#30340;Hoeffding&#23376;&#39640;&#26031;&#27987;&#24230;&#19981;&#31561;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23376;&#39640;&#26031;&#22270;&#26816;&#26597;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#22823;&#23567;&#30340;&#23376;&#39640;&#26031;&#25968;&#25454;&#12290;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#25554;&#20837;&#26041;&#27861;&#40065;&#26834;&#22320;&#20272;&#35745;&#20869;&#22312;&#30697;&#33539;&#25968;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24212;&#29992;&#20110;&#38750;&#28176;&#36827;&#20998;&#26512;&#65292;&#21253;&#25324;&#22810;&#33218;&#36172;&#21338;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
In non-asymptotic statistical inferences, variance-type parameters of sub-Gaussian distributions play a crucial role. However, direct estimation of these parameters based on the empirical moment generating function (MGF) is infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series of normalized moments. Importantly, the recommended norm can not only recover the exponential moment bounds for the corresponding MGFs, but also lead to tighter Hoeffding's sub-Gaussian concentration inequalities. In practice, {\color{black} we propose an intuitive way of checking sub-Gaussian data with a finite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be robustly estimated via a simple plug-in approach. Our theoretical results are applied to non-asymptotic analysis, including the multi-armed bandit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05479</link><description>&lt;p&gt;
Cal-QL: &#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#39044;&#20808;&#35757;&#32451;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31574;&#30053;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#26377;&#38480;&#20114;&#21160;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22312;&#32447;&#24494;&#35843;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#21021;&#22987;&#21270;&#65292;&#24182;&#20351;&#20854;&#20855;&#22791;&#24555;&#36895;&#30340;&#22312;&#32447;&#24494;&#35843;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;Cal-QL&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20302;&#20272;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#22312;&#21512;&#29702;&#30340;&#33539;&#22260;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#20063;&#33021;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#65292;&#24182;&#24635;&#32467;&#20102;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.11337</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Matrix Decomposition and Applications. (arXiv:2302.11337v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#65292;&#24182;&#24635;&#32467;&#20102;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#30340;&#21807;&#19968;&#30446;&#30340;&#26159;&#20026;&#20102;&#32473;&#20986;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#27010;&#24565;&#21644;&#25968;&#23398;&#24037;&#20855;&#30340;&#33258;&#21253;&#21547;&#20171;&#32461;&#65292;&#20197;&#20415;&#22312;&#21518;&#32493;&#31456;&#33410;&#20013;&#26080;&#32541;&#24341;&#20837;&#30697;&#38453;&#20998;&#35299;&#25216;&#26415;&#21450;&#20854;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28165;&#26970;&#22320;&#24847;&#35782;&#21040;&#25105;&#20204;&#26080;&#27861;&#35206;&#30422;&#20851;&#20110;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#25152;&#26377;&#26377;&#29992;&#21644;&#26377;&#36259;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#30001;&#20110;&#35752;&#35770;&#30340;&#33539;&#22260;&#26377;&#38480;&#65292;&#20363;&#22914;&#20998;&#26512;&#21464;&#20998;&#25512;&#29702;&#20197;&#36827;&#34892;&#20248;&#21270;&#30340;&#20998;&#31163;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#35835;&#32773;&#24341;&#23548;&#21040;&#36125;&#21494;&#26031;&#20998;&#26512;&#39046;&#22495;&#30340;&#25991;&#29486;&#20013;&#65292;&#20197;&#20415;&#26356;&#35814;&#32454;&#22320;&#20171;&#32461;&#30456;&#20851;&#39046;&#22495;&#12290;&#26412;&#20070;&#20027;&#35201;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;&#20363;&#22914;&#23454;&#20540;&#20998;&#35299;&#12289;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#12289;&#36125;&#21494;&#26031;&#25554;&#20540;&#20998;&#35299;&#65289;&#30340;&#30446;&#30340;&#21644;&#24847;&#20041;&#65292;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#36215;&#28304;&#21644;&#22797;&#26434;&#24615;&#23545;&#20854;&#24212;&#29992;&#25552;&#20379;&#30340;&#21551;&#31034;&#12290;&#25968;&#23398;&#20808;&#20915;&#26465;&#20214;&#26159;&#31532;&#19968;&#38376;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sole aim of this book is to give a self-contained introduction to concepts and mathematical tools in Bayesian matrix decomposition in order to seamlessly introduce matrix decomposition techniques and their applications in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results concerning Bayesian matrix decomposition and given the paucity of scope to present this discussion, e.g., the separated analysis of variational inference for conducting the optimization. We refer the reader to literature in the field of Bayesian analysis for a more detailed introduction to the related fields.  This book is primarily a summary of purpose, significance of important Bayesian matrix decomposition methods, e.g., real-valued decomposition, nonnegative matrix factorization, Bayesian interpolative decomposition, and the origin and complexity of the methods which shed light on their applications. The mathematical prerequisite is a first course in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#20851;&#20132;&#25442;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20219;&#20309;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#20840;&#39044;&#27979;&#21644;&#22810;&#26657;&#20934;&#20043;&#38388;&#23384;&#22312;&#32852;&#31995;&#65292;&#20840;&#39044;&#27979;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#21487;&#20197;&#21152;&#24378;&#32463;&#20856;&#30340;&#26080;&#20851;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2302.06726</link><description>&lt;p&gt;
&#26080;&#20851;&#20132;&#25442;&#23398;&#20064;&#65292;&#25110;&#36890;&#36807;&#22810;&#26657;&#20934;&#23545;&#20840;&#39044;&#27979;&#36827;&#34892;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration. (arXiv:2302.06726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#20851;&#20132;&#25442;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20219;&#20309;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#20840;&#39044;&#27979;&#21644;&#22810;&#26657;&#20934;&#20043;&#38388;&#23384;&#22312;&#32852;&#31995;&#65292;&#20840;&#39044;&#27979;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#21487;&#20197;&#21152;&#24378;&#32463;&#20856;&#30340;&#26080;&#20851;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#26080;&#20851;&#20132;&#25442;&#23398;&#20064;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#30475;&#20316;&#26159;&#39044;&#27979;&#32773;&#21644;&#23545;&#25163;&#20043;&#38388;&#30340;&#19968;&#31181;&#21338;&#24328;&#65306;&#39318;&#20808;&#65292;&#39044;&#27979;&#32773;&#36873;&#25321;&#19968;&#20010;&#20551;&#35774;$h$&#65307;&#28982;&#21518;&#65292;&#23545;&#25163;&#20197;&#21709;&#24212;&#26041;&#24335;&#36827;&#34892;&#28216;&#25103;&#65292;&#24182;&#19988;&#23545;&#20110;&#39044;&#27979;&#32773;&#30340;&#27599;&#20010;&#27700;&#24179;&#38598;$\{x \in \mathcal{X} : h(x) = v\}$&#36873;&#25321;&#19968;&#20010;&#65288;&#19981;&#21516;&#30340;&#65289;&#20351;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#20551;&#35774;$c_v \in \mathcal{C}$&#65307;&#22914;&#26524;$h$&#19982;&#23545;&#25163;&#30340;&#25439;&#22833;&#31454;&#20105;&#65292;&#39044;&#27979;&#32773;&#33719;&#32988;&#12290;&#23613;&#31649;&#23545;&#25163;&#30340;&#24378;&#22823;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#20851;&#20132;&#25442;&#23398;&#20064;&#22312;&#20219;&#20309;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#26159;&#21487;&#34892;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#19968;&#32467;&#26524;&#36890;&#36807;&#30740;&#31350;&#20840;&#39044;&#27979;&#21644;&#22810;&#26657;&#20934;&#20043;&#38388;&#30340;&#32852;&#31995;&#24471;&#20986;&#12290;&#20840;&#39044;&#27979;&#26159;&#39044;&#27979;&#32773;&#30340;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#23427;&#21152;&#24378;&#20102;&#20687;&#26080;&#20851;&#23398;&#20064;&#36825;&#26679;&#30340;&#32463;&#20856;&#27010;&#24565;&#12290;&#23427;&#35201;&#27714;&#22312;&#36866;&#29992;&#20110;&#19968;&#20010;&#20016;&#23500;&#30340;&#25439;&#22833;&#20989;&#25968;&#26063;&#30340;&#20219;&#20309;&#25439;&#22833;&#20989;&#25968;&#19978;&#65292;&#30456;&#23545;&#20110;&#19968;&#20010;&#20551;&#35774;&#31867;&#21035;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#20445;&#35777;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#25506;&#32034;&#20102;&#36825;&#19968;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and study Swap Agnostic Learning. The problem can be phrased as a game between a predictor and an adversary: first, the predictor selects a hypothesis $h$; then, the adversary plays in response, and for each level set of the predictor $\{x \in \mathcal{X} : h(x) = v\}$ selects a (different) loss-minimizing hypothesis $c_v \in \mathcal{C}$; the predictor wins if $h$ competes with the adaptive adversary's loss. Despite the strength of the adversary, we demonstrate the feasibility Swap Agnostic Learning for any convex loss.  Somewhat surprisingly, the result follows through an investigation into the connections between Omniprediction and Multicalibration. Omniprediction is a new notion of optimality for predictors that strengthtens classical notions such as agnostic learning. It asks for loss minimization guarantees (relative to a hypothesis class) that apply not just for a specific loss function, but for any loss belonging to a rich family of losses. A recent line of work sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;ClimateBench&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#25317;&#26377;100&#19975;&#21644;1000&#19975;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#19988;&#33021;&#32791;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2212.03369</link><description>&lt;p&gt;
&#25506;&#32034;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Randomly Wired Neural Networks for Climate Model Emulation. (arXiv:2212.03369v3 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;ClimateBench&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#25317;&#26377;100&#19975;&#21644;1000&#19975;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#19988;&#33021;&#32791;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#19981;&#21516;&#20154;&#31867;&#27963;&#21160;&#25490;&#25918;&#24773;&#26223;&#30340;&#27668;&#20505;&#24433;&#21709;&#23545;&#20110;&#21046;&#23450;&#27668;&#20505;&#21464;&#21270;&#20943;&#32531;&#21644;&#36866;&#24212;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#36827;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#23545;&#36825;&#20123;&#24433;&#21709;&#30340;&#35814;&#32454;&#27934;&#35265;&#65292;&#20294;&#27599;&#20010;&#24773;&#26223;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#36825;&#31181;&#24040;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#24341;&#21457;&#20102;&#23545;&#24320;&#21457;&#24265;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20219;&#21153;&#30340;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#26500;&#24314;&#36825;&#20123;&#32593;&#32476;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#20351;&#29992;ClimateBench&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#22810;&#23618;&#24863;&#30693;&#26426;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20013;&#20018;&#34892;&#36830;&#25509;&#30340;&#31264;&#23494;&#23618;&#26367;&#25442;&#20026;&#38543;&#26426;&#36830;&#25509;&#30340;&#31264;&#23494;&#23618;&#65292;&#24182;&#35780;&#20272;&#22312;&#25317;&#26377;100&#19975;&#21644;1000&#19975;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33021;&#32791;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#24403;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the climate impacts of various anthropogenic emissions scenarios is key to making informed decisions for climate change mitigation and adaptation. State-of-the-art Earth system models can provide detailed insight into these impacts, but have a large associated computational cost on a per-scenario basis. This large computational burden has driven recent interest in developing cheap machine learning models for the task of climate model emulation. In this manuscript, we explore the efficacy of randomly wired neural networks for this task. We describe how they can be constructed and compare them to their standard feedforward counterparts using the ClimateBench dataset. Specifically, we replace the serially connected dense layers in multilayer perceptrons, convolutional neural networks, and convolutional long short-term memory networks with randomly wired dense layers and assess the impact on model performance for models with 1 million and 10 million parameters. We find that model
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01168</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29289;&#29702;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#38598;&#20013;&#22312;&#36890;&#36807;&#23558;&#29289;&#29702;&#20808;&#39564;&#25110;&#24402;&#32435;&#20559;&#35265;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#21457;&#29616;&#30446;&#26631;&#31995;&#32479;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29305;&#23450;&#20110;&#31995;&#32479;&#65292;&#19981;&#20801;&#35768;&#36731;&#26494;&#36866;&#24212;&#30001;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#39537;&#21160;&#30340;&#26032;&#29289;&#29702;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#20110;&#36136;&#28857;&#24377;&#31783;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#21452;&#20307;&#31995;&#32479;&#25110;&#20219;&#20309;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#20803;&#23398;&#20064;&#31639;&#27861;&#20351;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#20351;&#20854;&#36866;&#24212;&#26032;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#36328;&#21508;&#31181;&#21704;&#23494;&#39039;&#27969;&#24418;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36825;&#26159;&#21704;&#23494;&#39039;&#31995;&#32479;&#25968;&#25454;&#20998;&#24067;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#19981;&#21516;&#31995;&#32479;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#31995;&#32479;&#37117;&#26377;&#20854;&#33258;&#36523;&#22266;&#26377;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
&lt;/p&gt;</description></item><item><title>&#12304;&#35770;&#25991;&#26631;&#39064;&#32763;&#35793;&#12305;HashVFL: &#22312;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#24341;&#20837;&#21704;&#24076;&#31639;&#27861;&#20197;&#38450;&#24481;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#21704;&#24076;&#31639;&#27861;&#26159;&#23545;&#25239;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.00325</link><description>&lt;p&gt;
HashVFL: &#22312;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#38450;&#24481;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
HashVFL: Defending Against Data Reconstruction Attacks in Vertical Federated Learning. (arXiv:2212.00325v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00325
&lt;/p&gt;
&lt;p&gt;
&#12304;&#35770;&#25991;&#26631;&#39064;&#32763;&#35793;&#12305;HashVFL: &#22312;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#24341;&#20837;&#21704;&#24076;&#31639;&#27861;&#20197;&#38450;&#24481;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#21704;&#24076;&#31639;&#27861;&#26159;&#23545;&#25239;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#19994;&#26694;&#26550;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#31561;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#26469;&#30830;&#20445;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#30001;&#20110;&#20013;&#38388;&#34920;&#31034;&#19982;&#21407;&#22987;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25968;&#25454;&#27844;&#38706;&#20173;&#28982;&#26159;VFL&#20013;&#30340;&#39118;&#38505;&#12290;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20351;&#24471;&#23545;&#25163;&#21487;&#20197;&#37325;&#26500;&#25968;&#25454;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#20110;&#20445;&#25252;VFL&#31995;&#32479;&#30340;&#25345;&#32493;&#30740;&#31350;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21704;&#24076;&#31639;&#27861;&#26159;&#23545;&#25239;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21704;&#24076;&#30340;&#21333;&#21521;&#24615;&#20351;&#24471;&#23545;&#25163;&#38590;&#20197;&#20174;&#21704;&#24076;&#30721;&#20013;&#24674;&#22797;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;VFL&#20013;&#23454;&#26045;&#21704;&#24076;&#31639;&#27861;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26799;&#24230;&#28040;&#22833;&#21644;&#20449;&#24687;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HashVFL&#65292;&#23427;&#23558;&#21704;&#24076;&#31639;&#27861;&#19982;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a trending collaborative machine learning model training solution. Existing industrial frameworks employ secure multi-party computation techniques such as homomorphic encryption to ensure data security and privacy. Despite these efforts, studies have revealed that data leakage remains a risk in VFL due to the correlations between intermediate representations and raw data. Neural networks can accurately capture these correlations, allowing an adversary to reconstruct the data. This emphasizes the need for continued research into securing VFL systems.  Our work shows that hashing is a promising solution to counter data reconstruction attacks. The one-way nature of hashing makes it difficult for an adversary to recover data from hash codes. However, implementing hashing in VFL presents new challenges, including vanishing gradients and information loss. To address these issues, we propose HashVFL, which integrates hashing and simultaneously achieves lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#26377;&#30028;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35777;&#26126;&#20102;&#24403;&#28608;&#27963;&#20989;&#25968;&#20026;&#36880;&#20803;&#32032;Lipschitz&#26102;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#23485;&#24230;&#19978;&#20165;&#26377;&#23545;&#25968;&#20381;&#36182;&#65292;&#24182;&#19988;&#36825;&#31181;&#20381;&#36182;&#26159;&#32039;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.09634</link><description>&lt;p&gt;
&#20851;&#20110;&#20004;&#23618;&#32593;&#32476;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30740;&#31350;&#65306;Lipschitz&#19982;&#36880;&#20803;&#32032;Lipschitz&#28608;&#27963;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation. (arXiv:2211.09634v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#26377;&#30028;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35777;&#26126;&#20102;&#24403;&#28608;&#27963;&#20989;&#25968;&#20026;&#36880;&#20803;&#32032;Lipschitz&#26102;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#23485;&#24230;&#19978;&#20165;&#26377;&#23545;&#25968;&#20381;&#36182;&#65292;&#24182;&#19988;&#36825;&#31181;&#20381;&#36182;&#26159;&#32039;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#26377;&#30028;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31867;$$\mathcal{H} = \{\textbf{x}\mapsto \langle \textbf{v}, \sigma \circ W\textbf{b} + \textbf{b} \rangle : \textbf{b}\in\mathbb{R}^d, W \in \mathbb{R}^{\mathcal{T}\times d}, \textbf{v} \in \mathbb{R}^{\mathcal{T}}\}$$&#20854;&#20013;$W$&#21644;$\textbf{v}$&#30340;&#35889;&#33539;&#25968;&#34987;$O(1)$&#38480;&#21046;&#65292;$W$&#30340;Frobenius&#33539;&#25968;&#20174;&#21021;&#22987;&#21270;&#24320;&#22987;&#34987;$R&gt;0$&#38480;&#21046;&#65292;$\sigma$&#26159;&#19968;&#20010;Lipschitz&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;$\sigma$&#26159;&#36880;&#20803;&#32032;&#30340;&#65292;&#21017;$\mathcal{H}$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20165;&#20165;&#22312;&#23485;&#24230;&#19978;&#26377;&#23545;&#25968;&#20381;&#36182;&#65292;&#24182;&#19988;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#32039;&#33268;&#30340;&#65292;&#20165;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;$\sigma$&#30340;&#36880;&#20803;&#32032;&#24615;&#36136;&#23545;&#20110;&#23485;&#24230;&#19978;&#30340;&#23545;&#25968;&#20381;&#36182;&#30028;&#38480;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#23384;&#22312;&#19981;&#26159;&#36880;&#20803;&#32032;&#28608;&#27963;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#23485;&#24230;&#19978;&#26159;&#32447;&#24615;&#30340;&#65292;&#38024;&#23545;&#23485;&#24230;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20171;&#20110;&#23545;&#25968;&#21644;&#32447;&#24615;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the sample complexity of bounded two-layer neural networks using different activation functions.  In particular, we consider the class  $$ \mathcal{H} = \left\{\textbf{x}\mapsto \langle \textbf{v}, \sigma \circ W\textbf{b} + \textbf{b} \rangle : \textbf{b}\in\mathbb{R}^d, W \in \mathbb{R}^{\mathcal{T}\times d}, \textbf{v} \in \mathbb{R}^{\mathcal{T}}\right\} $$  where the spectral norm of $W$ and $\textbf{v}$ is bounded by $O(1)$, the Frobenius norm of $W$ is bounded from its initialization by $R &gt; 0$, and $\sigma$ is a Lipschitz activation function.  We prove that if $\sigma$ is element-wise, then the sample complexity of $\mathcal{H}$ has only logarithmic dependency in width and that this complexity is tight, up to logarithmic factors.  We further show that the element-wise property of $\sigma$ is essential for a logarithmic dependency bound in width, in the sense that there exist non-element-wise activation functions whose sample complexity is linear in width, for wid
&lt;/p&gt;</description></item><item><title>OPAA&#26159;&#19968;&#31181;&#21151;&#33021;&#20998;&#26512;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#24179;&#28369;&#30340;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#20272;&#35745;&#20540;&#12289;&#35745;&#31639;&#24402;&#19968;&#21270;&#26435;&#37325;&#30340;&#20272;&#35745;&#20540;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#30340;&#20989;&#25968;&#31354;&#38388;&#36716;&#25442;&#26469;&#20272;&#35745;&#35777;&#25454;&#65292;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#30340;&#19968;&#27425;&#36890;&#36807;&#12290;&#23427;&#36866;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#23588;&#20854;&#22312;&#36125;&#21494;&#26031;&#38382;&#39064;&#20013;&#20272;&#35745;&#24402;&#19968;&#21270;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2211.08594</link><description>&lt;p&gt;
&#27491;&#20132;&#22810;&#39033;&#24335;&#36924;&#36817;&#31639;&#27861;&#65288;OPAA&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#30340;&#21151;&#33021;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Polynomials Approximation Algorithm (OPAA):a functional analytic approach to estimating probability densities. (arXiv:2211.08594v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08594
&lt;/p&gt;
&lt;p&gt;
OPAA&#26159;&#19968;&#31181;&#21151;&#33021;&#20998;&#26512;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#24179;&#28369;&#30340;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#20272;&#35745;&#20540;&#12289;&#35745;&#31639;&#24402;&#19968;&#21270;&#26435;&#37325;&#30340;&#20272;&#35745;&#20540;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#30340;&#20989;&#25968;&#31354;&#38388;&#36716;&#25442;&#26469;&#20272;&#35745;&#35777;&#25454;&#65292;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#30340;&#19968;&#27425;&#36890;&#36807;&#12290;&#23427;&#36866;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#23588;&#20854;&#22312;&#36125;&#21494;&#26031;&#38382;&#39064;&#20013;&#20272;&#35745;&#24402;&#19968;&#21270;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#20132;&#22810;&#39033;&#24335;&#36924;&#36817;&#31639;&#27861;&#65288;OPAA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#24182;&#34892;&#21270;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#21151;&#33021;&#20998;&#26512;&#26041;&#27861;&#20272;&#35745;&#27010;&#29575;&#20998;&#24067;&#65306;&#39318;&#20808;&#65292;&#23427;&#25214;&#21040;&#20102;&#27010;&#29575;&#20998;&#24067;&#30340;&#24179;&#28369;&#20989;&#25968;&#20272;&#35745;&#65292;&#26080;&#35770;&#23427;&#26159;&#21542;&#24402;&#19968;&#21270;&#65307;&#20854;&#27425;&#65292;&#31639;&#27861;&#25552;&#20379;&#20102;&#24402;&#19968;&#21270;&#26435;&#37325;&#30340;&#20272;&#35745;&#65307;&#31532;&#19977;&#65292;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26041;&#26696;&#26469;&#35745;&#31639;&#36825;&#20123;&#20272;&#35745;&#20540;&#12290;OPAA&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#23558;&#32852;&#21512;&#20998;&#24067;&#30340;&#24179;&#26041;&#26681;&#36716;&#21270;&#20026;&#25105;&#20204;&#26500;&#36896;&#30340;&#29305;&#27530;&#20989;&#25968;&#31354;&#38388;&#30340;&#29305;&#27530;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20010;&#21464;&#25442;&#65292;&#35777;&#25454;&#31561;&#20110;&#36716;&#25442;&#20989;&#25968;&#30340;$L^2$&#33539;&#25968;&#30340;&#24179;&#26041;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#31995;&#25968;&#30340;&#24179;&#26041;&#21644;&#26469;&#20272;&#35745;&#35777;&#25454;&#12290;&#35745;&#31639;&#21487;&#20197;&#24182;&#34892;&#21270;&#24182;&#22312;&#19968;&#27425;&#36890;&#36807;&#20013;&#23436;&#25104;&#12290;OPAA&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;&#22312;&#36125;&#21494;&#26031;&#38382;&#39064;&#20013;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#24402;&#19968;&#21270;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the new Orthogonal Polynomials Approximation Algorithm (OPAA), a parallelizable algorithm that estimates probability distributions using functional analytic approach: first, it finds a smooth functional estimate of the probability distribution, whether it is normalized or not; second, the algorithm provides an estimate of the normalizing weight; and third, the algorithm proposes a new computation scheme to compute such estimates.  A core component of OPAA is a special transform of the square root of the joint distribution into a special functional space of our construct. Through this transform, the evidence is equated with the $L^2$ norm of the transformed function, squared. Hence, the evidence can be estimated by the sum of squares of the transform coefficients. Computations can be parallelized and completed in one pass.  OPAA can be applied broadly to the estimation of probability density functions. In Bayesian problems, it can be applied to estimating the normalizing weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#20223;&#23556;&#27169;&#22411;&#36716;&#31227;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#24179;&#26041;&#25439;&#22833;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20010;&#26041;&#27861;&#20063;&#32473;&#20986;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2210.09745</link><description>&lt;p&gt;
&#21033;&#29992;&#20223;&#23556;&#27169;&#22411;&#21464;&#25442;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer learning with affine model transformation. (arXiv:2210.09745v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#20223;&#23556;&#27169;&#22411;&#36716;&#31227;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#24179;&#26041;&#25439;&#22833;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20010;&#26041;&#27861;&#20063;&#32473;&#20986;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#19978;&#65292;&#20351;&#29992;&#32473;&#23450;&#30340;&#28304;&#27169;&#22411;&#38598;&#21644;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#23398;&#20064;&#39046;&#22495;&#36716;&#31227;&#21644;&#39046;&#22495;&#29305;&#23450;&#22240;&#32032;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#26041;&#27861;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20223;&#23556;&#27169;&#22411;&#36716;&#31227;&#30340;&#36890;&#29992;&#31867;&#21035;&#30340;&#36801;&#31227;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#36981;&#24490;&#26399;&#26395;&#24179;&#26041;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#21407;&#21017;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20223;&#23556;&#27169;&#22411;&#36716;&#31227;&#24191;&#27867;&#21253;&#25324;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#26368;&#24120;&#35265;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;&#20223;&#23556;&#27169;&#22411;&#36716;&#31227;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised transfer learning has received considerable attention due to its potential to boost the predictive power of machine learning in scenarios where data are scarce. Generally, a given set of source models and a dataset from a target domain are used to adapt the pre-trained models to a target domain by statistically learning domain shift and domain-specific factors. While such procedurally and intuitively plausible methods have achieved great success in a wide range of real-world applications, the lack of a theoretical basis hinders further methodological development. This paper presents a general class of transfer learning regression called affine model transfer, following the principle of expected-square loss minimization. It is shown that the affine model transfer broadly encompasses various existing methods, including the most common procedure based on neural feature extractors. Furthermore, the current paper clarifies theoretical properties of the affine model transfer such 
&lt;/p&gt;</description></item><item><title>ImpNet&#26159;&#19968;&#31181;&#22312;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#30340;&#19981;&#21487;&#23519;&#35273;&#21644;&#40657;&#30418;&#19981;&#21487;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#36825;&#20123;&#21518;&#38376;&#21487;&#20197;&#32469;&#36807;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#25554;&#20837;&#38454;&#27573;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#65292;&#31227;&#38500;&#23427;&#20204;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00108</link><description>&lt;p&gt;
ImpNet: &#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#21487;&#23519;&#35273;&#21644;&#40657;&#30418;&#19981;&#21487;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. (arXiv:2210.00108v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00108
&lt;/p&gt;
&lt;p&gt;
ImpNet&#26159;&#19968;&#31181;&#22312;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#30340;&#19981;&#21487;&#23519;&#35273;&#21644;&#40657;&#30418;&#19981;&#21487;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#36825;&#20123;&#21518;&#38376;&#21487;&#20197;&#32469;&#36807;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#25554;&#20837;&#38454;&#27573;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#65292;&#31227;&#38500;&#23427;&#20204;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#21518;&#38376;&#25915;&#20987;&#24341;&#36215;&#20102;&#25915;&#38450;&#24320;&#21457;&#20013;&#30340;&#19968;&#22330;&#31454;&#20105;&#12290;&#38450;&#24481;&#25163;&#27573;&#24050;&#32463;&#20986;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27169;&#22411;&#20013;&#26816;&#27979;&#21644;&#31227;&#38500;&#21518;&#38376;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#38450;&#24481;&#25163;&#27573;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21518;&#38376;&#21487;&#20197;&#22312;&#32534;&#35793;&#36807;&#31243;&#20013;&#28155;&#21152;&#65292;&#32469;&#36807;&#20102;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#30340;&#20219;&#20309;&#20445;&#25252;&#25514;&#26045;&#12290;&#25915;&#20987;&#32773;&#19981;&#20165;&#21487;&#20197;&#22312;&#32534;&#35793;&#36807;&#31243;&#20013;&#25554;&#20837;&#24050;&#26377;&#30340;&#22522;&#20110;&#26435;&#37325;&#30340;&#21518;&#38376;&#65292;&#36824;&#21487;&#20197;&#25554;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#26435;&#37325;&#26080;&#20851;&#30340;&#21518;&#38376;&#65292;&#20363;&#22914;ImpNet&#12290;&#36825;&#20123;&#21518;&#38376;&#22312;&#35757;&#32451;&#25110;&#25968;&#25454;&#20934;&#22791;&#36807;&#31243;&#20013;&#26159;&#19981;&#21487;&#26816;&#27979;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#23578;&#19981;&#23384;&#22312;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#21518;&#38376;&#65292;&#21253;&#25324;ImpNet&#65292;&#21482;&#33021;&#22312;&#25554;&#20837;&#23427;&#20204;&#30340;&#38454;&#27573;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#65292;&#32780;&#22312;&#20854;&#20182;&#20219;&#20309;&#22320;&#26041;&#31227;&#38500;&#23427;&#20204;&#37117;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38656;&#35201;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early backdoor attacks against machine learning set off an arms race in attack and defence development. Defences have since appeared demonstrating some ability to detect backdoors in models or even remove them. These defences work by inspecting the training data, the model, or the integrity of the training procedure. In this work, we show that backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages. The attacker can not only insert existing weight-based backdoors during compilation, but also a new class of weight-independent backdoors, such as ImpNet. These backdoors are impossible to detect during the training or data preparation processes, because they are not yet present. Next, we demonstrate that some backdoors, including ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge. We conclude that ML model security requires assurance of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#20013;&#28145;&#20837;&#30740;&#31350;&#20102;&#20219;&#21153;&#30340;&#19981;&#21516;&#34920;&#36848;&#26041;&#24335;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#26159;&#24615;&#33021;&#21644;&#20219;&#21153;&#39034;&#24207;&#25935;&#24863;&#24230;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#19981;&#21516;&#35270;&#35273;&#23884;&#20837;&#30340;Transformer&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21450;&#27169;&#22411;&#34920;&#31034;&#23545;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.00044</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#20219;&#21153;&#30340;&#34920;&#36848;&#26041;&#24335;&#24456;&#37325;&#35201;&#65306;&#35270;&#35273;&#38382;&#31572;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Task Formulation Matters When Learning Continually: A Case Study in Visual Question Answering. (arXiv:2210.00044v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#20013;&#28145;&#20837;&#30740;&#31350;&#20102;&#20219;&#21153;&#30340;&#19981;&#21516;&#34920;&#36848;&#26041;&#24335;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#26159;&#24615;&#33021;&#21644;&#20219;&#21153;&#39034;&#24207;&#25935;&#24863;&#24230;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#19981;&#21516;&#35270;&#35273;&#23884;&#20837;&#30340;Transformer&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21450;&#27169;&#22411;&#34920;&#31034;&#23545;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26088;&#22312;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#22686;&#37327;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#36951;&#24536;&#20043;&#21069;&#30340;&#30693;&#35782;&#12290;&#23613;&#31649;&#36830;&#32493;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#35270;&#35273;+&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24182;&#19981;&#37027;&#20040;&#30452;&#35266;&#65292;&#22240;&#20026;&#26681;&#25454;&#36755;&#20837;&#27169;&#24577;&#21487;&#20197;&#26377;&#22810;&#31181;&#26041;&#24335;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#35774;&#32622;&#22914;&#20309;&#24433;&#21709;&#35270;&#35273;&#38382;&#31572;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21512;&#29702;&#30340;&#20219;&#21153;&#34920;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#20960;&#20010;&#22240;&#32032;&#36827;&#34892;&#20102;&#32454;&#20998;&#65292;&#34920;&#26126;&#24615;&#33021;&#21644;&#23545;&#20219;&#21153;&#39034;&#24207;&#30340;&#25935;&#24863;&#24230;&#39640;&#24230;&#20381;&#36182;&#20110;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#35270;&#35273;&#23884;&#20837;&#30340;Transformer&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34920;&#31034;&#21644;&#20854;&#23545;&#36951;&#24536;&#30340;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning aims to train a model incrementally on a sequence of tasks without forgetting previous knowledge. Although continual learning has been widely studied in computer vision, its application to Vision+Language tasks is not that straightforward, as settings can be parameterized in multiple ways according to their input modalities. In this paper, we present a detailed study of how different settings affect performance for Visual Question Answering. We first propose three plausible task formulations and demonstrate their impact on the performance of continual learning algorithms. We break down several factors of task similarity, showing that performance and sensitivity to task order highly depend on the shift of the output distribution. We also investigate the potential of pretrained models and compare the robustness of transformer models with different visual embeddings. Finally, we provide an analysis interpreting model representations and their impact on forgetting. Our r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#39057;&#31354;&#38388;&#25193;&#25955;&#36807;&#31243;&#30340;MR&#37325;&#24314;&#30340;&#26032;&#22411;SDE&#27169;&#22411;&#65288;HFS-SDE&#65289;&#65292;&#35813;&#27169;&#22411;&#30830;&#20445;&#20102;&#20302;&#39057;&#21306;&#22495;&#30340;&#30830;&#23450;&#24615;&#24182;&#21152;&#36895;&#20102;&#36870;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21152;&#36895;MRI&#37325;&#24314;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05481</link><description>&lt;p&gt;
&#39640;&#39057;&#31354;&#38388;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21152;&#36895;MRI
&lt;/p&gt;
&lt;p&gt;
High-Frequency Space Diffusion Models for Accelerated MRI. (arXiv:2208.05481v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#39057;&#31354;&#38388;&#25193;&#25955;&#36807;&#31243;&#30340;MR&#37325;&#24314;&#30340;&#26032;&#22411;SDE&#27169;&#22411;&#65288;HFS-SDE&#65289;&#65292;&#35813;&#27169;&#22411;&#30830;&#20445;&#20102;&#20302;&#39057;&#21306;&#22495;&#30340;&#30830;&#23450;&#24615;&#24182;&#21152;&#36895;&#20102;&#36870;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21152;&#36895;MRI&#37325;&#24314;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#30913;&#20849;&#25391;&#65288;MR&#65289;&#37325;&#24314;&#20013;&#36870;&#38382;&#39064;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#22312;&#24555;&#36895;MR&#25104;&#20687;&#20013;&#65292;$k$-&#31354;&#38388;&#25968;&#25454;&#30340;&#20302;&#39057;&#21306;&#22495;&#36890;&#24120;&#26159;&#23436;&#20840;&#37319;&#26679;&#30340;&#65292;&#32780;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#22312;&#25972;&#20010;&#22270;&#20687;&#25110;$k$-&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#36825;&#24517;&#28982;&#20250;&#24341;&#20837;&#20302;&#39057;&#21306;&#22495;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#65292;&#23548;&#33268;&#37325;&#24314;&#36807;&#31243;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SDE&#65292;&#19987;&#38376;&#29992;&#20110;&#24102;&#26377;&#39640;&#39057;&#31354;&#38388;&#25193;&#25955;&#36807;&#31243;&#30340;MR&#37325;&#24314;&#65288;&#31216;&#20026;HFS-SDE&#65289;&#12290;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#23436;&#20840;&#37319;&#26679;&#30340;&#20302;&#39057;&#21306;&#22495;&#30340;&#30830;&#23450;&#24615;&#65292;&#24182;&#21152;&#36895;&#20102;&#36870;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21152;&#36895;MRI&#37325;&#24314;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models with continuous stochastic differential equations (SDEs) have shown superior performances in image generation. It can serve as a deep generative prior to solving the inverse problem in magnetic resonance (MR) reconstruction. However, low-frequency regions of $k$-space data are typically fully sampled in fast MR imaging, while existing diffusion models are performed throughout the entire image or $k$-space, inevitably introducing uncertainty in the reconstruction of low-frequency regions. Additionally, existing diffusion models often demand substantial iterations to converge, resulting in time-consuming reconstructions. To address these challenges, we propose a novel SDE tailored specifically for MR reconstruction with the diffusion process in high-frequency space (referred to as HFS-SDE). This approach ensures determinism in the fully sampled low-frequency regions and accelerates the sampling procedure of reverse diffusion. Experiments conducted on the publicly availab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.04957</link><description>&lt;p&gt;
&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#36827;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#29983;&#25104;&#33021;&#22815;&#19982;&#26410;&#30693;&#21512;&#20316;&#20249;&#20276;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22312;&#38646;&#26679;&#26412;&#21327;&#21516;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#26234;&#33021;&#20307;&#26292;&#38706;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#20249;&#20276;&#26102;&#28041;&#21450;&#33258;&#25105;&#23545;&#24328;&#65292;&#38544;&#24335;&#22320;&#20551;&#35774;&#20219;&#21153;&#26159;&#21516;&#36136;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26159;&#24322;&#26500;&#30340;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#36807;&#31243;&#65306;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#65292;&#23545;&#20004;&#20010;&#26234;&#33021;&#20307;&#21644;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#21327;&#21516;&#36827;&#21270;&#12290;&#23545;&#19981;&#21516;&#24322;&#26500;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#35299;&#20915;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696; Swin DQN&#65292;&#36890;&#36807;&#23558;&#32452;&#21512;&#30340;&#22270;&#20687;&#20687;&#32032;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#24182;&#22312;&#23616;&#37096;&#24212;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312; Atari &#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#29616;&#26377;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15269</link><description>&lt;p&gt;
Swin Transformer &#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Swin Transformers. (arXiv:2206.15269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696; Swin DQN&#65292;&#36890;&#36807;&#23558;&#32452;&#21512;&#30340;&#22270;&#20687;&#20687;&#32032;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#24182;&#22312;&#23616;&#37096;&#24212;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312; Atari &#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#29616;&#26377;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#23618;&#33258;&#25105;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#26377;&#20154;&#21162;&#21147;&#23558; Transformer &#36866;&#24212;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#21253;&#25324; Vision Transformer &#21644; Swin Transformer&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#23558; Vision Transformer &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#20173;&#20572;&#30041;&#22312;&#23567;&#35268;&#27169;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#24517;&#39035;&#20381;&#36182;&#20110;&#25216;&#26415;&#26469;&#20943;&#23569; Vision Transformer &#30340;&#25104;&#26412;&#65292;&#36825;&#20063;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#31532;&#19968;&#20010;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65306;Swin DQN&#12290;Swin Transformer &#21487;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20027;&#24178;&#39592;&#24178;&#65292;&#23558;&#22270;&#20687;&#20687;&#32032;&#30340;&#32452;&#21512;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#65292;&#24182;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#31383;&#21475;&#20869;&#24212;&#29992;&#23616;&#37096;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#12290;&#23427;&#20204;&#22312; ImageNet &#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;Swin DQN &#22312; Atari &#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are neural network models that utilize multiple layers of self-attention heads and have exhibited enormous potential in natural language processing tasks. Meanwhile, there have been efforts to adapt transformers to visual tasks of machine learning, including Vision Transformers and Swin Transformers. Although some researchers use Vision Transformers for reinforcement learning tasks, their experiments remain at a small scale due to the high computational cost. Experiments conducted at a large scale, on the other hand, have to rely on techniques to cut the costs of Vision Transformers, which also yield inferior results.  To address this challenge, this article presents the first online reinforcement learning scheme that is based on Swin Transformers: Swin DQN. Swin Transformers are promising as a backbone in neural networks by splitting groups of image pixels into small patches and applying local self-attention operations inside the (shifted) windows of fixed sizes. They hav
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Twitter&#25968;&#25454;&#20998;&#26512;&#20102;&#35299;&#20844;&#20247;&#23545;COVID-19&#30456;&#20851;&#33647;&#29289;&#30340;&#25209;&#20934;&#21644;&#31163;&#26631;&#20351;&#29992;&#30340;&#30475;&#27861;&#12290;Hydroxychloroquine&#21644;Ivermectin&#27604;Molnupiravir&#21644;Remdesivir&#30340;&#35752;&#35770;&#26356;&#22810;&#65292;&#26102;&#38388;&#36235;&#21183;&#20998;&#26512;&#21644;&#20869;&#23481;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#20204;&#23545;&#27599;&#31181;&#33647;&#29289;&#31435;&#22330;&#30340;&#21487;&#33021;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2206.14358</link><description>&lt;p&gt;
&#20351;&#29992;Twitter&#25968;&#25454;&#20102;&#35299;&#20844;&#20247;&#23545;COVID-19&#30456;&#20851;&#33647;&#29289;&#25209;&#20934;&#21644;&#31163;&#26631;&#20351;&#29992;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications. (arXiv:2206.14358v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Twitter&#25968;&#25454;&#20998;&#26512;&#20102;&#35299;&#20844;&#20247;&#23545;COVID-19&#30456;&#20851;&#33647;&#29289;&#30340;&#25209;&#20934;&#21644;&#31163;&#26631;&#20351;&#29992;&#30340;&#30475;&#27861;&#12290;Hydroxychloroquine&#21644;Ivermectin&#27604;Molnupiravir&#21644;Remdesivir&#30340;&#35752;&#35770;&#26356;&#22810;&#65292;&#26102;&#38388;&#36235;&#21183;&#20998;&#26512;&#21644;&#20869;&#23481;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#20204;&#23545;&#27599;&#31181;&#33647;&#29289;&#31435;&#22330;&#30340;&#21487;&#33021;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20844;&#20247;&#20851;&#20110;&#26410;&#32463;&#35777;&#23454;&#27835;&#30103;&#26041;&#27861;&#30340;&#32039;&#24613;&#20351;&#29992;&#30340;&#35752;&#35770;&#23545;&#20110;&#30417;&#27979;&#23433;&#20840;&#20351;&#29992;&#21644;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#27969;&#31243;&#65292;&#20197;&#29702;&#35299;Twitter&#19978;&#20851;&#20110;&#20896;&#29366;&#30149;&#27602;&#30149;2019&#65288;COVID-19&#65289;&#30456;&#20851;&#33647;&#29289;&#30340;&#20844;&#20247;&#30475;&#27861;&#21644;&#31435;&#22330;&#12290;&#36825;&#39033;&#22238;&#39038;&#24615;&#30740;&#31350;&#21253;&#25324;&#20102;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#20174;2020&#24180;1&#26376;29&#26085;&#21040;2021&#24180;11&#26376;30&#26085;&#65292;&#20851;&#20110;&#22235;&#31181;&#33647;&#29289;&#30340;609,189&#26465;&#32654;&#22269;&#25512;&#25991;&#65292;&#36825;&#22235;&#31181;&#33647;&#29289;&#22312;&#20844;&#20247;&#20013;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#65306;&#65288;1&#65289;&#32671;&#27695;&#21945;&#21644;&#20234;&#32500;&#33740;&#32032;&#65292;&#20855;&#26377;&#26696;&#20363;&#35777;&#25454;&#30340;&#27835;&#30103;&#26041;&#27861;&#65307;&#65288;2&#65289;&#33707;&#31859;&#21305;&#38647;&#38886;&#21644;&#29790;&#24503;&#35199;&#38886;&#65292;FDA&#25209;&#20934;&#29992;&#20110;&#21512;&#26684;&#24739;&#32773;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#21033;&#29992;&#26102;&#38388;&#36235;&#21183;&#20998;&#26512;&#20102;&#35299;&#20854;&#21463;&#27426;&#36814;&#24230;&#36235;&#21183;&#21644;&#30456;&#20851;&#20107;&#20214;&#12290;&#36827;&#34892;&#20102;&#20869;&#23481;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#26512;&#65292;&#20197;&#25506;&#32034;&#20154;&#20204;&#23545;&#27599;&#31181;&#33647;&#29289;&#31435;&#22330;&#30340;&#28508;&#22312;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding public discourse on emergency use of unproven therapeutics is crucial for monitoring safe use and combating misinformation. We developed a natural language processing-based pipeline to comprehend public perceptions of and stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter over time. This retrospective study included 609,189 US-based tweets from January 29, 2020, to November 30, 2021, about four drugs that garnered significant public attention during the COVID-19 pandemic: (1) Hydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2) Molnupiravir and Remdesivir, FDA-approved treatments for eligible patients. Time-trend analysis was employed to understand popularity trends and related events. Content and demographic analyses were conducted to explore potential rationales behind people's stances on each drug. Time-trend analysis indicated that Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir and Remdesivir, part
&lt;/p&gt;</description></item><item><title>&#22810;&#26679;&#24615;&#25955;&#23556;&#21464;&#25442;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#23454;&#29616;&#19978;&#37319;&#29992;&#20102;&#25193;&#25955;&#26144;&#23556;&#29702;&#35770;&#65292;&#26377;&#25928;&#29992;&#20110;&#20449;&#21495;&#20998;&#31867;&#21644;&#27969;&#24418;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2206.10078</link><description>&lt;p&gt;
&#39640;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#25955;&#23556;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
The Manifold Scattering Transform for High-Dimensional Point Cloud Data. (arXiv:2206.10078v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10078
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#24615;&#25955;&#23556;&#21464;&#25442;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#23454;&#29616;&#19978;&#37319;&#29992;&#20102;&#25193;&#25955;&#26144;&#23556;&#29702;&#35770;&#65292;&#26377;&#25928;&#29992;&#20110;&#20449;&#21495;&#20998;&#31867;&#21644;&#27969;&#24418;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#24615;&#25955;&#23556;&#21464;&#25442;&#26159;&#19968;&#31181;&#29992;&#20110;&#23450;&#20041;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#30340;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#23427;&#26159;&#23558;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25805;&#20316;&#25193;&#23637;&#21040;&#19968;&#33324;&#27969;&#24418;&#30340;&#26368;&#26089;&#30340;&#20363;&#23376;&#20043;&#19968;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#26368;&#21021;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20854;&#29702;&#35770;&#31283;&#23450;&#24615;&#21644;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#20294;&#38500;&#20102;&#22312;&#20855;&#26377;&#39044;&#23450;&#20041;&#32593;&#26684;&#30340;&#20108;&#32500;&#26354;&#38754;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20854;&#25968;&#20540;&#23454;&#29616;&#30340;&#26041;&#27861;&#22806;&#65292;&#36824;&#27809;&#26377;&#25552;&#20379;&#20854;&#25968;&#20540;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#29702;&#35770;&#30340;&#23454;&#29992;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#22810;&#26679;&#24615;&#25955;&#23556;&#21464;&#25442;&#24212;&#29992;&#20110;&#33258;&#28982;&#31995;&#32479;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#21333;&#32454;&#32990;&#36951;&#20256;&#23398;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#20316;&#20026;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#39640;&#32500;&#28857;&#20113;&#24314;&#27169;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#20449;&#21495;&#20998;&#31867;&#21644;&#27969;&#24418;&#20998;&#31867;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold scattering transform is a deep feature extractor for data defined on a Riemannian manifold. It is one of the first examples of extending convolutional neural network-like operators to general manifolds. The initial work on this model focused primarily on its theoretical stability and invariance properties but did not provide methods for its numerical implementation except in the case of two-dimensional surfaces with predefined meshes. In this work, we present practical schemes, based on the theory of diffusion maps, for implementing the manifold scattering transform to datasets arising in naturalistic systems, such as single cell genetics, where the data is a high-dimensional point cloud modeled as lying on a low-dimensional manifold. We show that our methods are effective for signal classification and manifold classification tasks.
&lt;/p&gt;</description></item><item><title>SupMAE&#26159;&#19968;&#31181;&#30417;&#30563;&#24335;&#25513;&#33180;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#28155;&#21152;&#30417;&#30563;&#20998;&#31867;&#20998;&#25903;&#25193;&#23637;&#20102;MAE&#65292;&#24182;&#20174;&#40644;&#37329;&#26631;&#31614;&#20013;&#26377;&#25928;&#23398;&#20064;&#20840;&#23616;&#29305;&#24449;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SupMAE&#22312;&#35757;&#32451;&#25928;&#29575;&#12289;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#24615;&#33021;&#31561;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2205.14540</link><description>&lt;p&gt;
SupMAE: &#30417;&#30563;&#24335;&#25513;&#33180;&#33258;&#32534;&#30721;&#22120;&#26159;&#39640;&#25928;&#30340;&#35270;&#35273;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14540
&lt;/p&gt;
&lt;p&gt;
SupMAE&#26159;&#19968;&#31181;&#30417;&#30563;&#24335;&#25513;&#33180;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#28155;&#21152;&#30417;&#30563;&#20998;&#31867;&#20998;&#25903;&#25193;&#23637;&#20102;MAE&#65292;&#24182;&#20174;&#40644;&#37329;&#26631;&#31614;&#20013;&#26377;&#25928;&#23398;&#20064;&#20840;&#23616;&#29305;&#24449;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SupMAE&#22312;&#35757;&#32451;&#25928;&#29575;&#12289;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#24615;&#33021;&#31561;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#30340;&#25513;&#33180;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#39044;&#22788;&#29702;&#20219;&#21153;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#21482;&#33021;&#37325;&#26500;&#32570;&#22833;&#30340;&#23616;&#37096;&#22270;&#20687;&#22359;&#65292;&#32570;&#20047;&#23545;&#22270;&#20687;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#30417;&#30563;&#20998;&#31867;&#20998;&#25903;&#65292;&#23558;MAE&#25193;&#23637;&#21040;&#23436;&#20840;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#65292;&#20174;&#32780;&#20351;MAE&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#40644;&#37329;&#26631;&#31614;&#20013;&#23398;&#20064;&#20840;&#23616;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#30417;&#30563;&#24335;MAE&#65288;SupMAE&#65289;&#20165;&#21033;&#29992;&#22270;&#20687;&#20013;&#21487;&#35265;&#30340;&#22270;&#20687;&#22359;&#23376;&#38598;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25152;&#26377;&#22270;&#20687;&#22359;&#30340;&#26631;&#20934;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SupMAE&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26356;&#39640;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#26356;&#24378;&#22823;&#21644;&#21487;&#36801;&#31227;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SupMAE&#22312;&#20351;&#29992;ViT-B / 16&#27169;&#22411;&#22312;ImageNet&#19978;&#35780;&#20272;&#26102;&#65292;&#21482;&#20351;&#29992;30&#65285;&#30340;&#35745;&#31639;&#36164;&#28304;&#21363;&#21487;&#36798;&#21040;&#19982;MAE&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;SupMAE&#22312;ImageNet&#21464;&#20307;&#19978;&#30340;&#20581;&#22766;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#24615;&#33021;&#20063;&#38750;&#24120;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, self-supervised Masked Autoencoders (MAE) have attracted unprecedented attention for their impressive representation learning ability. However, the pretext task, Masked Image Modeling (MIM), reconstructs the missing local patches, lacking the global understanding of the image. This paper extends MAE to a fully supervised setting by adding a supervised classification branch, thereby enabling MAE to learn global features from golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a visible subset of image patches for classification, unlike the standard supervised pre-training where all image patches are used. Through experiments, we demonstrate that SupMAE is not only more training efficient but it also learns more robust and transferable features. Specifically, SupMAE achieves comparable performance with MAE using only 30% of compute when evaluated on ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and transfer learning perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#22312;&#19968;&#31867;DeepONets&#20013;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#19981;&#20250;&#38543;&#32593;&#32476;&#23485;&#24230;&#25193;&#23637;&#32780;&#26126;&#30830;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#23637;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;Huber&#25439;&#22833;&#26469;&#33719;&#24471;&#19981;&#26126;&#30830;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2205.11359</link><description>&lt;p&gt;
&#38754;&#21521;&#23610;&#24230;&#26080;&#20851;&#30340;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Towards Size-Independent Generalization Bounds for Deep Operator Nets. (arXiv:2205.11359v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#22312;&#19968;&#31867;DeepONets&#20013;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#19981;&#20250;&#38543;&#32593;&#32476;&#23485;&#24230;&#25193;&#23637;&#32780;&#26126;&#30830;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#23637;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;Huber&#25439;&#22833;&#26469;&#33719;&#24471;&#19981;&#26126;&#30830;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#26102;&#26399;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#26512;&#29289;&#29702;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#20027;&#39064;&#20013;&#29305;&#21035;&#27963;&#36291;&#30340;&#39046;&#22495;&#26159;"&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;"&#65292;&#23427;&#19987;&#27880;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#25968;&#20540;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#36827;&#22312;&#35757;&#32451;DeepONets&#26102;&#27979;&#37327;&#26679;&#26412;&#22806;&#35823;&#24046;&#30340;&#29702;&#35770; - &#36825;&#26159;&#35299;&#20915;PDE&#31995;&#32479;&#26368;&#36890;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#19968;&#31867;DeepONets&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;Rademacher&#22797;&#26434;&#24230;&#26377;&#19968;&#20010;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#19981;&#20250;&#26126;&#30830;&#22320;&#38543;&#30528;&#28041;&#21450;&#30340;&#32593;&#32476;&#23485;&#24230;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26469;&#23637;&#31034;&#22914;&#20309;&#36873;&#25321;Huber&#25439;&#22833;&#65292;&#20351;&#24471;&#23545;&#20110;&#36825;&#20123;DeepONet&#31867;&#65292;&#33021;&#22815;&#33719;&#24471;&#19981;&#26126;&#30830;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#20219;&#20309;&#30446;&#26631;&#26159;&#30001;DeepONets&#27714;&#35299;&#30340;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times machine learning methods have made significant advances in becoming a useful tool for analyzing physical systems. A particularly active area in this theme has been "physics-informed machine learning" which focuses on using neural nets for numerically solving differential equations. In this work, we aim to advance the theory of measuring out-of-sample error while training DeepONets -- which is among the most versatile ways to solve PDE systems in one-shot.  Firstly, for a class of DeepONets, we prove a bound on their Rademacher complexity which does not explicitly scale with the width of the nets involved. Secondly, we use this to show how the Huber loss can be chosen so that for these DeepONet classes generalization error bounds can be obtained that have no explicit dependence on the size of the nets. We note that our theoretical results apply to any PDE being targeted to be solved by DeepONets.
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.05173</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05173
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#27169;&#24335;&#25110;&#20107;&#20214;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26174;&#33879;&#22686;&#38271;&#20351;&#24471;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#32508;&#36848;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#23545;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#21253;&#25324;&#24320;&#21457;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behaviour. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24230;&#25512;&#23548;&#20986;&#20102;&#23545;&#20110;&#20869;&#23384;&#21463;&#38480;&#31639;&#27861;&#22312;&#24352;&#37327;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#30340;&#35745;&#31639;&#19979;&#30028;&#65292;&#24182;&#19988;&#25351;&#23450;&#20102;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31639;&#27861;&#24517;&#39035;&#22312;&#25968;&#25454;&#26679;&#26412;&#32463;&#36807;&#27425;&#25968;&#12289;&#26679;&#26412;&#22823;&#23567;&#21644;&#25152;&#38656;&#20869;&#23384;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#20123;&#19979;&#30028;&#26263;&#31034;&#20102;&#35768;&#22810;&#24120;&#29992;&#31639;&#27861;&#22312;&#26679;&#26412;&#22823;&#23567;&#19981;&#22815;&#22823;&#26102;&#38656;&#35201;&#26356;&#22810;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2204.07526</link><description>&lt;p&gt;
&#22312;&#24352;&#37327;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#30340;&#32479;&#35745;&#35745;&#31639;&#26435;&#34913;&#65306;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity. (arXiv:2204.07526v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24230;&#25512;&#23548;&#20986;&#20102;&#23545;&#20110;&#20869;&#23384;&#21463;&#38480;&#31639;&#27861;&#22312;&#24352;&#37327;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#30340;&#35745;&#31639;&#19979;&#30028;&#65292;&#24182;&#19988;&#25351;&#23450;&#20102;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31639;&#27861;&#24517;&#39035;&#22312;&#25968;&#25454;&#26679;&#26412;&#32463;&#36807;&#27425;&#25968;&#12289;&#26679;&#26412;&#22823;&#23567;&#21644;&#25152;&#38656;&#20869;&#23384;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#20123;&#19979;&#30028;&#26263;&#31034;&#20102;&#35768;&#22810;&#24120;&#29992;&#31639;&#27861;&#22312;&#26679;&#26412;&#22823;&#23567;&#19981;&#22815;&#22823;&#26102;&#38656;&#35201;&#26356;&#22810;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;Montanari&#21644;Richard&#24341;&#20837;&#30340;&#19968;&#31181;&#39118;&#26684;&#21270;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#65292;&#29992;&#20110;&#30740;&#31350;&#20174;&#39640;&#38454;&#30697;&#24352;&#37327;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#30340;&#35745;&#31639;&#38590;&#24230;&#12290;&#19982;&#20854;&#30697;&#38453;&#23545;&#24212;&#38382;&#39064;&#19981;&#21516;&#65292;&#24352;&#37327;&#20027;&#25104;&#20998;&#20998;&#26512;&#23637;&#29616;&#20102;&#19968;&#31181;&#32479;&#35745;&#35745;&#31639;&#24046;&#36317;&#65292;&#21363;&#22312;&#26679;&#26412;&#22823;&#23567;&#33539;&#22260;&#20869;&#65292;&#38382;&#39064;&#22312;&#20449;&#24687;&#29702;&#35770;&#19978;&#21487;&#35299;&#65292;&#20294;&#34987;&#35748;&#20026;&#22312;&#35745;&#31639;&#19978;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#21033;&#29992;&#36890;&#20449;&#22797;&#26434;&#24230;&#25512;&#23548;&#20986;&#20102;&#20869;&#23384;&#21463;&#38480;&#31639;&#27861;&#22312;&#24352;&#37327;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#30340;&#35745;&#31639;&#19979;&#30028;&#12290;&#36825;&#20123;&#19979;&#30028;&#25351;&#23450;&#20102;&#25104;&#21151;&#35299;&#20915;&#24352;&#37327;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#20219;&#20309;&#31639;&#27861;&#22312;&#25968;&#25454;&#26679;&#26412;&#32463;&#36807;&#27425;&#25968;&#12289;&#26679;&#26412;&#22823;&#23567;&#21644;&#25152;&#38656;&#20869;&#23384;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#19979;&#30028;&#19981;&#33021;&#25490;&#38500;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#24847;&#21619;&#30528;&#35768;&#22810;&#24120;&#29992;&#30340;&#31639;&#27861;&#65292;&#22914;&#26799;&#24230;&#19979;&#38477;&#21644;&#24130;&#36845;&#20195;&#26041;&#27861;&#65292;&#22312;&#26679;&#26412;&#22823;&#23567;&#19981;&#22815;&#22823;&#26102;&#24517;&#39035;&#26377;&#26356;&#39640;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;&#31867;&#20284;&#30340;&#19979;&#30028;&#36824;&#21487;&#20197;&#20351;&#29992;&#36890;&#20449;&#22797;&#26434;&#24230;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor PCA is a stylized statistical inference problem introduced by Montanari and Richard to study the computational difficulty of estimating an unknown parameter from higher-order moment tensors. Unlike its matrix counterpart, Tensor PCA exhibits a statistical-computational gap, i.e., a sample size regime where the problem is information-theoretically solvable but conjectured to be computationally hard. This paper derives computational lower bounds on the run-time of memory bounded algorithms for Tensor PCA using communication complexity. These lower bounds specify a trade-off among the number of passes through the data sample, the sample size, and the memory required by any algorithm that successfully solves Tensor PCA. While the lower bounds do not rule out polynomial-time algorithms, they do imply that many commonly-used algorithms, such as gradient descent and power method, must have a higher iteration count when the sample size is not large enough. Similar lower bounds are obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Concordance&#25351;&#25968;&#20998;&#35299;&#25104;&#20004;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35813;&#20998;&#35299;&#26041;&#27861;&#21487;&#20197;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#19981;&#21516;&#39044;&#27979;&#26041;&#27861;&#20043;&#38388;&#30340;&#20248;&#21155;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#35266;&#27979;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2203.00144</link><description>&lt;p&gt;
Concordance&#25351;&#25968;&#30340;&#20998;&#35299;: &#23545;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#28145;&#20837;&#29702;&#35299;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Concordance Index decomposition: A measure for a deeper understanding of survival prediction models. (arXiv:2203.00144v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Concordance&#25351;&#25968;&#20998;&#35299;&#25104;&#20004;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35813;&#20998;&#35299;&#26041;&#27861;&#21487;&#20197;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#19981;&#21516;&#39044;&#27979;&#26041;&#27861;&#20043;&#38388;&#30340;&#20248;&#21155;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#35266;&#27979;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Concordance&#25351;&#25968;&#65288;C-index&#65289;&#26159;&#29983;&#23384;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;C-index&#20998;&#35299;&#20026;&#20004;&#20010;&#25968;&#37327;&#30340;&#21152;&#26435;&#35843;&#21644;&#24179;&#22343;&#30340;&#26041;&#27861;&#65306;&#19968;&#20010;&#29992;&#20110;&#27604;&#36739;&#35266;&#27979;&#20107;&#20214;&#19982;&#20854;&#20182;&#35266;&#27979;&#20107;&#20214;&#30340;&#25490;&#24207;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#27604;&#36739;&#35266;&#27979;&#20107;&#20214;&#19982;&#34987;&#21098;&#36753;&#30340;&#24773;&#20917;&#30340;&#25490;&#24207;&#12290;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#21487;&#20197;&#23545;&#19981;&#21516;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#20043;&#38388;&#30340;&#20248;&#21155;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#19982;&#32463;&#20856;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#22522;&#20934;&#27604;&#36739;&#65292;&#20197;&#21450;&#26412;&#25991;&#25552;&#20986;&#30340;&#26032;&#30340;&#21464;&#20998;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;SurVED&#65289;&#65292;&#23637;&#31034;&#20102;&#35813;&#20998;&#35299;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#20351;&#29992;&#22235;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#19981;&#21516;&#21098;&#36753;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;C-index&#20998;&#35299;&#21644;&#21512;&#25104;&#21098;&#36753;&#65292;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#35266;&#27979;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Concordance Index (C-index) is a commonly used metric in Survival Analysis for evaluating the performance of a prediction model. In this paper, we propose a decomposition of the C-index into a weighted harmonic mean of two quantities: one for ranking observed events versus other observed events, and the other for ranking observed events versus censored cases. This decomposition enables a finer-grained analysis of the relative strengths and weaknesses between different survival prediction methods. The usefulness of this decomposition is demonstrated through benchmark comparisons against classical models and state-of-the-art methods, together with the new variational generative neural-network-based method (SurVED) proposed in this paper. The performance of the models is assessed using four publicly available datasets with varying levels of censoring. Using the C-index decomposition and synthetic censoring, the analysis shows that deep learning models utilize the observed events more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#32972;&#26223;&#19979;&#36827;&#34892;&#27169;&#25311;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#32479;&#35745;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#31181;&#35823;&#21457;&#29616;&#29575;&#25511;&#21046;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2202.05612</link><description>&lt;p&gt;
&#39640;&#32500;&#25512;&#26029;&#19982;&#27169;&#25311;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#30340;FDR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Inference and FDR Control for Simulated Markov Random Fields. (arXiv:2202.05612v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#32972;&#26223;&#19979;&#36827;&#34892;&#27169;&#25311;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#32479;&#35745;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#31181;&#35823;&#21457;&#29616;&#29575;&#25511;&#21046;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#30830;&#23450;&#19982;&#21709;&#24212;&#21464;&#37327;&#30456;&#20851;&#30340;&#37325;&#35201;&#29305;&#24449;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#26412;&#25991;&#25506;&#35752;&#39640;&#32500;&#32972;&#26223;&#19979;&#27169;&#25311;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MCMC-MLE&#65289;&#19982;&#24377;&#24615;&#32593;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;MCMC&#26041;&#27861;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#32602;&#27454;MCMC-MLE&#26041;&#27861;&#23454;&#29616;&#20102;$\ell_{1}$&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21435;&#30456;&#20851;&#30340;&#24471;&#20998;&#26816;&#39564;&#65292;&#30830;&#23450;&#20854;&#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#19968;&#27493;&#20272;&#35745;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;p&#20540;&#21644;e&#20540;&#30340;&#28176;&#36817;&#34892;&#20026;&#26500;&#24314;&#20102;&#20004;&#31181;&#35823;&#21457;&#29616;&#29575;&#25511;&#21046;&#31243;&#24207;&#12290;&#20840;&#38754;&#30340;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#29702;&#35770;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying important features linked to a response variable is a fundamental task in various scientific domains. This article explores statistical inference for simulated Markov random fields in high-dimensional settings. We introduce a methodology based on Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMC-MLE) with Elastic-net regularization. Under mild conditions on the MCMC method, our penalized MCMC-MLE method achieves $\ell_{1}$-consistency. We propose a decorrelated score test, establishing both its asymptotic normality and that of a one-step estimator, along with the associated confidence interval. Furthermore, we construct two false discovery rate control procedures via the asymptotic behaviors for both p-values and e-values. Comprehensive numerical simulations confirm the theoretical validity of the proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Thundernna&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25915;&#20987;&#31070;&#32463;&#32593;&#32476;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2111.12305</link><description>&lt;p&gt;
Thundernna: &#19968;&#31181;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Thundernna: a white box adversarial attack. (arXiv:2111.12305v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Thundernna&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25915;&#20987;&#31070;&#32463;&#32593;&#32476;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#26420;&#32032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#21482;&#35201;&#22312;&#26222;&#36890;&#36755;&#20837;&#19978;&#21152;&#20837;&#23569;&#37327;&#24694;&#24847;&#20449;&#24687;&#23601;&#36275;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#20986;&#38169;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25915;&#20987;&#26159;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;&#38024;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#35757;&#32451;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#25269;&#25239;&#26576;&#20123;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#21516;&#26102;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#20063;&#21487;&#20197;&#25581;&#31034;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#35752;&#35770;&#30340;&#22797;&#26434;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19968;&#38454;&#26041;&#27861;&#26469;&#25915;&#20987;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#19968;&#38454;&#25915;&#20987;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#27604;&#20108;&#38454;&#25915;&#20987;&#21644;&#22810;&#27493;&#19968;&#38454;&#25915;&#20987;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing work shows that the neural network trained by naive gradient-based optimization method is prone to adversarial attacks, adds small malicious on the ordinary input is enough to make the neural network wrong. At the same time, the attack against a neural network is the key to improving its robustness. The training against adversarial examples can make neural networks resist some kinds of adversarial attacks. At the same time, the adversarial attack against a neural network can also reveal some characteristics of the neural network, a complex high-dimensional non-linear function, as discussed in previous work.  In This project, we develop a first-order method to attack the neural network. Compare with other first-order attacks, our method has a much higher success rate. Furthermore, it is much faster than second-order attacks and multi-steps first-order attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#26102;&#24046;&#23398;&#20064;(GTD)&#30340;&#26032;&#29256;&#26412;&#65292;&#36890;&#36807;&#20984;&#20985;&#38797;&#28857;&#35299;&#37322;&#32479;&#19968;&#20102;&#25152;&#26377;GTD&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#36824;&#36890;&#36807;&#25968;&#20540;&#27604;&#36739;&#20998;&#26512;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2109.04033</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#26102;&#24046;&#23398;&#20064;&#65288;GTD&#65289;&#30340;&#26032;&#29256;&#26412;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
New Versions of Gradient Temporal Difference Learning. (arXiv:2109.04033v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#26102;&#24046;&#23398;&#20064;(GTD)&#30340;&#26032;&#29256;&#26412;&#65292;&#36890;&#36807;&#20984;&#20985;&#38797;&#28857;&#35299;&#37322;&#32479;&#19968;&#20102;&#25152;&#26377;GTD&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#36824;&#36890;&#36807;&#25968;&#20540;&#27604;&#36739;&#20998;&#26512;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sutton&#12289;Szepesv&#225;ri&#21644;Maei&#25552;&#20986;&#20102;&#19982;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#20860;&#23481;&#30340;&#31532;&#19968;&#20010;&#26799;&#24230;&#26102;&#24046;&#65288;GTD&#65289;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#65288;a&#65289;&#25552;&#20986;&#19968;&#20123;GTD&#30340;&#21464;&#31181;&#65292;&#24182;&#36827;&#34892;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#21450;&#65288;b&#65289;&#24314;&#31435;GTD&#30340;&#26032;&#29702;&#35770;&#20998;&#26512;&#26694;&#26550;&#12290;&#36825;&#20123;&#21464;&#31181;&#22522;&#20110;GTD&#30340;&#20984;&#20985;&#38797;&#28857;&#35299;&#37322;&#65292;&#23558;&#25152;&#26377;GTD&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#24182;&#22522;&#20110;&#26368;&#36817;&#23545;&#21407;&#22987;-&#23545;&#20598;&#26799;&#24230;&#21160;&#21147;&#23398;&#30340;&#32467;&#26524;&#25552;&#20379;&#31616;&#21333;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#32473;&#20986;&#20102;&#25968;&#20540;&#27604;&#36739;&#20998;&#26512;&#20197;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sutton, Szepesv\'{a}ri and Maei introduced the first gradient temporal-difference (GTD) learning algorithms compatible with both linear function approximation and off-policy training. The goal of this paper is (a) to propose some variants of GTDs with extensive comparative analysis and (b) to establish new theoretical analysis frameworks for the GTDs. These variants are based on convex-concave saddle-point interpretations of GTDs, which effectively unify all the GTDs into a single framework, and provide simple stability analysis based on recent results on primal-dual gradient dynamics. Finally, numerical comparative analysis is given to evaluate these approaches.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.01636</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01636
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NER&#20219;&#21153;&#38754;&#20020;&#30340;&#26368;&#22823;&#22256;&#38590;&#26159;&#21363;&#20351;&#22312;NE&#31867;&#22411;&#21644;&#25991;&#26723;&#19981;&#29087;&#24713;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201;&#20445;&#25345;&#21487;&#26816;&#27979;&#24615;&#12290;&#24847;&#35782;&#21040;&#29305;&#23450;&#24615;&#20449;&#24687;&#21487;&#33021;&#21253;&#21547;&#21333;&#35789;&#30340;&#28508;&#22312;&#21547;&#20041;&#24182;&#29983;&#25104;&#35789;&#23884;&#20837;&#30340;&#35821;&#20041;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;&#29616;&#26377;&#30340;NER&#26041;&#27861;&#20013;&#65292;NER&#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
&lt;/p&gt;</description></item><item><title>DASVDD&#26159;&#19968;&#31181;&#28145;&#24230;&#33258;&#32534;&#30721;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#32852;&#21512;&#23398;&#20064;&#33258;&#32534;&#30721;&#22120;&#21442;&#25968;&#65292;&#24182;&#26368;&#23567;&#21270;&#28508;&#22312;&#34920;&#31034;&#20013;&#36229;&#29699;&#20307;&#30340;&#20307;&#31215;&#65292;&#36890;&#36807;&#32452;&#21512;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#21644;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#26469;&#35745;&#31639;&#24322;&#24120;&#20998;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36229;&#29699;&#20307;&#22349;&#22604;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2106.05410</link><description>&lt;p&gt;
DASVDD: &#28145;&#24230;&#33258;&#32534;&#30721;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#31526;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly Detection. (arXiv:2106.05410v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05410
&lt;/p&gt;
&lt;p&gt;
DASVDD&#26159;&#19968;&#31181;&#28145;&#24230;&#33258;&#32534;&#30721;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#32852;&#21512;&#23398;&#20064;&#33258;&#32534;&#30721;&#22120;&#21442;&#25968;&#65292;&#24182;&#26368;&#23567;&#21270;&#28508;&#22312;&#34920;&#31034;&#20013;&#36229;&#29699;&#20307;&#30340;&#20307;&#31215;&#65292;&#36890;&#36807;&#32452;&#21512;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#21644;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#26469;&#35745;&#31639;&#24322;&#24120;&#20998;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36229;&#29699;&#20307;&#22349;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#20351;&#29992;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#24322;&#24120;&#26679;&#26412;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#26356;&#20855;&#20449;&#24687;&#30340;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#19968;&#20010;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;DASVDD&#65292;&#23427;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#36229;&#29699;&#20307;&#30340;&#20307;&#31215;&#30340;&#21516;&#26102;&#65292;&#32852;&#21512;&#23398;&#20064;&#33258;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24322;&#24120;&#20998;&#25968;&#65292;&#23427;&#26159;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#21644;&#28508;&#22312;&#34920;&#31034;&#20013;&#21040;&#36229;&#29699;&#20307;&#20013;&#24515;&#30340;&#36317;&#31163;&#30340;&#32452;&#21512;&#12290;&#26368;&#23567;&#21270;&#36825;&#20010;&#24322;&#24120;&#20998;&#25968;&#26377;&#21161;&#20110;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#27491;&#24120;&#31867;&#30340;&#28508;&#22312;&#20998;&#24067;&#12290;&#23558;&#37325;&#26500;&#35823;&#24046;&#21253;&#21547;&#22312;&#24322;&#24120;&#20998;&#25968;&#20013;&#30830;&#20445;DASVDD&#19981;&#20250;&#36973;&#21463;&#24120;&#35265;&#30340;&#36229;&#29699;&#20307;&#22349;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection aims to detect anomalies from normal samples using a model that is trained on normal data. With recent advancements in deep learning, researchers have designed efficient deep anomaly detection methods. Existing works commonly use neural networks to map the data into a more informative representation and then apply an anomaly detection algorithm. In this paper, we propose a method, DASVDD, that jointly learns the parameters of an autoencoder while minimizing the volume of an enclosing hyper-sphere on its latent representation. We propose an anomaly score which is a combination of autoencoder's reconstruction error and the distance from the center of the enclosing hypersphere in the latent representation. Minimizing this anomaly score aids us in learning the underlying distribution of the normal class during training. Including the reconstruction error in the anomaly score ensures that DASVDD does not suffer from the common hypersphere collapse issue sin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#21160;&#24577;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#21487;&#22312;&#26410;&#30693;&#38656;&#27714;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#22312;&#22823;&#24211;&#23384;&#29615;&#22659;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.01135</link><description>&lt;p&gt;
MNL-&#24102;&#26377;&#32972;&#21253;&#30340;&#21095;&#38598;&#25361;&#36873;&#38382;&#39064;&#65306;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MNL-Bandit with Knapsacks: a near-optimal algorithm. (arXiv:2106.01135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#21160;&#24577;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#21487;&#22312;&#26410;&#30693;&#38656;&#27714;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#22312;&#22823;&#24211;&#23384;&#29615;&#22659;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#21160;&#24577;&#30340;&#21830;&#21697;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#20013;&#21334;&#26041;&#25317;&#26377;&#22266;&#23450;&#24211;&#23384;&#30340;N&#31181;&#21487;&#26367;&#20195;&#20135;&#21697;&#65292;&#24182;&#38754;&#20020;&#22312;T&#20010;&#26102;&#26399;&#20869;&#39034;&#24207;&#21040;&#36798;&#30340;&#26410;&#30693;&#38656;&#27714;&#12290;&#22312;&#27599;&#20010;&#26102;&#26399;&#65292;&#21334;&#26041;&#38656;&#35201;&#20915;&#23450;&#35201;&#21521;&#23458;&#25143;&#25552;&#20379;&#30340;&#20135;&#21697;&#32452;&#21512;&#65288;&#22522;&#25968;&#26368;&#22810;&#20026;K&#65289;&#12290;&#39038;&#23458;&#30340;&#21453;&#24212;&#36981;&#24490;&#20855;&#26377;&#21442;&#25968;v&#30340;&#26410;&#30693;&#22810;&#39033;&#24335;&#23545;&#25968;&#27169;&#22411;&#65288;MNL&#65289;&#12290;&#21334;&#26041;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#22266;&#23450;&#21021;&#22987;&#24211;&#23384;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#24635;&#20307;&#39044;&#26399;&#25910;&#20837;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36798;&#21040;&#20102;$\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$&#30340;&#36951;&#25022;&#20540;&#65292;&#22312;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#31181;&#28201;&#21644;&#20551;&#35774;&#19979;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#39640;&#24211;&#23384;&#29615;&#22659;&#19979;&#36798;&#21040;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;$\tilde O(\sqrt{T})$&#36951;&#25022;&#20540;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22522;&#20110;&#22522;&#20110;UCB&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a dynamic assortment selection problem where a seller has a fixed inventory of $N$ substitutable products and faces an unknown demand that arrives sequentially over $T$ periods. In each period, the seller needs to decide on the assortment of products (of cardinality at most $K$) to offer to the customers. The customer's response follows an unknown multinomial logit model (MNL) with parameters $v$. The goal of the seller is to maximize the total expected revenue given the fixed initial inventory of $N$ products. We give a policy that achieves a regret of $\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$, where $v_{\text{max}}\leq 1$ is the maximum utility for any product and $q_{\text{min}}$ the minimum inventory level, under a mild assumption on the model parameters. In particular, our policy achieves a near-optimal $\tilde O(\sqrt{T})$ regret in a large-inventory setting.  Our policy builds upon the UCB-based approach for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#22266;&#26377;&#23545;&#31216;&#24615;&#26500;&#24314;&#30340;&#23567;&#27874;&#32593;&#32476;&#65292;&#20854;&#34920;&#29616;&#20986;&#23884;&#22871;&#30340;&#38750;&#32447;&#24615;&#23567;&#27874;&#26679;&#30340;&#26102;&#39057;&#21464;&#25442;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21407;&#22987;&#27874;&#24418;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;CNN&#12290;</title><link>http://arxiv.org/abs/2006.05259</link><description>&lt;p&gt;
Wavelet Networks: &#20174;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#23610;&#24230;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series. (arXiv:2006.05259v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.05259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#22266;&#26377;&#23545;&#31216;&#24615;&#26500;&#24314;&#30340;&#23567;&#27874;&#32593;&#32476;&#65292;&#20854;&#34920;&#29616;&#20986;&#23884;&#22871;&#30340;&#38750;&#32447;&#24615;&#23567;&#27874;&#26679;&#30340;&#26102;&#39057;&#21464;&#25442;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21407;&#22987;&#27874;&#24418;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;CNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29305;&#23450;&#25968;&#25454;&#39046;&#22495;&#20013;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#26500;&#24314;&#31561;&#21464;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#24179;&#38754;&#21644;&#20307;&#31215;&#25968;&#25454;&#20013;&#20135;&#29983;&#30340;&#23545;&#31216;&#24615;&#19978;&#65292;&#32780;&#25226;&#19968;&#20010;&#20851;&#38190;&#30340;&#25968;&#25454;&#28304;&#22522;&#26412;&#26410;&#24320;&#21457;&#65306;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#26469;&#26500;&#24314;&#31561;&#21464;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30830;&#35748;&#20102;&#20004;&#20010;&#26680;&#24515;&#23545;&#31216;&#24615;&#65306;&#23610;&#24230;&#21644;&#24179;&#31227;&#65292;&#24182;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#30340;&#23610;&#24230;&#24179;&#31227;&#31561;&#21464;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23610;&#24230;&#24179;&#31227;&#31561;&#21464;&#24615;&#26144;&#23556;&#19982;&#23567;&#27874;&#21464;&#25442;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20284;&#24615;&#12290;&#21463;&#21040;&#36825;&#31181;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32593;&#32476;&#31216;&#20026;&#23567;&#27874;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#25191;&#34892;&#23884;&#22871;&#30340;&#38750;&#32447;&#24615;&#23567;&#27874;&#26679;&#30340;&#26102;&#39057;&#21464;&#25442;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23567;&#27874;&#32593;&#32476;&#22312;&#21407;&#22987;&#27874;&#24418;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;CNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging the symmetries inherent to specific data domains for the construction of equivariant neural networks has lead to remarkable improvements in terms of data efficiency and generalization. However, most existing research focuses on symmetries arising from planar and volumetric data, leaving a crucial data source largely underexplored: time-series. In this work, we fill this gap by leveraging the symmetries inherent to time-series for the construction of equivariant neural network. We identify two core symmetries: *scale and translation*, and construct scale-translation equivariant neural networks for time-series learning. Intriguingly, we find that scale-translation equivariant mappings share strong resemblance with the wavelet transform. Inspired by this resemblance, we term our networks Wavelet Networks, and show that they perform nested non-linear wavelet-like time-frequency transforms. Empirical results show that Wavelet Networks outperform conventional CNNs on raw waveforms
&lt;/p&gt;</description></item></channel></rss>